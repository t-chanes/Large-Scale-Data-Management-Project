{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from googlesearch import search\n",
    "import re\n",
    "import nltk\n",
    "import chardet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSFdata/NSF_CCF.csv {'encoding': 'ISO-8859-1', 'confidence': 0.7299916171744574, 'language': ''}\n",
      "NSFdata/NSF_CICI.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "NSFdata/NSF_CSSI.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "NSFdata/NSF_DIBBS.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "NSFdata/NSF_MRI.csv {'encoding': 'ISO-8859-1', 'confidence': 0.7299962504897843, 'language': ''}\n",
      "NSFdata/NSF_OAC.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "NSFdata/NSF_SI2.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "# Lets check what encoding we have for our NSFdata files\n",
    "NSF_csv_files = ['NSFdata/NSF_CCF.csv', 'NSFdata/NSF_CICI.csv', 'NSFdata/NSF_CSSI.csv', 'NSFdata/NSF_DIBBS.csv', 'NSFdata/NSF_MRI.csv', 'NSFdata/NSF_OAC.csv', 'NSFdata/NSF_SI2.csv']\n",
    "for file in NSF_csv_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        print(file, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "# 1. Read the CSV file and load it into a DataFrame\n",
    "#data = pd.read_csv('NSFdata/NSF_DIBBS.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      This project would automate the creation and d...\n",
      "1      The growing number of cyber attacks on the Int...\n",
      "2      Uranium-series geochronology plays a critical ...\n",
      "3      Cybersecurity has become a significant issue t...\n",
      "4      CIF21 DIBBs: Conceptualization of the Social a...\n",
      "                             ...                        \n",
      "125    ABSTRACT<br/><br/>OPP-9813312   OPP-9813442   ...\n",
      "126    ABSTRACT<br/><br/>OPP-9813312   OPP-9813442   ...\n",
      "127    ABSTRACT<br/><br/>OPP-9907197    OPP-9907469  ...\n",
      "128    Current general circulation models (GCMs) have...\n",
      "129    ABSTRACT<br/><br/>OPP-9907197    OPP-9907469  ...\n",
      "Name: Abstract, Length: 130, dtype: object\n"
     ]
    }
   ],
   "source": [
    "projects = data[[\"AwardNumber\", \"Title\", \"NSFOrganization\", \"PrincipalInvestigator\", \"PIEmailAddress\", \"Abstract\"]]\n",
    "print(projects[\"Abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_abstract(abstract):\n",
    "    if abstract is None or not isinstance(abstract, str):\n",
    "        return []\n",
    "    abstract = re.sub('<[^<]+?>', '', abstract)  # Remove HTML tags\n",
    "    abstract = abstract.lower()  # Convert to lowercase\n",
    "    abstract = re.sub(r'\\W+', ' ', abstract)  # Remove special characters and numbers\n",
    "    words = abstract.split()  # Tokenize\n",
    "    words = [word for word in words if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [project, would, automate, creation, data, ana...\n",
      "1      [growing, number, cyber, attack, internet, cri...\n",
      "2      [uranium, series, geochronology, play, critica...\n",
      "3      [cybersecurity, become, significant, issue, pr...\n",
      "4      [cif21, dibbs, conceptualization, social, inno...\n",
      "                             ...                        \n",
      "125    [abstractopp, 9813312, opp, 9813442, opp, 9813...\n",
      "126    [abstractopp, 9813312, opp, 9813442, opp, 9813...\n",
      "127    [abstractopp, 9907197, opp, 9907469, opp, 9907...\n",
      "128    [current, general, circulation, model, gcms, d...\n",
      "129    [abstractopp, 9907197, opp, 9907469, opp, 9907...\n",
      "Name: Abstract, Length: 130, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 3. Process the Abstract column\n",
    "abstracts = projects[\"Abstract\"].apply(preprocess_abstract)\n",
    "print(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<4137 unique tokens: ['000', '1440753', '24', '50', '500']...>\n"
     ]
    }
   ],
   "source": [
    "# 4. Create a dictionary representation of the documents.\n",
    "dictionary = corpora.Dictionary(abstracts)\n",
    "print(dictionary)\n",
    "# 5. Corpus is a list of bags of words. Each bag-of-words is a list of tuples (term_id, term_frequency).\n",
    "corpus = [dictionary.doc2bow(text) for text in abstracts]\n",
    "\n",
    "# 6. Define the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=8, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new colunm LDA_ABSTRACTS to be classified into keywords based on the LDA model\n",
    "def get_lda_keywords(lda_model, bow, num_keywords=8):\n",
    "    topic_dist = lda_model[bow]\n",
    "    dominant_topic = max(topic_dist, key=lambda x: x[1])[0]\n",
    "    topic_terms = lda_model.get_topic_terms(dominant_topic, topn=num_keywords)\n",
    "    return [dictionary[id] for id, prob in topic_terms]\n",
    "\n",
    "#projects[\"LDA_abstract_keywords\"] = projects[\"Abstract\"].apply(lambda x: get_lda_keywords(lda_model, dictionary.doc2bow(preprocess_abstract(x))))\n",
    "#projects[\"LDA_abstract_keywords\"] = projects[\"Abstract\"].apply(lambda x: lda_model[dictionary.doc2bow(preprocess_abstract(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [data, project, community, researcher, researc...\n",
      "1      [data, project, information, science, scientif...\n",
      "2      [data, research, project, science, new, commun...\n",
      "3      [data, science, project, research, tool, commu...\n",
      "4      [data, science, project, research, tool, commu...\n",
      "                             ...                        \n",
      "125    [record, atmospheric, ice, project, university...\n",
      "126    [record, atmospheric, ice, project, university...\n",
      "127    [snow, ice, chemistry, atmospheric, atmosphere...\n",
      "128    [data, material, project, system, science, arc...\n",
      "129    [snow, ice, chemistry, atmospheric, atmosphere...\n",
      "Name: LDA_abstract_keywords, Length: 130, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(projects[\"LDA_abstract_keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BING_API_KEY = \"4071d4e339c04e5b98c7ce1366a3366e\"\n",
    "# 7. Defining Search for news articles and other online sources\n",
    "def search_news(title):\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": BING_API_KEY}\n",
    "    params = {\n",
    "        \"q\": f'\"{title}\"',\n",
    "        \"count\": 3,\n",
    "        \"offset\": 0,\n",
    "        \"mkt\": \"en-US\",\n",
    "        \"safesearch\": \"Moderate\",\n",
    "    }\n",
    "    response = requests.get(\"https://api.bing.microsoft.com/v7.0/search\", headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    search_results = response.json()\n",
    "    if \"webPages\" not in search_results:\n",
    "        return ''\n",
    "    news_links = [result[\"url\"] for result in search_results[\"webPages\"][\"value\"] if \"news\" in result[\"url\"] or \"article\" in result[\"url\"]]\n",
    "    return ', '.join(news_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_news(title):\n",
    "    try:\n",
    "        search_results = search(title, num_results=3, sleep_interval=15, advanced=True)\n",
    "        news_links = [f'{result.title} ({result.url}) - {result.description}' for result in search_results]\n",
    "        return ', '.join(news_links)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during search: {e}\")\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       \n",
      "1       \n",
      "2       \n",
      "3       \n",
      "4       \n",
      "      ..\n",
      "125     \n",
      "126     \n",
      "127     \n",
      "128     \n",
      "129     \n",
      "Name: News, Length: 130, dtype: object\n"
     ]
    }
   ],
   "source": [
    "projects[\"News\"] = projects[\"Title\"].apply(search_news)\n",
    "print(projects[\"News\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create a new DataFrame with the desired columns\n",
    "def clean_abstract(abstract):\n",
    "    if abstract is None or not isinstance(abstract, str):\n",
    "        return []\n",
    "    abstract = re.sub('<[^<]+?>', '', abstract)  # Remove HTML tags\n",
    "    abstract = re.sub(r'\\s+', ' ', abstract)  # Remove extra whitespaces\n",
    "    return abstract.strip()\n",
    "\n",
    "# projects[\"Clean_Abstract\"] = projects[\"Abstract\"].apply(clean_abstract)\n",
    "\n",
    "# output = projects.rename(columns={\"Title\": \"Project_title\",\n",
    "#                                   \"NSFOrganization\": \"Funding_agency\",\n",
    "#                                   \"AwardNumber\": \"Award_number\",\n",
    "#                                   \"PrincipalInvestigator\": \"PI_Name\",\n",
    "#                                   \"PIEmailAddress\": \"PI_contact\",\n",
    "#                                   \"Clean_Abstract\": \"Abstracts\"})\n",
    "\n",
    "# output.drop(columns=[\"Abstract\"], inplace=True)\n",
    "# # Lets reorder the columns to have the following order - Project_title, Funding_agency, Award_number, PI_Name, PI_contact, LDA_abstract_keywords, Abstracts, News\n",
    "# output = output[[\"Project_title\", \"Funding_agency\", \"Award_number\", \"PI_Name\", \"PI_contact\", \"LDA_abstract_keywords\", \"Abstracts\", \"News\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"NSF_DIBBS_final3_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically now we have to merge them all into one file using DOE NSF and NIH datasets.\n",
    "NSF_csv_files = ['NSFdata/NSF_CCF.csv', 'NSFdata/NSF_CICI.csv', 'NSFdata/NSF_CSSI.csv', 'NSFdata/NSF_DIBBS.csv', 'NSFdata/NSF_MRI.csv', 'NSFdata/NSF_OAC.csv', 'NSFdata/NSF_SI2.csv']\n",
    "\n",
    "all_projects = pd.DataFrame()\n",
    "\n",
    "for file_path in NSF_csv_files:\n",
    "    data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    projects = data[[\"AwardNumber\", \"Title\", \"NSFOrganization\", \"PrincipalInvestigator\", \"PIEmailAddress\", \"Abstract\"]]\n",
    "    abstracts = projects[\"Abstract\"].apply(preprocess_abstract)\n",
    "    dictionary = corpora.Dictionary(abstracts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in abstracts]\n",
    "    lda_model = models.LdaModel(corpus, num_topics=8, id2word=dictionary, passes=3)\n",
    "    projects[\"Keyword\"] = projects[\"Abstract\"].apply(lambda x: get_lda_keywords(lda_model, dictionary.doc2bow(preprocess_abstract(x))))\n",
    "    projects[\"News\"] = projects[\"Title\"].apply(search_news)\n",
    "    projects[\"Clean_Abstract\"] = projects[\"Abstract\"].apply(clean_abstract)\n",
    "    \n",
    "    output = projects.rename(columns={\"Title\": \"Project_name\",\n",
    "                                      \"NSFOrganization\": \"Funding_agency\",\n",
    "                                      \"AwardNumber\": \"Award_number\",\n",
    "                                      \"PrincipalInvestigator\": \"PI_name\",\n",
    "                                      \"PIEmailAddress\": \"PI_contact\",\n",
    "                                      \"Clean_Abstract\": \"Description\"})\n",
    "    \n",
    "    output.drop(columns=[\"Abstract\"], inplace=True)\n",
    "    output = output[[\"Project_name\", \"Funding_agency\", \"Award_number\", \"PI_name\", \"PI_contact\", \"Keyword\", \"Description\", \"News\"]]\n",
    "    output_file = f\"NSF_{file_path.split('/')[-1].split('.')[0]}_processed.csv\"\n",
    "    output.to_csv(output_file, index=False)\n",
    "    print(f\"Saved processed data to: {output_file}\")\n",
    "    all_projects = all_projects.append(output, ignore_index=True)\n",
    "\n",
    "all_projects.to_csv(\"NSF_all_final.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      AwardNumber                                              Title  \\\n",
      "0         9820538      Undergraduate Engineering in Medical Research   \n",
      "1         1802188  Digitization TCN:  Collaborative Research: Cap...   \n",
      "2         1802199  Digitization TCN:  Collaborative Research:  Ca...   \n",
      "3         1802200  Digitization TCN:  Collaborative Research: Cap...   \n",
      "4         1802178  Digitization TCN:  Collaborative Research:  Ca...   \n",
      "...           ...                                                ...   \n",
      "2995      1814888  SHF: Small: Communication-Efficient Distribute...   \n",
      "2996      1816209  CIF: Small: Occlusion-Based Computational Imag...   \n",
      "2997      1810758  NSF-BSF: AF: Small: An Algorithmic Theory of B...   \n",
      "2998      1822342       CISE/SHF: Summer School on Formal Techniques   \n",
      "2999      1751400  CAREER:Enabling Scalable, Modular, and Efficie...   \n",
      "\n",
      "     NSFOrganization PrincipalInvestigator            PIEmailAddress  \\\n",
      "0                EEC         William Smith           smithw1@ccf.org   \n",
      "1                DBI              Amy Litt          amy.litt@ucr.edu   \n",
      "2                DBI         Philip Rundel           rundel@ucla.edu   \n",
      "3                DBI            Joshua Der        jder@fullerton.edu   \n",
      "4                DBI    Katherine Waselkov   kwaselkov@csufresno.edu   \n",
      "...              ...                   ...                       ...   \n",
      "2995             CCF  Maryam Mehri Dehnavi  maryam.mehri@rutgers.edu   \n",
      "2996             CCF       Gregory Wornell               gww@mit.edu   \n",
      "2997             CCF           Nancy Lynch       lynch@csail.mit.edu   \n",
      "2998             CCF     Natarajan Shankar       shankar@csl.sri.com   \n",
      "2999             CCF      Anthony Nowatzki           tjn@cs.ucla.edu   \n",
      "\n",
      "                                               Abstract  \\\n",
      "0     9820538<br/>The Cleveland Clinic Foundation   ...   \n",
      "1     Flowering time is an important biological phen...   \n",
      "2     Flowering time is an important biological phen...   \n",
      "3     Flowering time is an important biological phen...   \n",
      "4     Flowering time is an important biological phen...   \n",
      "...                                                 ...   \n",
      "2995  Advances in sensing and processing technologie...   \n",
      "2996  Humans have long sought ways to extend their v...   \n",
      "2997  Understanding how the brain works, as a comput...   \n",
      "2998  Formal verification covers a wide range of tec...   \n",
      "2999  Past exponential improvements to computer proc...   \n",
      "\n",
      "                                  LDA_abstract_keywords  \n",
      "0     [student, research, science, project, support,...  \n",
      "1     [time, plant, program, data, ccf, award, flowe...  \n",
      "2     [time, plant, program, data, ccf, award, flowe...  \n",
      "3     [time, plant, program, data, ccf, award, flowe...  \n",
      "4     [time, plant, program, data, ccf, award, flowe...  \n",
      "...                                                 ...  \n",
      "2995  [data, project, network, system, learning, com...  \n",
      "2996  [algorithm, problem, project, data, model, the...  \n",
      "2997  [algorithm, problem, project, data, model, the...  \n",
      "2998  [student, research, science, project, support,...  \n",
      "2999  [software, system, project, program, technique...  \n",
      "\n",
      "[3000 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(projects)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocv4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
