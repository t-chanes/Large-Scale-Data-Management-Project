{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import chardet\n",
    "import re\n",
    "import string\n",
    "from gensim import corpora, models\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frames loaded 5\n"
     ]
    }
   ],
   "source": [
    "# Load the NIHdata from CSV files\n",
    "NIH_csv_files = ['NIHdata/NIH_CCF.csv', 'NIHdata/NIH_CICI.csv', 'NIHdata/NIH_CSSI.csv', 'NIHdata/NIH_MRI.csv', 'NIHdata/NIH_OAC.csv']\n",
    "data_frames = []\n",
    "for file in NIH_csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    # Keep only the relevant columns (abstracts and titles)\n",
    "    df = df[['Title', 'Abstract']]\n",
    "    # Merge the data frames\n",
    "    data_frames.append(df)\n",
    "print('Data frames loaded', len(data_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSFdata/NSF_CCF.csv {'encoding': 'ISO-8859-1', 'confidence': 0.7299916171744574, 'language': ''}\n",
      "NSFdata/NSF_CICI.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "NSFdata/NSF_CSSI.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "NSFdata/NSF_DIBBS.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "NSFdata/NSF_MRI.csv {'encoding': 'ISO-8859-1', 'confidence': 0.7299962504897843, 'language': ''}\n",
      "NSFdata/NSF_OAC.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "NSFdata/NSF_SI2.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "# Lets check what encoding we have for our NSFdata files\n",
    "NSF_csv_files = ['NSFdata/NSF_CCF.csv', 'NSFdata/NSF_CICI.csv', 'NSFdata/NSF_CSSI.csv', 'NSFdata/NSF_DIBBS.csv', 'NSFdata/NSF_MRI.csv', 'NSFdata/NSF_OAC.csv', 'NSFdata/NSF_SI2.csv']\n",
    "for file in NSF_csv_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        print(file, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frames loaded 12\n"
     ]
    }
   ],
   "source": [
    "# Load the NSFdata from CSV files\n",
    "NSF_csv_files = ['NSFdata/NSF_CCF.csv', 'NSFdata/NSF_CICI.csv', 'NSFdata/NSF_CSSI.csv', 'NSFdata/NSF_DIBBS.csv', 'NSFdata/NSF_MRI.csv', 'NSFdata/NSF_OAC.csv', 'NSFdata/NSF_SI2.csv']\n",
    "for file in NSF_csv_files:\n",
    "    df = pd.read_csv(file, encoding='ISO-8859-1')\n",
    "    # Keep only the relevant columns (abstracts and titles)\n",
    "    df = df[['Title', 'Abstract']]\n",
    "    # Merge the data frames\n",
    "    data_frames.append(df)\n",
    "print('Data frames loaded', len(data_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOEdata/DOE_CCF.xls {'encoding': 'Windows-1252', 'confidence': 0.73, 'language': ''}\n",
      "DOEdata/DOE_MRI.xls {'encoding': 'Windows-1254', 'confidence': 0.3892575204703193, 'language': 'Turkish'}\n",
      "DOEdata/DOE_OAC.xls {'encoding': 'Windows-1254', 'confidence': 0.4989260344799692, 'language': 'Turkish'}\n"
     ]
    }
   ],
   "source": [
    "# Lets check what encoding we have for our DOEdata files\n",
    "DOE_xls_files = ['DOEdata/DOE_CCF.xls', 'DOEdata/DOE_MRI.xls', 'DOEdata/DOE_OAC.xls']\n",
    "for file in DOE_xls_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        print(file, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frames loaded 15\n"
     ]
    }
   ],
   "source": [
    "# Load the DOEdata from xls files\n",
    "DOE_xls_files = ['DOEdata/DOE_CCF.xls', 'DOEdata/DOE_MRI.xls', 'DOEdata/DOE_OAC.xls']\n",
    "for file in DOE_xls_files:\n",
    "    df = pd.read_excel(file)\n",
    "    # Keep only the relevant columns (abstracts and titles)\n",
    "    df = df[['Title', 'Abstract']]\n",
    "    # Merge the data frames\n",
    "    data_frames.append(df)\n",
    "print('Data frames loaded', len(data_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the data frames\n",
    "df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Save the concatenated dataframe as a CSV file\n",
    "df.to_csv('Merged_data_DOE3_NIH5_NSF7_15DF.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML tags\n",
    "df['Title'] = df['Title'].apply(lambda x: re.sub('<.*?>', '', x))\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: re.sub('<.*?>', '', x))\n",
    "\n",
    "# Remove other non-alphanumeric characters\n",
    "df['Title'] = df['Title'].apply(lambda x: re.sub('[^0-9a-zA-Z]+', ' ', x))\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: re.sub('[^0-9a-zA-Z]+', ' ', x))\n",
    "\n",
    "# Remove numbers and punctuation\n",
    "translator = str.maketrans('', '', string.punctuation + string.digits)\n",
    "df['Title'] = df['Title'].apply(lambda x: x.translate(translator))\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: x.translate(translator))\n",
    "\n",
    "# Remove URLs and email addresses\n",
    "df['Title'] = df['Title'].apply(lambda x: re.sub(r'\\S+@\\S+', '', x))\n",
    "df['Title'] = df['Title'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: re.sub(r'\\S+@\\S+', '', x))\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "\n",
    "# Preprocess the text\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english') + ['a', 'an', 'the'])\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize the text into individual words or phrases\n",
    "df['Title'] = df['Title'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "# Remove stop words\n",
    "df['Title'] = df['Title'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "# Stem the words\n",
    "df['Title'] = df['Title'].apply(lambda x: [porter_stemmer.stem(word) for word in x])\n",
    "# Tokenize the text into individual words or phrases\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "# Remove stop words\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "# Stem the words\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: [porter_stemmer.stem(word) for word in x])\n",
    "\n",
    "# Create a dictionary of the terms and their frequencies\n",
    "texts = df['Title'] + df['Abstract']\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Create a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=10)\n",
    "\n",
    "# Print the topics and their top keywords\n",
    "for topic in lda_model.print_topics():\n",
    "    print(topic)\n",
    "\n",
    "# This code saves the topics and their top keywords to a file named topics.txt in the current working directory. The print function redirects the output to the file using the file argument.\n",
    "# Save the topics and their top keywords to a file\n",
    "with open('topics.txt', 'w') as f:\n",
    "    for topic in lda_model.print_topics():\n",
    "        print(topic, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated triple-quoted string literal (detected at line 10) (3341524094.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[30], line 10\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\"\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated triple-quoted string literal (detected at line 10)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Evaluate the coherence of the topics for different values of num_topics\n",
    "for num_topics in range(5, 20):\n",
    "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "    print(f\"Number of topics: {num_topics}, coherence score: {coherence_score}\")\n",
    "\n",
    "\"\"\"\"\n",
    "This code trains an LDA model for different values of num of topics, evaluates the coherence of the resulting topics using the CoherenceModel class with the 'c_v' coherence measure, and prints the coherence score for each value of num_topics. \n",
    "You can use the coherence score to select the best value of num_topics that produces the most coherent and interpretable topics.\n",
    "\n",
    "For the passes parameter, you can start with a small number (e.g., 5) and gradually increase it to see if the topics become more stable and consistent across different runs. \n",
    "The passes parameter controls the number of times the model goes through the entire dataset, so increasing it can improve the stability and quality of the topics, but also increases the computational cost. \n",
    "You can use the coherence score as a guide to determine the optimal number of passes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocv4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
