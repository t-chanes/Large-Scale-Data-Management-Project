{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from googlesearch import search\n",
    "import re\n",
    "import nltk\n",
    "import chardet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the abstracts so they dont have HTML tags, special characters, numbers, stopwords, and lemmatize the words for better results in the LDA model\n",
    "def preprocess_abstract(abstract):\n",
    "    if abstract is None or not isinstance(abstract, str):\n",
    "        return []\n",
    "    abstract = re.sub('<[^<]+?>', '', abstract)  # Remove HTML tags\n",
    "    abstract = abstract.lower()  # Convert to lowercase\n",
    "    abstract = re.sub(r'\\W+', ' ', abstract)  # Remove special characters and numbers\n",
    "    words = abstract.split()  # Tokenize\n",
    "    words = [word for word in words if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new colunm \"Keywords\" to be classified into keywords based on the LDA model\n",
    "def get_lda_keywords(lda_model, bow, num_keywords=8):\n",
    "    topic_dist = lda_model[bow]\n",
    "    dominant_topic = max(topic_dist, key=lambda x: x[1])[0]\n",
    "    topic_terms = lda_model.get_topic_terms(dominant_topic, topn=num_keywords)\n",
    "    return [dictionary[id] for id, prob in topic_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cleans the final abstracts (note the difference is this one is for the final abstracts and the other one is for the abstracts ran for the LDA model they have diffrent processing)\n",
    "def clean_abstract(abstract):\n",
    "    if abstract is None or not isinstance(abstract, str):\n",
    "        return []\n",
    "    abstract = re.sub('<[^<]+?>', '', abstract)  # Remove HTML tags\n",
    "    abstract = re.sub(r'\\s+', ' ', abstract)  # Remove extra whitespaces\n",
    "    return abstract.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the DOEdata from xls files\n",
    "# DOE_xls_files = ['DOEdata/DOE_CCF.xls', 'DOEdata/DOE_MRI.xls', 'DOEdata/DOE_OAC.xls']\n",
    "# for file in DOE_xls_files:\n",
    "#     df = pd.read_excel(file)\n",
    "#     # Keep only the relevant columns (abstracts and titles)\n",
    "#     df = df[['Title', 'Abstract']]\n",
    "#     # Merge the data frames\n",
    "#     data_frames.append(df)\n",
    "# print('Data frames loaded', len(data_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the NIHdata from CSV files\n",
    "# NIH_csv_files = ['NIHdata/NIH_CCF.csv', 'NIHdata/NIH_CICI.csv', 'NIHdata/NIH_CSSI.csv', 'NIHdata/NIH_MRI.csv', 'NIHdata/NIH_OAC.csv']\n",
    "# data_frames = []\n",
    "# for file in NIH_csv_files:\n",
    "#     df = pd.read_csv(file)\n",
    "#     # Keep only the relevant columns (abstracts and titles)\n",
    "#     df = df[['Title', 'Abstract']]\n",
    "#     # Merge the data frames\n",
    "#     data_frames.append(df)\n",
    "# print('Data frames loaded', len(data_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we merge them all into one file using DOE NSF or NIH datasets, in the process we will save each segment keyword Ex. NSF_CCF.csv will have a .csv file etc. and a final .csv file of course\n",
    "NSF_csv_files = ['NSFdata/NSF_CCF.csv', 'NSFdata/NSF_CICI.csv', 'NSFdata/NSF_CSSI.csv', 'NSFdata/NSF_DIBBS.csv', 'NSFdata/NSF_MRI.csv', 'NSFdata/NSF_OAC.csv', 'NSFdata/NSF_SI2.csv']\n",
    "\n",
    "all_projects = pd.DataFrame()\n",
    "\n",
    "for file_path in NSF_csv_files:\n",
    "    data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    projects = data[[\"AwardNumber\", \"Title\", \"NSFOrganization\", \"PrincipalInvestigator\", \"PIEmailAddress\", \"Abstract\"]]\n",
    "    abstracts = projects[\"Abstract\"].apply(preprocess_abstract)\n",
    "    dictionary = corpora.Dictionary(abstracts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in abstracts]\n",
    "    lda_model = models.LdaModel(corpus, num_topics=8, id2word=dictionary, passes=3)\n",
    "    projects[\"Keyword\"] = projects[\"Abstract\"].apply(lambda x: get_lda_keywords(lda_model, dictionary.doc2bow(preprocess_abstract(x))))\n",
    "    #projects[\"News\"] = projects[\"Title\"].apply(search_news)\n",
    "    projects[\"Clean_Abstract\"] = projects[\"Abstract\"].apply(clean_abstract)\n",
    "    \n",
    "    output = projects.rename(columns={\"Title\": \"Project_name\",\n",
    "                                      \"NSFOrganization\": \"Funding_agency\",\n",
    "                                      \"AwardNumber\": \"Award_number\",\n",
    "                                      \"PrincipalInvestigator\": \"PI_name\",\n",
    "                                      \"PIEmailAddress\": \"PI_contact\",\n",
    "                                      \"Clean_Abstract\": \"Description\"})\n",
    "    \n",
    "    output.drop(columns=[\"Abstract\"], inplace=True)\n",
    "    output = output[[\"Project_name\", \"Funding_agency\", \"Award_number\", \"PI_name\", \"PI_contact\", \"Keyword\", \"Description\"]]\n",
    "    output_file = f\"NSF_{file_path.split('/')[-1].split('.')[0]}_processed.csv\"\n",
    "    output.to_csv(output_file, index=False)\n",
    "    print(f\"Saved processed data to: {output_file}\")\n",
    "    all_projects = all_projects.append(output, ignore_index=True)\n",
    "\n",
    "all_projects.to_csv(\"NSF_all_final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projecttrace",
   "language": "python",
   "name": "projecttrace"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
