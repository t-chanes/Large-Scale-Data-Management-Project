{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "#from googlesearch import search\n",
    "import re\n",
    "import nltk\n",
    "import chardet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gra248/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/gra248/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the abstracts so they dont have HTML tags, special characters, numbers, stopwords, and lemmatize the words for better results in the LDA model\n",
    "def preprocess_abstract(abstract):\n",
    "    if abstract is None or not isinstance(abstract, str):\n",
    "        return []\n",
    "    abstract = re.sub('<[^<]+?>', '', abstract)  # Remove HTML tags\n",
    "    abstract = abstract.lower()  # Convert to lowercase\n",
    "    abstract = re.sub(r'\\W+', ' ', abstract)  # Remove special characters and numbers\n",
    "    words = abstract.split()  # Tokenize\n",
    "    words = [word for word in words if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new colunm \"Keywords\" to be classified into keywords based on the LDA model\n",
    "def get_lda_keywords(lda_model, bow, num_keywords=8):\n",
    "    topic_dist = lda_model[bow]\n",
    "    dominant_topic = max(topic_dist, key=lambda x: x[1])[0]\n",
    "    topic_terms = lda_model.get_topic_terms(dominant_topic, topn=num_keywords)\n",
    "    return [dictionary[id] for id, prob in topic_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cleans the final abstracts (note the difference is this one is for the final abstracts and the other one is for the abstracts ran for the LDA model they have diffrent processing)\n",
    "def clean_abstract(abstract):\n",
    "    if abstract is None or not isinstance(abstract, str):\n",
    "        return []\n",
    "    abstract = re.sub('<[^<]+?>', '', abstract)  # Remove HTML tags\n",
    "    abstract = re.sub(r'\\s+', ' ', abstract)  # Remove extra whitespaces\n",
    "    return abstract.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the DOEdata from xls files\n",
    "# DOE_xls_files = ['DOEdata/DOE_CCF.xls', 'DOEdata/DOE_MRI.xls', 'DOEdata/DOE_OAC.xls']\n",
    "# for file in DOE_xls_files:\n",
    "#     df = pd.read_excel(file)\n",
    "#     # Keep only the relevant columns (abstracts and titles)\n",
    "#     df = df[['Title', 'Abstract']]\n",
    "#     # Merge the data frames\n",
    "#     data_frames.append(df)\n",
    "print('Data frames loaded', len(data_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14466/1460694118.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  projects[\"Clean_Abstract\"] = projects[\"Abstract\"].apply(clean_abstract)\n",
      "/tmp/ipykernel_14466/1460694118.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  projects[\"Clean_Abstract\"] = projects[\"Abstract\"].apply(clean_abstract)\n",
      "/tmp/ipykernel_14466/1460694118.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  projects[\"Clean_Abstract\"] = projects[\"Abstract\"].apply(clean_abstract)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed data to: NIH_CCF_processed.csv\n",
      "Saved processed data to: NIH_CICI_processed.csv\n",
      "Saved processed data to: NIH_CSSI_processed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14466/1460694118.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  projects[\"Clean_Abstract\"] = projects[\"Abstract\"].apply(clean_abstract)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed data to: NIH_MRI_processed.csv\n",
      "Saved processed data to: NIH_OAC_processed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14466/1460694118.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  projects[\"Clean_Abstract\"] = projects[\"Abstract\"].apply(clean_abstract)\n"
     ]
    }
   ],
   "source": [
    "# Now we merge them all into one file using DOE NSF or NIH datasets, in the process we will save each segment keyword Ex. NSF_CCF.csv will have a .csv file etc. and a final .csv file of course\n",
    "NIH_csv_files = [r'/home/gra248/projects/ProjectTrace/NIHdata/NIH_CCF.csv', r'/home/gra248/projects/ProjectTrace/NIHdata/NIH_CICI.csv', r'/home/gra248/projects/ProjectTrace/NIHdata/NIH_CSSI.csv', r'/home/gra248/projects/ProjectTrace/NIHdata/NIH_MRI.csv', r'/home/gra248/projects/ProjectTrace/NIHdata/NIH_OAC.csv']\n",
    "\n",
    "all_projects = pd.DataFrame()\n",
    "\n",
    "for file_path in NIH_csv_files:\n",
    "    data = pd.read_csv(file_path)\n",
    "    # Keep only the first 8 terms in the \"Project Terms\" column\n",
    "    data[\"Project Terms\"] = data[\"Project Terms\"].apply(lambda x: \";\".join(x.split(\";\")[:8]))\n",
    "\n",
    "    # Extract the first and last name from the \"Contact PI / Project Leader\" column\n",
    "    data[\"First Name\"] = data[\"Contact PI / Project Leader\"].apply(lambda x: x.split(\",\")[1].strip())\n",
    "    data[\"Last Name\"] = data[\"Contact PI / Project Leader\"].apply(lambda x: x.split(\",\")[0].strip())\n",
    "\n",
    "    # Create a new column \"PI_contact\" by merging the extracted first and last name with the \"Organization Name\" column\n",
    "    data[\"PI_contact\"] = data[\"First Name\"] + \".\" + data[\"Last Name\"] + \"@\" + data[\"Organization Name\"].str.replace(\" \", \"\").apply(lambda x: x.lower())\n",
    "    #projects = data[[\"AwardNumber\", \"Title\", \"NSFOrganization\", \"PrincipalInvestigator\", \"PIEmailAddress\", \"Abstract\"]]\n",
    "    projects = data[[\"Title\",\"Project Terms\", \"Application ID\", \"Department\", \"Contact PI / Project Leader\", \"PI_contact\", \"Abstract\"]]\n",
    "    # abstracts = projects[\"Abstract\"].apply(preprocess_abstract)\n",
    "    # dictionary = corpora.Dictionary(abstracts)\n",
    "    # corpus = [dictionary.doc2bow(text) for text in abstracts]\n",
    "    # lda_model = models.LdaModel(corpus, num_topics=8, id2word=dictionary, passes=3)\n",
    "    # projects[\"Keyword\"] = projects[\"Abstract\"].apply(lambda x: get_lda_keywords(lda_model, dictionary.doc2bow(preprocess_abstract(x))))\n",
    "    #projects[\"News\"] = projects[\"Title\"].apply(search_news)\n",
    "    projects[\"Clean_Abstract\"] = projects[\"Abstract\"].apply(clean_abstract)\n",
    "    \n",
    "    output = projects.rename(columns={\"Title\": \"Project_name\",\n",
    "                                      \"Project Terms\": \"Keyword\",\n",
    "                                      \"Department\": \"Funding_agency\",\n",
    "                                      \"Application ID\": \"Award_number\",\n",
    "                                      \"Contact PI / Project Leader\": \"PI_name\",\n",
    "                                      \"Clean_Abstract\": \"Description\"})\n",
    "    \n",
    "    output.drop(columns=[\"Abstract\"], inplace=True)\n",
    "    output = output[[\"Project_name\", \"Funding_agency\", \"Award_number\", \"PI_name\", \"PI_contact\", \"Keyword\", \"Description\"]]\n",
    "    output_file = f\"{file_path.split('/')[-1].split('.')[0]}_processed.csv\"\n",
    "    output.to_csv(output_file, index=False)\n",
    "    print(f\"Saved processed data to: {output_file}\")\n",
    "    all_projects = pd.concat([all_projects, output], ignore_index=True)\n",
    "\n",
    "all_projects.to_csv(\"NIH_all_final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projecttrace",
   "language": "python",
   "name": "projecttrace"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
