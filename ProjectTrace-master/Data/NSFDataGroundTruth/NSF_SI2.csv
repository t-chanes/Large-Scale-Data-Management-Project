"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1702722","2017 Software Infrastructure for Sustained Innovation (SI2) Principal Investigator Workshop","OAC","Software Institutes","12/01/2016","06/15/2017","Ganesh Gopalakrishnan","UT","University of Utah","Standard Grant","Rajiv Ramnath","11/30/2017","$118,543.00","Yung-Hsiang Lu, Matthew Knepley, Kyle Niemeyer, Matthew Turk","ganesh@cs.utah.edu","201 PRESIDENTS CIR","SALT LAKE CITY","UT","841129049","8015816903","CSE","8004","7433, 7556, 8004","$0.00","This project will host a 1.5-day workshop in Arlington, VA, which will bring together the community of SI2 awardees (with the goal of involving one principal investigator from each SSE and SSI project,many of which are collaborative awards) from approximately 250 awards. The workshop will have participation from CDS&E, CRISP and Venture funded PIs as well as SI2 EAGER and RAPID awardees and selected awardees of the ACI VOSS program that examines cyberinfrastructures from the social and organizational perspective. In addition, the proximity to NSF will encourage participation by Program Officers from across the Foundation. Goals of this workshop include: (a) providing a focused forum for PIs to share technical information with each other and with NSF Program Officers, (b) encouraging exploration of emerging topics, (c) identifying emerging best practices across the supported software projects, (d) stimulating thinking on new ways of achieving software sustainability, and (d) disseminating the shared experiences of the researchers via an online web portal. <br/><br/>The workshop is expected to host close to 150 SI2 and other awardees, other speakers and panelists. The workshop will use a hybrid style, blending a traditional, top-down driven agenda with a interactive, adaptive, participant-driven agenda. <br/><br/>The proposed workshop will support the exchange of ideas among the current software cyberinfrastructure development projects. It will provide guidance on issues related to the development of robust software and to the problem of software sustainability. Involvement of program officers across NSF is expected to help the interdisciplinary SI2 awardees understand the relevance and impact of cyberinfrastructure throughout the NSF. The participation of these researchers and program officers in a common forum will help ensure that the cyberinfrastructure software developed as part of SI2 projects will be relevant and broadly applicable to the most science and engineering domains possible. The hybrid approach is innovative and holds the promise of creating rich interactions among PIs, resulting in richer collaboration and learning. The results of this workshop thus have the potential to guide cyberinfrastructure development and cyberinfrastructure driven research for both the participating projects and for the wider software development community."
"1148127","SI2-SSE: Correctness Verification Tools for Extreme Scale Hybrid Concurrency","OAC","Information Technology Researc, Software Institutes","06/01/2012","04/24/2015","Ganesh Gopalakrishnan","UT","University of Utah","Standard Grant","Rajiv Ramnath","05/31/2016","$490,279.00","Mary Hall","ganesh@cs.utah.edu","201 PRESIDENTS CIR","SALT LAKE CITY","UT","841129049","8015816903","CSE","1640, 8004","1640, 7433, 7942, 8004, 8005, 9150, 9251","$0.00","High Performance Computing is strategically important to national competitiveness.  Advances in computational capabilities involve the use of unprecedented levels of parallelism: programming methods that involve billions of concurrent activities.  Multiple styles of concurrency involving shared and distributed memory programming (""hybrid"") are necessary. Unfortunately, such programs are very difficult to debug using existing methods. This project develops formal (mathematically based) verification tools that can debug hybrid concurrent programs with very high certainty of bug elimination, while consuming only modest computational resources for verification.  <br/><br/>The project develops execution-based tools that eliminate search over semantically equivalent alternative schedules as well as solver-based techniques that eliminate classes of bugs over single runs. Scalable methods based on non-determinism classification and heuristic execution-space reduction are also being developed. <br/><br/>Expected results include: (1) development of tools based on formal algorithmic techniques that verify large-scale hybrid programs; (2) amalgamation of incisive bug-hunting methods developed at other research organizations within formally based tools developed in our group; (3) incorporation of our verification tools and techniques within popular tool-integration frameworks; (4) large-scale case studies handled using our tools; and (5) training of undergraduate and graduate students on these advanced verification methods, building the talent pool vital to continued progress in high performance computing with applications to science and engineering, energy/sustainability, and homeland security."
"1831393","2018 Software Infrastructure for Sustained Innovation (SI2) Principal Investigators Workshop","OAC","Software Institutes","06/01/2018","09/15/2018","Francis Timmes","AZ","Arizona State University","Standard Grant","Rob Beverly","05/31/2019","$85,065.00","Rafael Ferreira da Silva, Sandra Gesing, Paul Bauman, Kyle Niemeyer","fxt44@mac.com","660 S MILL AVE STE 312","TEMPE","AZ","852813670","4809655479","CSE","8004","026Z, 7556, 8004","$0.00","This project will host a 2-day workshop in Washington, DC, which will bring together the community of Software Infrastructure for Sustained Innovation (SI2) awardees (with the goal of involving one principal investigator from each Scientific Software Elements (SSE), Scientific Software Integration (SSI), and Scientific Software Innovation Institutes (S2I2) project, many of which are collaborative awards) from approximately 250 awards. The workshop will have participation from Computational and Data-Enabled Science and Engineering (CDS&E), Critical Resilient Interdependent Infrastructure Systems and Processes (CRISP), and Venture funded PIs as well as SI2 Early Concept Grants for Exploratory Research (EAGER) and Rapid Response Research (RAPID) awardees. In addition, the proximity to NSF will encourage participation by Program Officers from across the Foundation. Goals of this workshop include: (a) providing a focused forum for PIs to share technical information with each other and with NSF Program Officers, (b) encouraging exploration of emerging topics, (c) identifying emerging best practices across the supported software projects, (d) stimulating thinking on new ways of achieving software sustainability, and (d) disseminating the shared experiences of the researchers via an online web portal. The workshop is expected to host close to 150 SI2 and other awardees, other speakers and panelists. <br/><br/>The proposed workshop will support the exchange of ideas among the current software cyberinfrastructure development projects. It will provide guidance on issues related to the development of robust software and to the problem of software sustainability with the broader agenda for national software ecosystem. Involvement of program officers across NSF is expected to help the interdisciplinary SI2 awardees understand the relevance and impact of cyberinfrastructure throughout the NSF. The participation of these researchers and program officers in a common forum will help ensure that the cyberinfrastructure software developed as part of SI2 projects will be relevant and broadly applicable to the most science and engineering domains possible. The results of this workshop thus have the potential to guide cyberinfrastructure development and cyberinfrastructure driven research for both the participating projects and for the wider software development community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1419131","Collaborative Research:  Software Sustainability: an SI^2 PI Workshop","OAC","Software Institutes","01/15/2014","01/13/2014","Beth Plale","IN","Indiana University","Standard Grant","Daniel Katz","12/31/2014","$66,224.00","","plale@cs.indiana.edu","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","CSE","8004","7433, 7556","$0.00","This award will support a 1.5 day workshop in Arlington, VA to bring together the community of SI2 awardees with the aims of: 1) serving as a forum for focused PI technical exchange, through an early evening poster session; 2) serving as a forum for discussion of topics of relevance to the PIs from topics emerging both from within NSF and from the broader community, by informing the attendees of emerging best practices, and stimulating thinking on new ways of achieving sustainability and of ensuring that the foundation laid by SI2 is preserved into the future; and 3) gathering experiences and a shared sense of best practice that results in a published workshop report.<br/> <br/>The workshop will bring together researchers who are a proto-community of NSF open source software developers. The meeting will examine the characteristics of the community, and consider whether the products from the program can be enhanced by giving the community a new identify and new way of looking at itself. The meeting will also address citation, attribution, and reproducibility, which are three related topics often discussed in the context of data, but less so in the context of software. The attendees will consider practical steps that could be taken to advance software citation and science reproducibility. Finally, sustainability of software is a major topic for NSF and for the SI2 PIs. The meeting will highlight new ways of thinking about software sustainability, drawing on experts in the field and on recent SI2 EAGER funded projects that are studying the community to help the workshop attendees in their thinking about sustainability.<br/><br/>The community outputs of the workshop will be: posters developed by the SI2 PIs that will be shared amongst the attendees and shared more broadly on the workshop web site; an experiences report (licensed under a Creative Commons license) produced by the award PIs, distributed via the workshop web site, via email to participants who will be asked to disseminate among their project colleagues and peers, and via an archive repository through which it will be accessible through a persistent ID; and attendee journalism during the event in the form of a public Google doc and public Twitter stream."
"1256100","The Role of Software and Software Institutes in Computational Science Over Time","OAC","CI-TEAM, Software Institutes","10/01/2012","09/04/2012","Ewa Deelman","CA","University of Southern California","Standard Grant","Daniel Katz","09/30/2014","$74,430.00","Miron Livny","deelman@isi.edu","3720 S FLOWER ST FL 3","LOS ANGELES","CA","900074304","2137407762","CSE","7477, 8004","7477, 8005, 8009","$0.00","The workshop will bring together Principle Investigators of the leading software cyberinfrastructure projects and discuss issues relevant to the community as we move into the future.  In 2011 and 2012 the OCI Software Infrastructure for Sustained Innovation (SI2) program funded software efforts in small development efforts that can provide software pieces that can be integrated into the larger cyberinfrastructure, and larger collaborations that were delivering significant community software. In addition, new SI2 awards aimed at conceptualizing large-scale software institutes, aimed at providing a fabric for the software needed by domain scientists to achieve breakthroughs in their intra and inter-disciplinary efforts, will be awarded. New NSF initiatives such as EarthCube are defining roadmaps for cyberinfrastructure development in Earth sciences. This workshop will bring together the Principle Investigators of the recent SI2 awards to discuss potential synergies and collaborations, define challenges ahead, discuss the relationship of the SI2 efforts to the planned Software Institutes, and explore the relationship of the OCI-funded software in the context of the broad NSF initiatives such as EarthCube, DataWay, and other planned community-focused efforts.<br/><br/>To achieve its goals, the workshop will focus on these main themes: (i) Discussing SI2 projects within the context of SI2 Institutes and NSF-wide initiative such as EarthCube, DataWay, and others; (ii) sharing experiences in building quality software and services; (iii) fostering collaboration and providing incentives for collaboration and cyberinfrastructure development as a career; and (iv) sustaining the software capabilities in the long term, defining software value.<br/><br/>The workshop will solicit participation from major science and engineering projects that rely on the national cyberinfrastructure for their computations and data management needs. Their participation will help ensure that the cyberinfrastructure software developed as part of SI2 projects will be relevant and broadly applicable to a number of science and engineering domains. The results of this workshop will then potentially guide cyberinfrastructure development and testing in the future."
"1153402","NSF Workshop: Building Communities for Software Infrastructure for Sustained Innovation","OAC","Software Institutes","09/01/2011","09/08/2011","Jay Alameda","IL","University of Illinois at Urbana-Champaign","Standard Grant","Daniel Katz","08/31/2014","$54,851.00","","alameda@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","8004","7556, 8004","$0.00","The workshop for ""Building Communities for Software Infrastructure for Sustained Innovation"" brings together the practitioners of software sustainability, including NSF SI2 participants, to discuss issues and challenges of common interest, to formulate responses benefitting the software enterprise as a whole, and working to build a community of software developers, maintainers, supporters, and users.  This workshop also builds concrete affiliations between SI2 projects and other NSF CIF21 projects to help these programs function as coherent providers of cyberinfrastructure in the service of science and engineering research.<br/><br/>This workshop addresses numerous challenges to success with software; for instance, sustaining a community of developers, building a community of users, fostering innovation and transforming innovation into a reliable, high quality component of software, building liaisons/affiliations with other efforts, developing and maintaining software requirements, managing change against an unstable software substrate, targeting innovative new software and hardware platforms; developing mechanisms to train new users and educate the next generations in ways to use software in scientific and engineering discovery, as well as to be developers of the next set of software capabilities, etc.  The workshop will cultivate the software sustainability community, in order to  bring innovative, useable, sustainable software to the user community, as diverse as they are, and to foster even more innovation, both in science and engineering research, as well as in the development of new software to enable better utilization of a comprehensive cyberinfrastructure."
"1606994","2016 Software Infrastructure for Sustained Innovation (SI2) Principal Investigators Workshop","OAC","Software Institutes","01/15/2016","02/08/2016","Francis Timmes","AZ","Arizona State University","Standard Grant","Rajiv Ramnath","12/31/2017","$104,947.00","Stanley Ahalt, Shaowen Wang, Matthew Turk","fxt44@mac.com","660 S MILL AVE STE 312","TEMPE","AZ","852813670","4809655479","CSE","8004","7433, 7556, 8004","$0.00","This project will host a 1.5-day workshop in Arlington, VA, which will bring together the community of SI2 awardees (with the goal of involving one principal investigator from each SSE and SSI project, many of which are collaborative awards) from approximately 250 awards. The workshop will have participation from SI2 EAGER and RAPID awardees and selected awardees of the ACI VOSS program that examines cyberinfrastructures from the social and organizational perspective. In addition, the proximity to NSF will encourage participation by Program Officers from across the Foundation. Goals of this workshop include: (a) providing a focused forum for PIs to share technical information with each other and with NSF Program Officers, (b) encouraging exploration of emerging topics, (c) identifying emerging best practices across the supported software projects, (d) stimulating thinking on new ways of achieving software sustainability, and (d) disseminating the shared experiences of the researchers via an online web portal.<br/><br/>The workshop is expected to host close to 150 SI2 and other awardees, other speakers and panelists. The workshop will use a hybrid style, blending a traditional, top-down driven agenda with a interactive, adaptive, participant-driven agenda. <br/><br/>The proposed workshop will support the exchange of ideas among the current software cyberinfrastructure development projects. It will provide guidance on issues related to the development of robust software and to the problem of software sustainability.   Involvement of program officers across NSF is expected to help the interdisciplinary SI2 awardees understand the relevance and impact of cyberinfrastructure throughout the NSF. The participation of these researchers and program officers in a common forum will help ensure that the cyberinfrastructure software developed as part of SI2 projects will be relevant and broadly applicable to the most science and engineering domains possible. The hybrid approach is innovative and holds the promise of creating rich interactions among PIs, resulting in richer collaboration and learning. The results of this workshop thus have the potential to guide cyberinfrastructure development and cyberinfrastructure driven research for both the participating projects and for the wider software development community.<br/>"
"1521388","Software Infrastructure for Sustained Innovation - A 2015 SI^2 PI Workshop","OAC","Software Institutes","01/15/2015","01/29/2015","Francis Timmes","AZ","Arizona State University","Standard Grant","Rajiv Ramnath","12/31/2016","$75,527.00","Stanley Ahalt, Matthew Turk","fxt44@mac.com","660 S MILL AVE STE 312","TEMPE","AZ","852813670","4809655479","CSE","8004","7433, 7556, 8004","$0.00","This project will host a 1.5-day workshop in Arlington, VA, which will bring together the community of SI2 awardees (with the goal of involving one principal investigator from each SSE and SSI project, many of which are collaborative awards) from 214 awards. The workshop will also solicit participation from NSF-supported science and engineering projects that rely on the national research cyberinfrastructure for their computation, communication, and data management needs. In addition, the proximity to NSF will encourage participation by Program Officers from across the Foundation. Goals of this workshop include: (1) Providing a focused forum for PIs to share technical information with each other and with NSF Program Officers; (2) Encouraging exploration of emerging topics; (3) Identifying emerging best practices across the supported software projects; (4) Stimulating thinking on new ways of achieving software sustainability; and (5) Disseminating the shared experiences of the researchers via an online web portal. The workshop is expected to host 85 SI2 awardees and several other speakers and panelists. The workshop will use a hybrid style, blending a traditional, top-down driven agenda with a interactive, adaptive, participant-driven agenda. <br/><br/>The proposed workshop will support the exchange of ideas among the current software cyberinfrastructure development projects. It will provide guidance on issues related to the development of robust software and to the problem of software sustainability.   Involvement of program officers across NSF is expected to help the interdisciplinary SI2 awardees understand the relevance and impact of cyberinfrastructure throughout the NSF. The participation of these researchers and program officers in a common forum will help ensure that the cyberinfrastructure software developed as part of SI2 projects will be relevant and broadly applicable to the most science and engineering domains possible. The hybrid approach is innovative and holds the promise of creating rich interactions among PIs, resulting in richer collaboration and learning. The results of this workshop thus have the potential to guide cyberinfrastructure development and cyberinfrastructure driven research for both the participating projects and for the wider software development community."
"1419132","Collaborative Research:  Software Sustainability: an SI^2 PI Workshop","OAC","Software Institutes","01/15/2014","01/13/2014","Douglas Thain","IN","University of Notre Dame","Standard Grant","Daniel Katz","12/31/2014","$19,868.00","","dthain@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","8004","7433, 7556","$0.00","This award will support a 1.5 day workshop in Arlington, VA to bring together the community of SI2 awardees with the aims of: 1) serving as a forum for focused PI technical exchange, through an early evening poster session; 2) serving as a forum for discussion of topics of relevance to the PIs from topics emerging both from within NSF and from the broader community, by informing the attendees of emerging best practices, and stimulating thinking on new ways of achieving sustainability and of ensuring that the foundation laid by SI2 is preserved into the future; and 3) gathering experiences and a shared sense of best practice that results in a published workshop report.<br/> <br/>The workshop will bring together researchers who are a proto-community of NSF open source software developers. The meeting will examine the characteristics of the community, and consider whether the products from the program can be enhanced by giving the community a new identify and new way of looking at itself. The meeting will also address citation, attribution, and reproducibility, which are three related topics often discussed in the context of data, but less so in the context of software. The attendees will consider practical steps that could be taken to advance software citation and science reproducibility. Finally, sustainability of software is a major topic for NSF and for the SI2 PIs. The meeting will highlight new ways of thinking about software sustainability, drawing on experts in the field and on recent SI2 EAGER funded projects that are studying the community to help the workshop attendees in their thinking about sustainability.<br/><br/>The community outputs of the workshop will be: posters developed by the SI2 PIs that will be shared amongst the attendees and shared more broadly on the workshop web site; an experiences report (licensed under a Creative Commons license) produced by the award PIs, distributed via the workshop web site, via email to participants who will be asked to disseminate among their project colleagues and peers, and via an archive repository through which it will be accessible through a persistent ID; and attendee journalism during the event in the form of a public Google doc and public Twitter stream."
"1419139","Collaborative Research:  Software Sustainability: an SI^2 PI Workshop","OAC","Software Institutes","01/15/2014","01/13/2014","Matthew Jones","CA","University of California-Santa Barbara","Standard Grant","Daniel Katz","12/31/2014","$13,635.00","","jones@nceas.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","8004","7433, 7556","$0.00","This award will support a 1.5 day workshop in Arlington, VA to bring together the community of SI2 awardees with the aims of: 1) serving as a forum for focused PI technical exchange, through an early evening poster session; 2) serving as a forum for discussion of topics of relevance to the PIs from topics emerging both from within NSF and from the broader community, by informing the attendees of emerging best practices, and stimulating thinking on new ways of achieving sustainability and of ensuring that the foundation laid by SI2 is preserved into the future; and 3) gathering experiences and a shared sense of best practice that results in a published workshop report.<br/> <br/>The workshop will bring together researchers who are a proto-community of NSF open source software developers. The meeting will examine the characteristics of the community, and consider whether the products from the program can be enhanced by giving the community a new identify and new way of looking at itself. The meeting will also address citation, attribution, and reproducibility, which are three related topics often discussed in the context of data, but less so in the context of software. The attendees will consider practical steps that could be taken to advance software citation and science reproducibility. Finally, sustainability of software is a major topic for NSF and for the SI2 PIs. The meeting will highlight new ways of thinking about software sustainability, drawing on experts in the field and on recent SI2 EAGER funded projects that are studying the community to help the workshop attendees in their thinking about sustainability.<br/><br/>The community outputs of the workshop will be: posters developed by the SI2 PIs that will be shared amongst the attendees and shared more broadly on the workshop web site; an experiences report (licensed under a Creative Commons license) produced by the award PIs, distributed via the workshop web site, via email to participants who will be asked to disseminate among their project colleagues and peers, and via an archive repository through which it will be accessible through a persistent ID; and attendee journalism during the event in the form of a public Google doc and public Twitter stream."
"1265920","Collaborative Proposal: SI2-CHE: ExTASY Extensible Tools for Advanced Sampling and analYsis","CHE","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","09/01/2013","08/28/2013","Mauro Maggioni","NC","Duke University","Standard Grant","Evelyn Goldfield","01/31/2017","$148,000.00","","mauro.maggioni@jhu.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1253, 8004","5918, 5946, 5950, 7433, 8009","$0.00","Collaborative Research: SI2-CHE<br/>ExTASY Extensible Tools for Advanced Sampling and analYsis<br/><br/>An international team consisting of  Cecilia Clementi(Rice University), Mauro Maggioni (Duke University) Shantenu Jha (Rutgers University), Glenn Martyna (BM T. J. Watson Laboratory ), Charlie Laughton (University of Nottingham), Ben Leimkuhler ( University of Edinburgh), Iain Bethune (University of Edinburgh) and Panos Parpas(Imperial College) are supported through the SI2-CHE program for the development of ExTASY -- Extensible Toolkit for Advanced Sampling and analYsis, -- a conceptual and software framework that provides a step-change in the sampling of the conformational space of macromolecular systems. Specifically, ExTASY is a lightweight toolkit to enable first-class support for ensemble-based simulations and their seamless integration with dynamic analysis capabilities and ultra-large time step integration methods, whilst being extensible to other community software components via well-designed and standard interfaces. <br/><br/> The primary impacts of this project are in the biological sciences.  This software advances our understanding of biologically important systems, as it can be used to obtain fast and accurate sampling of the conformational dynamics of stable proteins; a prerequisite for the accurate prediction of thermodynamic parameters and biological functions. It also allows tackling systems like intrinsically disordered proteins, which can be beyond the reach of classical structural biology. Along with the research itself, the PIs are involved with outreach programs to attract high school students to science."
"1739657","SI2-SSE: Collaborative Research: A Sustainable Future for the Glue Multi-Dimensional Linked Data Visualization Package","OAC","EXTRAGALACTIC ASTRON & COSMOLO, OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","10/01/2017","08/29/2017","Alyssa Goodman","MA","Harvard University","Standard Grant","Seung-Jong Park","09/30/2020","$332,369.00","","agoodman@cfa.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","CSE","1217, 1253, 8004","1206, 7433, 7569, 7659, 8004, 8005, 9102","$0.00","Glue is a free and open-source application that allows scientists and data scientists to explore relationships within and across related datasets. Glue makes it easy create a wide variety of visualizations (such as scatter plots, bar charts, images) of data, including three dimensional views. What makes Glue unique is its ability to connect datasets together, without merging them into one. Thus, for example, two Earth-based mapping data sets may be connected and jointly visualized by using the coordinates (e.g. latitude and longitude) to glue the maps together, so that when a  user selects (e.g. with a lasso tool) regions in one data set, the corresponding selected subset of data will highlight in all related visualizations simultaneously. These ?linked views"" are especially powerful across wide varieties of plot types. For example, if a user interested in air traffic control glues a data set with information about the 3D locations of all airplanes to a second data set giving weather information, that user could make a combination of selections that would highlight (on maps, in 3D views, or any other display) planes at particular altitudes where thunderstorms might be likely to occur within a specific period of time. In particular, Glue makes it easy for users to create their own kinds of visualizations, which is important because different disciplines often need very specialized ways of looking at data. The software is already being used widely across several disciplines, in particular, astronomy and medicine, for which has been specially optimized. This project will add new features to make Glue more useful in more fields of science (e.g. bioinformatics, epidemiology) where there is demand for linked-view visualization, as well as making it more accessible as an educational tool. In addition, this project will train new users and developers, who will expand Glue into a much more sustainable community effort. <br/><br/>Glue is an open-source package that allows scientists to explore relationships within and across related datasets, by making it easy for them to make multi-dimensional linked visualizations of datasets, select subsets of data interactively or programmatically in 1, 2, or 3 dimensions, and see those selections propagate live across all open visualizations of the data (e.g. graphs, maps, diagnostics charts). A unique feature of glue is that datasets from different sources can be linked to each other, using user-defined mathematical relationships between sets of data components, which makes it possible to carry out selections across datasets. Glue, written in Python, is designed from the ground-up for multidisciplinary work, and it is currently helping researchers make discoveries in geoscience, genomics, astronomy, and medicine. It is also giving insights into data from outside academia, including open data provided by governments and cities. To become sustainable in the long term, glue development needs to become a community-driven effort. Through tutorial and developer workshops, coding sprints, and strategic collaborations with researchers in several disciplines and experienced open source developers, the glue team will help user communities extend glue by developing new functionality useful within particular fields of research. The team will help users contribute the most widely-needed functionality back to glue, and will recruit active contributors to participate in core glue development. As the community grows, glue development will be guided to focus on several major features useful to the broad research community, including: support for very large datasets, support for running glue fully in the browser (inside Jupyter notebooks and Jupyter Lab), and improved interoperability with third-party tools.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria"
"1708353","Collaborative Proposal: SI2-CHE: ExTASY Extensible Tools for Advanced Sampling and analYsis","CHE","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","07/01/2016","12/08/2016","Mauro Maggioni","MD","Johns Hopkins University","Standard Grant","Evelyn Goldfield","08/31/2017","$145,643.00","","mauro.maggioni@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","MPS","1253, 8004","5918, 5946, 5950, 7433, 8009","$0.00","Collaborative Research: SI2-CHE<br/>ExTASY Extensible Tools for Advanced Sampling and analYsis<br/><br/>An international team consisting of  Cecilia Clementi(Rice University), Mauro Maggioni (Duke University) Shantenu Jha (Rutgers University), Glenn Martyna (BM T. J. Watson Laboratory ), Charlie Laughton (University of Nottingham), Ben Leimkuhler ( University of Edinburgh), Iain Bethune (University of Edinburgh) and Panos Parpas(Imperial College) are supported through the SI2-CHE program for the development of ExTASY -- Extensible Toolkit for Advanced Sampling and analYsis, -- a conceptual and software framework that provides a step-change in the sampling of the conformational space of macromolecular systems. Specifically, ExTASY is a lightweight toolkit to enable first-class support for ensemble-based simulations and their seamless integration with dynamic analysis capabilities and ultra-large time step integration methods, whilst being extensible to other community software components via well-designed and standard interfaces. <br/><br/> The primary impacts of this project are in the biological sciences.  This software advances our understanding of biologically important systems, as it can be used to obtain fast and accurate sampling of the conformational dynamics of stable proteins; a prerequisite for the accurate prediction of thermodynamic parameters and biological functions. It also allows tackling systems like intrinsically disordered proteins, which can be beyond the reach of classical structural biology. Along with the research itself, the PIs are involved with outreach programs to attract high school students to science."
"1265929","Collaborative Research: SI2-CHE: ExTASY Extensible Tools for Advanced Sampling and analYsis","CHE","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","09/01/2013","06/28/2018","Cecilia Clementi","TX","William Marsh Rice University","Standard Grant","Evelyn Goldfield","08/31/2019","$585,000.00","","cecilia@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1253, 8004","5918, 5946, 5950, 7433, 8009","$0.00","Collaborative Research: SI2-CHE<br/>ExTASY Extensible Tools for Advanced Sampling and analYsis<br/><br/>An international team consisting of  Cecilia Clementi(Rice University), Mauro Maggioni (Duke University) Shantenu Jha (Rutgers University), Glenn Martyna (BM T. J. Watson Laboratory ), Charlie Laughton (University of Nottingham), Ben Leimkuhler ( University of Edinburgh), Iain Bethune (University of Edinburgh) and Panos Parpas(Imperial College) are supported through the SI2-CHE program for the development of ExTASY: Extensible Toolkit for Advanced Sampling and analYsis, a conceptual and software framework that provides a step-change in the sampling of the conformational space of macromolecular systems. Specifically, ExTASY is a lightweight toolkit to enable first-class support for ensemble-based simulations and their seamless integration with dynamic analysis capabilities and ultra-large time step integration methods, whilst being extensible to other community software components via well-designed and standard interfaces. <br/><br/> The primary impacts of this project are in the biological sciences.  This software advances our understanding of biologically important systems, as it can be used to obtain fast and accurate sampling of the conformational dynamics of stable proteins; a prerequisite for the accurate prediction of thermodynamic parameters and biological functions. It also allows tackling systems like intrinsically disordered proteins, which can be beyond the reach of classical structural biology. Along with the research itself, the PIs are involved with outreach programs to attract high school students to science."
"1265788","Collaborative Research: SI2-CHE: ExTASY Extensible Tools for Advanced Sampling and analYsis","CHE","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","09/01/2013","08/28/2013","Shantenu Jha","NJ","Rutgers University New Brunswick","Standard Grant","Evelyn Goldfield","08/31/2018","$617,005.00","","shantenu.jha@rutgers.edu","3 RUTGERS PLZA","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1253, 8004","5918, 5946, 5950, 7433, 8009","$0.00","Collaborative Research: SI2-CHE<br/>ExTASY Extensible Tools for Advanced Sampling and analYsis<br/><br/>An international team consisting of  Cecilia Clementi(Rice University), Mauro Maggioni (Duke University) Shantenu Jha (Rutgers University), Glenn Martyna (BM T. J. Watson Laboratory ), Charlie Laughton (University of Nottingham), Ben Leimkuhler ( University of Edinburgh), Iain Bethune (University of Edinburgh) and Panos Parpas(Imperial College) are supported through the SI2-CHE program for the development of ExTASY -- Extensible Toolkit for Advanced Sampling and analYsis, -- a conceptual and software framework that provides a step-change in the sampling of the conformational space of macromolecular systems. Specifically, ExTASY is a lightweight toolkit to enable first-class support for ensemble-based simulations and their seamless integration with dynamic analysis capabilities and ultra-large time step integration methods, whilst being extensible to other community software components via well-designed and standard interfaces. <br/><br/> The primary impacts of this project are in the biological sciences.  This software advances our understanding of biologically important systems, as it can be used to obtain fast and accurate sampling of the conformational dynamics of stable proteins ? a prerequisite for the accurate prediction of thermodynamic parameters and biological functions. It also allows tackling systems like intrinsically disordered proteins, which can be beyond the reach of classical structural biology. Along with the research itself, the PIs are involved with outreach programs to attract high school students to science."
"1047875","Collaborative Research SI2-SSE: Comprehensive Sustained Innovation in Acceleration of Molecular Dynamics Simulation and Analysis on Graphics Processing Units.","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","10/01/2010","09/15/2010","Ross Walker","CA","University of California-San Diego","Standard Grant","Evelyn Goldfield","03/31/2012","$73,469.00","","rcw@sdsc.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","CSE","1253, 8004","1253, 1978, 9216, 9263","$0.00","This collaborative pilot project between the San Diego Supercomputer Center at the University of California San Diego, the Quantum Theory Project at the University of Florida and industrial partners NVIDIA Inc. is focused on developing innovative, comprehensive open source software element libraries for accelerating condensed phase Molecular Dynamics (MD) simulations of biomolecules using Graphics Processing Units (GPU). By porting MD techniques to GPUs this project is enabling users to both attain substantial increases in their own local calculations without the need for substantial investment in hardware or infrastructure, and to make effective use of GPU acceleration provided by new machines within the NSF supercomputing centers. The software elements being developed and distributed both within the AMBER MD package and as open source libraries are providing critical software infrastructure in support of transformative research in the fields of chemistry, life science, materials science, environmental and renewable energy research.<br/><br/>The software elements being created in this project have very broad impact. For example, the integration of single and multi-GPU acceleration within the AMBER software alone benefits a very large and established national and international user base. Over 8,000 downloads of the AMBER Tools package from unique IP addresses and more than 500 sites which use the AMBER MD engine testify to the scope of the community of researchers this work impacts. Additionally the open source GPU MD acceleration libraries being produced provide broad impact across multiple domains while outreach workshops are helping to train the next generation of scientists not just in the use and potential benefits of GPU MD acceleration libraries but also in modern MD simulation techniques.<br/><br/>This award is co-funded by the Office of Cyberinfrastructure, the Division of Chemistry and the Office of Multidisciplinary Activites of the Directorate of Mathematical Sciences."
"1148458","Collaborative Research: SI2-SSI: Empowering the Scientific Community with Streaming Data Middleware: Software Integration into Complex Science Environments","OAC","ADVANCES IN BIO INFORMATICS, ECOSYSTEM STUDIES, Software Institutes, Cybersecurity Innovation","08/01/2012","09/14/2015","Tony Fountain","CA","University of California-San Diego","Standard Grant","Rajiv Ramnath","07/31/2016","$1,455,429.00","Ilya Zaslavsky, Sameer Tilak","fountain@sdsc.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","CSE","1165, 1181, 8004, 8027","1165, 1181, 7434, 8004, 8009, 8027","$0.00","This project will catalyze a new generation of sensor-based streaming data applications by building on the foundation of the Open Source DataTurbine (OSDT) middleware (www.dataturbine.org). This will involve strategic software development, systems integration and testing, science experiments, and education and outreach. The software developments will include (A) tools for configuring, testing, and controlling distributed sensor systems, (B) software interfaces that are compliant with the Open Geospatial Consortium Sensor Web Enablement standards, (C) a new OSDT-Matlab interface, and (D) OSDT cybersecurity enhancements. These software products will be integrated into the production research infrastructures at Purdue, the University of Connecticut, the North Temperate Lake LTER Network site, and the Monterey Bay Aquarium Research Institute. The software will be professionally developed and managed as community resources. The intellectual merit of this project will be advances in software engineering, interoperability standards, and systems management for complex real-time sensor systems as well as advances on important open questions in civil engineering, marine science, and ecology. This project will yield broader impacts by enabling new science applications, enhancing productivity, and accelerating innovation across NSF directorates. To ensure that these broader impacts are realized, the team will provide student research opportunities, develop curriculum materials and training courses, and cultivate a community of software users."
"1047577","Collaborative Research: SI2-SSI: Developments in High Performance Electronic Structure Theory","OAC","OFFICE OF MULTIDISCIPLINARY AC, PROJECTS, Chem Thry, Mdls & Cmptnl Mthds, Software Institutes","10/01/2010","07/18/2013","Todd Martinez","CA","Stanford University","Continuing Grant","Evelyn Goldfield","09/30/2015","$832,000.00","","Todd.Martinez@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","CSE","1253, 1978, 6881, 8004","1253, 1303, 1712, 1765, 1978, 6881, 7564, 7569, 7573, 7644, 8004, 9216, 9263","$0.00","This project focuses on implementing newly developed numerical methods and software engineering approaches into widely used computer codes used in biology, chemistry, materials science and engineering, physics, chemical and mechanical engineering and many other science and engineering fields. GAMESS (General Atomic and Molecular Electronic Structure System) is the most broadly used highly scalable computational chemistry program with more than 100,000 users worldwide. NWChem is likewise a very popular computational chemistry code, as are MPQC (Massively Parallel Quantum Chemistry) and AIMS (Ab Initio Multiple Spawning).  All of these codes are distributed at no cost via the Web.  While these codes have all been developed by computational chemists, they impact a multitude of disciplines in science and engineering. <br/><br/>The new software is highly scalable, thereby enabling the study of fundamentally critical problems, including the structure of liquids (water among them), the formation of atmosrpheric aerosols, heterogeneous catalysis, and photochemistry and photobiology. In addition, the new software is implemented so as to take advantage of new hardware such as the graphical processing units (GPU). Three of the principal investigators (Gordon, Martinez, Windus) are among the world leaders in such developments.<br/><br/>All of the new developments are to be incorporated into both graduate and undergraduate courses in both chemistry and computer science. In addition, downloadable modules are placed on an accessible web site.  The results of the research are to be reported at national meetings, such as those organized by the American Chemical Society, IEEE, and the Materials Research Society. Videos of these presentation are placed on the Web, so that researchers and educators who are unable to attend the meetings can access the results of this project. The Web site contains also a set of lessons learned to aid subsequent researchers.<br/><br/>This award pertains to the Software Infrastructure for Sustained Innovation (SI2) solicitation. Its funding sources include the Division of Chemistry (MPS/CHE), the Office of Multidisciplinary Activities (OMA) of the Directorate of Mathematical and Physical Sciences (MPS), the Office of Cyberinfrastructure (OCI), the Division of Civil, Mechanical and Manufacturing Innovation (ENG/CMMI), and the Division of Materials Research (MPS/DMR)."
"1148443","SI2-SSE: Enhancement and Support of Swift Parallel Scripting","OAC","Software Institutes","04/01/2012","04/16/2012","Michael Wilde","IL","University of Chicago","Standard Grant","Rajiv Ramnath","03/31/2015","$499,135.00","","wilde@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","CSE","8004","8004, 8005","$0.00","The Swift parallel scripting language enables scientists, engineers, and data analysts to express and coordinate parallel invocations of application programs on distributed and parallel computing platforms: one of the dominant modes of performing computation in science and engineering.  Swift runs on a variety of HPC clusters and clouds, and enables users to move their application scripts between computing environments with relative ease.<br/><br/>Swift comprises a programming model, scripting language, and runtime engine.  Its implicitly parallel programming model allows users with minimal programming expertise to transparently utilize parallel and distributed systems. The scripting language is simple, minimal and standalone; the programming model has also been embedded into the Python and R languages.<br/><br/>Swift is employed in many diverse domains, including biochemistry, neuroscience, climate model analysis, earthquake simulation, hydrology, energy forecasting, economics modeling, mass media analysis, materials science, and astronomy.<br/><br/>This project makes usability and programmatic expressiveness enhancements to Swift, broadens its utility library, performs the testing and hardening needed to serve a large-scale national and global community, and extends existing documentation and training material in a manner that will create and serve a broad user base. It also develops enhancements in configuration, script debugging, exception handling, parallel collection processing, and data typing and mapping to make Swift increasingly productive.<br/><br/>Swift enables users with little or no experience in parallel programming to leverage parallel and distributed computing environments ranging in scale from desktops to petascale supercomputers. It opens up powerful cyberinfrastructure like XSEDE, Open Science Grid, FutureGrid, and Blue Waters to a wide range and scale of scientific user communities, thus broadening participation in high performance computing."
"1047916","SI2-SSI: CyberGIS Software Integration for Sustained Geospatial Innovation","OAC","Methodology, Measuremt & Stats, Geography and Spatial Sciences, Cross-Directorate  Activities, Software Institutes, Sustainable Energy Pathways","10/01/2010","09/26/2017","Shaowen Wang","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Cheryl Eavey","09/30/2018","$4,804,821.00","Timothy Nyerges, Nancy Wilkins-Diehr, Luc Anselin, Budhendra Bhaduri","shaowen@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","1333, 1352, 1397, 8004, 8026","004Z, 1333, 1352, 1397, 7433, 7556, 7969, 8004, 8009","$0.00","Originally developed by geographers in the mid-1960s, Geographic Information Systems (GIS) have flourished since that time.  In the foreseeable future, GIS software will continue to play essential roles for breaking through scientific challenges in numerous fields and improving decision-making practices with broad societal impacts.  However, fulfilling such roles is increasingly dependent on the ability to handle very large spatiotemporal data sets and complex analysis software based on synthesizing computational and spatial thinking enabled by cyberinfrastructure, which conventional GIS-based software approaches do not provide.  This project will establish CyberGIS as a fundamentally new software framework comprising a seamless integration of cyberinfrastructure, GIS, and spatial analysis/modeling capabilities.  Specifically, the project will: 1) engage a multidisciplinary community through a participatory approach in evolving CyberGIS software requirements; 2) integrate and sustain a core set of composable, interoperable, manageable, and reusable CyberGIS software elements based on community-driven and open source strategies; 3) empower high-performance and scalable CyberGIS by exploiting spatial characteristics of data and analytical operations for achieving unprecedented capabilities for geospatial knowledge discovery; 4) enhance an online geospatial problem solving environment to allow for the contribution, sharing, and learning of CyberGIS software by numerous users, which fosters the development of education, outreach, and training programs crosscutting multiple disciplines; 5) deploy and test CyberGIS software by linking with national and international cyberinfrastructure to achieve scalability to significant sizes of geospatial problems, cyberinfrastructure resources, and user communities; and 6) evaluate and improve the CyberGIS framework through domain science applications and vibrant partnerships to gain better understanding of the complexity of coupled human-natural systems.<br/><br/>The CyberGIS software framework will shift the current paradigm of GIS and associated spatial analysis/modeling software to create scalable and sustainable software ecosystems while achieving groundbreaking scientific advances in understanding coupled human-natural systems that would be impossible otherwise.  These advances will, for example, dramatically advance the understanding of disaster preparedness and response and impacts of global climate change.  This framework will empower high-performance and collaborative geospatial problem solving and serve as a key driver for the interoperability of international cyberinfrastructure based on broad engagement of user communities related to GIS for both research and education purposes.  The project will establish an industrial partnership with the Environmental Systems Research Institute (ESRI), collaborations with the Department of Energy's Oak Ridge National Laboratory (ORNL) and the U.S. Geological Survey (USGS) National Map Project, and international partnerships with several institutions in Australia, China, and the United Kingdom to effectively extend the benefits to the nation and society in significant ways."
"1147843","Collaborative Research: SI2-SSI: Sustainable Development of Next-Generation Software in Quantum Chemistry","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, CHEMISTRY PROJECTS, Software Institutes","06/01/2012","07/06/2012","Charles Sherrill","GA","Georgia Tech Research Corporation","Standard Grant","Rajiv Ramnath","05/31/2017","$614,654.00","Edmond Chow","sherrill@gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","CSE","1253, 1712, 1991, 8004","1253, 1712, 1991, 7433, 7683, 8004, 8009, 9216, 9263","$0.00","Quantum chemistry can provide highly accurate results for arbitrary  molecular systems, making it a vital component in many different  disciplines such as materials science, biology, physics, chemical  engineering, mechanical engineering, environmental science, geology,  and others.  It is particularly critical in  the rational design of drugs, catalysts, organic  electronics, nanostructured materials, and other designed materials.  Because of their steep computational costs, quantum  chemistry codes must exploit parallel computing and must constantly  adapt to rapidly changing high performance computing technologies.  This creates a significant barrier for the adoption of new technologies into quantum chemistry codes.  Our project involves the development of a parallel, highly reusable library for advanced numerical approximations in quantum chemistry.  This will be the first  unified library of such techniques, designed for high performance and  also reusability by independent research groups.   The PANACHE (PArallel  Numerical Approximations in CHemistry Engine) library will fill this need.   To maximize its impact, PANACHE is being designed to be used by multiple quantum chemistry software packages. PANACHE dramatically speeds up quantum computations, making it much easier to gain insight into a wide array of problems, from studies of reaction mechanisms in catalysis to the design of improved organic photoelectronic devices.  <br/><br/>Our highly interdisciplinary project (involving two theoretical chemists and one computational scientist as co-PI?s) provides excellent opportunities for training graduate students and postdocs in the areas of numerical methods, high-performance computing, quantum mechanics, and computational chemistry. Computer code resulting from this project will be released as freely-available open-source software, enabling its use with any other software package.  Workshops on the new software will be held to introduce these new tools to other software developers, and online training material and graduate course material will be developed to improve education in the use of numerical methods in computational science and quantum chemistry.<br/><br/>This award pertains to the Software Infrastructure for Sustained Innovation (SI2) solicitation."
"1535651","SI2-SSE: yt: Reusable Components for Simulating, Analyzing and Visualizing Astrophysical Systems","OAC","EXTRAGALACTIC ASTRON & COSMOLO, OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","11/01/2014","04/28/2017","Matthew Turk","IL","University of Illinois at Urbana-Champaign","Standard Grant","Stefan Robila","09/30/2018","$511,989.00","","mjturk@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","1217, 1253, 8004","1206, 7433, 8004, 8005, 8009","$0.00","Computational modeling of astrophysical phenomena has grown in sophistication and realism, leading to a diversity of complex simulation platforms, each utilizing its own mechanism and format for representing particles and fluids.  Similarly, most of the data analysis is conducted with tools developed in isolation and targeted to a specific simulation platform or research domain; very little systematic and direct technology transfer between astrophysical researchers exists.  The yt project is a parallel analysis and visualization toolkit designed to support a collaborative community of researchers as they focus on answering physical questions, rather than the technical mechanics of reading, processing and visualizing data formats.  This project will enable the development of advanced, physics-based modules that apply universally across simulation codes, advancing scientific inquiry and enabling more efficient utilization of computational and human resources.  In doing so, it will help advance a myriad of research goals from the study of black hole binaries to the growth of cosmic structure.  In addition, the project will serve as a touchstone for collaboration and cross-code utilization between many groups studying diverse phenomena. Moreover, the project will be developed through a community-oriented process, engaging a wide range of participants.<br/><br/>The infrastructure development in this research will enable these capabilities by broadening the applicable simulation platforms within yt, enabling cross-code utilization of microphysical solvers and physics modules and in situ analysis, and developing collaborative platforms for the exploration of astrophysical datasets.   In particular, it will develop the capabilities of yt in three primary mechanisms.  The first is to enable support for additional, fundamentally different simulation platforms such as smoothed particle hydrodynamics, unstructured mesh, and non-Cartesian coordinate systems.  The second is to provide simulation instrumentation components to ease the process of developing simulation codes, interfacing and exchanging technology between those simulation codes, and to enable deeper, on-the-fly integration of astrophysical simulation codes with yt and other analysis toolkits.  The final focus is on developing interface components to enable collaborative and interactive exploration of data utilizing web-based platforms.  An explicit goal of this SI2-SSE project is the development of collaborative relationships between scientists, furthering the development of the field as a whole.  By conducting all business in the open with a focus on developing and encouraging collaborative, welcoming environments for contributors and researchers, this SSE will help to foster a level playing field that is more accessible to all parties, particularly women and underrepresented minorities.  An explicit milestone of this project is to streamline the process of conducting direct outreach through scientific visualization, greatly expanding the domains and individuals engaged in STEM-based public outreach."
"1047764","SI2-SSE: Collaborative Research: Lagrangian Coherent Structures for Accurate Flow Structure Analysis","OAC","OFFICE OF MULTIDISCIPLINARY AC, DYNAMICAL SYSTEMS, COFFES, Software Institutes","09/15/2010","09/07/2010","John Hart","IL","University of Illinois at Urbana-Champaign","Standard Grant","Daniel Katz","08/31/2014","$251,643.00","","jch@cs.uiuc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","1253, 7478, 7552, 8004","1253, 1630, 7478, 7552","$0.00","The Lagrangian Coherent Structures (LCS) software elements developed by this project will provide a valuable tool set for fluid mechanics research to extract new discoveries from the vast and growing body of computational and experimental fluid mechanics data. The computation of LCS enables a systematic approach to accurately characterize transport phenomena in complex systems that pose insurmountable challenges to traditional Eulerian approaches. Prior, ah hoc implementations of LCS have already helped in important, real-world challenges including, tracking pollutants in the ocean, developing novel diagnoses and therapies for cardiovascular disease, and helping airplanes to avoid turbulence. We will produce an open LCS software system to provide a modular, extensible and flexible infrastructure to broaden the community of scientists and engineers that benefit from LCS, in problems ranging from fluid dynamics to general dynamical systems. Prof. Shadden will lead the LCS algorithm design and numerical analysis, and Prof. Hart will oversee the package's architectural design and the efficient parallel implementation of its elements."
"1265704","Collaborative Research: SI2 CHE:  Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces","CHE","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","05/15/2013","05/15/2013","David Case","NJ","Rutgers University New Brunswick","Standard Grant","Evelyn Goldfield","04/30/2016","$328,229.00","","case@biomaps.rutgers.edu","3 RUTGERS PLZA","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1253, 8004","5918, 5946, 5950, 7433, 8009","$0.00","An international team consisting of Teresa Head-Gordon and Martin Head-Gordon (University of California, Berkeley), Paul Nerenberg (Claremont McKenna College), David Case (Rutgers University), Jay Ponder (Washington University), Mark Tuckerman (New York University) with their UK collaborators: Lorna Smith and Neil Chue Hong (University of Edinburgh), Chris-Kriton Skylaris and Jonathan W. Essex (University of Southampton), Ilian Todorov (Daresbury Laboratory), Mario Antonioletti (EPCC) are are supported through the SI2-CHE program to develop and deploy robust and sustainable software for advanced potential energy surfaces.  The greater accuracy introduced by improvements in the new generation of potential energy surfaces opens up several challenges in their manifestation as algorithms and software on current or emergent hardware platforms that in turn limits their wide adoption by the computational chemistry community. The research team is overcoming these obstacles via multiple but integrated directions: (1) to optimally implement advanced potential energy surfaces across multi-core and GPU enabled systems, (2) to develop a hierarchy of advanced polarizable models that alter the tradeoff between accuracy and computational speed,(3) to create new multiple time stepping methods; (4) to write a Quantum Mechanics/Molecular Mechanics (QM/MM ) application programing interface (API) that fully supports mutual polarization, (5) to adopt software best practices to ensure growth of a self-sustaining community and (6) to provide exemplar calculations with the new software in the several emerging application areas.<br/><br/>Molecular simulation and quantum chemistry software is an integral part of chemistry and chemical biology, and has been broadly adopted by academic researchers and industry scientists. Next generation scientific breakthroughs that utilize chemical software will be enabled by the deployment of state of the art theoretical models and algorithms that are translated into a sustainable software framework rapidly implemented on emergent high performance computing platforms. Potential energy surfaces describe the interactions between atoms. Advanced and highly accurate potential energy surfaces encounter software-related obstacles that inhibit their application to grand challenge chemistry problems. This UK and US consortium, representing a broad cross section of the computational chemistry software community, is working to directly tackle these obstacles.   This US and UK collaboration between universities and High Performance Computing centers works to endure that chemical software investments made in advanced potential energy surface models has a long term payoff in community sustainability and the training of the next generation of scientists. Outreach and training workshops are organized around the emergence of the advanced potential energy software including an introductory molecular simulation software boot camp for undergraduate students.<br/><br/>The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC."
"1047772","Collaborative Research: SI2-SSI: Developments in High Performance Electronic Structure Theory","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, PROJECTS, Chem Thry, Mdls & Cmptnl Mthds, International Research Collab, CCSS-Comms Circuits & Sens Sys, Software Institutes","10/01/2010","07/15/2013","Mark Gordon","IA","Iowa State University","Continuing Grant","Evelyn Goldfield","09/30/2015","$1,816,394.00","Theresa Windus, Masha Sosonkina","mark@si.msg.chem.iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","1253, 1712, 1978, 6881, 7298, 7564, 8004","1253, 1303, 1712, 1765, 1978, 5912, 5978, 6881, 7433, 7564, 7569, 7573, 7644, 8004, 8009, 9150, 9215, 9216, 9263","$0.00","This project focuses on implementing newly developed numerical methods and software engineering approaches into widely used computer codes used in biology, chemistry, materials science and engineering, physics, chemical and mechanical engineering and many other science and engineering fields. GAMESS (General Atomic and Molecular Electronic Structure System) is the most broadly used highly scalable computational chemistry program with more than 100,000 users worldwide. NWChem is likewise a very popular computational chemistry code, as are MPQC (Massively Parallel Quantum Chemistry) and AIMS (Ab Initio Multiple Spawning).  All of these codes are distributed at no cost via the Web.  While these codes have all been developed by computational chemists, they impact a multitude of disciplines in science and engineering. <br/><br/>The new software is highly scalable, thereby enabling the study of fundamentally critical problems, including the structure of liquids (water among them), the formation of atmosrpheric aerosols, heterogeneous catalysis, and photochemistry and photobiology. In addition, the new software is implemented so as to take advantage of new hardware such as the graphical processing units (GPU). Three of the principal investigators (Gordon, Martinez, Windus) are among the world leaders in such developments.<br/><br/>All of the new developments are to be incorporated into both graduate and undergraduate courses in both chemistry and computer science. In addition, downloadable modules are placed on an accessible web site.  The results of the research are to be reported at national meetings, such as those organized by the American Chemical Society, IEEE, and the Materials Research Society. Videos of these presentation are placed on the Web, so that researchers and educators who are unable to attend the meetings can access the results of this project. The Web site contains also a set of lessons learned to aid subsequent researchers.<br/><br/>This award pertains to the Software Infrastructure for Sustained Innovation (SI2) solicitation. Its funding sources include the Division of Chemistry (MPS/CHE), the Office of Multidisciplinary Activities (OMA) of the Directorate of Mathematical and Physical Sciences (MPS), the Office of Cyberinfrastructure (OCI), the Division of Civil, Mechanical and Manufacturing Innovation (ENG/CMMI), and the Division of Materials Research (MPS/DMR)."
"1148213","Collaborative Research:  SI2-SSI:  Empowering the Scientific Community with Streaming Data Middleware:  Software Integration into Complex Science Environments","OAC","ADVANCES IN BIO INFORMATICS, ECOSYSTEM STUDIES, Software Institutes, Cybersecurity Innovation","08/01/2012","08/07/2012","Duane Edgington","CA","Monterey Bay Aquarium Research Institute","Standard Grant","Rajiv Ramnath","07/31/2016","$101,710.00","","duane@mbari.org","7700 SANDHOLDT RD","MOSS LANDING","CA","950399644","8317751803","CSE","1165, 1181, 8004, 8027","1165, 1181, 7434, 8004, 8009, 8027","$0.00","This project will catalyze a new generation of sensor-based streaming data applications by building on the foundation of the Open Source DataTurbine (OSDT) middleware (www.dataturbine.org). This will involve strategic software development, systems integration and testing, science experiments, and education and outreach. The software developments will include (A) tools for configuring, testing, and controlling distributed sensor systems, (B) software interfaces that are compliant with the Open Geospatial Consortium Sensor Web Enablement standards, (C) a new OSDT-Matlab interface, and (D) OSDT cybersecurity enhancements. These software products will be integrated into the production research infrastructures at Purdue, the University of Connecticut, the North Temperate Lake LTER Network site, and the Monterey Bay Aquarium Research Institute. The software will be professionally developed and managed as community resources. The intellectual merit of this project will be advances in software engineering, interoperability standards, and systems management for complex real-time sensor systems as well as advances on important open questions in civil engineering, marine science, and ecology. This project will yield broader impacts by enabling new science applications, enhancing productivity, and accelerating innovation across NSF directorates. To ensure that these broader impacts are realized, the team will provide student research opportunities, develop curriculum materials and training courses, and cultivate a community of software users."
"1339624","SI2-SSE: yt: Reusable Components for Simulating, Analyzing and Visualizing Astrophysical Systems","OAC","EXTRAGALACTIC ASTRON & COSMOLO, OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","10/01/2013","10/02/2014","Matthew Turk","NY","Columbia University","Standard Grant","Rajiv Ramnath","04/30/2015","$493,793.00","Greg Bryan, Matthew Turk","mjturk@illinois.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","1217, 1253, 8004","1206, 7433, 8005","$0.00","Computational modeling of astrophysical phenomena has grown in sophistication and realism, leading to a diversity of complex simulation platforms, each utilizing its own mechanism and format for representing particles and fluids.  Similarly, most of the data analysis is conducted with tools developed in isolation and targeted to a specific simulation platform or research domain; very little systematic and direct technology transfer between astrophysical researchers exists.  The yt project is a parallel analysis and visualization toolkit designed to support a collaborative community of researchers as they focus on answering physical questions, rather than the technical mechanics of reading, processing and visualizing data formats.  This project will enable the development of advanced, physics-based modules that apply universally across simulation codes, advancing scientific inquiry and enabling more efficient utilization of computational and human resources.  In doing so, it will help advance a myriad of research goals from the study of black hole binaries to the growth of cosmic structure.  In addition, the project will serve as a touchstone for collaboration and cross-code utilization between many groups studying diverse phenomena. Moreover, the project will be developed through a community-oriented process, engaging a wide range of participants.<br/><br/>The infrastructure development in this research will enable these capabilities by broadening the applicable simulation platforms within yt, enabling cross-code utilization of microphysical solvers and physics modules and in situ analysis, and developing collaborative platforms for the exploration of astrophysical datasets.   In particular, it will develop the capabilities of yt in three primary mechanisms.  The first is to enable support for additional, fundamentally different simulation platforms such as smoothed particle hydrodynamics, unstructured mesh, and non-Cartesian coordinate systems.  The second is to provide simulation instrumentation components to ease the process of developing simulation codes, interfacing and exchanging technology between those simulation codes, and to enable deeper, on-the-fly integration of astrophysical simulation codes with yt and other analysis toolkits.  The final focus is on developing interface components to enable collaborative and interactive exploration of data utilizing web-based platforms.  An explicit goal of this SI2-SSE project is the development of collaborative relationships between scientists, furthering the development of the field as a whole.  By conducting all business in the open with a focus on developing and encouraging collaborative, welcoming environments for contributors and researchers, this SSE will help to foster a level playing field that is more accessible to all parties, particularly women and underrepresented minorities.  An explicit milestone of this project is to streamline the process of conducting direct outreach through scientific visualization, greatly expanding the domains and individuals engaged in STEM-based public outreach."
"1265817","SI2-CHE:  CCP-SAS - Collaborative Computing consortium for advanced analyses of structural data in chemical biology and soft condensed matter","CHE","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","06/01/2013","05/31/2013","Emre Brookes","TX","University of Texas Health Science Center San Antonio","Standard Grant","Evelyn Goldfield","05/31/2018","$263,926.00","","emre.brookes@umontana.edu","7703 FLOYD CURL DR","San Antonio","TX","782293901","2105672340","MPS","1253, 8004","5918, 5946, 5950, 7433, 8009","$0.00","An international team consisting of Paul Butler (University of Tennessee, Knoxville and NIST), Jianhan Chen (Kansas State University), Emre Brooks (University of Texas Health Science Center, San Antonio) and their UK collaborators led by Stephen J. Perkins (University College, London) are supported through the SI2-CHE program.  The object of this project is to provide a web-based GUI front-end with a high-performance back-end to increase the accessibility of advanced atomistic modeling of scattering data by novice users.   Advanced analysis modules and new simulation methods including implicit solvent models are being developed to increase the accuracy of scattering calculations and simulation protocols.  Software is being developed and disseminated with continual feedback from the research teams' large international user-base.  These efforts leverage an international user and developer community that use high-performance computing resources on a wide-range of cutting-edge chemical problems.     <br/><br/>A typical bench scientist purifies and characterizes samples, collects the small angle x-ray scattering (SAXS), small angle neutron scattering (SANS),  or analytical ultracentrifugation (AUC) data, and interprets the results using simplistic models.   It is rare that the same individual also has the skills to use advanced atomistic simulation software. The CCP-SAS project is focused on developing an easy-to-use modeling package that enables users to generate physically accurate atomistic models, calculate scattering profiles and compare results to experimental scattering data sets in a single web-based software suite.  This enables a broad range of scattering scientists to access often complicated simulation and scattering analysis methods seamlessly thus providing a significant acceleration to the discovery process. <br/><br/>The US based investigators are supported by the CHE and ACI divisions within NSF; the  UK based investigators are supported by the EPSRC."
"1265850","SI2-CHE:  CCP-SAS - Collaborative Computing consortium for advanced analyses of structural data in chemical biology and soft condensed matter","CHE","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","06/01/2013","05/31/2013","Jianhan Chen","KS","Kansas State University","Standard Grant","Evelyn Goldfield","05/31/2017","$224,492.00","","jianhanc@umass.edu","2 FAIRCHILD HALL","Manhattan","KS","665061100","7855326804","MPS","1253, 8004","5918, 5946, 5950, 7433, 8009, 9150","$0.00","An international team consisting of Paul Butler (University of Tennessee, Knoxville and NIST), Jianhan Chen (Kansas State University), Emre Brooks (University of Texas Health Science Center, San Antonio) and their UK collaborators led by Stephen J. Perkins (University College, London) are supported through the SI2-CHE program.  The object of this project is to provide a web-based GUI front-end with a high-performance back-end to increase the accessibility of advanced atomistic modeling of scattering data by novice users.   Advanced analysis modules and new simulation methods including implicit solvent models are being developed to increase the accuracy of scattering calculations and simulation protocols.  Software is being developed and disseminated with continual feedback from the research teams' large international user-base.  These efforts leverage an international user and developer community that use high-performance computing resources on a wide-range of cutting-edge chemical problems.     <br/><br/>A typical bench scientist purifies and characterizes samples, collects the small angle x-ray scattering (SAXS), small angle neutron scattering (SANS),  or analytical ultracentrifugation (AUC) data, and interprets the results using simplistic models.   It is rare that the same individual also has the skills to use advanced atomistic simulation software. The CCP-SAS project is focused on developing an easy-to-use modeling package that enables users to generate physically accurate atomistic models, calculate scattering profiles and compare results to experimental scattering data sets in a single web-based software suite.  This enables a broad range of scattering scientists to access often complicated simulation and scattering analysis methods seamlessly thus providing a significant acceleration to the discovery process. <br/><br/>The US based investigators are supported by the CHE and ACI divisions within NSF; the  UK based investigators are supported by the EPSRC."
"1265712","Collaborative Research: SI2-CHE:  Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces","CHE","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","05/15/2013","05/15/2013","Jay Ponder","MO","Washington University","Standard Grant","Evelyn Goldfield","04/30/2017","$257,400.00","","ponder@dasher.wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","MPS","1253, 8004","5918, 5946, 5950, 7433, 8009","$0.00","An international team consisting of Teresa Head-Gordon and Martin Head-Gordon (University of California, Berkeley), Paul Nerenberg (Claremont McKenna College), David Case (Rutgers University), Jay Ponder (Washington University), Mark Tuckerman (New York University) with their UK collaborators: Lorna Smith and Neil Chue Hong (University of Edinburgh), Chris-Kriton Skylaris and Jonathan W. Essex (University of Southampton), Ilian Todorov (Daresbury Laboratory), Mario Antonioletti (EPCC) are are supported through the SI2-CHE program to develop and deploy robust and sustainable software for advanced potential energy surfaces.  The greater accuracy introduced by improvements in the new generation of potential energy surfaces opens up several challenges in their manifestation as algorithms and software on current or emergent hardware platforms that in turn limits their wide adoption by the computational chemistry community. The research team is overcoming these obstacles via multiple but integrated directions: (1) to optimally implement advanced potential energy surfaces across multi-core and GPU enabled systems, (2) to develop a hierarchy of advanced polarizable models that alter the tradeoff between accuracy and computational speed,(3) to create new multiple time stepping methods; (4) to write a Quantum Mechanics/Molecular Mechanics (QM/MM ) application programing interface (API) that fully supports mutual polarization, (5) to adopt software best practices to ensure growth of a self-sustaining community and (6) to provide exemplar calculations with the new software in the several emerging application areas.<br/><br/>Molecular simulation and quantum chemistry software is an integral part of chemistry and chemical biology, and has been broadly adopted by academic researchers and industry scientists. Next generation scientific breakthroughs that utilize chemical software will be enabled by the deployment of state of the art theoretical models and algorithms that are translated into a sustainable software framework rapidly implemented on emergent high performance computing platforms. Potential energy surfaces describe the interactions between atoms. Advanced and highly accurate potential energy surfaces encounter software-related obstacles that inhibit their application to grand challenge chemistry problems. This UK and US consortium, representing a broad cross section of the computational chemistry software community, is working to directly tackle these obstacles.   This US and UK collaboration between universities and High Performance Computing centers works to endure that chemical software investments made in advanced potential energy surface models has a long term payoff in community sustainability and the training of the next generation of scientists. Outreach and training workshops are organized around the emergence of the advanced potential energy software including an introductory molecular simulation software boot camp for undergraduate students.<br/><br/>The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC."
"1265821","SI2-CHE:  CCP-SAS - Collaborative Computing consortium for advanced analyses of structural data in chemical biology and soft condensed matter","CHE","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","06/01/2013","05/31/2013","Paul Butler","TN","University of Tennessee Knoxville","Standard Grant","Evelyn Goldfield","05/31/2018","$611,582.00","","pbutler@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","MPS","1253, 8004","5918, 5946, 5950, 7433, 8009, 9150","$0.00","An international team consisting of Paul Butler (University of Tennessee, Knoxville and NIST), Jianhan Chen (Kansas State University), Emre Brooks (University of Texas Health Science Center, San Antonio) and their UK collaborators led by Stephen J. Perkins (University College, London) are supported through the SI2-CHE program.  The object of this project is to provide a web-based GUI front-end with a high-performance back-end to increase the accessibility of advanced atomistic modeling of scattering data by novice users.   Advanced analysis modules and new simulation methods including implicit solvent models are being developed to increase the accuracy of scattering calculations and simulation protocols.  Software is being developed and disseminated with continual feedback from the research teams' large international user-base.  These efforts leverage an international user and developer community that use high-performance computing resources on a wide-range of cutting-edge chemical problems.     <br/><br/>A typical bench scientist purifies and characterizes samples, collects the small angle x-ray scattering (SAXS), small angle neutron scattering (SANS),  or analytical ultracentrifugation (AUC) data, and interprets the results using simplistic models.   It is rare that the same individual also has the skills to use advanced atomistic simulation software. The CCP-SAS project is focused on developing an easy-to-use modeling package that enables users to generate physically accurate atomistic models, calculate scattering profiles and compare results to experimental scattering data sets in a single web-based software suite.  This enables a broad range of scattering scientists to access often complicated simulation and scattering analysis methods seamlessly thus providing a significant acceleration to the discovery process. <br/><br/>The US based investigators are supported by the CHE and ACI divisions within NSF; the  UK based investigators are supported by the EPSRC."
"1265660","Collaborative: SI2-CHE: Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces","CHE","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","05/15/2013","05/15/2013","Paul Nerenberg","CA","Claremont McKenna College","Standard Grant","Evelyn Goldfield","10/31/2014","$56,983.00","","pnerenb@calstatela.edu","500 E. Ninth St.","Claremont","CA","917115929","9096077085","MPS","1253, 8004","5918, 5946, 5950, 7433, 8009","$0.00","An international team consisting of Teresa Head-Gordon and Martin Head-Gordon (University of California, Berkeley), Paul Nerenberg (Claremont McKenna College), David Case (Rutgers University), Jay Ponder (Washington University), Mark Tuckerman (New York University) with their UK collaborators: Lorna Smith and Neil Chue Hong (University of Edinburgh), Chris-Kriton Skylaris and Jonathan W. Essex (University of Southampton), Ilian Todorov (Daresbury Laboratory), Mario Antonioletti (EPCC) are are supported through the SI2-CHE program to develop and deploy robust and sustainable software for advanced potential energy surfaces.  The greater accuracy introduced by improvements in the new generation of potential energy surfaces opens up several challenges in their manifestation as algorithms and software on current or emergent hardware platforms that in turn limits their wide adoption by the computational chemistry community. The research team is overcoming these obstacles via multiple but integrated directions: (1) to optimally implement advanced potential energy surfaces across multi-core and GPU enabled systems, (2) to develop a hierarchy of advanced polarizable models that alter the tradeoff between accuracy and computational speed,(3) to create new multiple time stepping methods; (4) to write a Quantum Mechanics/Molecular Mechanics (QM/MM ) application programing interface (API) that fully supports mutual polarization, (5) to adopt software best practices to ensure growth of a self-sustaining community and (6) to provide exemplar calculations with the new software in the several emerging application areas.<br/><br/>Molecular simulation and quantum chemistry software is an integral part of chemistry and chemical biology, and has been broadly adopted by academic researchers and industry scientists. Next generation scientific breakthroughs that utilize chemical software will be enabled by the deployment of state of the art theoretical models and algorithms that are translated into a sustainable software framework rapidly implemented on emergent high performance computing platforms. Potential energy surfaces describe the interactions between atoms. Advanced and highly accurate potential energy surfaces encounter software-related obstacles that inhibit their application to grand challenge chemistry problems. This UK and US consortium, representing a broad cross section of the computational chemistry software community, is working to directly tackle these obstacles.   This US and UK collaboration between universities and High Performance Computing centers works to endure that chemical software investments made in advanced potential energy surface models has a long term payoff in community sustainability and the training of the next generation of scientists. Outreach and training workshops are organized around the emergence of the advanced potential energy software including an introductory molecular simulation software boot camp for undergraduate students.<br/><br/>The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC."
"1148215","Collaborative Research: SI2-SSI: Empowering the Scientific Community with Streaming Data Middleware: Software Integration into Complex Science Environments","OAC","ADVANCES IN BIO INFORMATICS, ECOSYSTEM STUDIES, Software Institutes, Cybersecurity Innovation","08/01/2012","08/07/2012","Richard Christenson","CT","University of Connecticut","Standard Grant","Rajiv Ramnath","07/31/2016","$30,000.00","","rchriste@engr.uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","CSE","1165, 1181, 8004, 8027","1165, 1181, 7434, 8004, 8009, 8027","$0.00","This project will catalyze a new generation of sensor-based streaming data applications by building on the foundation of the Open Source DataTurbine (OSDT) middleware (www.dataturbine.org). This will involve strategic software development, systems integration and testing, science experiments, and education and outreach. The software developments will include (A) tools for configuring, testing, and controlling distributed sensor systems, (B) software interfaces that are compliant with the Open Geospatial Consortium Sensor Web Enablement standards, (C) a new OSDT-Matlab interface, and (D) OSDT cybersecurity enhancements. These software products will be integrated into the production research infrastructures at Purdue, the University of Connecticut, the North Temperate Lake LTER Network site, and the Monterey Bay Aquarium Research Institute. The software will be professionally developed and managed as community resources. The intellectual merit of this project will be advances in software engineering, interoperability standards, and systems management for complex real-time sensor systems as well as advances on important open questions in civil engineering, marine science, and ecology. This project will yield broader impacts by enabling new science applications, enhancing productivity, and accelerating innovation across NSF directorates. To ensure that these broader impacts are realized, the team will provide student research opportunities, develop curriculum materials and training courses, and cultivate a community of software users."
"2039142","Collaborative Research: SI2-SSI: Expanding Volunteer Computing","OAC","Software Institutes","06/17/2020","06/26/2020","Ritu Ritu","TX","University of Texas at San Antonio","Standard Grant","Seung-Jong Park","04/30/2021","$203,817.00","","ritu@wayne.edu","1 UTSA CIRCLE","SAN ANTONIO","TX","782491644","2104584340","CSE","8004","077Z, 7433, 8004, 8009","$0.00","Volunteer computing (VC) uses donated computing time consumer devices such as home computers and smartphones to do scientific computing. It has been shown that VC can provide greater computing power, at lower cost, than conventional approaches such as organizational computing centers and commercial clouds. BOINC is the most common software framework for VC. Essentially, donors of computing time simply have to load BOINC on their computer or smartphone, and then register to donate at the BOINC web site. VC provides ""high throughput computing"": handling lots of independent jobs, with performance goals based on the rate of job completion rather than completion time for individual jobs. This type of computing (all known as high-throughput computing) is in great demand in most areas of science. Until now, the adoption of VC has been limited by its structure. For example, VC projects (such as Einstein@home and Rosetta@home) are operated by individual research groups, and volunteers must browse and choose from among many such projects. As a result, there are relatively few VC projects, and volunteers are mostly tech-savvy computer enthusiasts.  This project aims to solve these problems using two complementary development efforts: First, it will add BOINC-based VC conduits to two major high-performance computing providers: (a) the Texas Advanced Computing Center, a supercomputer center, and (b) nanoHUB, a web portal for nano science that provides computing capabilities.Also, a unified control interface to VC will be developed, tentatively called Science United, where donors can register.  The project  will benefit thousands of scientists who use these facilities, and it will create technology that makes it easy for other HPC providers to add their own VC back ends. Also, Science United will provide a simpler interface to BOINC volunteers where they will register to support scientific areas, rather than specific projects. Science United will also serve as an allocator of computing power among projects. Thus, new projects will no longer have to do their own marketing and publicity to recruit volunteers. Finally, the creation of a single VC ""brand"" (i.e Science United) will allow coherent marketing of VC to the public. By creating a huge pool of low-cost computing power that will benefit thousands of scientists, and increasing public awareness of and interest in science, the project plans to establish VC as a central and long-term part of the U.S. scientific cyber infrastructure.<br/><br/>Adding VC to an existing HPC facility involves several technical issues, which will be addressed as follows: (1) Packaging science applications (which typically run on Linux cluster nodes) to run on home computers (mostly Windows, some Mac and Linux): the team is developing an approach using VirtualBox and Docker, in which the application and its environment (Linux distribution, libraries, executables) are represented as a set of layers comprising a Docker image, which is then run as a container within a Linux virtual machine on the volunteer device. This has numerous advantages: it reduces the work of packaging applications to near zero; it minimizes network traffic because a given Docker layer is downloaded to a host only once; and it provides a strong security sandbox so that volunteer computers are protected from buggy or malicious applications, (2) File management: Input and output files must be moved between existing private servers and public-facing servers that are accessible to the outside Internet. A file management system will be developed, based on Web RPCs, for this purpose. This system will use content-based naming so that a given file is transferred and stored only once. It also maintains job/file associations so that files can be automatically deleted from the public server when they are no longer needed. (3) Submitting and monitoring jobs:  BOINC provides a web interface for efficiently submitting and monitoring large batches of jobs. These were originally developed as part of a system to migrate HTCondor jobs to BOINC. This project is extending it to support the additional requirements of TACC and nanoHUB. Note that these new capabilities are not specific to TACC or nanoHUB: they provide the glue needed to easily add BOINC-based VC to any existing HTC facility. The team is also developing RPC bindings in several languages (Python, C++, PHP). The other component of the project, Science United, is a database-driven web site and an associated web service for the BOINC clients. Science United will control volunteer hosts (i.e. tell them which projects to work for) using BOINC's ""Account Manager"" mechanism, in which the BOINC client on each host periodically contacts Science United and is told what projects to run. Project servers, not Science United, will distribute jobs and files. Science United will define a set of ""keywords"" for science areas (physics, biomedicine, environment, etc.) and for location (country, institution). Projects will be labelled with appropriate keywords. Volunteers will have a yes/no/maybe interface for specifying the types of jobs they want to run. Science United will thus provide a mechanism in which a fraction of total computing capacity can be allocated to a project for a given period. Because total capacity changes slowly over time, this allows near-certain guaranteed allocations. Science United will embody a scheduling system that attempts to enforce allocations, honor volunteer preferences, and maximize throughput. Finally, Science United will do detailed accounting of computing. Volunteer hosts will tell Science United how much work (measured by CPU time and FLOPs, GPU time and FLOPs, and number of jobs) they have done for each project. Science United will maintain historical records of this data for volunteers and projects, and current totals with finer granularity (e.g. for each host/project combination). Finally, Science United will provide web interfaces letting volunteers see their contribution status and history, and letting administrators add projects, control allocations, and view accounting data."
"1265731","Collaborative Research: SI2-CHE: Development and Deployment of Chemical  Software for Advanced Potential Energy Surfaces","CHE","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","05/15/2013","05/15/2013","Teresa Head-Gordon","CA","University of California-Berkeley","Standard Grant","Evelyn Goldfield","04/30/2016","$531,527.00","Martin Head-Gordon","thg@berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1253, 8004","5918, 5946, 5950, 7433, 8009","$0.00","An international team consisting of Teresa Head-Gordon and Martin Head-Gordon (University of California, Berkeley), Paul Nerenberg (Claremont McKenna College), David Case (Rutgers University), Jay Ponder (Washington University), Mark Tuckerman (New York University) with their UK collaborators: Lorna Smith and Neil Chue Hong (University of Edinburgh), Chris-Kriton Skylaris and Jonathan W. Essex (University of Southampton), Ilian Todorov (Daresbury Laboratory), Mario Antonioletti (EPCC) are are supported through the SI2-CHE program to develop and deploy robust and sustainable software for advanced potential energy surfaces.  The greater accuracy introduced by improvements in the new generation of potential energy surfaces opens up several challenges in their manifestation as algorithms and software on current or emergent hardware platforms that in turn limits their wide adoption by the computational chemistry community. The research team is overcoming these obstacles via multiple but integrated directions: (1) to optimally implement advanced potential energy surfaces across multi-core and GPU enabled systems, (2) to develop a hierarchy of advanced polarizable models that alter the tradeoff between accuracy and computational speed,(3) to create new multiple time stepping methods; (4) to write a Quantum Mechanics/Molecular Mechanics (QM/MM ) application programing interface (API) that fully supports mutual polarization, (5) to adopt software best practices to ensure growth of a self-sustaining community and (6) to provide exemplar calculations with the new software in several emerging application areas.<br/><br/>Molecular simulation and quantum chemistry software is an integral part of chemistry and chemical biology, and has been broadly adopted by academic researchers and industry scientists. Next generation scientific breakthroughs that utilize chemical software will be enabled by the deployment of state of the art theoretical models and algorithms that are translated into a sustainable software framework rapidly implemented on emergent high performance computing platforms. Potential energy surfaces describe the interactions between atoms. Advanced and highly accurate potential energy surfaces encounter software-related obstacles that inhibit their application to grand challenge chemistry problems. This UK and US consortium, representing a broad cross section of the computational chemistry software community, is working to directly tackle these obstacles.   This US and UK collaboration between universities and High Performance Computing centers works to endure that chemical software investments made in advanced potential energy surface models has a long term payoff in community sustainability and the training of the next generation of scientists. Outreach and training workshops are organized around the emergence of the advanced potential energy software including an introductory molecular simulation software boot camp for undergraduate students.<br/><br/>The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC."
"1453123","Collaborative: SI2-CHE: Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces","CHE","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","07/01/2014","09/03/2014","Paul Nerenberg","CA","California Institute of Technology","Standard Grant","Evelyn Goldfield","04/30/2016","$38,417.00","","pnerenb@calstatela.edu","1200 E CALIFORNIA BLVD","PASADENA","CA","91125","6263956219","MPS","1253, 8004","5918, 5946, 5950, 7433, 8009","$0.00","An international team consisting of Teresa Head-Gordon and Martin Head-Gordon (University of California, Berkeley), Paul Nerenberg (Claremont McKenna College), David Case (Rutgers University), Jay Ponder (Washington University), Mark Tuckerman (New York University) with their UK collaborators: Lorna Smith and Neil Chue Hong (University of Edinburgh), Chris-Kriton Skylaris and Jonathan W. Essex (University of Southampton), Ilian Todorov (Daresbury Laboratory), Mario Antonioletti (EPCC) are are supported through the SI2-CHE program to develop and deploy robust and sustainable software for advanced potential energy surfaces.  The greater accuracy introduced by improvements in the new generation of potential energy surfaces opens up several challenges in their manifestation as algorithms and software on current or emergent hardware platforms that in turn limits their wide adoption by the computational chemistry community. The research team is overcoming these obstacles via multiple but integrated directions: (1) to optimally implement advanced potential energy surfaces across multi-core and GPU enabled systems, (2) to develop a hierarchy of advanced polarizable models that alter the tradeoff between accuracy and computational speed,(3) to create new multiple time stepping methods; (4) to write a Quantum Mechanics/Molecular Mechanics (QM/MM ) application programing interface (API) that fully supports mutual polarization, (5) to adopt software best practices to ensure growth of a self-sustaining community and (6) to provide exemplar calculations with the new software in the several emerging application areas.<br/><br/>Molecular simulation and quantum chemistry software is an integral part of chemistry and chemical biology, and has been broadly adopted by academic researchers and industry scientists. Next generation scientific breakthroughs that utilize chemical software will be enabled by the deployment of state of the art theoretical models and algorithms that are translated into a sustainable software framework rapidly implemented on emergent high performance computing platforms. Potential energy surfaces describe the interactions between atoms. Advanced and highly accurate potential energy surfaces encounter software-related obstacles that inhibit their application to grand challenge chemistry problems. This UK and US consortium, representing a broad cross section of the computational chemistry software community, is working to directly tackle these obstacles.   This US and UK collaboration between universities and High Performance Computing centers works to endure that chemical software investments made in advanced potential energy surface models has a long term payoff in community sustainability and the training of the next generation of scientists. Outreach and training workshops are organized around the emergence of the advanced potential energy software including an introductory molecular simulation software boot camp for undergraduate students.<br/><br/>The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC."
"1047696","Collaborative Research: SI2-SSI: Developments in High Performance Electronic Structure Theory","OAC","OFFICE OF MULTIDISCIPLINARY AC, PROJECTS, Chem Thry, Mdls & Cmptnl Mthds, Software Institutes","10/01/2010","08/07/2013","Edward Valeev","VA","Virginia Polytechnic Institute and State University","Continuing Grant","Evelyn Goldfield","09/30/2015","$396,000.00","","evaleev@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","CSE","1253, 1978, 6881, 8004","1253, 1303, 1712, 1765, 1978, 6881, 7564, 7569, 7573, 7644, 8004, 9216, 9263","$0.00","This project focuses on implementing newly developed numerical methods and software engineering approaches into widely used computer codes used in biology, chemistry, materials science and engineering, physics, chemical and mechanical engineering and many other science and engineering fields. GAMESS (General Atomic and Molecular Electronic Structure System) is the most broadly used highly scalable computational chemistry program with more than 100,000 users worldwide. NWChem is likewise a very popular computational chemistry code, as are MPQC (Massively Parallel Quantum Chemistry) and AIMS (Ab Initio Multiple Spawning).  All of these codes are distributed at no cost via the Web.  While these codes have all been developed by computational chemists, they impact a multitude of disciplines in science and engineering. <br/><br/>The new software is highly scalable, thereby enabling the study of fundamentally critical problems, including the structure of liquids (water among them), the formation of atmosrpheric aerosols, heterogeneous catalysis, and photochemistry and photobiology. In addition, the new software is implemented so as to take advantage of new hardware such as the graphical processing units (GPU). Three of the principal investigators (Gordon, Martinez, Windus) are among the world leaders in such developments.<br/><br/>All of the new developments are to be incorporated into both graduate and undergraduate courses in both chemistry and computer science. In addition, downloadable modules are placed on an accessible web site.  The results of the research are to be reported at national meetings, such as those organized by the American Chemical Society, IEEE, and the Materials Research Society. Videos of these presentation are placed on the Web, so that researchers and educators who are unable to attend the meetings can access the results of this project. The Web site contains also a set of lessons learned to aid subsequent researchers.<br/><br/>This award pertains to the Software Infrastructure for Sustained Innovation (SI2) solicitation. Its funding sources include the Division of Chemistry (MPS/CHE), the Office of Multidisciplinary Activities (OMA) of the Directorate of Mathematical and Physical Sciences (MPS), the Office of Cyberinfrastructure (OCI), the Division of Civil, Mechanical and Manufacturing Innovation (ENG/CMMI), and the Division of Materials Research (MPS/DMR)."
"1147794","Collaborative Research: SI2-SSI: Sustainable Development of  Next-Generation Software in Quantum Chemistry","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, CHEMISTRY PROJECTS, Software Institutes","06/01/2012","07/06/2012","Thomas Crawford","VA","Virginia Polytechnic Institute and State University","Standard Grant","Daniel Katz","05/31/2015","$325,140.00","","crawdad@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","CSE","1253, 1712, 1991, 8004","1253, 1712, 1991, 7433, 7683, 8004, 8009, 9216, 9263","$0.00","Quantum chemistry can provide highly accurate results for arbitrary  molecular systems, making it a vital component in many different  disciplines such as materials science, biology, physics, chemical  engineering, mechanical engineering, environmental science, geology,  and others.  It is particularly critical in  the rational design of drugs, catalysts, organic  electronics, nanostructured materials, and other designed materials.  Because of their steep computational costs, quantum  chemistry codes must exploit parallel computing and must constantly  adapt to rapidly changing high performance computing technologies.  This creates a significant barrier for the adoption of new technologies into quantum chemistry codes.  Our project involves the development of a parallel, highly reusable library for advanced numerical approximations in quantum chemistry.  This will be the first  unified library of such techniques, designed for high performance and  also reusability by independent research groups.   The PANACHE (PArallel  Numerical Approximations in CHemistry Engine) library will fill this need.   To maximize its impact, PANACHE is being designed to be used by multiple quantum chemistry software packages. PANACHE dramatically speeds up quantum computations, making it much easier to gain insight into a wide array of problems, from studies of reaction mechanisms in catalysis to the design of improved organic photoelectronic devices.  <br/><br/>Our highly interdisciplinary project (involving two theoretical chemists and one computational scientist as co-PI?s) provides excellent opportunities for training graduate students and postdocs in the areas of numerical methods, high-performance computing, quantum mechanics, and computational chemistry. Computer code resulting from this project will be released as freely-available open-source software, enabling its use with any other software package.  Workshops on the new software will be held to introduce these new tools to other software developers, and online training material and graduate course material will be developed to improve education in the use of numerical methods in computational science and quantum chemistry.<br/><br/>This award pertains to the Software Infrastructure for Sustained Innovation (SI2) solicitation."
"1216696","Collaborative Research: Software Infrastructure for Accelerating Grand Challenge Science with Future Computing Platforms","OAC","Software Institutes","10/01/2012","09/13/2012","Manish Parashar","NJ","Rutgers University New Brunswick","Standard Grant","Rudolf Eigenmann","09/30/2014","$50,000.00","Shantenu Jha","manish.parashar@utah.edu","3 RUTGERS PLZA","NEW BRUNSWICK","NJ","089018559","8489320150","CSE","8004","7433, 8211","$0.00","Solving scientific grand challenges requires effective use of cyber infrastructure. Future computing platforms, including Field Programmable Gate Arrays (FPGAs), General Purpose Graphics Processing Units (GPGPUs), multi-core and multi-threaded processors, and Cloud computing platforms, can dramatically accelerate innovation to solve complex problems of societal importance when supported by a critical mass of sustainable software.<br/><br/>This project will organize scientific communities to help leverage the disruptive potential of future computing platforms through sustainable software. Grand challenge problems in biological science, social science, and security domains will be targeted based on their under-served needs and demonstrated possibilities. Users will be engaged through interdisciplinary workshops that bring together domain experts with software technologists with the goals of identifying core opportunity areas, determining critical software infrastructure, and discovering software sustainability challenges. The outcome will be an in-depth conceptual design for a Center for Sustainable Software on Future Computing Platforms, as part of the Software Infrastructure for Sustained Innovation (SI2) program. The design, scoped toward grand challenge problems, will identify common and specialized software infrastructure, research, development and outreach priorities, and coordination with the SSE and SSI components of the SI2 program. The interactions will offer a comprehensive understanding of grand challenges that best map to future computing platforms and the software infrastructure to best support scientists' needs. The workshops will enhance understanding of future platforms' potential for transformative research and lead to key insights into cross-cutting problems in leveraging their potential. Published results will help guide future research and reduce barriers to entry for under-represented groups."
"1740309","NSCI SI2-SSE: Multiscale Software for Quantum Simulations of Nanostructured Materials and Devices","OAC","DMR SHORT TERM SUPPORT, CI REUSE, Software Institutes, DMREF","09/01/2017","08/29/2017","Jerzy Bernholc","NC","North Carolina State University","Standard Grant","Rob Beverly","08/31/2021","$500,000.00","Carl Kelley, Wenchang Lu, Emil Briggs","bernholc@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","CSE","1712, 6892, 8004, 8292","026Z, 6863, 6893, 7237, 7433, 8004, 8005, 9216","$0.00","Computational science is firmly established as a pillar of scientific discovery and technology, promising unprecedented new capabilities. The National Strategic Computing Initiative (NSCI) establishes an ambitious roadmap to advance Science and Technology (S&T) through support for sustained innovations in high performance computing and its use. Harnessing the power of millions of computer cores and/or compute accelerators (also called graphical processing units, GPUs) foreseen in future high performance computers requires a new generation of application software and algorithms, able to effectively utilize such resources and create the revolutionary S&T advances that underpin the nation's economic competitiveness. The proposed work will develop high-performance, scalable quantum simulation software for complex materials and devices, which will be portable and tunable across alternative exascale architectures. It will be able to address grand challenges in the design of quantum materials and devices, responding to one of the five strategic objectives of NSCI. Quantum materials and processes also underpin one of NSF's 10 Big Ideas for Future NSF Investments, The Quantum Leap: Leading the Next Quantum Revolution. Another important program in which materials simulation plays a key role is the Materials Genome Initiative. It seeks to ""deploy advanced materials at least twice as fast at a fraction of the cost"" and relies on computational materials design as the critical aspect, with computation guiding experiments. The goals of this project are to refactor and extend the open-source RMG software suite to future computer architectures at exascale, to enable transformational research on the design of quantum materials and devices from fundamental quantum-mechanical level. The RMG software will extend from desktops to the largest supercomputer systems, and will also perform well on a multitude of other systems, such as parallel computing clusters of various sizes, including those with GPUs. At the highest level of performance, it will enable predictive simulations at unprecedented scale, impact several areas of science and engineering and become a source of new discoveries and economic growth. RMG, already highly parallel and capable of multi-petaflops speeds, can provide a pathway towards reaching key NSCI goals. RMG has already been included in a benchmark suite which will be used to help select future supercomputers. At the same time, it's scalability means that it will be useful in classroom education running on students' laptops, to help individual researchers perform significant scientific or technological research on their accelerator- or GPU-equipped workstations, and, to run larger problems on a multitude of computer clusters with varying capabilities.<br/><br/>The goals of this project are to refactor and extend the open-source RMG software suite to exascale architectures, to enable transformational research on the design of quantum materials and devices from fundamental quantum-mechanical level. The RMG software will extend from desktops to the largest supercomputer systems, and will also perform well on a multitude of other systems, such as parallel clusters of various sizes, including those with GPUs. At the highest level of performance, it will enable predictive simulations at unprecedented scale, impact several areas of science and engineering and become a source of new discoveries and economic growth. RMG, already highly parallel and capable of multi-petaflops speeds, can provide a pathway towards reaching some of key NSCI goals. It has been included just as a part of NSF's Sustained Petascale Performance Benchmarks, which will be used to select NSF's future Leadership Class supercomputers. However, it will also be useful in classroom education, running on individual students' laptops, help individual researchers perform significant scientific or technological research on their accelerator- or GPU-equipped workstations, and also run on a multitude of clusters with varying capabilities. The extensible and portable exascale-capable software tools for simulations of complex quantum materials and devices will enable many scientific and technological endeavors that are currently too difficult to pursue, including dramatically accelerated discovery and design of complex quantum materials structures, such as nanostructured energy storage materials; nanoscale biosensors for electrical sequencing of DNA and nanoscale ""laboratories on a chip"" for monitoring health; as well as addressing fundamental questions about quantum behavior and the manipulation of quantum systems. Analogous accelerated progress is expected in other areas of science and technology that depend on nano and meso scales that are intermediate between those of molecules and bulk solids. Medium-size simulations will be enabled on local computing platforms, with an easy migration pathway to national facilities with the same input GUI. The exascale quantum simulation software will thus become a major resource to the national community. The easy availability of desktop binaries, supported source code, and optimized binaries at national facilities will lead to a major increase in high-end usage, dramatically enlarging the number and quality of simulations. The increase in users at all levels will stimulate their contributions both by new development and though incorporation of existing code elements into various materials frameworks. The national Cyberinfrastructure Community will be engaged through SI2 Software Institutes, Blue Waters and XSEDE projects, including live tutorials at workshops, as well tutorial sessions at conferences. STEM education and interests will be addressed by recruitment of undergraduate students, visually attractive presentations at libraries and science museums, and web-based presentation modules.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences."
"1216898","Collaborative Research: Software Infrastructure for Accelerating Grand Challenge Science with Future Computing Platforms","OAC","ADVANCES IN BIO INFORMATICS, Special Projects - CCF, Software Institutes","10/01/2012","09/13/2012","Viktor Prasanna","CA","University of Southern California","Standard Grant","Rudolf Eigenmann","09/30/2014","$350,000.00","Yogesh Simmhan","prasanna@usc.edu","3720 S FLOWER ST FL 3","LOS ANGELES","CA","900074304","2137407762","CSE","1165, 2878, 8004","1165, 2878, 7433, 8211","$0.00","Solving scientific grand challenges requires effective use of cyber infrastructure. Future computing platforms, including Field Programmable Gate Arrays (FPGAs), General Purpose Graphics Processing Units (GPGPUs), multi-core and multi-threaded processors, and Cloud computing platforms, can dramatically accelerate innovation to solve complex problems of societal importance when supported by a critical mass of sustainable software.<br/><br/>This project will organize scientific communities to help leverage the disruptive potential of future computing platforms through sustainable software. Grand challenge problems in biological science, social science, and security domains will be targeted based on their under-served needs and demonstrated possibilities. Users will be engaged through interdisciplinary workshops that bring together domain experts with software technologists with the goals of identifying core opportunity areas, determining critical software infrastructure, and discovering software sustainability challenges. The outcome will be an in-depth conceptual design for a Center for Sustainable Software on Future Computing Platforms, as part of the Software Infrastructure for Sustained Innovation (SI2) program. The design, scoped toward grand challenge problems, will identify common and specialized software infrastructure, research, development and outreach priorities, and coordination with the SSE and SSI components of the SI2 program. The interactions will offer a comprehensive understanding of grand challenges that best map to future computing platforms and the software infrastructure to best support scientists' needs. The workshops will enhance understanding of future platforms' potential for transformative research and lead to key insights into cross-cutting problems in leveraging their potential. Published results will help guide future research and reduce barriers to entry for under-represented groups."
"1265889","Collaborative Research: SI2-CHE: Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces","CHE","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","05/15/2013","05/15/2013","Mark Tuckerman","NY","New York University","Standard Grant","Evelyn Goldfield","04/30/2016","$225,375.00","","mark.tuckerman@nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","MPS","1253, 8004","5918, 5946, 5950, 7433, 8009","$0.00","An international team consisting of Teresa Head-Gordon and Martin Head-Gordon (University of California, Berkeley), Paul Nerenberg (Claremont McKenna College), David Case (Rutgers University), Jay Ponder (Washington University), Mark Tuckerman (New York University) with their UK collaborators: Lorna Smith and Neil Chue Hong (University of Edinburgh), Chris-Kriton Skylaris and Jonathan W. Essex (University of Southampton), Ilian Todorov (Daresbury Laboratory), Mario Antonioletti (EPCC) are are supported through the SI2-CHE program to develop and deploy robust and sustainable software for advanced potential energy surfaces.  The greater accuracy introduced by improvements in the new generation of potential energy surfaces opens up several challenges in their manifestation as algorithms and software on current or emergent hardware platforms that in turn limits their wide adoption by the computational chemistry community. The research team is overcoming these obstacles via multiple but integrated directions: (1) to optimally implement advanced potential energy surfaces across multi-core and GPU enabled systems, (2) to develop a hierarchy of advanced polarizable models that alter the tradeoff between accuracy and computational speed,(3) to create new multiple time stepping methods; (4) to write a Quantum Mechanics/Molecular Mechanics (QM/MM ) application programing interface (API) that fully supports mutual polarization, (5) to adopt software best practices to ensure growth of a self-sustaining community and (6) to provide exemplar calculations with the new software in the several emerging application areas.<br/><br/>Molecular simulation and quantum chemistry software is an integral part of chemistry and chemical biology, and has been broadly adopted by academic researchers and industry scientists. Next generation scientific breakthroughs that utilize chemical software will be enabled by the deployment of state of the art theoretical models and algorithms that are translated into a sustainable software framework rapidly implemented on emergent high performance computing platforms. Potential energy surfaces describe the interactions between atoms. Advanced and highly accurate potential energy surfaces encounter software-related obstacles that inhibit their application to grand challenge chemistry problems. This UK and US consortium, representing a broad cross section of the computational chemistry software community, is working to directly tackle these obstacles.   This US and UK collaboration between universities and High Performance Computing centers works to endure that chemical software investments made in advanced potential energy surface models has a long term payoff in community sustainability and the training of the next generation of scientists. Outreach and training workshops are organized around the emergence of the advanced potential energy software including an introductory molecular simulation software boot camp for undergraduate students.<br/><br/>The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC."
"1216504","Collaborative Research: Software Infrastructure for Accelerating Grand Challenge Science with Future Computing Platforms","OAC","Software Institutes","10/01/2012","09/13/2012","David Bader","GA","Georgia Tech Research Corporation","Standard Grant","Rudolf Eigenmann","09/30/2014","$104,386.00","Edward Riedy, Richard Vuduc","bader@njit.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","CSE","8004","7433, 8211","$0.00","Solving scientific grand challenges requires effective use of cyber infrastructure. Future computing platforms, including Field Programmable Gate Arrays (FPGAs), General Purpose Graphics Processing Units (GPGPUs), multi-core and multi-threaded processors, and Cloud computing platforms, can dramatically accelerate innovation to solve complex problems of societal importance when supported by a critical mass of sustainable software.<br/><br/>This project will organize scientific communities to help leverage the disruptive potential of future computing platforms through sustainable software. Grand challenge problems in biological science, social science, and security domains will be targeted based on their under-served needs and demonstrated possibilities. Users will be engaged through interdisciplinary workshops that bring together domain experts with software technologists with the goals of identifying core opportunity areas, determining critical software infrastructure, and discovering software sustainability challenges. The outcome will be an in-depth conceptual design for a Center for Sustainable Software on Future Computing Platforms, as part of the Software Infrastructure for Sustained Innovation (SI2) program. The design, scoped toward grand challenge problems, will identify common and specialized software infrastructure, research, development and outreach priorities, and coordination with the SSE and SSI components of the SI2 program. The interactions will offer a comprehensive understanding of grand challenges that best map to future computing platforms and the software infrastructure to best support scientists' needs. The workshops will enhance understanding of future platforms' potential for transformative research and lead to key insights into cross-cutting problems in leveraging their potential. Published results will help guide future research and reduce barriers to entry for under-represented groups."
"1450217","Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","08/01/2015","06/24/2015","Mark Gordon","IA","Iowa State University","Standard Grant","Seung-Jong Park","07/31/2021","$1,200,000.00","Theresa Windus","mark@si.msg.chem.iastate.edu","515 MORRILL RD, 1350 BEARDSHEAR","AMES","IA","500112105","5152945225","CSE","1253, 8004","7433, 8009, 9150","$0.00","Computational chemistry has become ubiquitous throughout the fields of science and engineering. Among the many uses of computational chemistry codes are to aid in the design of new materials with specific properties, to understand protein folding dynamics related to the human body, to understand the mechanisms of enzyme catalysis to produce biofuels, and to identify and characterize gaseous species in the atmosphere. Because computational chemistry simulations can require large amounts of computer, memory and disk space, one focus of this research is to reduce the demands on computer resources by exploiting methods that can efficiently fragment large molecules into smaller pieces and at the same time take advantage of computers that have many thousands of computer cores. The principal investigators are developers of several computational chemistry programs, each of which has unique functionalities that have taken multiple person-years to develop and implement. In order to minimize further development efforts and to maximize the utility of the unique features, a second key focus of this research will be to develop the ability of each program to access the unique features and data of the other programs. A major bottleneck in the effort to construct high performance computers with more and more compute cores is the power that is required to drive such systems. One way that the research team will address the power issue is to explore the utility of low power architectures, such as graphical processing units for computational chemistry calculations. All of the newly developed codes will be made available to the user community by web download at no cost.  This project is supported by programs in the Division of Chemistry in MPS and the Division of Advanced Cyberinfrastructure in CISE. <br/><br/>This project presents an integrated computational science approach to very high quality electronic structure and dynamics calculations that will (a) be broadly accessible to both the development and applications communities, (b) have the capability to address problems of great interest, such as the properties of liquids and solvent effects, and photochemical/photobiological dynamics with high accuracy, (c) provide interoperability and sustainability into the foreseeable future, and (d) solve bottlenecks including power consumption using accelerators. A primary goal is to provide to the broad community new, accurate approaches that may be easily used, with a clear path forward to further code improvement and development. As part of the proposed effort, in addition to the usual outlets of journal articles and public presentations, the investigators will organize workshops at prominent national meetings, so that the expertise and software developed will be available to as broad a group of users as possible.  All of the developed codes will be available on the web for easy downloads. The proposed research will develop new paradigms for interoperability, with a focus on the highly popular program suites GAMESS, NWChem, PSI4  and the AIMS dynamics code. Also included will be a cloud-based client-server model, a common quantum chemistry driver and novel data management approaches. The integration of the codes will be accomplished by making use of the combined expertise of the PIs in developing interoperable methods and data interfaces in computational science. In addition, several new methods will be developed, including novel explicit (R12) correlation methods that will be integrated with the most accurate levels of theory: multi-reference and the most accurate and novel coupled cluster methods, as well as the derivation and implementation of analytic derivatives. Applicability to large molecular systems will be made feasible by drawing upon novel fragmentation methods that scale nearly perfectly to the petascale."
"1642396","SI2-SSE: An Interactive Parallelization Tool","OAC","Software Institutes","11/01/2016","08/29/2017","Ritu Ritu","TX","University of Texas at Austin","Continuing Grant","Stefan Robila","10/31/2019","$463,302.00","","ritu@wayne.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","CSE","8004","7433, 8004, 8005","$0.00","Many scientists, engineers, and students use the programming languages C, C++, and Fortran. Scientists who use these languages to conduct their experiments set up their programs to run in sequence -- what computer scientists call ""serial processing.""  In order to make the best use of the state-of-the-art High Performance Computing (HPC) platforms to run more efficient experiments or to run very large simulations, the programs written in these languages need to be able to run simultaneously, what computer scientists call ""in parallel"" as distinct from ""serial"".  Enabling the parallelization of the applications and software, however, remains a challenge. This project proposes deployment of an Interactive Parallelization Tool (IPT), which helps scientists, engineers, and students build on their customary use of C/C++/Fortran to create more efficient programs that can run on advanced HPC platforms. The IPT will be made available to the community through a web-portal for convenient code generation and testing on computational resources of the national cyberinfrastructure (CI).  <br/><br/>The team has developed a prototype IPT to parallelize multiple design patterns written in C/C++ at a high-level of abstraction.  The proposed project will harden the IPT prototype into a production-quality tool and will extend support for the efficient parallelization of additional types of C/C++ applications.  The team will also prototype the parallelization of Fortran applications.  Carefully selected test cases will be provided with the IPT to engage and educate the users about the advantages and disadvantages of the various parallel programming strategies so that they can be appropriately applied in solving real-world problems.  The IPT can be extended to support additional parallel programming paradigms, applications classes, and languages.  It will be evaluated using code from multiple domains (e.g. biology, geosciences, civil engineering, and astronomy), and potential users will be engaged during the testing phase.  This will all be served through a web-based portal, and will enable a low-risk guided approach for researchers, practitioners, and students to incorporate HPC in their work."
"2001752","Collaborative Research: SI2-SSI: Expanding Volunteer Computing","OAC","Software Institutes","09/01/2019","10/31/2019","Michael Zentner","CA","University of California-San Diego","Standard Grant","Seung-Jong Park","04/30/2021","$172,474.00","","mzentner@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","CSE","8004","7433, 8004, 8009","$0.00","Volunteer computing (VC) uses donated computing time consumer devices such as home computers and smartphones to do scientific computing. It has been shown that VC can provide greater computing power, at lower cost, than conventional approaches such as organizational computing centers and commercial clouds. BOINC is the most common software framework for VC. Essentially, donors of computing time simply have to load BOINC on their computer or smartphone, and then register to donate at the BOINC web site. VC provides ""high throughput computing"": handling lots of independent jobs, with performance goals based on the rate of job completion rather than completion time for individual jobs. This type of computing (all known as high-throughput computing) is in great demand in most areas of science. Until now, the adoption of VC has been limited by its structure. For example, VC projects (such as Einstein@home and Rosetta@home) are operated by individual research groups, and volunteers must browse and choose from among many such projects. As a result, there are relatively few VC projects, and volunteers are mostly tech-savvy computer enthusiasts.  This project aims to solve these problems using two complementary development efforts: First, it will add BOINC-based VC conduits to two major high-performance computing providers: (a) the Texas Advanced Computing Center, a supercomputer center, and (b) nanoHUB, a web portal for nano science that provides computing capabilities.Also, a unified control interface to VC will be developed, tentatively called Science United, where donors can register.  The project  will benefit thousands of scientists who use these facilities, and it will create technology that makes it easy for other HPC providers to add their own VC back ends. Also, Science United will provide a simpler interface to BOINC volunteers where they will register to support scientific areas, rather than specific projects. Science United will also serve as an allocator of computing power among projects. Thus, new projects will no longer have to do their own marketing and publicity to recruit volunteers. Finally, the creation of a single VC ""brand"" (i.e Science United) will allow coherent marketing of VC to the public. By creating a huge pool of low-cost computing power that will benefit thousands of scientists, and increasing public awareness of and interest in science, the project plans to establish VC as a central and long-term part of the U.S. scientific cyber infrastructure.<br/><br/>Adding VC to an existing HPC facility involves several technical issues, which will be addressed as follows: (1) Packaging science applications (which typically run on Linux cluster nodes) to run on home computers (mostly Windows, some Mac and Linux): the team is developing an approach using VirtualBox and Docker, in which the application and its environment (Linux distribution, libraries, executables) are represented as a set of layers comprising a Docker image, which is then run as a container within a Linux virtual machine on the volunteer device. This has numerous advantages: it reduces the work of packaging applications to near zero; it minimizes network traffic because a given Docker layer is downloaded to a host only once; and it provides a strong security sandbox so that volunteer computers are protected from buggy or malicious applications, (2) File management: Input and output files must be moved between existing private servers and public-facing servers that are accessible to the outside Internet. A file management system will be developed, based on Web RPCs, for this purpose. This system will use content-based naming so that a given file is transferred and stored only once. It also maintains job/file associations so that files can be automatically deleted from the public server when they are no longer needed. (3) Submitting and monitoring jobs:  BOINC provides a web interface for efficiently submitting and monitoring large batches of jobs. These were originally developed as part of a system to migrate HTCondor jobs to BOINC. This project is extending it to support the additional requirements of TACC and nanoHUB. Note that these new capabilities are not specific to TACC or nanoHUB: they provide the glue needed to easily add BOINC-based VC to any existing HTC facility. The team is also developing RPC bindings in several languages (Python, C++, PHP). The other component of the project, Science United, is a database-driven web site and an associated web service for the BOINC clients. Science United will control volunteer hosts (i.e. tell them which projects to work for) using BOINC's ""Account Manager"" mechanism, in which the BOINC client on each host periodically contacts Science United and is told what projects to run. Project servers, not Science United, will distribute jobs and files. Science United will define a set of ""keywords"" for science areas (physics, biomedicine, environment, etc.) and for location (country, institution). Projects will be labelled with appropriate keywords. Volunteers will have a yes/no/maybe interface for specifying the types of jobs they want to run. Science United will thus provide a mechanism in which a fraction of total computing capacity can be allocated to a project for a given period. Because total capacity changes slowly over time, this allows near-certain guaranteed allocations. Science United will embody a scheduling system that attempts to enforce allocations, honor volunteer preferences, and maximize throughput. Finally, Science United will do detailed accounting of computing. Volunteer hosts will tell Science United how much work (measured by CPU time and FLOPs, GPU time and FLOPs, and number of jobs) they have done for each project. Science United will maintain historical records of this data for volunteers and projects, and current totals with finer granularity (e.g. for each host/project combination). Finally, Science United will provide web interfaces letting volunteers see their contribution status and history, and letting administrators add projects, control allocations, and view accounting data."
"1740333","SI2-SSE: Gunrock: High-Performance GPU Graph Analytics","OAC","Software Institutes","10/01/2017","08/24/2017","John Owens","CA","University of California-Davis","Standard Grant","Seung-Jong Park","09/30/2022","$400,000.00","","jowens@ece.ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Many sets of data can be represented as ""graphs"". Graphs express relationships between entities, and those entities and relationships can be used to solve problems of interest in many fields. For instance, a social graph (like Facebook's) links people (entities) by friendships (relationships), and with that graph, Facebook can suggest people to you who might be your friends. Amazon might use a graph made of people and items for sale (entities) connected by who bought those items (relationships) to suggest items you might want to buy. A credit card company might look at your pattern of purchases and detect possible fraud even before you know your credit card was stolen. Graphs are also useful in many fields of science, such as genomics, epidemiology, and economics. This project uses an emerging programmable processor, the graphics processor (GPU), to solve graph problems. GPUs are rapidly moving into our nation's largest data centers and supercomputers. The project team is building a system for computation on graphs that will significantly improve performance on these problems. In this project, the team will work with the computing community and the scientific community, both of whom have numerous interesting, challenging graph computation problems that this system will target. The system is open-source software and can be used freely by researchers and industry all over the world.<br/><br/><br/>This project, supported by the Office of Advanced Cyberinfrastructure seeks to develop the ""Gunrock"" programmable, high-performance, open-source graph analytics library for graphics processors (GPUs) from a working prototype to a robust, sustainable, open-source component of the GPU computing ecosystem. Gunrock's strengths are its programming model and highly optimized implementation. With this work the project team hopes to address Gunrock's usability in the computing and scientific communities by improving Gunrock's scalability, capabilities, core operators, and supported graph computations. In this work the team will collaborate with the GPU Open Analytics Initiative and the NSF-sponsored CINET project for network science to ensure that our work has the broadest possible impact.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1339884","Collaborative Research: SI2-SSE: A Petascale Numerical Library for Multiscale Phenomena Simulations","OAC","Software Institutes, CDS&E","10/01/2013","09/11/2013","Dmitry Pekurovsky","CA","University of California-San Diego","Standard Grant","Rajiv Ramnath","09/30/2017","$358,917.00","","dmitry@sdsc.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","CSE","8004, 8084","7433, 8005, 8084","$0.00","Multiscale phenomena are a grand challenge for theory, simulations and experiments. It is difficult to understand phenomena at both highest and lowest scales, as well as all those in between. This challenge shows up in diverse fields. One of the long-standing problems in cosmology is understanding cosmological structure formation. In computer simulations of this process the challenge is increasing grid resolution while retaining the essential physics. In all-atom molecular dynamics simulations of enzymes the challenge is simulating systems with a large number of atoms while resolving long-range interactions and having sufficiently high throughput. Such simulations are critical in understanding important biological processes and eventually designing new drugs. Another prime example of multiscale phenomena is turbulent flows, a rich and complex subject of great relevance to many of the main technological issues of the day, including climate, energy, and the management of oil and biohazards. Understanding turbulent flows is critical for design of new transportation vehicles, improving efficiency of combustion processes and managing their environment pollution. Here simulations have been historically limited, and remain so, due to extremely high computational cost, even using the high-end computational systems available to researchers today. Reducing this cost, and efficiently using the computational resources, often requires specialized expertise, as well as significant development time and cost many research groups cannot afford. This project will develop a powerful suite of critical software components to provide tools for performing simulations of multiscale phenomena. <br/><br/>The suite will implement state-of-the-art techniques for reducing communication cost, which has become the most important contributing factor to the total simulation cost, especially at larger scales. It will provide a flexible set of features that will make it usable in a great number of codes across the disciplines. In particular, the library will include user-friendly interfaces for Fourier transforms, spectral and compact differentiation in three dimensions, in addition to widely used communication routines (transposes, halo exchanges). This combination of emphasis on scalable performance and richness of features makes this suite unique among other libraries in existence today. Given the extraordinary challenge of simulations of multiscale phenomena, this library will provide a realistic path towards the Exascale. The suite will be available under an open source license. User outreach will be undertaken through the project website, mailing list, user surveys and presentations."
"1148473","SI2-SSE: Fingerprinting Scientific Codes to Verify and Create Compatible System Software Environments","OAC","Software Institutes","04/01/2012","04/11/2012","Philip Papadopoulos","CA","University of California-San Diego","Standard Grant","Rajiv Ramnath","03/31/2017","$500,000.00","","ppapadopoulos@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","CSE","8004","8004, 8005","$0.00","Every computational science application requires supporting software (code libraries, data files, other programs) to run properly. However, the computer systems that complex scientific applications run on are continuously updated by administrators to add new capabilities, fix bugs, and plug security holes. These are necessary changes, but a user needs to know if ""something changed in the system that will break his/her code"". A similar question arises when running a code on a different computer from which it was compiled or developed: ""does the new system have all the necessary support software to run my applications?"" This proposal describes a plan for developing the software capability to both statically and dynamically ""fingerprint"" a scientific application to answer those two questions and therefore test for compatibility of new system or determine if a new update could have deleterious effects. Our proposed software system will have five major components: fingerprint format, static analysis of an application to create one type of fingerprint, a system verifier that checks if a system satisfies the needs defined in a fingerprint, dynamic analysis to find applications dependencies only detectable at run time, and a composer capability to automatically define a Rocks cluster managed system that will fulfill the requirements of a specific fingerprint. All software developed will be open-source and freely available.<br/><br/>Nearly every person who uses a computer has seen messages similar to ""new updates have been installed, you must restart your computer."" Sometimes, applications break because of these updates. This proposal's broader impact is that, for a wide variety of scientific applications, we will be able to determine if a new system or an existing system is compatible by verifying the code's fingerprint. Since, scientific applications can be very sensitive to underlying software changes, it will be possible to detect incompatibility before wasting time and energy to run a large-scale application on an production systems. When these applications are used as the basis of scientific discovery, it becomes even more imperative that we view the computing environment and application together as an experimental apparatus whose configuration we need to better understand. We believe that the composer capability of this proposal can have the transformational impact by developing fully- descriptive catalogs of what an application needs to function properly. In the era in which simulation output is being used to drive policy, this kind of scientific reproducibility has impact well beyond the notions of academic completeness."
"1148276","Collaborative Research SI2-SSE:Sustained Innovation in Acceleration of Molecular Dynamics on Future Computational Environments: Power to the People in the Cloud and on Accelerators","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, CHEMISTRY PROJECTS, Software Institutes","06/01/2012","06/24/2014","Amitava Majumdar","CA","University of California-San Diego","Standard Grant","Rajiv Ramnath","05/31/2016","$320,833.00","","majumdar@sdsc.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","CSE","1253, 1712, 1991, 8004","7237, 7433, 7569, 7573, 7683, 8005, 9216, 9263","$0.00","This collaborative project between the San Diego Supercomputer Center at the University of California San Diego, the Quantum Theory Project at the University of Florida and industrial partners NVIDIA, Intel and Amazon is focused on developing innovative, comprehensive open source software element libraries for accelerating condensed phase Molecular Dynamics (MD) simulations of biomolecules using next generation accelerator hardware including Intel's MIC system and Graphics Processing Units (GPU). It will extend support to include all major MD techniques and develop open source accelerated analysis libraries. A priority is enhanced sampling techniques including Thermodynamic Integration, constant pH algorithms, Multi-Dimensional Hamiltonian Replica Exchange and Metadynamics. These elements will then be combined, in collaboration with Amazon to support MD as-a-service through easily accessible web front ends to cloud services, including Amazon's EC2 GPU hardware. Transitioning large scale MD workflows from requiring access to large supercomputer hardware to being accessible to all on desktop and cloud resources provides the critical software infrastructure to support transformative research in the fields of chemistry, life science, materials science, environmental and renewable energy.<br/><br/>The software elements created through this project have an extremely broad impact. The integration of comprehensive support for next generation hardware acceleration into the AMBER software alone benefits a very large user base. With over 10,000 downloads of the latest AMBER Tools package from unique IPs and >800 sites using the AMBER MD engines testify to the scope of the community of researchers this work impacts. The development of simple web based front ends for use of elastically scalable cloud resources makes simulations routine for all researchers. Meanwhile education and outreach efforts train the next generation of scientists not just in how to use the MD acceleration libraries and advanced MD simulation techniques developed here but also gets them thinking about how their approach can be transformed given that performance that was previously restricted to large scale supercomputers is now available on individual desktops."
"1147926","Collaborative Research: SI2-SSI: A Comprehensive Performance Tuning Framework for the MPI Stack","OAC","Information Technology Researc, Software Institutes","06/01/2012","06/04/2012","Amitava Majumdar","CA","University of California-San Diego","Standard Grant","Rajiv Ramnath","05/31/2016","$450,772.00","","majumdar@sdsc.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","CSE","1640, 8004","1640, 7433, 8004, 8009","$0.00","The Message Passing Interface (MPI) is a very widely used parallel programming model on modern High-End Computing (HEC) systems. Many performance aspects of MPI libraries, such as latency, bandwidth, scalability, memory footprint, cache pollution, overlap of computation and communication etc. are highly dependent on system configuration and application requirements. Additionally, modern clusters are changing rapidly with the growth of multi-core processors and commodity networking technologies such as InfiniBand and 10GigE/iWARP. They are becoming diverse and heterogeneous with varying number of processor cores, processor speed, memory speed, multi-generation network adapters/switches, I/O interface technologies, and accelerators (GPGPUs), etc.  Typically, any MPI library deals with the above kind of diversity in platforms and sensitivity of applications by employing various runtime parameters. These parameters are tuned during its release, or by<br/>system administrators, or by end-users.  These default parameters may or may not be optimal for all system configurations and applications.<br/><br/>The MPI library of a typical proprietary system goes through heavy performance tuning for a range of applications.  Since commodity clusters provide greater flexibility in their configurations (processor, memory and network), it is very hard to achieve optimal tuning using released version of any MPI library, with its default settings. This leads to the following broad challenge: ""Can a comprehensive performance tuning framework be designed for MPI library so that the next generation InfiniBand, 10GigE/iWARP and RoCE clusters and applications will be able to extract `bare-metal' performance and maximum scalability?""  The investigators, involving computer<br/>scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) as well as computational scientists from the Texas Advanced Computing Center (TACC) and San Diego Supercomputer Center (SDSC), University of California San Diego (UCSD), will be addressing the above challenge with innovative solutions.<br/><br/>The investigators will specifically address the following challenges: 1) Can a set of static tools be designed to optimize performance of an MPI library during installation time?  2) Can a set of dynamic tools with low overhead be designed to optimize performance on a per-user and per-application basis during production runs?  3) How to incorporate the proposed performance tuning framework with the upcoming MPIT interface?  4) How to configure MPI libraries on a given system to deliver different optimizations to a set of driving applications?  and 5) What kind of benefits (in terms of performance, scalability, memory efficiency and reduction in cache pollution) can be achieved by the proposed tuning framework?  The research will be driven by a set of applications from established NSF computational science researchers running large scale simulations on the TACC Ranger and other systems at OSC, SDSC and OSU.  The proposed designs will be integrated into the open-source MVAPICH2 library."
"1148424","Collaborative Research: SI2-SSI: A Comprehensive Performance Tuning Framework for the MPI Stack","OAC","Information Technology Researc, Software Institutes","06/01/2012","06/04/2012","William Barth","TX","University of Texas at Austin","Standard Grant","Rajiv Ramnath","05/31/2016","$449,995.00","Tommy Minyard","bbarth@tacc.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","CSE","1640, 8004","1640, 7433, 8004, 8009","$0.00","The Message Passing Interface (MPI) is a very widely used parallel programming model on modern High-End Computing (HEC) systems. Many performance aspects of MPI libraries, such as latency, bandwidth, scalability, memory footprint, cache pollution, overlap of computation and communication etc. are highly dependent on system configuration and application requirements. Additionally, modern clusters are changing rapidly with the growth of multi-core processors and commodity networking technologies such as InfiniBand and 10GigE/iWARP. They are becoming diverse and heterogeneous with varying number of processor cores, processor speed, memory speed, multi-generation network adapters/switches, I/O interface technologies, and accelerators (GPGPUs), etc.  Typically, any MPI library deals with the above kind of diversity in platforms and sensitivity of applications by employing various runtime parameters. These parameters are tuned during its release, or by<br/>system administrators, or by end-users.  These default parameters may or may not be optimal for all system configurations and applications.<br/><br/>The MPI library of a typical proprietary system goes through heavy performance tuning for a range of applications.  Since commodity clusters provide greater flexibility in their configurations (processor, memory and network), it is very hard to achieve optimal tuning using released version of any MPI library, with its default settings. This leads to the following broad challenge: ""Can a comprehensive performance tuning framework be designed for MPI library so that the next generation InfiniBand, 10GigE/iWARP and RoCE clusters and applications will be able to extract `bare-metal' performance and maximum scalability?""  The investigators, involving computer<br/>scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) as well as computational scientists from the Texas Advanced Computing Center (TACC) and San Diego Supercomputer Center (SDSC), University of California San Diego (UCSD), will be addressing the above challenge with innovative solutions.<br/><br/>The investigators will specifically address the following challenges: 1) Can a set of static tools be designed to optimize performance of an MPI library during installation time?  2) Can a set of dynamic tools with low overhead be designed to optimize performance on a per-user and per-application basis during production runs?  3) How to incorporate the proposed performance tuning framework with the upcoming MPIT interface?  4) How to configure MPI libraries on a given system to deliver different optimizations to a set of driving applications?  and 5) What kind of benefits (in terms of performance, scalability, memory efficiency and reduction in cache pollution) can be achieved by the proposed tuning framework?  The research will be driven by a set of applications from established NSF computational science researchers running large scale simulations on the TACC Ranger and other systems at OSC, SDSC and OSU.  The proposed designs will be integrated into the open-source MVAPICH2 library."
"1550493","Collaborative Research: SI2-SSI: Sustaining Innovation in the Linear Algebra Software Stack for Computational Chemistry and Other Sciences","OAC","Software Institutes","07/15/2016","03/29/2017","Robert van de Geijn","TX","University of Texas at Austin","Standard Grant","Amy Walton","12/31/2019","$899,902.00","John Stanton, Don Batory, Margaret Myers, Victor Eijkhout","rvdg@cs.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","CSE","8004","7433, 8004, 8009","$0.00","Scientific discovery now often involves computer simulation in addition to, or instead of, laboratory experimentation. This can accelerate, improve, and/or expand scientific insight, often at a great reduction in cost. Many such computer simulations spend much or most of their time solving linear algebra (matrix) problems. For these simulations, linear algebra problems constitute the most basic building blocks of the computation. As a result, software libraries (bundles of specialized code) that efficiently solve linear algebra problems fundamentally support sustained innovation in science. The project aims to create a next generation of software libraries for this domain and will make these libraries available to the scientific community as open source software that can be easily ported to current and future computer architectures. This will directly and indirectly impact discovery in academia, at the national labs, and in industry. The project will also impact affordable education through open course ware that is expected to reach a broad audience. The involvement of undergraduate and graduate students will strengthen the pool of qualified individuals trained to support scientific computing.  The project involves research staff and students who are members of traditionally underrepresented groups.<br/><br/><br/>The BLAS (Basic Linear Algebra Subprograms) are well-known routines that provide standard building blocks for performing basic vector and matrix operations. The Level 1 BLAS perform scalar, vector and vector-vector operations, the Level 2 BLAS perform matrix-vector operations, and the Level 3 BLAS perform matrix-matrix operations. Because the BLAS are efficient, portable, and widely available, they are commonly used in the development of high quality linear algebra software, such as the well-known Linear Algebra PACKage (LAPACK), as an example. However, the BLAS libraries that exist today have not evolved to new computing architectures, and hence do not perform as well as they could. The technical goal and scope of this project, therefore, is to develop a new high-performance dense linear algebra library with broad functionality that can be easily ported to current and future multi-core and many-core processors. The project builds on the BLAS-like Library Instantiation Software (BLIS) effort that has exposed low-level primitives that facilitate the high-performance implementation of BLAS. By implementing the higher-level dense linear algebra functionality in terms of these low-level primitives, portable high performance will be achieved for higher-level functionality needed by many scientific computing applications. Contributions will include the to-be developed techniques for implementing such software, the resulting open source software, and pedagogical artifacts that will include open course ware."
"1550588","Collaborative Research: SI2-SSI: Swift/E: Integrating Parallel Scripted Workflow into the Scientific Software Ecosystem","OAC","Information Technology Researc, Software Institutes","10/01/2016","09/01/2021","Kyle Chard","IL","University of Chicago","Standard Grant","Varun Chandola","09/30/2022","$2,770,286.00","Michael Wilde, Daniel Katz, Dmitry Karpeev, Ravi Madduri, Justin Wozniak","chard@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","CSE","1640, 8004","7433, 8004, 8009, 9102, CL10","$0.00","Science and engineering research increasingly relies on repeated execution of a complex series of steps (i.e., workflows) to form hypotheses; conduct experiments; analyze results; and refine theory.   Computation is often essential throughout the workflow and in this case, software can improve productivity by managing the computational and data workflow.  Swift is one such open-source workflow system that has been developed and widely used in diverse areas ranging from materials simulations and climate modeling to neuroscience and genomics. This project extends the capabilities of Swift by integrating it with other software systems that enable collaboration, usability, maintainability, and productivity. The new ecosystem, Swift/E, will enable scientists and engineers to more productively create and run computational workflow campaigns of larger scale, and debug, execute, adapt, and disseminate them faster and easier than has been possible to date. These workflows embody and communicate the computational methods specific to each domain of scientific inquiry. Swift/E achieves community engagement and extensive productivity benefits for a large user community through an integrated program of research, education, and software dissemination. The project engages and serves science and engineering communities by creating patterns of practice for building and sharing reusable workflow libraries, and by training students, educators, and researchers in their use.  To advance the education of the next generation of computationally trained scientists, Swift/E powers a network of NSF-supported ""e-Labs"" that teach the concepts of collaborative parallel computational science at high school and undergraduate levels, reaching over a thousand students annually.<br/><br/>The open-source Swift/E ""ecosystem"" integrates Swift with several scientific software elements that play a major role in the national and global cyberinfrastructure of today. These elements are: Swift for the parallel scripting of scientific workflow; Globus for data cataloging, management, and high-speed wide-area transport; the Web-based Galaxy workflow portal for workflow composition, execution, and collaborative sharing; Jupyter for the interactive development, testing, debugging, and assembly of high level programming and workflow languages; Python and R for productively expressing high-level computational logic; and ""git"" and related tools and Web portals for revision control, code dissemination and sharing, and for the collaborative engagement of developers.  Swift's implicitly parallel programming language is minimal and compact.  Swift provides a facility for embedding other scripting languages (currently Python, R, Julia and Tcl) into its runtime environment.  This project merges newer extreme-scale ""Swift/T"" capabilities with the flexible and portable original ""Swift/K"" version to make the core Swift/E software element more powerful and flexible while lowering it?s ongoing support cost. Swift/E enhances usability by extending Swift's troubleshooting and inter-language integration facilities.  And with enhanced and innovative workflow sharing archives, new training materials, and a sustained program for user support and self-sustaining and expanding community engagement, the Swift/E project engages, supports, and sustains a large global science and engineering user base."
"1664018","Collaborative Research: SI2-SSI:  Cyberinfrastructure for Advancing Hydrologic Knowledge through Collaborative Integration of Data Science, Modeling and Analysis","OAC","Special Initiatives, EAR-Earth Sciences Research, XC-Crosscutting Activities Pro, Software Institutes, EarthCube","10/01/2017","11/25/2019","Hong Yi","NC","University of North Carolina at Chapel Hill","Standard Grant","Seung-Jong Park","09/30/2022","$540,000.00","Hong Yi, Michael Stealey","hongyi@renci.org","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","CSE","1642, 6898, 7222, 8004, 8074","026Z, 7433, 8004, 8009","$0.00","Researchers across the country and around the world expend tremendous resources to gather and analyze vast stores of hydrologic data and populate a myriad of models to better understand hydrologic phenomena and find solutions to vexing water problems. Each of those researchers has limited money, time, computational capacity, data storage, and ability to put that data to productive use. What if they could combine their efforts to make collaboration easier? What if those collected data sets and processed model outputs could be used collaboratively to help advance hydrologic understanding beyond their original purpose? HydroShare is a system to advance hydrologic science by enabling the scientific community to more easily and freely share products resulting from their research, not just the scientific publication summarizing a study, but also the data and models used to create the scientific publication. HydroShare supports the sharing and publication of hydrologic data and models. This capability is necessary for community model development, execution, and evaluation and to improve reproducibility and community trust in scientific findings through transparency. As a platform for collaboration and running models on advanced computational infrastructure, HydroShare enhances the capability for data intensive research in hydrology and other aligned sciences. HydroShare is designed to help researchers easily meet the sharing requirements of data management plans while at the same time providing value added functionality that makes metadata capture more effective and helps researchers improve their work productivity. This project will extend the capabilities of the HydroShare cyberinfrastructure to enhance support for scientific methods, advance the social capabilities of HydroShare to enable improved collaborative research, integrate with 3rd party consumer data storage systems to provide more flexible and sustainable data storage. and establish an application testing environment to empower researchers to develop their own computer programs to act on and work with data in HydroShare. Empowering HydroShare users with the ability to rapidly develop web application programs opens the door to unforeseen, innovative combinations of data and models. WRF-Hydro, the framework for the NOAA National Water Model, will be used as a use case for collaboration on model development. Since WRF-Hydro is used by NOAA as part of the National Water Model (NWM), this collaboration opens possibilities for transfer of research to operations. Collectively, this functionality will provide a computing framework for transforming the practice of broad science communities to leverage advances in data science and computation and accelerate discovery.<br/><br/>HydroShare is a system for sharing hydrologic data and models aimed at giving hydrologists the cyberinfrastructure needed to manage data, innovate and collaborate in research to solve water problems. It addresses the challenges of sharing data and hydrologic models to support collaboration and reproducible hydrologic science through the publication of hydrologic data and models. With HydroShare users can: (1) share data and models with colleagues; (2) manage who has access to shared content; (3) share, access, visualize and manipulate a broad set of hydrologic data types and models; (4) use the web services interface to program automated and client access; (5) publish data and models to meet the requirements of research project data management plans; (6) discover and access data and models published by others; and (7) use web apps to visualize, analyze, and run models on data. This project will extend the capabilities of HydroShare to: (1) enhance support for scientific methods enabling systematic data and model analysis and hypothesis testing; (2) advance the social capabilities of HydroShare to enable improved collaborative research; (3) integrate with 3rd party consumer data storage systems to provide more flexible and sustainable data storage; and (4) establish an application testing environment to empower researchers to develop their own computer programs to act on and work with data in HydroShare. Under development since 2012 and first released in 2014, HydroShare supports the sharing and publication of hydrologic data and models. This capability is necessary for community model development, execution, and evaluation. As a platform for collaboration and cloud based computation on network servers remote from the user, HydroShare enhances the capability for data intensive research in hydrology and other aligned sciences. HydroShare is innovative from a computer science and CI perspective in the way computation and data sharing are framed as a network computing platform that integrates data storage, organization, discovery, and programmable actions through web applications (web apps). Support for these three key elements of computation allows researchers to easily employ services beyond the desktop to make data storage and manipulation more reliable and scalable, while improving ability to collaborate and reproduce results. The generation of new understanding, through integration of information from multiple sources and reuse and collaborative enrichment of research data and models, will be enhanced. Structured and systematic model process intercomparisons and alternative hypothesis testing will be enabled, bringing, through user friendly CI, the latest thinking in advancing hydrologic modeling to a broad community of earth science researchers, thereby transforming research practices and the knowledge generated from this research. Interoperability with consumer cloud storage will greatly ease entry of content into HydroShare and support its sustainability. This meshing of the rigorous metadata model of HydroShare with consumer file sharing will enhance reproducibility as well as provide an innovative mechanism for sharing and collaboration. Empowering HydroShare users with the ability to rapidly develop web apps opens the door to unforeseen, innovative combinations of data and models. WRF- Hydro will be used as a use case for collaboration on model development. WRF-Hydro provides a reach-based high resolution representation of hydrologic processes, and offers the potential to bring together scientists working at scales from research catchments on the order of 1 to 100s of square kilometers as well as those working at regional to continental scales and cut across disciplines from environmental engineering to aquatic ecologists. Since WRF-Hydro is used by NOAA as part of the National Water Model (NWM), this collaboration opens possibilities for transfer of research to operations. This project will adapt current best practices in CI for interoperability and extensibility to serve this multidisciplinary community of scientists. HydroShare has already had a broader impact, with documented rapid growth in use and uptake by other projects including in EarthCube. It will become sustainable community CI through operation as part of the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI) Water Data Center (WDC) facility. The use of WRF- Hydro/NWM, as a driving use case, will advance CI for community based model improvement. Through the Summer Young Innovators Program at the National Water Center (NWC), supported by the National Weather Service (NWS) and operated by CUAHSI, a pathway already exists to translate research findings to the operational needs of federal agencies participating in the NWC. HydroShare already touches a broad and diverse community, with user base including Native American tribes, hydrologic science students, and faculty researchers across the U.S. This proposal builds on the success of HydroShare to extend its capabilities and broaden model hypothesis testing, collaborative data sharing, and open app development across earth science research and education."
"1716828","SI2-SSE: GraphPack: Unified Graph Processing with Parallel Boost Graph Library, GraphBLAS, and High-Level Generic Algorithm Interfaces","OAC","Software Institutes","10/01/2016","08/20/2021","Andrew Lumsdaine","WA","University of Washington","Standard Grant","Rob Beverly","09/30/2022","$499,386.00","","al75@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","8004","026Z, 7433, 7942, 8004, 8005","$0.00","Modeling and simulating physical phenomena with computers is now an important tool for development and discovery in almost all fields of science and engineering.  Joining theory and experimentation, computation is now recognized as the ""third pillar"" of scientific research.  More recently, data analytics (the science of examining raw data with the purpose of drawing conclusions about that information) has emerged as an important computational tool for scientific discovery - a tool that is likely to be as important as, if not more important than, modeling and simulation.  Within the broad domain of data analytics, the use of graphs is a powerful conceptual tool that describes relationships between discrete objects.  Because of the growing importance of data analytics, many research groups have turned their attention to developing new approaches for solving large-scale graph problems.  While the research results in this area have been valuable, the software products that have been produced tend to be limited in scope and/or not of sufficient quality to be reused. This project will address this problem by creation of GraphPack, a comprehensive unified graph library with a coherent user interface and support for multiple state-of-the-art compute platforms. This work will have broad impacts in scientific and engineering application areas, larger social and economic areas depending on graph analytics, and in education. GraphPack will improve the ease of use and broaden the applicability of graph algorithms. Application areas include such diverse areas as knowledge discovery, genomics, proteomics, electronic design automation, forest management, Internet routing, power grid management, and many more.<br/><br/>GraphPack will be a reliable and comprehensive toolkit applicable across a wide variety of problems and architectures that will unleash the capabilities of the community by making the current state of the art readily available. GraphPack will develop a consistent and comprehensive set of abstractions necessary to express a wide variety of (generic) graph algorithms and data structures in the context of unimodal as well as hybrid parallelism. These abstractions will be incorporated as abstract concepts, and selected concrete efficient implementations will be provided. While genericity is an important goal, GraphPack will also provide a simplified user interface for graph algorithms for the situations where simplicity is more important than fully tuned performance. GraphPack will also provide a GraphBLAS interface based on the recent efforts to provide a standardized set of graph operations based on the concepts of linear algebra. By providing multiple interfaces with efficient parallel implementations, GraphPack will enable a wide variety of applications to take advantage of high-performance graph algorithms."
"1663671","SI2-SSI Collaborative Research: The SimCardio Open Source Multi-Physics Cardiac Modeling Package","OAC","FD-Fluid Dynamics, Engineering of Biomed Systems, BMMB-Biomech & Mechanobiology, Software Institutes, Smart and Connected Health, CDS&E","09/01/2017","05/04/2017","Alison Marsden","CA","Stanford University","Standard Grant","Rob Beverly","08/31/2023","$1,431,169.00","","amarsden@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","CSE","1443, 5345, 7479, 8004, 8018, 8084","026Z, 028E, 7433, 7479, 8004, 8009, 9102, 9263","$0.00","Cardiovascular (CV) simulations have become a crucial component of fundamental research in surgical planning, device design, diagnosis, and disease mechanisms. The project team has previously developed SimVascular (www.simvascular.org), which is currently the only open source software package providing a complete pipeline from medical image data segmentation to patient specific blood flow simulation and analysis in arteries and veins. The SimCardio open source project will extend and enhance the functionality of SimVascular to the realm of heart modeling, providing the first fully integrated computer model of cardiac physiology and function. This will help basic science and medical researchers perform computer modeling in numerous diseases affecting heart function in children and adults. This computer modeling software will enable researchers to build models of the heart and vascular anatomy directly from medical imaging data, which can be used for personalized treatment planning and medical device design, ultimately leading to new treatments for patients with cardiovascular disease. <br/><br/>The SimCardio project will create a unique open source software package for multi-physics cardiac modeling and simulations. SimCardio will include a new multi-physics finite element solver with capabilities for large-deformation fluid-structure interaction (FSI) to capture ventricular contraction and heart valve dynamics, non-linear and visco-elastic material models, cardiac mechanics models of active heart contraction, and electrophysiology. This will be facilitated by sustainable software infrastructure bridging the cardiovascular fluid and solid mechanics communities. The project will provide a new user-interface for high-throughput construction of patient-specific cardiac and vascular models. SimCardio will broaden the applicability of SimVascular to problems including heart valves, heart failure, cardiomyopathy, aortic dissection, structural congenital heart defects and medical devices. To facilitate adoption, the project will publicly provide accompanying educational and training materials, and maintain a sustainable software ecosystem to increase the user community and ensure continued availability and evolution."
"1740229","SI2-SSE: Collaborative Research: A Sustainable Future for the Glue Multi-Dimensional Linked Data Visualization Package","OAC","EXTRAGALACTIC ASTRON & COSMOLO, OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","10/01/2017","08/29/2017","Michelle Borkin","MA","Northeastern University","Standard Grant","Ashok Srinivasan","09/30/2022","$164,142.00","","m.borkin@northeastern.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","1217, 1253, 8004","1206, 7433, 7569, 8004","$0.00","Glue is a free and open-source application that allows scientists and data scientists to explore relationships within and across related datasets. Glue makes it easy create a wide variety of visualizations (such as scatter plots, bar charts, images) of data, including three dimensional views. What makes Glue unique is its ability to connect datasets together, without merging them into one. Thus, for example, two Earth-based mapping data sets may be connected and jointly visualized by using the coordinates (e.g. latitude and longitude) to glue the maps together, so that when a  user selects (e.g. with a lasso tool) regions in one data set, the corresponding selected subset of data will highlight in all related visualizations simultaneously. These ?linked views"" are especially powerful across wide varieties of plot types. For example, if a user interested in air traffic control glues a data set with information about the 3D locations of all airplanes to a second data set giving weather information, that user could make a combination of selections that would highlight (on maps, in 3D views, or any other display) planes at particular altitudes where thunderstorms might be likely to occur within a specific period of time. In particular, Glue makes it easy for users to create their own kinds of visualizations, which is important because different disciplines often need very specialized ways of looking at data. The software is already being used widely across several disciplines, in particular, astronomy and medicine, for which has been specially optimized. This project will add new features to make Glue more useful in more fields of science (e.g. bioinformatics, epidemiology) where there is demand for linked-view visualization, as well as making it more accessible as an educational tool. In addition, this project will train new users and developers, who will expand Glue into a much more sustainable community effort. <br/><br/>Glue is an open-source package that allows scientists to explore relationships within and across related datasets, by making it easy for them to make multi-dimensional linked visualizations of datasets, select subsets of data interactively or programmatically in 1, 2, or 3 dimensions, and see those selections propagate live across all open visualizations of the data (e.g. graphs, maps, diagnostics charts). A unique feature of glue is that datasets from different sources can be linked to each other, using user-defined mathematical relationships between sets of data components, which makes it possible to carry out selections across datasets. Glue, written in Python, is designed from the ground-up for multidisciplinary work, and it is currently helping researchers make discoveries in geoscience, genomics, astronomy, and medicine. It is also giving insights into data from outside academia, including open data provided by governments and cities. To become sustainable in the long term, glue development needs to become a community-driven effort. Through tutorial and developer workshops, coding sprints, and strategic collaborations with researchers in several disciplines and experienced open source developers, the glue team will help user communities extend glue by developing new functionality useful within particular fields of research. The team will help users contribute the most widely-needed functionality back to glue, and will recruit active contributors to participate in core glue development. As the community grows, glue development will be guided to focus on several major features useful to the broad research community, including: support for very large datasets, support for running glue fully in the browser (inside Jupyter notebooks and Jupyter Lab), and improved interoperability with third-party tools.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria"
"1339839","SI2-SSE: BenchLab: Open Community Tools and Infrastructure for Performance Research in Cloud, Mobile and Green Computing","OAC","Special Projects - CCF, Software Institutes","10/01/2013","08/29/2013","Prashant Shenoy","MA","University of Massachusetts Amherst","Standard Grant","Alan Sussman","09/30/2018","$500,000.00","David Irwin","shenoy@cs.umass.edu","COMMONWEALTH AVE","AMHERST","MA","01003","4135450698","CSE","2878, 8004","7433, 8005","$0.00","The emergence of large-scale Internet applications and services has driven a surge in research on cloud platforms, virtualization, data center architectures, and green computing.  However, realistic performance evaluation of new research prototypes continues to be a major challenge.  Home-grown performance evaluation tools that are often used by researchers are no longer able to capture the complexity of today's real systems and applications.  To address this drawback, the project seeks to develop BenchLab, an open, flexible community infrastructure comprising applications, workloads and tools to enable realistic performance evaluation and benchmarking by systems researchers.  BenchLab is an open framework where source code and workload datasets are freely available for modification and use by researchers for their specific experiments. The framework consists of a suite of server-side benchmark applications and workloads that represent cloud, mobile web, and green computing environments.  BenchLab employs a modular, extensible architecture that is designed to support a range of server applications and workloads, with the ability to add support for newer applications and workloads and retire outdated ones.  BenchLab is designed to be easy to use for experimental systems research ""at scale"" in commercial clouds, such as Amazon EC2, or virtualized clusters in laboratory settings. <br/> BenchLab will provide open tools and workloads for the research community to enable researchers to run larger and more realistic performance evaluation experiments that better emulate today's real-word systems. BenchLab will be incorporated into hands-on lab assignments to teach students the science and art of experimental performance evaluation."
"1562450","Collaborative Research: SI2-SSI: A Sustainable Open Source Software Pipeline for Patient Specific Blood Flow Simulation and Analysis","OAC","Software Institutes, CDS&E","07/01/2015","09/21/2015","Alison Marsden","CA","Stanford University","Standard Grant","Stefan Robila","09/30/2018","$812,810.00","","amarsden@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","CSE","8004, 8084","7433, 8009, 8084","$0.00","The SimVascular package is a crucial research tool for cardiovascular modeling and simulation, and has contributed to numerous advances in personalized medicine, surgical planning and medical device design. SimVascular is currently the only comprehensive software package that provides a complete pipeline from medical image data segmentation to patient specific blood flow simulation and analysis. This software now forms the backbone in cardiovascular simulation research in a small but active group of domestic and international academic labs. However, since its original release there have been several critical barriers preventing wider adoption by new users, application to large-scale research studies, and educational access. These include 1) the cost and complications associated with embedded commercial components, 2) the need for more efficient geometric model construction tools, 3) lack of sustainable architecture and infrastructure, and 4) a lack of organized maintenance. <br/><br/>This project is addressing the above roadblocks through the following aims:  1) create a sustainable and modular open source SimVascular 2.0 project housed at Stanford Simbios? simtk.org, with documentation, benchmarking and test suites, 2) provide alternatives to all commercial components in the first truly open source release of SimVascular, 3) improve the image segmentation methods and efficiency of model construction to enable high-throughput studies, and 4) enhance functionality by merging state of the art research in optimization, flow analysis, and multiscale modeling. The project leverages existing resources and infrastructure at simtk.org, and builds upon the significant previous investment that enabled the initial open source release of SimVascular. Access is further enhanced by cross-linking with the NIH funded Vascular Model Repository. This project will increase the user base and build a sustainable software platform supported by an active open source community.   Releasing the first fully open source version of SimVascular will enable greater advances in cardiovascular medicine, provide open access to state of the art simulation tools for educational purposes, and facilitate training of young investigators. These efforts will also further promote diversity and attract students to science and engineering by leveraging this software to enable high school field trips to the UCSD StarCAVE to view simulation data using virtual reality."
"1550329","SI2-SSI:   Collaborative Research: ParaTreet: Parallel Software for Spatial Trees in Simulation and Analysis","OAC","Software Institutes","09/01/2016","08/19/2016","Orion Lawlor","AK","University of Alaska Fairbanks Campus","Standard Grant","Rob Beverly","08/31/2018","$54,311.00","","lawlor@alaska.edu","2145 N. TANANA LOOP","FAIRBANKS","AK","997750001","9074747301","CSE","8004","7433, 8004, 8009, 9150","$0.00","Many scientific and visualization methods involve organizing the data they are processing into a hierarchy (also known as a ""tree"").   These applications and methods include: astronomical simulations of particles moving under the influence of gravity, analysis of spatial data (that is, data that describes objects with respect to their relative position in space), photorealistic rendering of virtual environments,reconstruction of surfaces from laser scans, collision detection when simulating the movement of physical objects, and many others.   Tree data structures, and the algorithms used to work on these structures, are heavily used in these applications because they help to make these applications run much faster on supercomputers. However, implementing tree-based algorithms can require a significant effort, particularly on modern highly parallel computers.  This project will create ParaTreet, a software toolkit for parallel trees, that will enable rapid development of such applications.  Details of the parallel aspects will be hidden from the programmer, who will be able to quickly evaluate the relative merits of different trees and algorithms even when applied to large datasets and very computation-intensive applications. The combination of such an abstract and extensible framework with a portable adaptive runtime system will allow scientists to effectively use parallel hardware ranging from small clusters to petascale-class machines, for a wide variety of tree-based applications. This project will demonstrate the feasibility of such an approach as well as generate evidence of community adoption of this technology. If successful, this project will enable NSF-supported researchers to solve science problems faster as well as to tackle more complex problems, thus serving NSF's science mission.<br/><br/><br/>This project builds upon an existing collaboration on Computational Astronomy and the resultant software base in the ChaNGa (Charm N-body GrAvity solver) code. ChaNGa is a software package that performs collisionless N-body simulations, and can perform cosmological simulations with periodic boundary conditions in co-moving coordinates or simulations of isolated stellar systems. This project will extend ChaNGa with a parallel tree toolkit called ParaTreet and associated applications, that will allow scientists to effectively utilize small clusters as well as very large supercomputers for parallel tree-based calculations.  The key data structure in ParaTreet is an asynchronous software-based tree data cache, which maintains a writeback local copy of remote tree data. We plan to support a variety of spatial decomposition methods and the associated trees, including Oct-trees, KD-trees, inside-outside trees, ball trees, R-trees, and their combinations. Different trees are useful in different application circumstances, and the software will allow their relative merits to be evaluated with relative ease. The framework will support a variety of parallel work decomposition methods, including those based on space filling curves, and support dynamic rearrangement of parallel work at runtime. The algorithms supported will range from Barnes-Hut with various multipole expansions, data clustering, collision detection, surface reconstruction, ray intersection, etc. The software includes a collection of dynamic load balancing strategies in the Charm++ framework that can be tuned for specific problem structures. It also includes support for clusters of accelerators, such as GPGPUs. This project will demonstrate the feasibility of such an approach as well as generate evidence of community adoption of this technology."
"1535130","SI2-SSE: A Software Element for Neutrino Radiation Hydrodynamics in GenASiS","OAC","COMPUTATIONAL PHYSICS, Software Institutes","09/01/2015","08/12/2015","Reuben Budiardja","TN","University of Tennessee Knoxville","Standard Grant","Bogdan Mihaila","08/31/2019","$432,488.00","Eirik Endeve, Christian Cardall, Anthony Mezzacappa","reubendb@utk.edu","201 ANDY HOLT TOWER","KNOXVILLE","TN","379960001","8659743466","CSE","1798, 7244, 8004","1206, 7433, 8005, 8084, 9150","$0.00","Multi-physics computational modeling is an integral part of the scientific study of many complex natural phenomena. These phenomena often involve the physics of radiation transport. For example, neutrino radiation hydrodynamics is a key element of the physics governing environments with hot and dense nuclear matter. Such extreme environments include the Early Universe, during primordial nucleosynthesis of light nuclei such as hydrogen through lithium just after the Big Bang; the merger through inspiraling of neutron star-neutron star or neutron star-black hole binaries; and the death throes of massive stars, more than ten times the mass of the Sun, in stellar explosions known as core-collapse supernovae, which are responsible for elements such as oxygen and calcium without which life as we know it would not exist. Radiation transport and kinetic theory of particles besides neutrinos---photons, electrons, or neutrons---are also relevant to many areas of astrophysics, as well as a broad range of other science applications, including materials science, plasma physics, neutron transport, multiphase flows, and high-energy-density physics. As such, the availability of a software element to solve radiation transport problems is highly valuable to researchers.<br/><br/>This project will create and deploy a software element to solve radiation hydrodynamics problems on modern supercomputers featuring ""hybrid"" architectures that include traditional CPUs plus ""accelerators"" or ""coprocessors,"" such as GPUs or Intel Many Integrated Core processors, respectively. This radiation hydrodynamics functionality will be developed within GenASiS (General Astrophysical Simulation System), a new framework being developed to facilitate the simulation of astrophysical phenomena on the world's leading capability supercomputers. In particular, the radiation transport solver will utilize the extant capabilities of GenASiS for adaptive computational ""mesh refinement,"" whereby the representation of the natural continuum is captured adaptively on a mesh of points foundational to any computational model in order to maximize the fidelity of the computational model for a given computational cost. We will use the so-called M1 approach, solving directly for the zeroth and first angular moments (energy density and momentum) of the radiation field, with higher-order moments given by ""closure relations,"" expressing them in terms of the zeroth and first moments. The energy dependence of the radiation field will be retained, with the zeroth and first angular moments discretized into ""energy bins."" Our computational approach to neutrino radiation transport will be an ""implicit-explicit"" (IMEX) scheme. Interactions between radiation and matter will be handled with a time-implicit subsolver, which will involve the inversion of dense matrices local to each node of the machine to exploit all available hardware in the node, including accelerators and coprocessors when available.  Algorithms and software resulting from this project will be made available to the community. GenASiS, as an extensible, object-oriented simulation framework, will be valuable to researchers seeking to experiment with and implement different kinds of solvers for multi-physics problems. In particular, the neutrino hydrodynamics solver developed in this project is of high interest to astrophysics modelers."
"1148188","Collaborative Research:  SI2-SSI:  Open Source Support for Massively Parallel, Generic Finite Element Methods","OAC","Software Institutes","08/01/2012","08/09/2012","Michael Heroux","MN","College of Saint Benedict","Standard Grant","Stefan Robila","07/31/2018","$181,586.00","","mheroux@csbsju.edu","37 COLLEGE AVE S","SAINT JOSEPH","MN","563742001","3203635690","CSE","8004","7433, 8004, 8009","$0.00","Partial differential equations are used in a wide variety of applications as<br/>mathematical models. Their numerical solution is, consequently, of prime<br/>importance for the accurate simulation and optimization of processes in the<br/>sciences, engineering, and beyond.<br/>The last decade saw the emergence of large and successful libraries that<br/>support such applications. While these libraries provide most of what such<br/>codes need for small-scale computations, many realistic applications yield<br/>problems of hundreds of millions or billions of unknowns and require clusters<br/>with thousands of processor cores, but there is currently little generic<br/>support for such problems, limiting access to the many large publicly<br/>supported computing facilities to experts in computational science and<br/>excluding scientists from many fields for whom computational simulation would<br/>be a useful tool.  This project intends to build the software infrastructure that will allow a<br/>wide cross section of scientists to utilize these large resources.<br/><br/><br/>This project intends to support the software infrastructure for the<br/>large-scale solution of partial differential equations on massively parallel<br/>computational resources in a generic way. It will build on two of the most<br/>successful libraries for scientific computing, the finite element library<br/>deal.II, and Trilinos that provides the parallel linear algebra capabilities<br/>for the former. Specifically, we will: (i) Make support for massively parallel<br/>computations ubiquitous in deal.II; (ii) Research and develop seamless support<br/>for problems with billions of unknowns in both libraries and improve the<br/>interaction between the two; (iii) Exploit intra-node parallelism on today's<br/>clusters; (iv) Ensure the applicability of our work on a broad basis by<br/>implementing two real-world applications. <br/>Both deal.II and Trilinos have large, active and diverse developer and user<br/>communities, and this project will actively engage these communities through<br/>user meetings, short courses, regularly taught classes, mailing lists, and<br/>direct contact in focused projects."
"1440571","Collaborative Research:  SI2-SSE:  Pythia Network Diagnosis Infrastructure (PuNDIT)","OAC","Software Institutes, Campus Cyberinfrastructure","09/01/2014","08/14/2014","Shawn McKee","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Rajiv Ramnath","08/31/2017","$231,187.00","","smckee@umich.edu","503 THOMPSON STREET","ANN ARBOR","MI","481091340","7347636438","CSE","8004, 8080","7433, 8005","$0.00","In today's world of distributed collaborations of scientists, there are many challenges to providing effective infrastructures to couple these groups of scientists with their shared computing and storage resources. The Pythia Network Diagnostic InfrasTructure (PuNDIT) project will integrate and scale research tools and create robust code suitable for operational needs to address the difficult challenge of automating the detection and location of network problems. <br/><br/>PuNDIT will build upon the de-facto standard perfSONAR network measurement infrastructure to gather and analyze complex real-world network topologies coupled with their corresponding network metrics to identify possible signatures of network problems from a set of symptoms. For example if the symptoms suggest a router along the path has buffers configured too small for high performance, Pythia will return a diagnosis of ""Small Buffer"". If symptoms indicate non-congestive packet-loss for a particular network segment, the user can be notified of a possible ""Bad Network Segment"". A primary goal for PuNDIT is to convert complex network metrics into easily understood diagnoses in an automated way."
"1440530","SI2-SSE: LC/DC: Lockless Containers and Data Concurrency","OAC","Special Projects - CCF, Software Institutes","01/01/2015","02/03/2017","Damian Dechev","FL","The University of Central Florida Board of Trustees","Standard Grant","Bogdan Mihaila","12/31/2018","$516,000.00","","dechev@cs.ucf.edu","4000 CENTRAL FLORIDA BLVD","ORLANDO","FL","328168005","4078230387","CSE","2878, 8004","7433, 8004, 8005, 9251","$0.00","Multicore programming demands a change in the way we design and use fundamental algorithms and data structures. This research represents a forward-looking and pragmatic approach that will lead to the discovery of the key principles for effective data and resource management for multiprocessor application development. In the course of this project the PI will create new methodologies and tools for the design, verification, and effective use of lock-free and wait-free data structures and algorithms. The proposed methodology will allow for the construction and use of lightweight multiprocessor algorithms with minimal overhead by supporting only the minimal set of operations and guarantees required by the user's application. The ideas advanced in this work will allow first-of-a-kind technology that will deliver immense boost in performance and software reuse; thus productivity will increase for developers of commercial and scientific applications. This research will pave the way for tool-based specification and verification of nonblocking algorithms, which will help reliability of multiprocessor programs. <br/><br/>This work will create novel multiprocessor data structures that provide wait-free and lock-free progress. A concurrent object is lock-free if it guarantees that some thread makes progress. A wait-free algorithm guarantees that all threads make progress, thus eliminating performance bottlenecks and entire classes of safety hazards such as starvation, deadlock, and order violations. Unlike a sequential data structure, a concurrent container must maintain correctness when multiple threads are performing its operations. Achieving this correctness adversely affects the complexity and performance of the operations. As a result, users of concurrent containers are often forced to sacrifice functionality or safety guarantees to achieve desired performance. Here, the PI will introduce the use of alternative function models that will deliver high performance in parts of the program that require less functionality and more functionality in other fragments of the program that need it. The deliverables of this research include: a collection of formally verified multiprocessor data structure designs including queues, vectors, ring buffers, sets, and hash maps; a wait-free database; a multiple resource lock manager; a set of unified concurrent APIs to assist the end users of the data structures; and a technique for specification of the key progress and correctness properties of these containers. All software developed under this project will be released under BSD License and will be made available to the broad research and development community."
"1148331","SI2-SSE: Interdisciplinary Software Infrastructure for Differential Geometry, Lie Theory and their Applications","OAC","OFFICE OF MULTIDISCIPLINARY AC, DYNAMICAL SYSTEMS, Software Institutes, CDS&E-MSS","06/01/2012","05/29/2012","Ian Anderson","UT","Utah State University","Standard Grant","Rajiv Ramnath","05/31/2016","$360,845.00","Charles Torre","ian.anderson@usu.edu","1000 OLD MAIN HILL","LOGAN","UT","843221000","4357971226","CSE","1253, 7478, 8004, 8069","1253, 7433, 7478, 7683, 8004, 8005, 9150","$0.00","The goal of this proposal is to further develop symbolic software for the field of differential geometry and those areas of mathematics, physics and engineering where <br/>differential geometry plays an essential role. The proposed work will: provide new functionalities requested by the user community, complete packages currently under <br/>development, redesign critical components for improved computational efficiency, develop upgrades of existing algorithms and code whose performance does not support the demands of research. Specific objectives include [1] the development of a new coordinate-free computational environment for work with abstract differential forms and for tensor analysis on homogeneous spaces; [2] software for the structure theory of real/complex Lie algebras and their representations; [3] implementation of the theory of Young tableaux for tensors with symmetry to address resource and performance problems arising in large tensor computations; [4] new programs for symbolic computations for sub-manifold theory in Riemannian geometry, complex manifolds and Kahler geometry, and symplectic geometry; [5] a comprehensive new package for exterior differential systems; and [6] expansion of various data-bases of Lie algebras, differential equations, and exact solutions in general relativity.<br/><br/>Of all the core disciplines in mathematics, differential geometry is unique in that it interfaces with so many other subjects in pure mathematics, applied mathematics, physics, engineering, and even computer science. The PI's DifferentialGeometry (DG) software package has laid the foundation for a single, unified symbolic computational environment for research and teaching in differential geometry and its many application areas. The goal of this proposal is to add new computational environments to address specific application needs, to add basic functionalities that will bring various sub-packages to maturity, to upgrade routines with performance limitation, and significantly extend the DG data-bases of Lie algebras, group actions, integrable systems, and solutions of the Einstein equations. Earlier versions of this software have established a significant user community. Community feedback has dictated much of the specific program agenda in this proposal. A unique partnership between Utah State University and Maplesoft insures that the DG software meets the high standards of reliability, ease of use, documentation and support, and longevity that a extended user community (with diverse levels of symbolic computational experience) demands.<br/><br/>While originially designed as a research tool, DG also provides an innovative approach to teaching differential geometry and its applications in the classroom. All developments in DG are implemented with this in mind.<br/><br/>The PI will host a workshop at Utah State University entitled: Symbolic Methods in Differential Geometry, Lie Theory and Applications. This workshop will consist of <br/>hands-on training sessions, and lectures on applications of symbolic methods to problems in  differential geometry. This workshop will also provide an ideal <br/>venue to survey participant research interests to drive future code development.<br/><br/>Student involvement at the undergraduate and graduate levels is an important component of this project. The experience gained in working with computer algebra systems in general, and differential geometry in particular, is valuable to the student for future educational activities and/or future employment."
"1550423","SI2-SSI: Collaborative Research: A Robust High-Throughput Ab Initio Computation and Analysis Software Framework for Interface Materials Science","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes, DMREF","09/01/2016","08/31/2016","Yifei Mo","MD","University of Maryland, College Park","Standard Grant","Rob Beverly","08/31/2020","$300,000.00","","yfmo@umd.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","CSE","1253, 1712, 8004, 8292","7237, 7433, 7569, 8004, 8009, 8400, 9216","$0.00","A set of techniques scientists call ""ab initio"" methods, which are derived from the fundamental laws of physics with minimal assumptions and approximations, has become a critical tool in the study and design of materials. With computing advances and software innovations, the automation of high-throughput ab initio calculations has, in particular, heralded an explosion of computed data for a large variety of materials. However, these high-throughput efforts are limited to specific properties. In contrast, materials interfaces, one of the fastest growing research areas in materials science and engineering, are showing an increasing relevance in many areas of materials applications such as catalysis and electronics. This project will develop a software framework that enables novel high-throughput interface materials investigations and design. The developed software platform will expand the genome of materials by including the computed interfacial properties of interface materials. This community-based software can potentially become a critical component of the Materials Genome Initiative and serve not just the large and diverse materials research community, but also the physics and chemistry communities. Besides featuring heavily in existing and planned courses taught by the Principal Investigators in their home institutions, the proposed framework will facilitate the training of undergraduates and graduates in the ab initio methodologies in other institutions as well. This project will also conduct public outreach activities to increase awareness of the importance of sustainable software development for data-driven interface materials science. <br/><br/>The project will develop necessary workflow management, error correction schemes, and systematic analysis tools to support ab initio studies of thermodynamics, kinetics, diffusion, and electronic property of interface materials including hetero-structures and grain boundary. It targets developmental efforts on three key focus areas of great interest to interface materials science: (i) Ab initio thermodynamics of surfaces and interfaces; ii) Advanced methods for materials kinetics and diffusion at materials interfaces; and iii) Automated algorithms for structural construction of grain boundary and post data-processing and analysis. In doing so, this project will greatly expand the suite of interfacial materials properties that are amenable to a high-throughput ab initio treatment, paving the way for materials investigations and design in a broad spectrum of technological applications, including energy generation and storage, catalysis and electronics. In addition, by interfacing with classical-mechanics simulation codes, this framework will bridge the gap between the ab initio and classical force-field approach, which is expected to significantly advance the high-throughput simulations of materials interfaces. <br/><br/>This award by the Advanced Cyberinfrastructure Division is jointly supported by the NSF Directorate for Mathematical and Physical Sciences (Division of Materials Research and Office of Multidisciplinary Activities)."
"1450170","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","OAC","Physical & Dynamic Meteorology, Software Institutes, EarthCube","08/01/2015","08/04/2015","William Capehart","SD","South Dakota School of Mines and Technology","Standard Grant","Bogdan Mihaila","07/31/2019","$183,956.00","","William.Capehart@sdsmt.edu","501 E SAINT JOSEPH ST","RAPID CITY","SD","577013901","6053941218","CSE","1525, 8004, 8074","4444, 7433, 8009, 9150","$0.00","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities."
"1535065","SI2-SSE: Collaborative Research: An Intelligent and Adaptive Parallel CPU/GPU Co-Processing Software Library for Accelerating Reactive-Flow Simulations","OAC","CFS-Combustion & Fire Systems, Software Institutes","09/01/2015","05/08/2018","Kyle Niemeyer","OR","Oregon State University","Standard Grant","Bogdan Mihaila","08/31/2020","$314,287.00","","kyle.niemeyer@oregonstate.edu","1500 SW JEFFERSON ST","CORVALLIS","OR","973318655","5417374933","CSE","1407, 8004","026Z, 148E, 7433, 8004, 8005, 9251","$0.00","In order to develop the next generation of clean and efficient vehicle engines and power-generating combustors, engineers need the next generation of computational modeling tools. Accurately describing the chemistry of conventional and alternative liquid transportation fuels is vital to predict harmful emission levels and other important quantities, but the high computational cost of detailed models for chemistry poses a significant barrier to use by designers. In order to use such accurate models, software is needed that can efficiently handle chemistry in practical simulations. This collaborative project aims to develop such tools, employing the computational power of modern parallelized central processing units (CPUs) and graphics processing units (GPUs). In addition to helping designers create clean and efficient engine technology, the advances made in this project are widely applicable to other computational modeling problems including astrophysics, nuclear reactions, atmospheric chemistry, biochemical networks, and even cardiac electrophysiology.<br/><br/>The objective of the proposed effort is to develop software elements specifically targeted at co-processing on GPUs, CPUs, and other many-core accelerator devices to reduce the computational cost of using detailed chemistry and enable high-fidelity yet affordable reactive-flow simulations. This will be achieved by (1) developing and comparing chemical kinetics integration algorithms for parallel operation on CPUs and GPUs/accelerators, (2) developing a method for detecting local stiffness due to chemical kinetics and adaptively selecting the most efficient solver based on available hardware, (3) implementing a computational cell clustering strategy to group similar spatial locations, (4) demonstrating the improved performance offered by these software elements using commercial and open-source computational fluid dynamics codes for modeling reactive flows, and (5) designing a portable and sustainable software library based on the above software elements, including building a community of users. The result of this program will be an open source software library that significantly decreases the cost of using detailed, accurate chemistry in realistic combustion simulations; the success of the program will be determined based on achieving order-of-magnitude performance improvement or better."
"1047828","Collaborative Research: SI2-SSI: Development of an Integrated Molecular Design Environment for Lubrication Systems (iMoDELS)","OAC","NANOSCALE: INTRDISCPL RESRCH T, Software Institutes","10/01/2011","07/23/2017","Peter Cummings","TN","Vanderbilt University","Continuing Grant","Amy Walton","09/30/2018","$2,542,681.00","Clare McCabe, Gabor Karsai, Akos Ledeczi","peter.cummings@vanderbilt.edu","110 21ST AVE S","NASHVILLE","TN","372032416","6153222631","CSE","1674, 8004","1674, 8004, 9150","$0.00","This project is focused on developing, deploying and distributing the Integrated Molecular Design Environment for Lubrication Systems (iMoDELS), an open-source simulation and design environment (SDE) that encapsulates the expertise of specialists in first principles, forcefields and molecular simulation related to nanoscale lubrication in a simple web-based interface.  The iMoDELS SDE is being developed using model-integrated computing (MIC), a state-of-the-art powerful, well-established, extensible, community-supported, and application-hardened software engineering framework that supports scientific and engineering workflows.  Making iMoDELS broadly accessible is motivated by the high cost (over $800B/yr in the US) of friction and wear, which, along with the methodology to overcome them, lubrication, are collectively known as tribology.  Tribology involves molecular mechanisms occurring on a nanometer scale, and hence understanding tribological behavior on this scale is critical to developing new technologies for reducing wear due to friction.  Deployment of iMoDELS will enable non-computational specialists to be able to evaluate, design and optimize nanoscale lubrication systems, such as hard disk drives, NEMS (nanoelectromechanical systems) and MEMS (microelectromechanical systems), and experiments involving rheological measurements via atomic force microscopes (AFMs) and surface force apparatuses (SFAs).<br/><br/>The iMoDELS SDE brings together a unique combination of materials and computer scientists who will combine their skills to abstract the deep human expertise currently required for the development of simulation-based experiments, thus making broadly available easy-to-use tools to an empirically driven area of science and engineering (nanotribology) of rapidly growing technological importance.  iMoDELS includes the creation and open dissemination of forcefield and simulation results databases that will benefit the simulation community worldwide and catalyze broad-based activity in this area.  The proposed research also includes the interdisciplinary training of undergraduate and graduate students, as well as postdoctoral researchers at the interface of tribology, computational materials sciences, and computer science.  The PIs will use the iMoDELS SDE, and results from it, in presentations used in outreach to local area high school and in classes given to undergraduate students.  The iMoDELS SDE will be vigorously promoted through workshops and presentations at national conferences, and via the dedicated website for development and dissemination."
"1339757","SI2-SSE:  Collaborative Research:  Software Elements for Transfer and Analysis of Large-Scale Scientific Data","OAC","Special Projects - CCF, Software Institutes","09/01/2013","08/29/2013","Gagan Agrawal","OH","Ohio State University","Standard Grant","Rob Beverly","08/31/2017","$400,000.00","","gagrawal@augusta.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","CSE","2878, 8004","7433, 8005","$0.00","As science has become increasingly data-driven, and as data volumes and velocity are increasing,  scientific advance in many areas will  only be feasible if critical `big-data' problems are addressed  - and even more importantly,  software tools embedding these solutions are readily available to the scientists. Particularly, the major challenge being faced by current data-intensive scientific research efforts    is that while the dataset sizes continue to grow rapidly, neither among network bandwidths,   memory capacity of parallel machines,   memory access speeds,    and disk bandwidths are increasing at the same rate.<br/><br/>Building on top of recent research at Ohio State University, which includes work on automatic data virtualization, indexing methods for scientific data,   and a novel bit-vectors based sampling method, the goal of this project is to fully develop, disseminate, deploy, and support  robust  software elements addressing challenges in data transfers and analysis.   The prototypes that have been already developed at Ohio State are being extended into two  robust   software elements: <br/><br/>Software Element 1:   GridPFTP (Grid Partial-File Transport Protocol):   An Extention to GridFTP: an extention of GridFTP that allows users to specify a subset of the file to be transferred, avoiding unnecessary transfer of the entire file.<br/>Software Element 2: Parallel Readers  for NetCDF and HDF5 for Paraview and VTK:  Data subsetting and sampling tools for  NetCDF and  HDF5 that perform data selection and  sampling at  the I/O level, and in parallel.<br/><br/>This project impacts a number of scientific areas,   i.e., any area that involves   big (and growing) dataset sizes and  need for data transfers and/or visualization.  This project also contributes to computer science research in `big data',  including scientific (array-based)  databases,  and visualization.  Another contribution will be towards preparation  of the broad science and engineering research community  for big data handling and analytics."
"1663688","Collaborative Research: SI2-SSI: Modules for Experiments in Stellar Astrophysics","OAC","STELLAR ASTRONOMY & ASTROPHYSC, OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","05/01/2017","06/02/2020","Lars Bildsten","CA","University of California-Santa Barbara","Standard Grant","Rob Beverly","04/30/2023","$826,773.00","","bildsten@itp.ucsb.edu","3227 CHEADLE HALL","SANTA BARBARA","CA","931060001","8058934188","CSE","1215, 1253, 7244, 8004","1206, 7433, 7569, 8004, 8009, 8084","$0.00","Stars are the most commonly observed celestial objects, and remain at the forefront of astronomical research.  The goal of this project is to sustain the MESA software framework as a key piece of software infrastructure for the astronomy community while building new innovative scientific capabilities and educational networks. MESA (Modules for Experiments in Stellar Astrophysics) is a software instrument that solves the equations governing the evolution of stars.  The MESA project 1) has attracted over 900 registered users world-wide; 2) has over 10,000 downloads of the source code; 3) has received over 12,000 archived and searchable posts about community discussions of MESA; 4) has been cited about 1000 times and has a current citation rate of about 300/year; 5) papers that cite MESA have themselves generated over 10,000 citations; 6) provides a Software Development Kit to build MESA across a variety of platforms; 7) delivers an annual Summer School program that now has over 150 graduates; 8) hosts a web-portal for the astronomy community to share knowledge and tools; 9) offers a prototype of a cloud resource for education, MESA-Web.  These metrics provide evidence that MESA is becoming standard software for understanding evolving stars.  <br/><br/>This project supports the MESA software framework and its user community. MESA provides solvers for one-dimensional fully coupled structures and composition equations governing stellar evolution. It is based on an implicit finite volume scheme with adaptive mesh refinement and sophisticated time-step controls; state-of-the-art modules provide equation of state, opacity, nuclear reaction rates, element diffu- sion, boundary conditions, and changes to the mass of the star. MESA is an open source library that employs contemporary numerical approaches, supports shared memory parallelism based on OpenMP, and is designed with present and future multi-core and multi-thread architectures in mind. MESA combines composable, interoperable, robust, efficient, thread-safe numerical and physics modules for provenance-capable simulations of a wide range of stellar evolution scenarios ranging from giant planets to low mass single stars to massive star binaries. MESA?s domain of applicability continues to grow, with recent extensions enabling users to model oscillations, rota- tion, binary stars, and explosions. Recent technical innovations allow for user plugins and provide bit-for-bit consistent results across all supported platforms.  The products of the MESA project has driven and will continue to drive significant innovation in the stellar, gravitational wave, nuclear, exoplanet, galactic, and cosmological communities."
"1148359","Collaborative Research SI2 SSE: Pipeline Framework for Ensemble Runs on Clouds","OAC","Software Institutes","05/01/2012","05/07/2012","Beth Plale","IN","Indiana University","Standard Grant","Daniel Katz","04/30/2014","$292,863.00","","plale@cs.indiana.edu","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","CSE","8004","8004, 8005","$0.00","Cloud computing is an attractive computational resource for e-Science because of the ease with which cores can be accessed on demand, and because the virtual machine implementation that underlies cloud computing reduces the cost of porting a numeric or analysis code to a new platform. It is difficult to use cloud computing resources for large-scale, high throughput ensemble jobs however. Additionally, the computationally oriented researcher is increasingly encouraged to make data sets available to the broader community. For the latter to be achieved, using capture tools during experimentation to harvest metadata and provenance reduces the manual burden of marking up results. Better automatic capture of metadata and provenance is the only means by which sharing of scientific data can scale to meet the burgeoning explosion of data.<br/><br/>This project develops a pipeline framework for running ensemble simulations on the cloud; the framework has two key components: ensemble deployment and metadata harvest. Regarding the former, on commercial cloud platforms typically a much smaller number of jobs than desired can be started at any one time. An ensemble run will need to be pipelined to a cloud resource, that is, executed in well-controlled batches over a period of time. We will use platform features of Azure, and employ machine learning techniques to continuously refine the pipeline submission strategy and workflow strategies for ensemble parameter specification, pipelined deployment, and metadata capture. Regarding the latter key component, we expect to reduce the burden of sharing scientific datasets resulting from the use of cloud resources through automatic metadata and provenance capture and representation that aligns the metadata with emerging best practices in data sharing and discovery. Ensemble simulations result in complex data sets, whose reuse could be increased by expressive, granule and collection level metadata, including the lineage of the resulting products, to contribute towards trust.<br/><br/>In this project we focus on a compelling and timely application from climate research: One of the more immediate and dangerous impacts of climate change could be a change in the strength of storms that form over the oceans. In addition, as sea level rises due to global warming and melting of the polar ice caps, coastal communities will become increasingly vulnerable to storm surge. There have already been indications that even modest changes in ocean surface temperature can have a disproportionate effect on hurricane strength and the damage inflicted by these storms. In an effort to understand these impacts, modelers turn to predictions generated by hydrodynamic coastal ocean models such as the Sea, Lake and Overland Surges from Hurricanes (SLOSH) model. The proposed research advances the knowledge and understanding of probabilistic storm surge products by enhancements to the SLOSH model itself and through mechanisms that take advantage of commercial cloud resources. This knowledge is expected to have application in research, the classroom, and in operational settings.<br/><br/>The broader significance of the project is several-fold. Cloud computing is an important economic driver but it remains difficult for use in computationally driven scientific research. This project lowers the barriers to conducting e-Science research that utilizes cloud resources, specifically Azure. It will contribute tools to help researchers share, preserve, and publicize the scientific data sets that result from their research. Because we focus on and improve an application that predicts storm surge in response to sea level changes and severe storms, our work contributes to societal responses and adaptations to climate change, including planning and building the sustainable, hazard-resilient coastal communities of the future."
"1339822","SI2-SSI: Collaborative Research: Sustained Innovation for Linear Algebra Software (SILAS)","OAC","OFFICE OF MULTIDISCIPLINARY AC, Special Projects - CCF, Software Institutes, CDS&E-MSS","10/01/2013","08/25/2015","Jack Dongarra","TN","University of Tennessee Knoxville","Continuing Grant","Rajiv Ramnath","09/30/2016","$1,427,955.00","","dongarra@icl.utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","1253, 2878, 8004, 8069","7433, 8009, 9150","$0.00","As the era of computer architectures dominated by serial processors comes to a close, the convergence of several unprecedented changes in processor design has produced a broad consensus that much of the essential software infrastructure of computational science and engineering is utterly obsolete. Math libraries have historically been in the vanguard of software that must be quickly adapted to such design revolutions because they are the common, low-level software workhorses that do all the most basic mathematical calculations for many different types of applications. The Sustained Innovation for Linear Algebra Software (SILAS) project updates two of the most widely used numerical libraries in the history of Computational Science and Engineering---LAPACK and ScaLAPACK, (abbreviated Sca/LAPACK)---enhancing and hardening them for this ongoing revolution in processor architecture and system design. SILAS creates a layered package of software components, capable of running at every level of the platform deployment pyramid, from the desktop to the largest supercomputers in the world. It achieves three complementary objectives: 1) Wherever possible, SILAS delivers seamless access to the most up-to-date algorithms, numerical implementations, and performance, by way of Sca/LAPACK programming interfaces that are familiar to many computational scientists; 2) Wherever necessary, SILAS makes advanced algorithms, numerical implementations and performance capabilities available through new interface extensions; and 3) SILAS provides a well engineered conduit through which new discoveries at the frontiers of research in these areas can be channeled as quickly as possible to all the application communities that depend on high performance linear algebra. The improvements and innovations included in SILAS derive from a variety of sources. They represent the results (including designs and well tested prototypes) of the PIs' own algorithmic and software research agenda, which has targeted multicore, hybrid and extreme scale system architectures. They are an outcome of extensive and on-going interactions with users, vendors, and the management of large NSF and DOE supercomputing facilities. They flow from cross-disciplinary engagement with other areas of computer science and engineering, anticipating the demands and opportunities of new architectures and programming models. And finally, they come from the enthusiastic participation of the research community in developing and offering enhanced versions of existing Sca/LAPACK codes.<br/><br/>The primary impact of SILAS is a direct function of the importance of the Sca/LAPACK libraries to many branches of computational science. The Sca/LAPACK libraries are the community standard for dense linear algebra and have been adopted and/or supported by a large community of users, computing centers, and HPC vendors. Learning to use them is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. Application domains where Sca/LAPACK have historically been heavily used include (among a host of other examples) airplane wing design, radar cross-section studies, flow around ships and other off-shore constructions, diffusion of solid bodies in a liquid, noise reduction, and diffusion of light through small particles. Moreover, the list of application partners working with SILAS to enhance and transform these libraries for next generation platforms expands this traditional list to include quantum chemistry, adaptive mesh refinement schemes, computational materials science, geophysical flows, stochastic simulation and database research for ""big data"". No other numerical library can claim this breadth of integration with the community. Thus, there is every reason to believe that enhancing these libraries with state of the art methods and algorithms and adapting them for new and emerging platforms (reaching up to extreme scale), will have a correspondingly large impact on the research and education community, government laboratories, and private industry."
"1440677","SI2-SSE: RADICAL Cybertools: Scalable, Interoperable and Sustainable Tools for Science","OAC","Leadership-Class Computing, Software Institutes","01/01/2015","11/18/2019","Shantenu Jha","NJ","Rutgers University New Brunswick","Standard Grant","Bogdan Mihaila","12/31/2020","$526,915.00","Matteo Turilli","shantenu.jha@rutgers.edu","3 RUTGERS PLZA","NEW BRUNSWICK","NJ","089018559","8489320150","CSE","7781, 8004","026Z, 7433, 8004, 8005, 9251","$0.00","To support science and engineering applications that are the basis of many societal and intellectual challenges in the 21st century, there is a need for comprehensive, balanced, and flexible distributed cyberinfrastructure (DCI).  The process of designing and deploying such large-scale DCI, however, presents a critical and challenging research agenda.  One specific challenge is to produce tools that provide a step change in the sophistication of problems that can be investigated using DCI, while being extensible, easy to deploy and use, as well as being compatible with a variety of other established tools.  RADICAL Cybertools will meet these requirements by providing an abstractions-based suite of well-defined capabilities that are architected to support scalable, interoperable and sustainable science on a range of high-performance and distributed computing infrastructure.  RADICAL Cybertools builds upon important theoretical advances, production-software-development best practices, and carefully-analyzed usage and programming models.  RADICAL Cybertools is posed to play a role in grand-challenge problems, ranging from personalized medicine and health to understanding long-term global and regional climate.  All software developed through the project will be open source and will be licensed under the MIT License (MIT).  Version control on the SVN repository will be accessible via http://radical.rutgers.edu.<br/><br/>Existing and current utilization of RADICAL Cybertools is built upon preliminary research prototypes of RADICAL Cybertools.  There is a significant difference, however, in the quality and capability required to support scalable end-usage science, compared to that of a research prototype.  It is the aim of this project to bridge this gap between the ability to serve as a research prototype versus the challenges of supporting scalable end-usage science.  This will be achieved by addressing existing limitations of usability, functionality, and scalability.  We will do so by utilizing conceptual and theoretical advances in the understanding of distributed systems and middleware, resulting in a scalable architecture and robust design.  We will employ advances in performance engineering, data-intensive methods, and cyberinfrastructure to deliver the next generation of RADICAL Cybertools.  This project will take the existing research prototypes of RADICAL Cybertools to the next level towards becoming a hardened, extensible, and sustainable tool that will support a greater number of users, application types, and resource types."
"1550486","Collaborative Research:  SI2-SSI: Sustaining Innovation in the Linear Algebra Software Stack for Computational Chemistry and Other Sciences","OAC","Software Institutes","07/15/2016","07/26/2016","Tze Meng Low","PA","Carnegie-Mellon University","Standard Grant","Amy Walton","06/30/2019","$265,000.00","","lowt@andrew.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","CSE","8004","7433, 8004","$0.00","Scientific discovery now often involves computer simulation in addition to, or instead of, laboratory experimentation. This can accelerate, improve, and/or expand scientific insight, often at a great reduction in cost. Many such computer simulations spend much or most of their time solving linear algebra (matrix) problems. For these simulations, linear algebra problems constitute the most basic building blocks of the computation. As a result, software libraries (bundles of specialized code) that efficiently solve linear algebra problems fundamentally support sustained innovation in science. The project aims to create a next generation of software libraries for this domain and will make these libraries available to the scientific community as open source software that can be easily ported to current and future computer architectures. This will directly and indirectly impact discovery in academia, at the national labs, and in industry. The project will also impact affordable education through open course ware that is expected to reach a broad audience. The involvement of undergraduate and graduate students will strengthen the pool of qualified individuals trained to support scientific computing.  The project involves research staff and students who are members of traditionally underrepresented groups.<br/><br/><br/>The BLAS (Basic Linear Algebra Subprograms) are well-known routines that provide standard building blocks for performing basic vector and matrix operations. The Level 1 BLAS perform scalar, vector and vector-vector operations, the Level 2 BLAS perform matrix-vector operations, and the Level 3 BLAS perform matrix-matrix operations. Because the BLAS are efficient, portable, and widely available, they are commonly used in the development of high quality linear algebra software, such as the well-known Linear Algebra PACKage (LAPACK), as an example. However, the BLAS libraries that exist today have not evolved to new computing architectures, and hence do not perform as well as they could. The technical goal and scope of this project, therefore, is to develop a new high-performance dense linear algebra library with broad functionality that can be easily ported to current and future multi-core and many-core processors. The project builds on the BLAS-like Library Instantiation Software (BLIS) effort that has exposed low-level primitives that facilitate the high-performance implementation of BLAS. By implementing the higher-level dense linear algebra functionality in terms of these low-level primitives, portable high performance will be achieved for higher-level functionality needed by many scientific computing applications. Contributions will include the to-be developed techniques for implementing such software, the resulting open source software, and pedagogical artifacts that will include open course ware."
"1534836","SI2-SSE: TestRig 2.0","OAC","Special Projects - CCF, Software Institutes, Campus Cyberinfrastructure","09/01/2015","06/29/2015","Christopher Rapier","PA","Carnegie-Mellon University","Standard Grant","Rajiv Ramnath","02/28/2017","$287,308.00","","rapier@psc.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","CSE","2878, 8004, 8080","7433, 8005","$0.00","Advances in computation, modeling, and digital collection have led to an exponential growth in the size of scientific data sets. This 'Big Data' is transforming the process of scientific discovery. However, many scientific workflows require that the data be transferred, via networks like the Internet, to an optimal location for analysis and collaboration. Unfortunately, scientists often encounter performance problems on these networks. To resolve these problems, researchers depend on the expertise of engineers at Network Operations Centers (NOCs) for diagnosis and resolution. Resolving these problems often requires a cycle of interactions between the user and engineer that can last for days if not weeks, seriously hindering the scientific workflow. This project, known as TestRig 2.0, will short circuit this cycle by deploying a simple, easy to use system that will automatically collect a wide variety of important network diagnostic information. TestRig 2.0 will distribute dynamically generated test environments that are tuned to the specific needs of NOC engineers. TestRig 2.0 will automatically runs a series of tests against the infrastructure, collect the results, and transfer them back to the NOC engineer. TestRig 2.0 will also conducted a coarse analysis of the collected data so as to give the engineers some initial insight into the collected metrics.<br/><br/>In this project, researchers at Pittsburgh Supercomputing Center (PSC) propose to build a system, TestRig 2.0, that will quickly and easily gather a wide range of important network diagnostic information. This system includes a centralized management server that dynamically creates and configures client optical disk image files (as defined by ISO 9660). This server will be at PSC; it will provide TestRig 2.0 services to multiple geographically diverse NOCs via a free subscription model. These bootable images will provide a known good TCP stack configuration and system environment. TestRig 2.0 will use this baseline environment to perform tests against the perfSONAR monitoring infrastructure and also collect other pertinent data. The resulting data will undergo an initial automated coarse analysis by the client in order to provide engineers initial insight into the data. The resulting analysis and raw data will then be packaged and transferred back to the appropriate NOC for analysis. The client ISO creation process will include a unique dynamically generated public/private key pair. This key pair will be used for the authentication of the client ISO, distribution management, and user/test management. The rapid collection of pertinent data will enhance the process of scientific discovery by allowing researchers to focus on their research, instead of running tests for network engineers. By providing a framework for the use and development of post-collection data analysis, the system will lower the bar required for effective network diagnostics across a wide range of support personnel."
"1550597","SI2-SSI: Lidar Radar Open Software Environment (LROSE)","OAC","Physical & Dynamic Meteorology, Software Institutes, EarthCube","08/01/2016","09/06/2016","Michael Bell","HI","University of Hawaii","Standard Grant","Rob Beverly","12/31/2016","$2,499,996.00","Gary Barnes, WENCHAU LEE, Michael Dixon","mmbell@hawaii.edu","2425 CAMPUS RD SINCLAIR RM 1","HONOLULU","HI","96822","8089567800","CSE","1525, 8004, 8074","4444, 7433, 8004, 8009, 9150","$0.00","Modern radars and lidars are a diverse class of instruments, capable of detecting molecules, aerosols, birds, bats and insects, winds, moisture, clouds, and precipitation. Scientists and engineers use them to perform research into air quality and pollution, dangerous biological plumes, cloud physics, cloud extent, climate models, numerical weather prediction, road weather, aviation safety, severe convective storms, tornadoes, hurricanes, floods, and movement patterns of birds, bats and insects. Radars and lidars are critical for protecting society from high impact weather and understanding the atmosphere and biosphere, but they are complex instruments that produce copious quantities of data that pose many challenges for researchers, students, and instrument developers. This project will develop a new set of tools called the Lidar Radar Open Software Environment (LROSE) to meet these challenges and help address the 'big data' problem faced by users in the research and education communities. This project will open new avenues of scientific investigation, including data assimilation to improve weather forecasts, and help to maximize returns on NSF investments in weather and climate research by providing better software tools to researchers, students, and educators. Improving the effectiveness of NSF research will provide significant scientific and societal benefits through an improved understanding of many diverse scientific topics that are relevant to public safety, national defense, and the global economy.<br/><br/>The LROSE project will develop a 'Virtual Toolbox' with a set of software tools needed for a diverse set of scientific applications. LROSE will be packaged so that it can be run on a virtual machine (VM), either locally or in the cloud, and stocked with core algorithm modules for those typical processing steps that are well understood and documented in the peer-reviewed literature. LROSE will enable the user community to use the core toolset to develop new research modules that address the specific needs of the latest scientific research. Through the VM Toolbox and a core software framework, other developers of open-source radar software can then provide their own compatible software tools to the set. By combining the open source approach with recent developments in virtual machines and cloud computing, we will develop a system that is both highly capable and easy to run on virtually any hardware, without the complexity of a compilation environment. The LROSE project will build on existing prototypes and available software elements, while facilitating community development of new techniques and algorithms to distribute a suite of documented software modules for performing radar and lidar analysis. These modules will each implement accredited scientific methods referencing published papers. The infrastructure and modules will allow researchers to run standard procedures, thereby improving the efficiency and reproducibility of the analyses, and encourage researchers to jointly develop new scientific approaches for data analysis. The use of collaborative open source methods will lead to a suite of available algorithmic modules that will allow scientists to explore radar and lidar data in new, innovative ways. Researchers will benefit from the improved toolset for advancing understanding of weather and climate, leading to a positive outcome in the advancement of scientific knowledge and societal benefits."
"1740330","SI2-SSE: Software Elements to Enable Immersive Simulation","OAC","Software Institutes, CDS&E","09/01/2017","08/28/2017","Kenneth Jansen","CO","University of Colorado at Boulder","Standard Grant","Seung-Jong Park","08/31/2021","$499,997.00","Kurt Maute, Alireza Doostan, John Evans","jansenke@colorado.edu","3100 MARINE ST STE 481 572 UCB","BOULDER","CO","803090001","3034926221","CSE","8004, 8084","029E, 030E, 036E, 067E, 7433, 8004, 8005, 9263","$0.00","Parallel computers have grown so powerful that they are now able to solve extremely complex fluid flow or structures problems in seconds.  Unfortunately, it may take a researcher many hours or even days to set up a complex problem before it can be solved.  Furthermore it may take hours or often weeks to extract insight from the volume of data the simulation produces, if using standard techniques. For discovery and design questions, where the next variant of the problem requires a change to the problem definition, these delays disrupt the flow of experimentation and the associated intuition and learning about how the change in the problem definition relates to a change in the solution. To address this issue, a paradigm shift, referred to here as ""immersive simulation"", is planned to enable new approaches to problem definition editing that allow practitioners to interact with the simulations (visual model iteration) in a manner where they can dynamically experience the influence of parameter variations from a single, live, and ongoing simulation.  Examples include a surgeon virtually altering the shape of a bypass graft on one computer monitor and then virtually observing the change in the blood flow patterns not only within the bypass but throughout the vascular system. Likewise, an engineer altering the shape of a virtual car to see if the flow pattern improves or worsens. These applied research examples have parallels in fundamental research where live insight into the flow physics of unsteady, turbulent flows and their sensitivity to live parameter changes will be made available to researchers for the first time. Visually connecting the solution change to the visually iterated geometry and/or parameter change will enable a new age of intuition-driven discovery and design. This paradigm shift will also be incorporated into foundational undergraduate and graduate courses to enable deeper, experiential-based learning.  <br/><br/>The central goal of this project is to advance state-of-the-art tools into generic components that, when integrated, will make the following capabilities available to any partial differential equation solver: 1) live, reconfigurable visualization of ongoing simulations, 2) live, reconfigurable problem definition to allow the dynamic solution insight to guide the choice of key problem parameters, 3) real-time parameter sensitivity feedback, 4) adaptive simulation control to account for discretization errors and geometry changes, and 5) integration and demonstration of reliable, immersive simulation. The first communities that these software components will be developed with include cardiovascular flow and aerodynamic flow control. They have already articulated a need for software to more rapidly explore the performance of their systems under a broad parameter space with intuitive and quantitative parameter sensitivity. This software will enable not only design (applied research e.g., exploring bypass vs. stent type and placement for a particular patient's diseased vasculature or flow control actuator placement), but also discovery (fundamental research e.g., explore physics of flow response to discover completely new surgical procedures and flow control processes and devices). This twofold and complementary software application will have a similar impact on education, where foundational courses will use the integrated software modules to create immersive simulations that build intuition about flow physics, and then reinforce that learning in an applied nature in capstone design courses.  While the ideas will be prototyped and proven within the field of fluid dynamics, they will be developed generally, with sustainable software engineering, for easy adoption by other fields that make use of simulation. The successful development, integration, and demonstration of these tools at scale will transform massively parallel simulation from a series of I/O-intensive steps to live, reconfigurable discovery using carefully designed interfaces that blaze the trail for all simulation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1440620","SI2-SSE: The Next Generation of the Montage Mosaic Engine","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","10/01/2014","08/14/2014","Graham Berriman","CA","California Institute of Technology","Standard Grant","Rajiv Ramnath","09/30/2017","$499,902.00","","gbb@ipac.caltech.edu","1200 E CALIFORNIA BLVD","PASADENA","CA","91125","6263956219","CSE","1253, 1798, 8004","1206, 7433, 8005","$0.00","Images produced by the new generation of astronomical instruments are addressing fundamental questions about the Universe, such as the formation of the very first galaxies after the Big Bang, and the very first stages of the formation of stars in massive dust clouds in our Galaxy. Exploiting this new generation of data is difficult because the data sets they produce are sufficiently complex and large as to demand new approaches to data processing that lag far behind developments in instrumentation. A growing community is working to rectify this state-of-affairs. This project will deliver software tools that will aggregate data from the new instruments into images of large scale regions of the sky so that astronomers can fully study scientific questions such as those identified above. This approach of studying aggregated images, or mosaics, is a powerful tool in astronomy. The project will deliver the next generation of an existing mosaic-building engine, Montage, which is in wide use in astronomy and in educational activities.  It will support processing of the new data sets such that they can be visualized in immersive tools such as the World Wide Telescope, widely  used in developing innovative approaches to education, and such that that they can generate data used by Citizen Science services such as Zooniverse. Montage will come be bundled with a set of tools that will enable astronomers to process massive collections images on powerful ""cloud computing"" platforms. These tools will be applicable to many data-intensive problems in fields such as earthquake prediction, DNA sequencing, and climate modeling. Finally, Montage is in wide use in developing and testing national cyberinfrastructure to benefit the U.S. science community. We anticipate that the next-generation Montage will be used in the same way to develop ever more powerful cyberinfrastructure as data volumes grow rapidly in all fields.<br/><br/>In greater detail, the project will deliver the next generation of the Montage image mosaic engine, which will offer new capabilities that respond to the changing astronomy data and computing landscapes. These capabilities, requested by the user community, are: 1. Support for mosaicking of data cubes, now routinely generated by modern instrumentation; 2. Support for two widely used sky-partitioning schemes, HEALPix and TOAST; 3. An API to enable users to call Montage directly in Python and other languages.  The work to develop memory management and subsetting techniques to support mosaicking will be available for others to use and extend. Support for HEALPix will enable integration and analysis of far-infrared, cosmic background data sets with other image data sets. TOAST will enable essentially any image data set to be incorporated into the WWT.  Montage will be bundled with a turnkey package of open source tools that provision resources and run applications on cloud platforms. This package will build on knowledge gained in creating data products at scale with cloud platforms. These tools will bring cloud computing to scientists who have little system configuration knowledge, one of the biggest barriers to entry; these tools are general purpose and will be applicable to data intensive applications in may fields. Thus Montage will  provide powerful new capabilities to astronomers, to projects analyzing data at scale to create new data products, and to scientists in data-intensive fields outside astronomy.  The next-generation toolkit will inherit the sustainable Montage architecture, which has attracted a large user base among astronomers, E/PO specialists, and computer technologists. Montage is written in C, is portable across all common Unix platforms, highly scalable and delivered as components that are easy to incorporate into pipelines and processing environments. Montage is the only mosaic engine with all these characteristics. The project will use the evolutionary delivery lifecycle model. The code will be a made accessible on the GitHub repository, and released as Open Source code with a BSD 3-clause license. A Users' Panel will advise on detailed specifications."
"1550547","Collaborative Research: SI2-SSI: Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion","OAC","PROBABILITY, APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, Software Institutes, CDS&E-MSS","09/01/2016","08/10/2016","Noemi Petra","CA","University of California - Merced","Standard Grant","Seung-Jong Park","08/31/2021","$475,000.00","","npetra@ucmerced.edu","5200 N LAKE RD","MERCED","CA","953435001","2092012039","CSE","1263, 1266, 1271, 8004, 8069","4444, 7433, 8004, 8009, 8251, 9263","$0.00","Scientists often use mathematical models to predict the behavior of natural and engineered systems. These models are therefore fundamental to scientific and engineering progress and hence relevant to NSF's science mission. Most models of realistic physical systems  use complex formulae (such as, partial differential equations) involving many variables. When using such a model for predicting the future behavior of a system, a scientist has to provide initial values for all the variables.  This can be difficult because input values may not be directly measureable. Thus, scientists often must use ""inverse"" computations to calculate the initial input values of the variables of a system model based on external observations of the real world. In other words, scientists seek to infer inputs to a computer model of a physical process from real observational data of the outputs. There are many examples of inverse computations, ranging from computing the important dimensions of an organ from its CAT scan, reconstructing the source of a sound by measuring its volume and frequency at various places, calculating the density of the Earth from measurements of its gravity field, or calculating the initial condition of the atmosphere (temperature, pressure, etc.) from satellite and weather station observations over a time interval. Inverse problems are ubiquitous across all of science and engineering (and beyond). Many solutions exist for inverse problems, i.e. solutions that fit the data to the observations. However, there are variations in the solutions identified. That is, the solutions of an inverse problem are subject to uncertainty. Bayesian inferencing provides a systematic mathematical framework for characterizing this uncertainty. However, the Bayesian solution of inverse problems for large-scale complex models require enormous computational power. Only recently have algorithms begun to emerge that are computationally tractable. However, these algorithms have remained out of the reach of the mainstream of scientists who solve inverse problems, due to their complexity and the need for deeper information from the forward model. This project aims to develop, distribute, and support open-source software that encodes state-of-the-art algorithms for the solution of large-scale complex Bayesian inverse problems and is robust, scalable, flexible, modular, widely accessible, and easy to use.<br/><br/>The project builds heavily on two complementary open-source software libraries the team has been developing: MUQ at MIT, and hIPPYlib at UT-Austin/UC-Merced. MUQ provides a spectrum of powerful Bayesian inversion models and algorithms, but expects forward models to come equipped with gradients/Hessians to permit large-scale solution. hIPPYlib implements powerful large-scale gradient/Hessian-based inverse solvers in an environment that can automatically generate needed derivatives, but it lacks full Bayesian capabilities. By integrating these two complementary libraries, the project will result in a robust, scalable, and efficient software framework that realizes the benefits of each to tackle complex large-scale Bayesian inverse problems across a broad spectrum of scientific and engineering disciplines. The resulting software, that will be distributed under an open-source license, will provide an environment for rapid development of inverse models equipped with gradient/Hessian information; benchmark problems for evaluation and comparison of algorithms; and tutorial problems for training and testing purposes."
"1450468","SI2-SSI Integration of Synchrotron X-Ray Analysis Software Methods into the Larch Framework","OAC","OFFICE OF MULTIDISCIPLINARY AC, Geophysics, DMR SHORT TERM SUPPORT, Software Institutes, EarthCube","10/01/2015","09/16/2015","Matthew Newville","IL","University of Chicago","Standard Grant","Seung-Jong Park","09/30/2019","$540,969.00","","newville@cars.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","CSE","1253, 1574, 1712, 8004, 8074","7433, 7574, 8004, 8009, 9216","$0.00","The solutions to many of the outstanding problems in geology, environmental science, material science, and biology require understanding the chemical state and detailed atomic structure of the molecules and solids that make up our world.  Such problems range from understanding the molecular forms of arsenic in rice, determining the chemical composition of the earths interior, and improving the performance and reducing the environmental impact of batteries that are in our laptops, cellphones, and cars.  The nation's synchrotron facilities provide powerful X-ray facilities that allow researchers to study these questions by investigating the chemical makeup and crystal structure of complex, real-world materials such as plant seeds, contaminated soils, human and animal tissue, minerals and meteorites, and working batteries and catalysts.  Synchrotron measurement techniques have developed very rapidly over the past few decades, and are being used by many more researchers.  The ability to handle and interpret the large and complex datasets now being routinely generated at these facilities is often a significant challenge, even for experts.  The work here will develop the Larch X-ray analysis framework to provide open-source software that is easy to use and specific enough to correctly interpret several categories of synchrotron X-ray data.  The approach will provide tools that are flexible enough to enable researchers to explore and interpret new combinations of data easily enough to make new connections and discoveries in a wide variety of scientific areas.<br/><br/><br/>This project will integrate visualization and analysis software for multiple synchrotron X-ray techniques into the open source and extensible Larch X-ray Analysis framework.  The immediate focus of the work is to support visualization and quantitative analysis of the rich and complex data from X-ray microprobes, including X-ray fluorescence imaging, fluorescence and absorption spectroscopies, and X-ray diffraction.  The Larch framework already provides a suite of analysis procedures for X-ray absorption spectroscopy and fluorescence imaging, and has been designed to be readily extensible by adding plug-ins in Python, widely used in scientific computing and being embraced in the synchrotron user communities.  Existing state-of-the-art analysis procedures for X-ray fluorescence, X-ray absorption, and X-ray diffraction have been identified to be integrated into the Larch framework, adapting and translating the software as needed to be compatible with the open-source Python framework.  With the combination of state-of-the-art analysis methods for multiple data types, Larch will provide a single well-supported and -documented analysis package with robust, easy to use analytic methods for a range of synchrotron X-ray data. By being easily extensible, the Larch package can also accommodate methods for other synchrotron X-ray techniques."
"1440638","SI2-SSE: GEM3D: Open-Source Cartesian Adaptive Complex Terrain Atmospheric Flow Solver for GPU Clusters","OAC","Physical & Dynamic Meteorology, Software Institutes, EarthCube","10/01/2014","08/31/2017","Inanc Senocak","ID","Boise State University","Standard Grant","Alan Sussman","09/30/2018","$500,000.00","Grady Wright, Donna Calhoun, Elena Sherman","senocak@pitt.edu","1910 UNIVERSITY DR","BOISE","ID","837250001","2084261574","CSE","1525, 8004, 8074","7433, 8005, 9150","$0.00","The U.S. Government invests in leadership supercomputing facilities through several agencies to advance scientific discovery in many fronts. This project is motivated by this national commitment to supercomputing research and the increasing availability of many-core computing hardware from workstations to supercomputers. Today scientists and engineers have access to extreme-scale computing resources. However, many legacy codes do not take advantage of recent innovations in computing hardware, and there is a lack of open-source simulation science software that can effectively leverage the many-core computing paradigm. Computational fluid dynamics (CFD) solvers have advanced many fields such as aerospace engineering and atmospheric sciences. Many current open-source CFD models and numerical weather prediction models do not take full advantage of the superior compute performance of graphics processing units (GPUs). By creating an open-source community model that can execute on multi-GPU workstations and large GPU clusters, the project team expects to broaden the use of high-performance computing in fluid dynamics applications. The immediate target application is wind modeling over complex terrain, to support research and development in wind resource assessment, power forecasting, atmospheric research, and air pollution. Through this project, the PIs will continue to transfer and expand the knowledge bases in GPU computing, computational mathematics, and software engineering to new students. Skill sets that transcend traditional disciplines are highly prized by national laboratories as there is a critical shortage of workforce who can conduct scientific research using supercomputers. Students and postdoctoral researchers who are involved in this project will contribute toward this critical workforce. <br/><br/>This project brings together engineers, applied mathematicians, and computer scientists. The entire suite of software elements will be designed for GPU clusters with an MPI-CUDA implementation that overlaps computation with communications using a three-dimensional decomposition for enhanced scalability. The implementation will balance performance and further development and ownership by a broader community of academic researchers. The team will follow modern software engineering practices for concurrent applications. An adaptive mesh refinement strategy that can scale on GPU clusters will be developed. A novel projection method based on radial basis functions will impose the divergence-free constraint on a hierarchy of adaptively refined grids. Software elements will be tested using unit testing and verification techniques for concurrent programs, and against data available from benchmark numerical problems.  The flow solver will include modules for the immersed boundary approach for arbitrarily complex terrain and the dynamic large-eddy simulation technique. The software implementation and syntax will be intuitive to allow contributions from a larger community. The project team expects the proposed software to help reduce modeling errors with very high resolution simulations and contribute toward a fundamental understanding of turbulent winds over complex terrain. The PIs of this project will continue their teaching efforts in Parallel Scientific Computing, Computational Mathematics, and Software Engineering. The results will be disseminated through conference presentations and via a wiki site for the open-source project. Software elements will be released under an open-source GNU General Public License."
"1740204","Collaborative Research: NSCI: SI2-SSE: Time Stepping and Exchange-Correlation Modules for Massively Parallel Real-Time Time-Dependent DFT","OAC","DMR SHORT TERM SUPPORT, Software Institutes","09/01/2017","08/24/2017","Yosuke Kanai","NC","University of North Carolina at Chapel Hill","Standard Grant","Rob Beverly","08/31/2021","$250,000.00","","ykanai@UNC.EDU","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","CSE","1712, 8004","026Z, 054Z, 7433, 8004, 8005, 9216","$0.00","Recent advances in high-performance (HPC) computing allow simulations of quantum dynamics of electrons in complex materials, and such simulations are central to advancing various medical and semiconductor technologies, ranging from proton beam cancer therapy to fabricating faster and smaller electronics. At the same time, the increasing scale and complexity of modern high-performance computers exposed a need for development of scientific software that is tailored for computers with large numbers of processors so that simulations can efficiently take advantage of increasing computing power. This project advances scientific software for simulating quantum dynamics of electrons for high-performance computers with tens and hundreds of thousands of processors that are becoming widely available. This work builds the HPC academic research community around the proposed software by extending the existing software available for quantum dynamics simulation with better user-friendly features and analysis techniques. In the process, this project engages graduate students and early-career researchers to use and further develop scientific software for high-performance computers in general. Additionally, a summer school for hands-on training will be conducted. The open source software will be made available to the community on Github (public repository). <br/><br/>Real-time propagation in time-dependent density functional theory (RT-TDDFT) is becoming increasingly popular for studying non-equilibrium electronic dynamics both in the linear regime and beyond linear response. RT-TDDFT can be combined to study coupled dynamics of quantum-mechanical electrons with the movement of classical ions within Ehrenfest dynamics. In spite of its great promise, RT-TDDFT is computationally very demanding, especially for studying large condensed-matter systems. The large cost arises from small time steps of numerical integration of the electron dynamics, rendering accurate (hybrid) exchange-correlation (XC) functionals unfeasible, despite their clear benefits. In addition, while modern high-performance computing (HPC) helps tackling great scientific questions, massively parallel, hybrid-paradigm architectures present new challenges. Theoretical and algorithmic methods need to be developed in order to take full advantage of modern massively parallel HPC. This work builds new modules for the RT-TDDFT software component of the Qb@ll code, that enables a large community of researchers to perform advanced first-principles simulations of non-equilibrium electron dynamics in complex condensed-phase systems, using massively parallel HPC. This is done through developing (1) new modules for numerical integration that propagate the underlying non-linear partial differential equations in real time with high efficiency and accuracy, and (2) new modules for improved approximations of the underlying electronic structure, using a modern meta-generalized-gradient XC functional. Furthermore, the work builds the HPC academic research community around RT-TDDFT within the Qb@ll code through (1) development of user-friendly features that interface Qb@ll with other code and analysis techniques and (2) engagement of early-career scientists by incorporating hands-on training on RT-TDDFT using the Qb@ll code in TDDFT summer school.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering, the Materials Research Division and Chemistry Division in the Directorate of Mathematical and Physical Sciences."
"1448360","Workshop on Supporting Scientific Discovery through Norms and Practices for Software and Data Citation and Attribution","OAC","SciSIP-Sci of Sci Innov Policy, Software Institutes, STAR Metrics","09/01/2014","08/18/2014","Stanley Ahalt","NC","University of North Carolina at Chapel Hill","Standard Grant","Daniel Katz","08/31/2015","$99,935.00","Thomas Carsey","ahalt@renci.org","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","CSE","7626, 8004, 8022","7433, 7556, 8004","$0.00","Scientific researchers, and particularly academic researchers, are embedded in a reputation economy. Tenure, promotion, and acclaim are achieved through influential research. There are few incentives for scientists to share data and software, and tenure and promotion decisions lack consideration of such activities; and to compound the problem, there are disincentives such as risking the loss of attribution. Some scientists distrust the public access model for software and data, and prefer to share data and software only by personal request, which assures attribution through personal contact and implicit social contract. There is also a lack of well-developed metrics with which to assess the impact and quality of scientific software and data.  New practices and incentives are needed in the research community for software and data citation and attribution, so that data producers, software and tool developers, and data curators are credited for their contributions.<br/> <br/>This workshop will facilitate a national, interdisciplinary exploration of new norms and practices for software and data citation and attribution, with the goal of informing the Science of Science and Innovation Policy (SciSIP) and Software Infrastructure for Sustained Innovation (SI2) NSF programs. Social and technical challenges facing current software development and data generation efforts will be identified and participants will explore viable methods and metrics to support software and data attribution in the scientific research community. This workshop will address registration of software and data, repositories for software and data, methods for tracking software and data usage, software and data annotation, collecting and curating metadata on software and data, ensuring appropriate attribution by software and data users, alternatives to traditional publication models for attribution, adaptation of commercial models for software and data attribution, proportioning attribution metrics to match degree of effort and role in software development and data generation, and establishing reward metrics for open science. Workshop outcomes will include actionable plans to enable the broader research community to implement the software and data attribution practices that are identified and advanced by the participants of the workshop."
"1460334","SI2-SSE: Parallel and Adaptive Simulation Infrastructure for Biological Fluid-Structure Interaction","OAC","INTERFAC PROCESSES & THERMODYN, Mechanics of Materials and Str, DYNAMICAL SYSTEMS, Software Institutes","07/01/2014","09/11/2014","Boyce Griffith","NC","University of North Carolina at Chapel Hill","Standard Grant","Rajiv Ramnath","12/31/2015","$75,361.00","","boyceg@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","CSE","1414, 1630, 7478, 8004","","$0.00","The immersed boundary (IB) method is both a mathematical formulation and a numerical approach to problems of fluid-structure interaction, treating the specific case in which an elastic structure is immersed in a viscous incompressible fluid.  The IB method was introduced to describe the fluid dynamics of heart valves, but this methodology has also been applied to a wide range of problems in biological and non-biological fluid dynamics.  The IB method typically requires high spatial resolution to resolve the viscous boundary layers at fluid-structure interfaces and, at higher Reynolds numbers, to resolve vortices shed from such interfaces.  To improve the efficiency of the IB method, the principal investigator has developed an adaptive version of the IB method that employs block-structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where it is needed.  IBAMR software is a distributed-memory parallel implementation of this adaptive scheme.  The key goal of this project is to make IBAMR the unifying software framework for users of the IB method, thereby establishing a community of researchers who employ a common software infrastructure for biofluids model development and simulation.  The project aims to enhance IBAMR substantially by (1) developing and implementing implicit IB schemes that will allow for the efficient use of large numerical timesteps; (2) developing and implementing extensions of the basic IB methodology, including a new variable-viscosity version of the IB method, and an existing stochastic version for microscale and nanoscale problems in which Brownian motion is important; (3) optimizing IBAMR for use with modern as well as projected-future high performance computing systems comprised of multi-core compute nodes interconnected by a high-speed network; and (4) developing front-end tools for model construction, validation, and execution, thereby facilitating the adoption and use of IBAMR, especially by students and researchers with limited computational experience.<br/><br/>From the writhing and coiling of DNA, to the beating and pumping motions of cilia and flagella, to the flow of blood in the heart and throughout the circulation, coupled fluid-structure systems are ubiquitous in biology and physiology.  This project aims to enhance significantly the IBAMR software developed by the principal investigator.  IBAMR is a framework for performing computer simulations of biological fluid mechanics, and this project seeks to establish IBAMR as a unifying software infrastructure that will serve as a common ""language"" for developing and exchanging such models.  IBAMR is already being actively used within several independent research projects that aim to model different aspects of cardiovascular dynamics, such as platelet aggregation and the fluid dynamics of natural and prosthetic heart valves.  Such simulations promise ultimately to improve the efficacy of devices and procedures for treating cardiovascular disease.  This software also is being used within projects that study other problems in biofluid mechanics, including insect flight, aquatic locomotion, and the dynamics of phytoplankton.  By enhancing IBAMR, this project will also enhance significantly the ability of these and other research groups to construct detailed biofluids models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations.  This project will enhance the IBAMR software substantially, extending the range of problems to which it may be applied, and improving the methods implemented within the software as well as the efficiency of the implementation.  The work of this project will extend greatly the community of students and researchers who are able to use IBAMR to model biological fluid-structure interaction, in part by implementing graphical software tools for building IB models and running IB simulations."
"1739423","SI2-SSE: A parallel computing framework for large-scale real-space and real-time TDDFT excited-states calculations","OAC","DMR SHORT TERM SUPPORT, Software Institutes","02/15/2018","02/02/2018","Eric Polizzi","MA","University of Massachusetts Amherst","Standard Grant","Rob Beverly","01/31/2022","$485,854.00","","polizzi@ecs.umass.edu","COMMONWEALTH AVE","AMHERST","MA","01003","4135450698","CSE","1712, 8004","026Z, 7237, 7569, 8004, 8005","$0.00","The ability to control electronic materials and understand their properties has been a driving force for technological breakthroughs. The technology for electronic devices has been on a rapidly rising trajectory since the 1960s with the ability to fabricate ever smaller silicon transistors (`Moore's Law'), with today's device sizes in the nanometer range. With the rise of nanotechnology, atom-by-atom quantum simulations of emerging materials are becoming increasingly important to reliably supplement the current experimental investigations. Modeling and simulations of atomic systems are essential to assist the everyday work of numerous engineers and scientists and can universally impact a wide range of disciplines (engineering, physics, chemistry, and biology) spanning the technological fields of computing, sensing and energy. This project will accelerate the development of quantum technologies and their impacts in the global economy. A new software will be produced to help capture many fundamental quantum effects which are increasingly important in nanotechnology for exploring and prototyping new revolutionary electronic materials and devices.<br/><br/>This project aims at developing and offering a new open source software, NESSIE, that can address the modern challenges encountered in material and device nano-engineering applications.  NESSIE will use the most cost-effective method to perform excited states calculations, the time-dependent density functional theory (TDDFT), in conjunction with a novel combination of numerical algorithms and physical and mathematical modeling techniques.  NESSIE will be capable of performing excited-state TDDFT calculations using full-potential (all-electron) in real-space (using finite element) and real-time. A new hierarchical parallelization strategy will allow NESSIE to tackle unprecedented atomistic finite size systems at this level of theory. The outcome of this project will open new perspectives for addressing the numerical challenges in real-time TDDFT excited-states calculations to operate the full range of electronic spectroscopy, and study the nanoscopic many-body effects in arbitrary complex molecules and very large-scale finite-size nanostructures. It is expected that the NESSIE software and associated numerical components will become a new valuable new tool for the scientific community, that could be applied to investigate the fundamental electronic properties of numerous nanostructured materials.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences."
"1740211","SI2-SSE: Highly Efficient and Scalable Software for Coarse-Grained Molecular Dynamics","OAC","DMR SHORT TERM SUPPORT, Software Institutes","09/01/2017","02/20/2018","Gregory Voth","IL","University of Chicago","Standard Grant","Rob Beverly","08/31/2021","$500,000.00","Hakizumwami B. Runesha","gavoth@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","CSE","1712, 8004","026Z, 054Z, 7433, 8004, 8005, 9216","$0.00","Molecular simulation provides a powerful complement to conventional experimental techniques, offering both high-resolution information and unusual levels of control over the experimental conditions. Whileatomic-resolution molecular simulations are well established and widely used, it is also possible to remove extraneous detail from molecular representations to create highly efficient ""coarse-grained"" (CG) models.CG approaches can expand the potential applications of molecular simulations far beyond atomic-resolution models: the computational efficiency of CG models allows the scientist to investigate not only significantlylarger systems, but also phenomena that require significantly longer time scales. These CG approaches are of particular interest in the study of systems where key aspects of various processes emerge frominteractions between large numbers of molecules over relatively long distances. CG models can therefore provide crucial insight into the molecular basis of such systems, e.g., new materials. However, CG models can require significant scientific understanding to create and use effectively. To bring cutting-edge CG methodologies into a wider degree of use, this project willimplement key algorithmic advances and associated CG functionalities into the widely-used LAMMPS molecular dynamics simulation code. Furthermore, the project will implement a publically-accessible repository for CGmodel parameters and input files to accelerate the dissemination of exemplar CG models throughout the scientific community.<br/><br/><br/>The project will integrate key functionalities for very large-scale and dynamic CG models into the LAMMPS molecular dynamics package. These functionalities include not only sparse memory optimizations (e.g.,template molecular topology descriptions and spatial data structures for link cell algorithms) but also user-defined transition information for the propagation of ""ultra-coarse-grained"" (UCG) models; parameterizationof the latter can be achieved by using the integrated multi-scale coarse-grained force matching code (MSCGFM). Furthermore, direct incorporation of experimental data into CG models will be assisted byimplementations of the ""experiment directed metadynamics"" (EDM) and ""experiment directed simulation"" (EDS) algorithms. Taken together, these enhancements will provide cutting-edge CG model generation and simulation techniques to a wide user community. To complement the extended functionality of the LAMMPS code, a user-driven data and metadata repository for CG models will be provided to assist with efficient dissemination of model parameters andsimulation/validation data to the scientific community."
"1450339","SI2-SSI: Collaborative Research:  Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS, CDS&E","08/01/2015","08/11/2015","Matthew Knepley","IL","University of Chicago","Standard Grant","Rob Beverly","02/29/2016","$262,655.00","","knepley@buffalo.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","CSE","1253, 8004, 8069, 8084","7433, 8004, 8009, 8084","$0.00","Many biological and biomedical systems involve the interaction of a flexible structure and a fluid. These systems range from the writhing and coiling of DNA, to the beating and pumping of cilia and flagella, to the flow of blood in the body, to the locomotion of fish, insects, and birds. This project aims to develop advanced software infrastructure for performing dynamic computer simulations of such biological and biomedical systems. To facilitate the deployment of this software in a range of scientific and engineering applications, this project will develop new software capabilities in concert with new computer models that use the software. Specific application domains to be advanced in this project include models of aquatic locomotion that can be used to understand the neural control of movement and ultimately to develop new treatments for neurological pathologies such as spinal cord injuries, and models that simulate the interaction between the electrophysiology of the heart and the contractions of the heart that pump blood throughout the body, which could lead to improved approaches to treating heart disease. The software to be developed within the project is freely available online and is used by a number of independent research groups in a variety of scientific and engineering domains. It is being actively used in projects that model different aspects of cardiovascular dynamics, such as platelet aggregation and the dynamics of natural and prosthetic heart valves, and in projects that study other biological problems, including cancer dynamics, insect flight, aquatic locomotion, and the dynamics of phytoplankton. The software is also being applied to non-biological problems, including nanoscale models of colloidal suspensions and models of active particles. The improved methods and software to be developed in this project will thereby have a broad and sustained impact on a large number of ongoing research efforts in the biological and biomedical sciences and other scientific and engineering disciplines.<br/><br/>The immersed boundary (IB) method is a broadly applicable framework for modeling and simulating fluid-structure interaction (FSI). The IB method was introduced to model the fluid dynamics of heart valves, and subsequent development initially focused on simulating cardiac fluid dynamics. This methodology is broadly useful, however, and has been applied to a variety of problems in which a fluid flow interacts with immersed structures, including elastic bodies, bodies with known or prescribed deformational kinematics, and rigid bodies. Extensions of the IB method have also been developed to model electrophysiological systems and systems with chemically active structures. To improve the efficiency of the IB method, the PI has developed adaptive versions of the IB method that employ structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where needed. These methods have been implemented within the IBAMR software framework, which provides parallel implementations of the IB method and its extensions that leverage high-quality computational libraries including SAMRAI, PETSc, and libMesh. This project will further extend the IBAMR software by implementing modeling and discretization technologies required by the research applications of current and prospective users of the software, by developing improved solver infrastructure facilitated by the implementation of native support for structured AMR discretizations in the PETSc library, and by integrating with existing high-quality software tools for model development, deployment, and analysis. IBAMR is freely distributed online and is used within a number of independent research groups both to the further development of the IB method and also to its application to simulate diverse problems in fluid dynamics and FSI. By enhancing IBAMR, this project will also enhance the ability of these and other researchers to construct detailed models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations. This project will also develop general-purpose support for AMR discretizations in PETSc, a software library with thousands of active users, ~400 downloads per month, and numerous applications. The work of this project will help to grow the IBAMR user community of students and researchers by developing UI tools for building models, running simulations, and analyzing results. Students will be actively engaged in all aspects of the project, including code, method, and model development."
"1636501","Collaborative Research: SI2-SSE: UT Wrangler: Understanding the Software Needs of High End Computer Users","OAC","Software Institutes","10/01/2015","03/15/2016","Mark Fahey","IL","University of Chicago","Standard Grant","Rob Beverly","09/30/2016","$46,194.00","","markrfahey@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","CSE","8004","7433, 8005, 9150","$0.00","This research addresses two important questions: what software do researchers actually use on high-end computers, and how successful are they in their efforts to use it? It is a plan to improve our understanding of individual users' software needs, then leverage that understanding to help stakeholders conduct business in a more efficient, effective, and systematic way. The signature product, UTWrangler, builds on work that is already improving the user experience and enhancing support programs for thousands of users on twelve supercomputers across the United States and Europe.   For the first time, complete, accurate, detailed, and continuous ground truth information about software needs, trends, and issues at the level of the individual job are being delivered.<br/> <br/>UTWrangler will instrument, monitor, and analyze individual jobs on high-end computers to generate a picture of the compilers, libraries, and other software that users need to run their jobs successfully. It will highlight the products our researchers need and do not need, and alert users and support staff to the root causes of software configuration issues as soon as the problems occur. UTWrangler's prototypes prove its value and future impact: simplifying end users' workflows; improving support, training and documentation; saving money; and helping administrators prioritize maintenance of their large base of installed software.  UTWrangler will build on the capabilities of its prototypes, providing a robust, sustainable, second generation mechanism that will help the computational research community make the most effective use of limited computing cycles and labor hours.  And UTWrangler will mitigate the difficulties new users encounter, reporting configuration problems as soon as jobs begin, and identifying opportunities to improve documentation, education and outreach programs."
"1148428","Collaborative Research:  SI2-SSE: Correctness Verification Tools for Extreme Scale Hybrid Concurrency","OAC","Software Institutes","06/01/2012","06/19/2012","Rajeev Thakur","IL","University of Chicago","Standard Grant","Rudolf Eigenmann","05/31/2014","$44,346.00","","thakur@anl.gov","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","CSE","8004","8004, 8005","$0.00","High Performance Computing is strategically important to national competitiveness.  Advances in computational capabilities involve the use of unprecedented levels of parallelism: programming methods that involve billions of concurrent activities.  Multiple styles of concurrency involving shared and distributed memory programming (?hybrid?) are necessary. Unfortunately, such programs are very difficult to debug using existing methods. This project develops formal (mathematically based) verification tools that can debug hybrid concurrent programs with very high certainty of bug elimination, while consuming only modest computational resources for verification.  <br/><br/>The project develops execution-based tools that eliminate search over semantically equivalent alternative schedules as well as solver-based techniques that eliminate classes of bugs over single runs. Scalable methods based on non-determinism classification and heuristic execution-space reduction are also being developed. <br/><br/>Expected results include: (1) development of tools based on formal algorithmic techniques that verify large-scale hybrid programs; (2) amalgamation of incisive bug-hunting methods developed at other research organizations within formally based tools developed in our group; (3) incorporation of our verification tools and techniques within popular tool-integration frameworks; (4) large-scale case studies handled using our tools; and (5) training of undergraduate and graduate students on these advanced verification methods, building the talent pool vital to continued progress in high performance computing with applications to science and engineering, energy/sustainability, and homeland security."
"1148144","SI2-SSE: Software Tools for Biomolecular Free Energy Calculations","OAC","OFFICE OF MULTIDISCIPLINARY AC, MATERIALS AND SURFACE ENG, DMR SHORT TERM SUPPORT, CHEMISTRY PROJECTS, Software Institutes","07/01/2012","07/06/2012","Celeste Sagui","NC","North Carolina State University","Standard Grant","Rajiv Ramnath","06/30/2016","$497,797.00","Christopher Roland","sagui@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","CSE","1253, 1633, 1712, 1991, 8004","1253, 1633, 1712, 1982, 1991, 7237, 7433, 7569, 7573, 8004, 8005, 9215, 9216, 9263, HPCC","$0.00","TECHNICAL SUMMARY<br/><br/>The Office of Cyberinfrastructure, Division of Materials Research, Chemistry Division, and Division of Civil, Mechanical, and Manufacturing Innovation contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation.  This award supports the development of a new set of software tools for calculating biomolecular free energies, transition paths, and reaction rates that will be based on and will augment the PIs' ABMD/REMD/SMD suite of codes. New capabilities are to be added to the existing codes, which should greatly enhance the applicability of the software to challenging biomolecular problems. In particular, the PIs aim to implement and develop: (i) the capability for dealing with dynamically coupled multiple walkers or replicas for enhanced sampling; (ii) the introduction of Transition Path Theory and Sampling methodology for calculating reaction rates and related physical quantities; and (iii) the introduction of the ""string method"" for multi-dimensional free energy calculations and the calculation of the important minimum free energy path. These software tools will be released under the open source GPL, and released as part of the AMBER software package. The codes will also be released as stand-alone modules via the web for other users to integrate into their own programs.<br/><br/>To test and showcase the workings of the software package, the PIs will apply these methods to biomolecular systems where changes in handedness play an important role, including: proline-rich systems, mainly collagen and a new class of cell-penetrating peptides, and DNA and RNA in B and Z double helices. <br/><br/>This award will also enhance education through: the mentoring and recruitment of minority students, the development of a Biophysics option, the integration of research topics into current educational forums, working with local schools, and the development of a new Institute for Computational Science and Engineering.<br/><br/>NON-TECHNICAL SUMMARY<br/>The Office of Cyberinfrastructure, Division of Materials Research, Chemistry Division, and Division of Civil, Mechanical, and Manufacturing Innovation contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation.  This award supports the development of a new set of software tools that will enable the calculation of rates of chemical reactions involving biomolecules. The codes will also be able to calculate the energy that can be converted into useful work in a biochemical system. The codes will be distributed through a widely used software package called AMBER which contributes to the software cyberinfrastructure of computational chemistry, biochemistry, chemical engineering, materials, and biological physics communities. It will enable a wide range of computational research involving biological molecules. Among the possible applications are the study of the structure and transformations of biomolecules which provides information on their possible functions, the simulation of antibiotics, and simulating nanoscale technology applications. <br/><br/>This award will also enhance education through: the mentoring and recruitment of minority students, the development of a Biophysics option, the integration of research topics into current educational forums, working with local schools, and the development of a new Institute for Computational Science and Engineering."
"1664084","Collaborative Research: SI2-SSI: Expanding Volunteer Computing","OAC","Software Institutes","05/15/2017","05/08/2017","Michael Zentner","IN","Purdue University","Standard Grant","Stefan Robila","11/30/2019","$499,996.00","","mzentner@ucsd.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","CSE","8004","7433, 8004, 8009","$0.00","Volunteer computing (VC) uses donated computing time consumer devices such as home computers and smartphones to do scientific computing. It has been shown that VC can provide greater computing power, at lower cost, than conventional approaches such as organizational computing centers and commercial clouds. BOINC is the most common software framework for VC. Essentially, donors of computing time simply have to load BOINC on their computer or smartphone, and then register to donate at the BOINC web site. VC provides ""high throughput computing"": handling lots of independent jobs, with performance goals based on the rate of job completion rather than completion time for individual jobs. This type of computing (all known as high-throughput computing) is in great demand in most areas of science. Until now, the adoption of VC has been limited by its structure. For example, VC projects (such as Einstein@home and Rosetta@home) are operated by individual research groups, and volunteers must browse and choose from among many such projects. As a result, there are relatively few VC projects, and volunteers are mostly tech-savvy computer enthusiasts.  This project aims to solve these problems using two complementary development efforts: First, it will add BOINC-based VC conduits to two major high-performance computing providers: (a) the Texas Advanced Computing Center, a supercomputer center, and (b) nanoHUB, a web portal for nano science that provides computing capabilities.Also, a unified control interface to VC will be developed, tentatively called Science United, where donors can register.  The project  will benefit thousands of scientists who use these facilities, and it will create technology that makes it easy for other HPC providers to add their own VC back ends. Also, Science United will provide a simpler interface to BOINC volunteers where they will register to support scientific areas, rather than specific projects. Science United will also serve as an allocator of computing power among projects. Thus, new projects will no longer have to do their own marketing and publicity to recruit volunteers. Finally, the creation of a single VC ""brand"" (i.e Science United) will allow coherent marketing of VC to the public. By creating a huge pool of low-cost computing power that will benefit thousands of scientists, and increasing public awareness of and interest in science, the project plans to establish VC as a central and long-term part of the U.S. scientific cyber infrastructure.<br/><br/>Adding VC to an existing HPC facility involves several technical issues, which will be addressed as follows: (1) Packaging science applications (which typically run on Linux cluster nodes) to run on home computers (mostly Windows, some Mac and Linux): the team is developing an approach using VirtualBox and Docker, in which the application and its environment (Linux distribution, libraries, executables) are represented as a set of layers comprising a Docker image, which is then run as a container within a Linux virtual machine on the volunteer device. This has numerous advantages: it reduces the work of packaging applications to near zero; it minimizes network traffic because a given Docker layer is downloaded to a host only once; and it provides a strong security sandbox so that volunteer computers are protected from buggy or malicious applications, (2) File management: Input and output files must be moved between existing private servers and public-facing servers that are accessible to the outside Internet. A file management system will be developed, based on Web RPCs, for this purpose. This system will use content-based naming so that a given file is transferred and stored only once. It also maintains job/file associations so that files can be automatically deleted from the public server when they are no longer needed. (3) Submitting and monitoring jobs:  BOINC provides a web interface for efficiently submitting and monitoring large batches of jobs. These were originally developed as part of a system to migrate HTCondor jobs to BOINC. This project is extending it to support the additional requirements of TACC and nanoHUB. Note that these new capabilities are not specific to TACC or nanoHUB: they provide the glue needed to easily add BOINC-based VC to any existing HTC facility. The team is also developing RPC bindings in several languages (Python, C++, PHP). The other component of the project, Science United, is a database-driven web site and an associated web service for the BOINC clients. Science United will control volunteer hosts (i.e. tell them which projects to work for) using BOINC's ""Account Manager"" mechanism, in which the BOINC client on each host periodically contacts Science United and is told what projects to run. Project servers, not Science United, will distribute jobs and files. Science United will define a set of ""keywords"" for science areas (physics, biomedicine, environment, etc.) and for location (country, institution). Projects will be labelled with appropriate keywords. Volunteers will have a yes/no/maybe interface for specifying the types of jobs they want to run. Science United will thus provide a mechanism in which a fraction of total computing capacity can be allocated to a project for a given period. Because total capacity changes slowly over time, this allows near-certain guaranteed allocations. Science United will embody a scheduling system that attempts to enforce allocations, honor volunteer preferences, and maximize throughput. Finally, Science United will do detailed accounting of computing. Volunteer hosts will tell Science United how much work (measured by CPU time and FLOPs, GPU time and FLOPs, and number of jobs) they have done for each project. Science United will maintain historical records of this data for volunteers and projects, and current totals with finer granularity (e.g. for each host/project combination). Finally, Science United will provide web interfaces letting volunteers see their contribution status and history, and letting administrators add projects, control allocations, and view accounting data."
"1642391","SI2-SSE: Hearing the Signal through the Static: Realtime Noise Reduction in the Hunt for Binary Black Holes and other Gravitational Wave Transients","OAC","LIGO RESEARCH SUPPORT, COMPUTATIONAL PHYSICS, Software Institutes","11/01/2016","09/04/2018","Chad Hanna","PA","Pennsylvania State Univ University Park","Continuing Grant","Bogdan Mihaila","10/31/2020","$400,000.00","","crh184@psu.edu","201 Old Main","University Park","PA","168021503","8148651372","CSE","1252, 7244, 8004","7433, 7483, 7569, 8004, 8005, 8084","$0.00","Gravitational waves - tiny ripples in the fabric of space - were detected for the first time in history on September 14, 2015 by the US-led gravitational wave observatory, LIGO. This watershed event has ushered in a new era of gravitational wave astronomy, which will transform our understanding of the Universe by providing information through a previously inaccessible channel. LIGO operates at the very edge of the sensitivity required to detect gravitational waves, and therefore, non-stationary noise caused by the environment, e.g., weather and man-made noise, limits LIGO's sensitivity to short-duration gravitational wave transient signals such as the one detected in September. In order to ensure the most opportunity for the advancement of science, this project aims to mine the extensive auxiliary information available in the LIGO observatories in realtime in order to mitigate the impact of non-stationary noise and increase the rate of transient gravitational wave detections. Doing so will afford increased opportunities for joint gravitational wave and electromagnetic observations, which are thought to be rare.<br/><br/>This project aims to address a key piece of missing software infrastructure to use machine learning and inference techniques to utilize auxiliary information such as seismometers, microphones and various control loop signals, to identify non-stationary noise that couples to the gravitational-wave channel in near realtime. The software will be broken into three components: a signal decomposer, a signal classifier and a signal trainer. The signal classifier will determine, given the decomposed, instantaneous output of auxiliary channels and the training data, the probability that non-stationary noise is present. These software components will be built from reusable resources that can be applied across the time-domain gravitational wave community in data calibration, data transfer, and analysis.<br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division in the Directorate of Mathematical and Physical Sciences."
"1642341","SI2-SSE: Foundations for MATPOWER as an Extensible Tool for Power Systems Research and Education","OAC","CCSS-Comms Circuits & Sens Sys, Software Institutes","11/01/2016","09/07/2016","Ray Zimmerman","NY","Cornell University","Standard Grant","Amy Walton","10/31/2020","$449,998.00","","rz10@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","CSE","7564, 8004","026Z, 155E, 7433, 8004, 8005, 8084","$0.00","The electric power system is one of the most fundamental and critical infrastructures underlying modern society, and the economic, environmental and societal impacts of advances in its planning and operations are potentially enormous. MATPOWER is a set of open-source scientific software elements for electric power system simulation, analysis and optimization. MATPOWER is already widely used in its current form, especially for research and education. The purpose of this project is to restructure the internals of MATPOWER as well as software development platform to make it easier to customize and extend by its community of users and developers. This restructuring will serve to increase MATPOWER's future impact as a successful research tool for designing and analyzing the power systems of the future. As power grids evolve toward more sustainable, economically efficient and environmentally friendly systems, positioning MATPOWER to expand its role as a flexible research and educational tool in this arena of innovation and change has the potential for far reaching and transformative impact both nationally and globally.<br/><br/>MATPOWER addresses some of the most fundamental classes of problems in power systems analysis, namely the power flow (PF), optimal power flow (OPF) and related problems used to determine the steady state voltages, currents and power flows arising from the interactions between system conditions, operator control actions and the laws of physics. The aim of this work is to broaden and extend MATPOWER's reach as a research-enabling tool for tackling future power systems problems in two specific ways. The first is to develop the project infrastructure needed to transition to and sustain a community-driven, collaborative development paradigm. The second is to redesign the core MATOWER software around a general modular architecture that will enable more flexible user customization and facilitate significant user contributions, while retaining and enhancing the simplicity that makes it attractive in education. Specifically, the new architecture will greatly simplify the process of adding arbitrary new device models, such as three-winding transformers, voltage dependent loads, flexible AC transmission system (FACTS) devices and more sophisticated models of HVDC lines. It will also facilitate the inclusion of additional controls, such as remote voltage regulation, switched shunts and transformer and phase shifter tap changing. Finally, it will serve as the basis to expand MATPOWER to handle three-phase, unbalanced network modeling, as required for many low-voltage distribution systems, opening the door for use in many additional current and emerging research areas."
"1047932","Collaborative Research: SI2-SSE: Software for integral equation solvers on manycore and heterogeneous architectures","OAC","OFFICE OF MULTIDISCIPLINARY AC, DYNAMICAL SYSTEMS, COFFES, Software Institutes","09/15/2010","09/07/2010","Denis Zorin","NY","New York University","Standard Grant","Daniel Katz","12/31/2014","$250,000.00","","dzorin@mrl.nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","CSE","1253, 7478, 7552, 8004","1253, 7478, 7552","$0.00","We propose to develop and deploy mathematical software for boundary-value problems in three-dimensional complex geometries.  The algorithms in the library will be based on integral equation formulations.  The library will be designed to scale on novel computing platforms that comprise special accelerators and manycore architectures. <br/><br/>Integral equations can be used to conduct simulations on many problems in science and engineering with significant societal impact.  Three example applications on which the proposed simulation technologies will have an impact in this project are microfluidic chips, biomolecular electrostatics, and plasma physics.  First, microfluidic chips are submillimeter-sized devices used for medical diagnosis and drug design.  Optimizing the function of such devices at low cost requires efficient computer simulation tools, such as the ones we propose to develop.  Second, understanding the structure and function of biomolecules such as DNA and proteins is crucial in biotechnology.  The proposed technologies can be used to resolve bimolecular electrostatic interactions.  Third, plasma physics, which is related to fusion nuclear reactors, includes electrostatic interactions in complex geometries, and the proposed work will enable large-scale three-dimensional simulations. <br/><br/>The key features of the proposed software are: (1) parallel fast multipole methods, (2) efficient geometric modeling techniques for complex geometries, (3) simple library interfaces that allow use of the proposed software by non-experts, and (4) scalability on heterogeneous architectures.<br/><br/>Along with our research activities, an educational and dissemination program will be designed to communicate the results of this work to students and researchers.  Several postdoctoral, graduate, and undergraduate students will be involved with the project.  Additional educational activities will include research experiences for undergraduates, leveraging ongoing programs such as NSF REUs.  We will encourage participation by women, minorities, and underrepresented groups."
"1740310","Collaborative Proposal:  SI2-SSE:  An open source multi-physics platform to advance fundamental understanding of plasma physics and enable impactful application of plasma systems","OAC","PLASMA PHYSICS, OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","09/01/2017","08/30/2017","Davide Curreli","IL","University of Illinois at Urbana-Champaign","Standard Grant","Rob Beverly","08/31/2019","$160,001.00","","dcurreli@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","1242, 1253, 8004","004Z, 026Z, 1062, 7433, 7569, 8004, 8005, 8396","$0.00","As the world moves toward a more sustainable life cycle for vital resources, new techniques for the synthesis, modification, or remediation of materials will be needed. Techniques that utilize plasma discharges will make significant contributions to a more sustainable nexus spanning food, water, and energy. To advance the fundamental understanding of these plasma-based systems and how they interact with the materials that will drive this higher level of sustainability, the ability to simulate both the complex interactions within the plasma itself and the complex interaction of the plasma with surrounding materials is needed. This project will provide a powerful simulation platform to the scientific community that will enable the study of plasma chemistry formation and plasma material interaction with a level of fidelity that is not currently available to researchers around the world. The open-source framework for this platform will enable researchers from institutions around the world to contribute to the capabilities of this framework and advance the underlying science of these systems to move toward a more sustainable food, energy, and water nexus.<br/><br/><br/>To advance plasma-based technology that will enable greater sustainability in the future food, energy, and water nexus, there exists an overarching need for advances in simulation capability that address four unifying research challenges, 1.) Plasma Produced Selectivity in Reaction Mechanisms in the Volume and on Surfaces, 2.) Interfacial Plasma Phenomena, 3.) Multiscale, Non-Equilibrium Chemical Physics, and 4.) Synergy and Complexity in Plasmas. This research effort will expand, deploy, and support a powerful open-source multi-physics platform that will enable advanced simulation in these unifying research areas. A plasma science simulation application will be expanded to include complex multi-phase chemistries, multiple-domain simulation of the interface between plasmas and other material phases, and fully coupled electro-magnetic treatment of plasma systems that will link plasma formation mechanisms with underlying chemical and electrical multi-phase interactions. Zapdos will be supported on the existing multi-physics Object Oriented Simulation Environment (MOOSE) and will leverage the existing support, verification, revision tracking, and training infrastructure and best known methods employed by both MOOSE and the 22 developed applications (including Zapdos) that currently reside on the MOOSE framework. This proposal will leverage collaboration not only between the two partnering universities, but with framework developers (Idaho National Laboratory), existing users (Oak Ridge National Laboratory), and the broader plasma community (APS Topical Meeting on Gaseous Electronics) to develop efficient development, deployment, support, and training of this impactful simulation tool.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering, the Physics Division and the Office of Multidisciplinary Activities in the Directorate of Mathematical and Physical Sciences, and the Division of Chemical, Bioengineering, Environmental, and Transport Systems in the Directorate of Engineering."
"1339745","SI2-SSI: Collaborative: The XScala Project: A Community Repository for Model-Driven Design and Tuning of Data-Intensive Applications for Extreme-Scale Accelerator-Based Systems","OAC","CYBERINFRASTRUCTURE","10/01/2013","09/16/2013","David Bader","GA","Georgia Tech Research Corporation","Standard Grant","Alan Sussman","09/30/2018","$1,188,710.00","Edward Riedy, Richard Vuduc","bader@njit.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","CSE","7231","7433, 8009, 9145","$0.00","The increasing gap between processor and memory performance -- referred to as the memory wall -- has led high-performance computing vendors to design and incorporate new accelerators into their next-generation systems. Representative accelerators include recon&#64257;gurable hardware such as FPGAs, heterogeneous processors such as CPU+GPU processors, highly multicore and multithreaded processors, and manycore co-processors and general-purpose graphics processing units, among others. These accelerators contain myriad innovative architectural features, including explicit control of data motion, large-scale SIMD/vector processing, and multithreaded stream processing. Such features provide abundant opportunities for developers to achieve high-performance for applications that were previously deemed hard to optimize. This project aims to develop tools that will assist developers in using hardware accelerators (co-processors) productively and effectively.<br/> <br/>This project's specific technical focus is on data-intensive kernels including large dictionary string matching, dynamic programming, graph theory, and sparse matrix computations that arise in the domains of biology, network security, and the social sciences. The project is developing XScala, a software framework for designing efficient accelerator kernels. The framework contains a variety of design time and run-time performance optimization tools. The project concentrates on data-intensive kernels, bound by data movement. It proposes optimization techniques including (a) enhancing and exploiting maximal concurrency to hide data movement; (b) algorithmic reorganization to improve spatial and/or temporal locality; (c) data structure transformations to improve locality or reduce the size of the data (compressed structures); and (d) prefetching, among others. The project is also developing a public software repository and forum, called the XBazaar, for community-developed accelerator kernels. This project includes workshops, tutorials, and the PIs class and summer projects as various means by which to increase community involvement. The broader impacts include productive use of emerging classes of accelerator-augmented computer systems; creation of an open and accessible community repository, the XBazaar, for distributing accelerator-tuned computational kernels, software, and models; training of graduate and undergraduate students; and dissemination through publications, presentations at scientific meetings, lectures, workshops, and tutorials. The framework itself will be released as open-source code and as precompiled binaries for several common platforms, through the XBazaar, as an initial step toward building a community around accelerator kernels."
"1102418","SI2-SSE: Adaptive Software for Quantum Chemistry","OAC","OFFICE OF MULTIDISCIPLINARY AC, PROJECTS, Software Institutes","10/01/2010","12/07/2010","So Hirata","IL","University of Illinois at Urbana-Champaign","Standard Grant","Daniel Katz","08/31/2014","$391,079.00","","sohirata@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","1253, 1978, 8004","","$0.00","The goal of this project is to establish a new paradigm of scientific software, electing quantum chemistry as the domain science. The new software does not have a static, compiled code, but instead consists of an expert system and code generator. At every execution, it analyzes the hardware and application parameters, determines(parallel) algorithms, and implements them for one-time use. This strategy not only allows unprecedented flexibility in algorithm optimization but can also realize ideas that are impossible otherwise. Since the approach makes no assumption about hardware or application, it is more extensible, maintainable, and portable. It is particularly well suited for chemistry, where a variety of molecules and reactions is infinite.<br/><br/>The expected long-term impact of this project is a change in the way scientific and engineering computing software is developed and defined. It promises novel software technology, which simultaneously achieves development efficiency, high product quality, and increased ability to optimize the code and enhance the methodological capabilities, by having no fixed source code. This project also offers unique, interdisciplinary education for chemistry graduate students, which places exceptionally large focus on computing, the field that has been a driving force of the 21st century economy.<br/><br/>This is an award within the solicitation of Software Infrastructure for Sustained Innovation. The award is co-funded by the Office of Cyberinfrastructure, the Division of Chemistry and the Office of Multidisciplinary Activities."
"1450440","SI2-SSI: Collaborative Research: A Software Infrastructure for MPI Performance Engineering: Integrating MVAPICH and TAU via the MPI Tools Interface","OAC","Software Institutes","09/01/2015","08/31/2015","Dhabaleswar Panda","OH","Ohio State University","Standard Grant","Stefan Robila","08/31/2020","$1,200,000.00","","panda@cse.ohio-state.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","CSE","8004","7433, 8009","$0.00","Message-Passing Interface (MPI) continues to dominate the supercomputing landscape, being the primary parallel programming model of choice. A large variety of scientific applications in use today are based on MPI.  On the current and next-generation High-End Computing (HEC) systems, it is essential to understand the interaction between time-critical applications and the underlying MPI implementations in order to better optimize them for both scalability and performance. Current users of HEC systems develop their applications with high-performance MPI implementations, but analyze and fine tune the behavior using standalone performance tools.  Essentially, each software component views the other as a blackbox, with little sharing of information or access to capabilities that might be useful in optimization strategies.  Lack of a standardized interface that allows interaction between the profiling tool and the MPI library has been a big impediment.  The newly introduced MPI_T interface in the MPI-3 standard provides a simple mechanism that allows MPI implementers to expose variables representing configuration parameters or performance measurements from within the implementation for the benefit of tools, tuning frameworks, and other support libraries.  However, few performance analysis and tuning tools take advantage of the MPI_T interface and none do so to dynamically optimize at execution time. This research and development effort aims to build a software infrastructure for MPI performance engineering using the new MPI_T interface.<br/><br/>With the adoption of MPI_T in the MPI standard, it is now possible to take positive steps to realize close interaction between and integration of MPI libraries and performance tools.  This research, undertaken by a team of computer scientists from OSU and UO representing the open source MVAPICH and TAU projects, aims to create an open source integrated software infrastructure built on the MPI_T interface which defines the API for interaction and information interchange to enable fine grained performance optimizations for HPC applications.  The challenges addressed by the project include: 1) enhancing existing support for MPI_T in MVAPICH to expose a richer set of performance and control variables; 2) redesigning TAU to take advantage of the new MPI_T variables exposed by MVAPICH; 3) extending and enhancing TAU and MVAPICH with the ability to generate recommendations and performance engineering reports; 4) proposing fundamental design changes to make MPI libraries like MVAPICH ``reconfigurable'' at runtime; and 5) adding support to MVAPICH and TAU for interactive performance engineering sessions.  The framework will be validated on a variety of HPC benchmarks and applications.  The integrated middleware and tools will be made publicly available to the community.  The research will have a significant impact on enabling optimizations of HPC applications that have previously been difficult to provide.  As a result, it will contribute to deriving ""best practice"" guidelines for running on next-generation Multi-Petaflop and Exascale systems.  The research directions and their solutions will be used in the curriculum of the PIs to train undergraduate and graduate students."
"1740263","SI2-SSE: Expanding the Scope of Materials Modeling with Electron Phonon Wannier (EPW) Software","OAC","DMR SHORT TERM SUPPORT, Software Institutes, DMREF","09/01/2017","08/24/2017","Elena Roxana Margine","NY","SUNY at Binghamton","Standard Grant","Rob Beverly","08/31/2021","$500,000.00","Madhusudhan Govindaraju","rmargine@binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139024400","6077776136","CSE","1712, 8004, 8292","026Z, 054Z, 7433, 8004, 8005, 8400, 9102, 9216","$0.00","Introduction of efficient non-empirical computational methods for modeling and predicting advanced materials properties is at the heart of the on-going effort to accelerate theory-guided materials discovery. The open-source software Electron-Phonon-Wannier (EPW) code offers unique capabilities for high-accuracy calculations of properties at the quantum mechanical level. In particular, EPW provides insight into the microscopic mechanisms that govern the interaction between electrons and atomic vibrations. Within this project, the existing capabilities will be extended to model a wider range of materials with complex electronic and magnetic properties. The knowledge can be used to design new-generation materials for harvesting of solar and thermal energy, making transition from electronics to spintronics, or realizing exotic states of matter. EPW will serve the broad electronic structure community of physicists, materials scientists, chemists, and engineers who work on modeling and designing next-generation materials for thermoelectric, photovoltaic, superconducting, spintronic, and other applications. The development of high-performance materials is crucial for addressing emergent societal challenges related to energy and environment, transportation, and information and communication technologies. The developed computational tools will be released under the GNU General Public License to ensure that the scientific community will directly and timely benefit from this technology. A broad spectrum of educational and outreach activities proposed within the project will promote and popularize scientific research in diverse communities. Planned hands-on workshops on EPW in the US and Europe will help create a strong EPW community for further development of the code and foster new research collaborations among participants from different countries. Interactive events for elementary school students in the upstate New York area will help attract a new generation of scientists from underrepresented groups into the STEM disciplines.<br/><br/><br/>The current focus of the electronic structure community is to introduce new capabilities enabling the design of emerging high-performance materials for thermoelectric, photovoltaic, superconducting, spintronic, and other applications. Function-defining properties in these applications are notoriously difficult to evaluate with desired accuracy using present density functional theory-based methods. The aim of this project is to expand the functionalities and broaden the impact of the open-source software Electron-Phonon-Wannier (EPW) in the area of materials research. EPW, now distributed as part of the Quantum ESPRESSO suite, has emerged as a unique computational tool that offers functionalities not available in standard electronic structure packages. By combining density-functional perturbation theory and maximally-localized Wannier functions methods, EPW makes it computationally feasible to calculate millions of electron-phonon matrix elements. The proposed work will expand the current capabilities of the EPW code to modeling an important class of spin-dependent materials properties. The proposed methodological, and user-oriented objectives are chosen to align with the focal directions of the SSE program and the DMR pertaining to creation, expansion, and deployment of robust software targeting a large user base. In particular, predictive calculations of spin transport, spin relaxation, and spin dynamics can yield fundamental insights into processes at the atomic scale and provide the necessary foundation to rationally design new materials and guide experimental work. The project will also provide easy management and execution of day to day scientific experiments on large-scale computing infrastructures. The introduction of workflows for automating, storing, managing, and sharing simulations will facilitate data transparency and communication as well as advance data-driven materials design to new frontiers. Successful implementation of these objectives will substantially enhance the functionalities of the EPW code and ensure the continued growth of the EPW user community. The developed computational tools will be released under the GNU General Public License to ensure that the scientific community will directly and timely benefit from this technology. The research program will be tightly integrated with educational and outreach activities. It will enable interdisciplinary training of students in advanced electronic structure methods, computational material science, and high-performance computing. Other efforts will include science demonstrations to elementary school students, development of a special course in materials modeling to incorporate computer simulations in the Binghamton University's undergraduate and graduate curriculum, and organization of workshops to teach the underlying theory and optimal usage of the EPW code.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences."
"1642440","SI2-SSE: PAPI Unifying Layer for Software-Defined Events (PULSE)","OAC","Software Institutes","11/01/2016","09/08/2016","Anthony Danalis","TN","University of Tennessee Knoxville","Standard Grant","Seung-Jong Park","10/31/2020","$499,997.00","Heike Jagode","adanalis@icl.utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Leading scientific domains, such as physics, chemistry, climate science, and advanced materials design, utilize high-performance computing (HPC) to understand and solve problems of unprecedented complexity. Overcoming such challenges requires the ability to perform advanced scientific and engineering simulations, and to analyze the extreme amount of data these computer models involve. But the ever increasing scale of these problems also means that the complexity of software systems needed to address them is rising, and this fact raises new challenges for the scientific application communities that require HPC in order to achieve their goals. In particular, HPC application developers who want to understand the performance characteristics of their application had previously been able to monitor the way it interacted with the underlying hardware, but they had no access to a standardized way of accessing the behavior of the complex software stack that their application depends on. The PULSE project will fill this major software infrastructure gap. It offers an integrated solution that enables different layers of a complex software stack to communicate with one another and provide information about their internal behavior. Thus, PULSE makes it possible for the scientific applications of the future to harness ever increasing amounts of computing power, despite anticipated increases in the complexity of both future hardware and software technologies. By providing the infrastructure that developers need in order to achieve an analytical understanding of the behavior of whole programs, it will substantially improve the insight computational scientists have into how various modern software systems interact with one another and the underlying hardware technologies. &#8232;<br/><br/>The abstraction and standardization layer provided by the Performance Application Programming Interface (PAPI) has played a critical role in enabling application profiling for over a decade. It has enabled performance conscious developers to gain insights about their application by simply instrumenting their code using a handful of PAPI functions that interoperate across different hardware substrates. At the same time, the abstraction layer offered by PAPI has enabled sophisticated profiling toolkits to focus on combining, organizing and visualizing information in a way that is useful to the end user, instead of re-implementing the hardware access layer for every new platform that comes to market. However, this abstraction layer that PAPI offers has been limited to profiling information generated by hardware. Information regarding the behavior of the software stack underneath the application that is being profiled has to be acquired either through low level binary instrumentation, or through custom APIs. Now, through this PULSE project, abstraction and unification layer for profiling software events has emerged. PULSE will extend the abstraction and unification layer that PAPI has provided to hardware events to also encompass software events. On one end, it will provide a standard, well defined and well documented API that high level profiling software can utilize to acquire and present to application developers performance information about the libraries used by their application. On the other end, it will provide standard APIs that library and runtime writers can utilize to communicate to higher software layers information about the behavior of their software. The project is expected to have a direct influence on the state of the art in whole program profiling and understanding. The success of PULSE will substantially improve the insight of computational scientists and engineers into the way that different modules of modern software interact with one another and the underlying hardware. Broadening the applicability of PAPI, as proposed under PULSE, is expected to dramatically increase PAPI's impact in this area."
"1642409","SI2-SSE: Scaling up Science on Cyberinfrastructure with the Cooperative Computing Tools","OAC","Software Institutes","09/01/2016","05/17/2018","Douglas Thain","IN","University of Notre Dame","Standard Grant","Seung-Jong Park","08/31/2020","$510,711.00","","dthain@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","8004","026Z, 062Z, 7433, 8004, 8005, 9251","$0.00","This project will support the continued development of the Cooperating Computing Tools (CCTools)software, which provides scientists with the ability to distribute existing simulation and analysis software codes to large numbers of computers in commercial clouds, advanced supercomputers, or both.  As a result, research in fields such as physics, biology, and chemistry can be accelerated, in some cases, by a hundred- or even a thousand-fold.  Staff and students involved in this project will develop new software capabilities and work closely with scientific communities to improve their productivity in advanced computing facilities.  The project will also help to develop a technical workforce by training graduate and undergraduate students in advanced software engineering skills.<br/><br/>Today's computational scientist has access to an extraordinary range of computing facilities, including thousands of cores and petabytes of storage drawn from commercial cloud providers, university clusters, or national computing resources. However, end users often want to connect these systems in ways that the designers did not anticipate. They may wish to access data at one facility, run software at a second facility, and store the results at yet another. However, combining infrastructure is much harder than it ought to be, because most resources assume that their users only live within that particular closed world. Programs built in one environment may not run in another; data stored in one system may not be accessible in another; specialized programming models may be fast on one machine, but useless on another machine. Researchers do not want to be tied down to a single system; rather, they want standard programs to execute easily across all environments. This project will create new features and support users of the Cooperating Computing Tools (CCTools) software which provides scientific users with the ability to harness large scale cyber-infrastructure for data intensive scientific applications. The key components are the Parrot virtual filesystem, the Makeflow workflow system, and the Work Queue application framework. These components can be used separately or together to take existing POSIX applications and scale them up from a single laptop all the way to national scale infrastructure. This project will advance the development of the CCTools software to meet the changing technology landscape in three key respects: exploiting container technologies, making efficient use of local concurrency, and performing capacity management at the workflow scale. This grant will support a small team of students and staff who will engage user communities to understand their needs, sustain and extend the software on current cyberinfrastructure, and perform outreach and education to users both new and old. The project will focus on active user communities in high energy physics, which rely on Parrot for global scale filesystem access in campus clusters and the Open Science Grid; bioinformatics users executing complex workflows via the VectorBase, LifeMapper, and CyVerse disciplinary portals, and ensemble molecular dynamics applications that harness GPUs from XSEDE and commercial clouds."
"1147522","SI2-SSE: A Unified Software Environment to Best Utilize Cache and Memory Systems on Multicores","OAC","Information Technology Researc, Software Institutes","06/01/2012","06/22/2012","Xiaodong Zhang","OH","Ohio State University","Standard Grant","Rob Beverly","05/31/2017","$500,000.00","","zhang@cse.ohio-state.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","CSE","1640, 8004","1640, 7942, 8004, 8005","$0.00","This project further develops and maintains a set of system and application software to benefit application users of multicores by providing a unified software environment where multicore system utilities can be easily used as common functions in various applications. The project consists of the following three tasks. First, we further improve our cache-partitioning OS utility, and make efforts to add two other critical system utilities: (1) Multicore buffer cache management to prevent the shared cache from thrashing and pollution; (2) multicore-aware synchronization lock management to effectively make process assignments such  that the co-running processes would minimize bandwidth consumption within a multicore chip and cross multiple multicore chips.  Second, we continue our efforts to develop a software runtime library that enables programmers to explicitly manage and optimize the shared cache usage and memory accesses by allocating proper cache space and memory modules for different data sets of different processes.  Finally, we provide a unified software environment for application users. With a set of easy interface functions, the users can access both middleware runtime library and the system utilities without a requirement of knowing architectural and system details. <br/><br/>The broader and transformative impact of the project can be significant: (1) Our software will provide effective and accessible solutions for significant performance improvement in multicores for a large scope of application community. (2) Gaining the insights into system interactions among applications, OS, and multicore architecture, we will provide valuable guidance for designs and implementations of application software. (3) The software is online with a maintenance for a public, wide, and sustained usage, which will directly impact open source software, and contribute to application users. (4) The research and software development of the project will train both undergraduate and graduate students for their future technical innovations in academia and industries."
"1549970","SI2-SSI:Collaborative Research: ENKI: Software Infrastructure that ENables Knowledge Integration for Modeling Coupled Geochemical and Geodynamical Processes","OAC","Petrology and Geochemistry","09/01/2016","08/19/2016","George Bergantz","WA","University of Washington","Standard Grant","Seung-Jong Park","08/31/2021","$141,957.00","","bergantz@u.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","1573","7433, 8004, 8009","$0.00","Earth scientists seek to understand the mechanisms of planetary evolution from a process perspective in order to promote the progress of science.  They model the chemistry of melting of the interiors of planets as a result of heat flow within the body.  They calculate the flows of energy and mass from the interior to the surface. They model the interaction of fluids and rocks, which drives chemical weathering and the formation of ore deposits.  They seek to understand the synthesis and stabilities of organic compounds and their economic and biological roles.  They study the interactions of atmosphere, oceans, biosphere and land as a dynamically coupled evolving chemical system. To achieve this level of understanding of planetary evolution, Earth scientists use software tools that encode two fundamentally different types of models: (1) thermodynamic models of naturally occurring materials, and (2) models of transport that track physical flows of both fluids and solids.  Much of the fundamental science of planetary evolution lies in understanding coupled thermodynamic and transport models. This grant funds development of a software infrastructure that supports this coupled modeling of the chemical evolution of planetary bodies.   It is their aim to establish an essential and active community resource that will engage a large number of researchers, especially early career scientists, in the exercise of model building and customization.  <br/><br/>This is a project to create ENKI, a collaborative model configuration and testing portal that will transform research and education in the fields of geochemistry, petrology and geophysics.  ENKI will provide software tools in computational thermodynamics and fluid dynamics.  It will support development and access to thermochemical models of Earth materials, and establish a standard infrastructure of web services and libraries that permit these models to be integrated into fluid dynamical transport codes. This infrastructure will allow scientific questions to be answered by quantitative simulations that are presently difficult to impossible because of the lack of interoperable software frameworks.  ENKI, via the adoption of state-of-the-art model interfacing (OpenMI) and deployment environments (HubZero), will modernize how thermodynamic and fluid dynamic models are used by the Earth science community in five fundamental ways: (1) provenance tracking will enable automatic documentation of model development and execution workflows, (2) new tools will assist users in updating thermochemical models as new data become available, with the ability to merge these data and models into existing repositories and frameworks, (3) automated code generation will eliminate the need for users to manually code web services and library modules, (4) visualization tools and standard test suites will facilitate validation of model outcomes against observational data, (5) collaborative groups will be able to share and archive models and modeling workflows with associated provenance for publication. With these tools we seek to transform the large community of model users, who currently depend on a small group of dedicated and experienced researchers for model development and maintenance, into an empowered ensemble of model developers who take ownership of the process and bring their own expertise, intuition and perspective to shaping the software tools they use in daily research.  ENKI development will be community driven.  Participation of a dedicated and diverse group of early career professionals will guide us in user interface development - insuring portal capabilities are responsive to user needs, and in development of a rich set of documentation, tutorials and examples.  All software associated with this project will be released as open source."
"1339727","SI2-SSE: Software Infrastructure for Revealing Gene and Genome Evolution, Anchored by Enhancement of Multiple Genome Alignment Software MCSCAN","OAC","Cross-BIO Activities, Software Institutes","02/01/2014","09/16/2013","Xiyin Wang","GA","University of Georgia Research Foundation Inc","Standard Grant","Bogdan Mihaila","01/31/2019","$493,692.00","Andrew Paterson","wangxy@uga.edu","310 EAST CAMPUS RD TUCKER HALL R","ATHENS","GA","306021589","7065425939","CSE","7275, 8004","7433, 8005","$0.00","The software MCSCAN, used to align multiple genomes, will be enhanced to contribute to deciphering the structure and evolutionary trajectories of eukaryotic genomes and genes, in particular addressing consequences of recursive whole-genome duplications. Burgeoning sets of eukaryotic genome sequences provide the foundation for a new spectrum of investigations into the functional and evolutionary consequences of gene and genome duplication, as well as the means to clarify knowledge of relationships among particular genes. The current software can only align small numbers of genomes; and layers of duplicated blocks produced by different genome duplication events are not readily deconvoluted, thus failing to provide crucial information toward understanding evolutionary trajectories of genomes and gene families. The enhanced software will mitigate these limitations.<br/><br/>The enhanced software will greatly help researchers to reconstruct the evolutionary trajectories of genomes and gene families, including the singularly challenging genomes of angiosperms and other taxa that have experienced polyploidization events. In particular, multiple alignment (of an expanded number of genomes) will be preceded by a multiple-way comparison of homologous regions at the DNA level, which will provide a holographic grasp of layers of homology produced by different duplication events. To reflect the evolutionary trajectories of structural changes, genomes will be input in a stepwise manner, with those of simple structures first. The resulting multiple alignment will much more accurately depict evolutionary relationships between chromosomal regions from diverse genomes, and easily be visualized and understood by users. The core part of the software will be implemented using the C++ programming language while the visualization module will be developed in Python language. The multiple and pairwise alignment information will be stored in MySQL or SQLite databases. The software will be tamed to work under multiple operating systems, including MS Windows, UNIX and Linux. Online service will be developed using the Django Web framework and jQuery (a concise JavaScript Library), and added to our NSF-supported PGDD. The software will be formed by several independent modules, which can be freely used by other researchers. A to-be-constructed web server accompanying the software will show figures illustrating genome structures, comparison between different plants, and evolutionary changes inferred to have occurred over millions of years. These intuitive visual resources will benefit researchers seeking to understand the evolution of plants, as well as elementary and middle school students, and readers at local libraries.  The program will regularly host visitors from other institutions, countries, and the public. The enhanced software and related results in genomic analysis will be reported in academic conferences."
"1441963","SI2-SSI: SAGEnext: Next Generation Integrated Persistent Visualization and Collaboration Services for Global Cyberinfrastructure","OAC","Software Institutes, Cybersecurity Innovation","05/16/2014","09/10/2020","Jason Leigh","HI","University of Hawaii","Continuing Grant","Bogdan Mihaila","09/30/2021","$5,204,441.00","","leighj@hawaii.edu","2425 CAMPUS RD SINCLAIR RM 1","HONOLULU","HI","96822","8089567800","CSE","8004, 8027","7433, 8004, 8009, 8211, 9150, 9251","$0.00","Cyberinfrastructure runs the gamut - from computing, networks, data stores, instruments, observatories, and sensors, to software and application codes - and promises to enable research at unprecedented scales, complexity, resolution, and accuracy. However, it is the research community that must make sense of all the data being amassed, so the SAGEnext (Scalable Adaptive Graphics Environment) framework is an innovative user-centered platform that connects people to their data and colleagues, locally and remotely, via tiled display walls, creating a portal, or wide-aperture ""digital lens,"" with which to view their data and one another. It enables Cyber-Mashups, or the ability to juxtapose and integrate information from multiple sources in a variety of resolutions, as easily as the Web makes access to lower-resolution images today. <br/><br/>SAGEnext expands on a vibrant partnership among national and international universities, supercomputer centers, government laboratories and industry; 100 institutions worldwide use the current version of SAGE. For computational scientists, from such diverse fields as biology, earth science, genomics, or physics, SAGEnext will transform the way they manage the scale and complexity of their data. For computer scientists, SAGEnext is a platform for conducting research in human-computer interaction, cloud computing, and advanced networking. SAGEnext capabilities, integrating visualization application codes, cloud documents, stereo 3D, and new user-interaction paradigms, is unprecedented and heretofore not available, and will have a transformative effect on data exploration and collaboration, making cyberinfrastructure more accessible to end users, in both the laboratory and in the classroom. SAGEnext, integrated with advanced cyberinfrastructure tools, will transform the way today's scientists and future scientists manage the scale and complexity of their data, enabling them to more rapidly address problems of national priority - such as global climate change or homeland security - which benefits all mankind. These same tools can better communicate scientific concepts to public policy and government officials, and via museum exhibitions, to the general public."
"1339600","Collaborative Research: SI2-SSE: Modules for Experiments in Stellar Astrophysics","OAC","STELLAR ASTRONOMY & ASTROPHYSC, OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","01/01/2014","08/29/2013","Francis Timmes","AZ","Arizona State University","Standard Grant","Rajiv Ramnath","12/31/2017","$306,840.00","","fxt44@mac.com","660 S MILL AVE STE 312","TEMPE","AZ","852813670","4809655479","CSE","1215, 1253, 8004","1206, 7433, 8005","$0.00","As the most commonly observed objects, stars remain at the forefront of astrophysical research. Technical advances in detectors, computer processing power, networks and data storage have enabled new sky surveys. Many of these search for transient events at optical wavelengths, such as the Palomar Transient Factory and Pan-STARRS1 that probe ever-larger areas of the sky and ever-fainter sources, opening up the vast discovery space of ""time domain astronomy"". The recent Kepler and COROT space missions achieved nearly continuous monitoring of more than 100,000 stars. The stellar discoveries from these surveys include revelations about stellar evolution, rare stars, unusual explosion outcomes, and remarkably complex binary star systems. The immediate future holds tremendous promise, as both the space-based survey Gaia and the ground based Large Synoptic Survey Telescope come to fruition. This tsunami of data has created a new demand for a reliable and publicly available research and education tool in computational stellar astrophysics that will reap the full scientific benefits of these discoveries while also creating a collaborative environment where theory, computation and interpretation can come together to address critical scientific issues. This demand by the stellar community led to our release of the Modules for Experiments in Stellar Astrophysics (MESA) software project in 2011.  MESA has driven, and will continue to drive with support from this award, innovation in the stellar community as well as the exoplanet, galactic, and cosmological communities. Educators have widely deployed MESA in their undergraduate and graduate stellar evolution courses because MESA is a community platform with an active support network for leading-edge scientific investigations. Stellar astrophysics research, and all the communities that rely on stellar astrophysics, will be significantly enhanced by sustaining innovative development of MESA.<br/><br/>This award supports the Modules for Experiments in Stellar Astrophysics (MESA) software project and user community.  MESA solves the 1D fully coupled structure and composition equations governing stellar evolution. It is based on an implicit finite difference scheme with adaptive mesh refinement and sophisticated timestep controls; state-of-the-art modules provide equation of state, opacity, nuclear reaction rates, element diffusion, boundary conditions, and changes to the mass of the star. MESA is an open source library that employs contemporary numerical approaches, supports shared memory parallelism based on OpenMP, and is written with present and future multi-core and multi-thread architectures in mind. MESA combines the robust, efficient, thread-safe numerical and physics modules for simulations of a wide range of stellar evolution scenarios ranging from very-low mass to massive stars. Innovations in MESA and its domain of applicability continues to grow, just recently extended to include giant planets, oscillations, and rotation.  This project will sustain MESA as a key piece of software infrastructure for stellar astrophysics while building new scientific and educational networks."
"1740097","SI2-SSE:   GenApp - A Transformative Generalized Application Cyberinfrastructure","OAC","Software Institutes","10/01/2017","08/29/2017","Emre Brookes","TX","University of Texas Health Science Center San Antonio","Standard Grant","Stefan Robila","03/31/2019","$269,601.00","","emre.brookes@umontana.edu","7703 FLOYD CURL DR","San Antonio","TX","782293901","2105672340","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Scientific computing and computational analysis are becoming integral aspects of virtually any field of science, be it exact sciences like Physics, Chemistry and Biology, or social sciences. Efforts of many research laboratories are focused on creation of scientific codes for data generation, analysis and interpretation. However, publicly funded and often hard won scientific codes developed in a typical research laboratory too frequently become unsustainable  beyond the lifetime of funding or shortly after staff rotation. Projects that are funded to afford expensive computer science expertise simply to maintain and update existing software divert scarce resources from the lab's primary goals and often translates the problem without resolving it.  Only a select number of researchers receive sufficient funding to maintain and update software, limiting the dissemination of new ideas and techniques. The diversity and continually changing nature of software environments compounds the issues.  Enabling user utilization presents hurdles in deployment, access and training.  These issues also create barriers to the implementation of new ideas embodied in new codes.  The GenApp project's goals are to address these issues. To begin with, GenApp enables the rapid dissemination of scientific codes to researchers with minimal software expertise. As more researchers use these codes, more of them become vested in the codes, which helps their sustainability. <br/><br/>The fundamental goal of this project is to advance the GenApp framework into a transformative tool to broadly benefit the scientific software developer community. GenApp is a generalized application generation framework intended for rapid deployment of scientific codes, which can generate both science gateways and stand-alone applications. Among the main unique features of GenApp are the minimal technical expertise requirement for the end user and an open-end design ensuring sustainability of generated applications. To produce fully functional applications, GenApp weaves libraries of fragments and user defined modules as directed by simple definition files, created from a uniform, logical, and simple-to-encode general interface definition file provided by GenApp.  This general definition file and the underlying software can be reused indefinitely to produce applications in a variety of existing and yet-to-be defined software environments. Preserving such simplicity with GenApp's maturation is one of the main developmental strategies. To achieve the goal of GenApp four focus Aims have been proposed. The first is infrastructure development, which includes general enhancements to the capabilities of GenApp. The second is documentation, training, dissemination, outreach and sustainability - all important aspects to produce a software product that is useful to the community. The third is simply feedback, since user and developer feedback will help drive the first two Aims.  The final Aim includes two structural biology domain science applications that will adopt and drive GenApp development.  GenApp will see its primary practical utilization in making highly demanding novel computational and analysis tools accessible to experimentalists and theoreticians working in the nuclear magnetic resonance (NMR) and small-angle scattering (SAS) domains of structural biology. The GenApp framework will serve as a platform for applications utilizing advanced tools requiring efficient use of HPC resources, tools for modeling SAS data with molecular simulations, and a large software suite for a combined analysis of NMR and SAS measurements coupled to computational modeling. Easy access to these powerful tools will enable hitherto impossible studies of a number of fundamental biological problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1741870","Collaborative Research: SI2-SSI: Open Source Support for Massively Parallel, Generic Finite Element Methods","OAC","Software Institutes","03/01/2017","04/25/2017","Wolfgang Bangerth","CO","Colorado State University","Standard Grant","Stefan Robila","07/31/2018","$393,703.00","","bangerth@colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","8004","7433, 8009","$0.00","Partial differential equations are used in a wide variety of applications as<br/>mathematical models. Their numerical solution is, consequently, of prime<br/>importance for the accurate simulation and optimization of processes in the<br/>sciences, engineering, and beyond.<br/>The last decade saw the emergence of large and successful libraries that<br/>support such applications. While these libraries provide most of what such<br/>codes need for small-scale computations, many realistic applications yield<br/>problems of hundreds of millions or billions of unknowns and require clusters<br/>with thousands of processor cores, but there is currently little generic<br/>support for such problems, limiting access to the many large publicly<br/>supported computing facilities to experts in computational science and<br/>excluding scientists from many fields for whom computational simulation would<br/>be a useful tool.  This project intends to build the software infrastructure that will allow a<br/>wide cross section of scientists to utilize these large resources.<br/><br/><br/>This project intends to support the software infrastructure for the<br/>large-scale solution of partial differential equations on massively parallel<br/>computational resources in a generic way. It will build on two of the most<br/>successful libraries for scientific computing, the finite element library<br/>deal.II, and Trilinos that provides the parallel linear algebra capabilities<br/>for the former. Specifically, we will: (i) Make support for massively parallel<br/>computations ubiquitous in deal.II; (ii) Research and develop seamless support<br/>for problems with billions of unknowns in both libraries and improve the<br/>interaction between the two; (iii) Exploit intra-node parallelism on today's<br/>clusters; (iv) Ensure the applicability of our work on a broad basis by<br/>implementing two real-world applications. <br/>Both deal.II and Trilinos have large, active and diverse developer and user<br/>communities, and this project will actively engage these communities through<br/>user meetings, short courses, regularly taught classes, mailing lists, and<br/>direct contact in focused projects."
"1048018","SI2-SSE: Software Infrastructure For Partitioning Sparse Graphs on Existing and Emerging Computer Architectures","OAC","CSR-Computer Systems Research, Software Institutes","09/15/2010","09/07/2010","George Karypis","MN","University of Minnesota-Twin Cities","Standard Grant","Sol Greenspan","08/31/2015","$499,784.00","Michael Whalen","karypis@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7354, 8004","7354","$0.00","Algorithms that find good partitionings of large, sparse, and unstructured graphs represent an important technique for developing effective and computationally efficient approaches for problems that need to process and analyze such graphs. As a result, they have found extensive applications in many diverse areas such as high-performance computing, scientific computing, VLSI design, data mining, pattern recognition, computer graphics, network analysis, database and geographical information systems, operations research, optimization, and scheduling. This project will develop and make available a software infrastructure that provides a broad range of graph partitioning tools for large, sparse, and unstructured graphs. This infrastructure will be built using modern object-oriented software engineering principles that will facilitate their modularity, user-extensibility, maintainability, and community development; and incorporate novel graph partitioning algorithms that can scale to graphs containing billions of nodes and facilitate the partitioning of different types of graphs on different computing architectures. This software infrastructure will enable the efficient execution of scientific numerical simulations on parallel systems containing tens of thousands of processing nodes and billions of mesh elements, the development of divide-and-conquer approaches for synthesizing very large VLSI circuits on different chip architectures, the clustering and analysis of very large graphs and networks, and the solution of a wide-range of partitioning problem instances involving different objectives and constraints.<br/><br/>This will positively impact numerous science & engineering disciplines, commercial companies, non-profit organizations, and individuals that benefit from the results of the computations that are enabled and facilitated by the various application domains that rely on graph partitioning. Finally, the project integrates the research with an educational plan focused on undergraduate and graduate education and mentoring through courses, software engineering projects, summer institutes, and research opportunities; and a community development and an outreach plan designed to promote broad adoption of the resulting software infrastructure by providing extensive documentation, online tutorials, and organizing meetings at relevant conferences and workshops."
"1047955","SI2-SSE: SciDB - A Scientific DataManagement System","OAC","CSR-Computer Systems Research, Software Institutes","09/15/2010","09/14/2010","Michael Stonebraker","MA","Massachusetts Institute of Technology","Standard Grant","Daniel Katz","08/31/2014","$500,000.00","Samuel Madden","stonebraker@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7354, 8004","7354","$0.00","Current relational database systems (RDBMSs) were engineered for the business data processing market, and not for scientific users (e.g, astronomers, physicists, chemists, oceanographers, or earth scientists). By and large, science users either grumble and use RDBMSs or, more often, ""roll their own"" data management software. Significant projects, such as the Large Hadron Collider (LHC) and the NASA Mission to Planet Earth, have spent millions of dollars on custom software systems, with limited applicability to other projects.  As such, after a generation of science applications, there is limited shared data management infrastructure. <br/><br/>SciDB is a project focused on building an open-source DBMS focused on the needs of science users. We have developed the requirements of SciDB based on a close collaboration with a number of scientists.  Data management features include a nested array data model (rather than the tabular model of RDBMSs) with operations attuned to scientific data, a no-overwrite storage model (allowing interaction with historical results), and support for uncertainty, named versions and provenance information.  <br/><br/>At this point, there is a distributed team of 17 programmers and scientists working actively on the design and implementation of SciDB, assisted by an advisory committee of 15 scientists. This team is primarily focused on the research issues surrounding the design of SciDB, and most contributors are involved in the project as volunteers or are paid by their individual organizations. <br/><br/>A demo of the first working proof-of-concept SciDB prototype was given at the VLDB conference in August 2009, and a first public release of SciDB is planned for September 2010. The purpose of this NSF grant is to enhance SciDB with additional science-oriented features, including time travel, versions, uncertainty and provenance.  With NSF?s help, we expect to develop a full-function system by the end of the grant period."
"1047962","SI2-SSE: A Tracing Virtual Machine for Statistical Computing","OAC","OFFICE OF MULTIDISCIPLINARY AC, Special Projects - CCF, CSR-Computer Systems Research, COFFES, Software Institutes","09/15/2010","09/08/2010","Jan Vitek","IN","Purdue University","Standard Grant","Sol Greenspan","08/31/2013","$489,084.00","Olga Vitek","j.vitek@neu.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","1253, 2878, 7354, 7552, 8004","1253, 1640, 2878, 7552","$0.00","Scripting languages are lightweight, dynamic computer programming languages designed to maximize productivity by offering high-level abstractions and reducing syntactic overhead. Scripting languages strive to optimize programmer time, rather than machine time, which is desirable early in the software life cycle. However, they lose their appeal when requirements stabilize and projects enter their deployment phase. The compromises made to reduce development time make it hard to scale to large data sets or to get the needed speed to perform computationally intensive tasks.  This is certainly the case for the R scripting language and its development environment. R is an extremely powerful tool for Biostatistics practitioners. Over the years, it has grown to become a mainstay for statistical computing in the life sciences.  This project's goal is to build an execution environment for the R programming language, which we call ReactoR, with particular emphasis on improving scalability for real-world applications in the life sciences. The proposed work will yield an implementation of R which delivers performance comparable to compiled native code and lets developers write code without having to resort to low-level intrinsics. This virtual machine will include a trace-based compiler for generating native code for the most frequently executed routines and a concurrent garbage collector to decrease footprint."
"1339737","SI2-SSE: Peer-to-Peer Overlay Virtual Network for Cloud Computing Research","OAC","Special Projects - CCF, Software Institutes","09/01/2013","05/26/2015","Renato Figueiredo","FL","University of Florida","Standard Grant","Alan Sussman","08/31/2018","$494,107.00","","renato@acis.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","2878, 8004","7433, 8005, 9251","$0.00","Modern cloud computing systems use virtual machine technologies to deliver unprecedented flexibility to users, enabling businesses and individuals to cost-effectively deploy computing and storage capacity on-demand, at scale, across multiple infrastructures distributed geographically. While the ability to deploy virtual machines for cloud computing is widely supported, researchers face increasing challenges in prototyping and deploying experimental research systems that span across multiple cloud providers. In particular, providing end-to-end network connectivity among distributed virtual machines in today's Internet environment (where nodes are often constrained by firewalls and network address translators) requires significant investment of time in development, testing and maintenance of code needed solely to provide connectivity. This project addresses these connectivity challenges in cloud computing by developing an open-source scientific software element that allows researchers and users of clouds to seamlessly create virtual networks on demand for distributed virtual machines. To this end, the project creates software-defined virtual networks that support the standard Internet Protocol (IP) and use tunneling of virtual network packets over Peer-to-Peer (P2P) links among virtual machines for scalable and resilient messaging. In addition to the core IP-over-P2P virtual networking, the software provides a framework for configuration, management and monitoring that enables easy deployment of user-defined overlays for inter-cloud research experiments.  <br/>The open-source software developed in this project enables advances in the state-of-the-art of research of cloud computing systems and applications. Complementary to research and development activities, this project delivers educational modules, tutorials, software packages, and pre-configured virtual machine images that allow non-expert users to deploy their own virtual networks over private, commercial and public clouds. Because cloud computing technologies are increasingly pervasive and of growing importance to the economy and society, the broader impacts of this project can reach Internet users at large who benefit from the ability to seamlessly interconnect cloud virtual machines across multiple providers. In particular, leveraging online social networking technologies, the virtual network software software enables individuals and small groups to easily create social virtual private networks connecting personal computers and multiple cloud resources."
"1339676","SI2 SSI: Collaborative Research: Sustained Innovations for Linear Algebra Software (SILAS)","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS","10/01/2013","09/02/2015","James Demmel","CA","University of California-Berkeley","Continuing Grant","Rajiv Ramnath","09/30/2016","$611,954.00","","demmel@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","1253, 8004, 8069","7433, 8009","$0.00","As the era of computer architectures dominated by serial processors comes to a close, the convergence of several unprecedented changes in processor design has produced a broad consensus that much of the essential software infrastructure of computational science and engineering is utterly obsolete. Math libraries have historically been in the vanguard of software that must be quickly adapted to such design revolutions because they are the common, low-level software workhorses that do all the most basic mathematical calculations for many different types of applications. The Sustained Innovation for Linear Algebra Software (SILAS) project updates two of the most widely used numerical libraries in the history of Computational Science and Engineering---LAPACK and ScaLAPACK, (abbreviated Sca/LAPACK)---enhancing and hardening them for this ongoing revolution in processor architecture and system design. SILAS creates a layered package of software components, capable of running at every level of the platform deployment pyramid, from the desktop to the largest supercomputers in the world. It achieves three complementary objectives: 1) Wherever possible, SILAS delivers seamless access to the most up-to-date algorithms, numerical implementations, and performance, by way of Sca/LAPACK programming interfaces that are familiar to many computational scientists; 2) Wherever necessary, SILAS makes advanced algorithms, numerical implementations and performance capabilities available through new interface extensions; and 3) SILAS provides a well engineered conduit through which new discoveries at the frontiers of research in these areas can be channeled as quickly as possible to all the application communities that depend on high performance linear algebra. The improvements and innovations included in SILAS derive from a variety of sources. They represent the results (including designs and well tested prototypes) of the PIs' own algorithmic and software research agenda, which has targeted multicore, hybrid and extreme scale system architectures. They are an outcome of extensive and on-going interactions with users, vendors, and the management of large NSF and DOE supercomputing facilities. They flow from cross-disciplinary engagement with other areas of computer science and engineering, anticipating the demands and opportunities of new architectures and programming models. And finally, they come from the enthusiastic participation of the research community in developing and offering enhanced versions of existing Sca/LAPACK codes.<br/><br/>The primary impact of SILAS is a direct function of the importance of the Sca/LAPACK libraries to many branches of computational science. The Sca/LAPACK libraries are the community standard for dense linear algebra and have been adopted and/or supported by a large community of users, computing centers, and HPC vendors. Learning to use them is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. Application domains where Sca/LAPACK have historically been heavily used include (among a host of other examples) airplane wing design, radar cross-section studies, flow around ships and other off-shore constructions, diffusion of solid bodies in a liquid, noise reduction, and diffusion of light through small particles. Moreover, the list of application partners working with SILAS to enhance and transform these libraries for next generation platforms expands this traditional list to include quantum chemistry, adaptive mesh refinement schemes, computational materials science, geophysical flows, stochastic simulation and database research for ""big data"". No other numerical library can claim this breadth of integration with the community. Thus, there is every reason to believe that enhancing these libraries with state of the art methods and algorithms and adapting them for new and emerging platforms (reaching up to extreme scale), will have a correspondingly large impact on the research and education community, government laboratories, and private industry."
"1440727","SI2-SSE Collaborative Research: Molecular Simulations of Polymer Nanostructures in the Cloud","OAC","DMR SHORT TERM SUPPORT, Software Institutes, CDS&E","10/01/2014","07/31/2014","Alejandro Strachan","IN","Purdue University","Standard Grant","Alan Sussman","09/30/2018","$349,804.00","Chunyu Li, Benjamin Haley","strachan@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","1712, 8004, 8084","024E, 085E, 7237, 7433, 8005, 8400, 9216","$0.00","The use of polymers, their composites, and nanostructures is growing at a fast pace, both by displacing traditional materials and by enabling emerging technologies. Examples range from the all-composite airframe of the Boeing 787 and the new Airbus 350 to wearable electronics. This project aims to develop a software infrastructure to simulate these materials with atomic resolution and make these tools universally accessible and useful via on-line computing. These simulations have the potential to accelerate the development of optimized material formulations that can benefit society and reduce the associated costs by combining physical with computational experiments. Making these advanced tools available for free online simulations and complementing them with tutorials and educational material will encourage their use in the classroom and will impact the education of new generations of engineers and scientists familiar with these powerful tools that will be required to address tomorrow's challenges.<br/><br/>The objective of this effort is to enable pervasive, reproducible molecular simulations of polymeric materials using state-of-the-art tools and with quantified uncertainties building on recent breakthroughs in molecular simulations, cyber-infrastructure and uncertainty quantification. The framework will consist of three main components: i) powerful simulation tools for polymer nano structures including: state-of-the-art molecular builders, a parallel MD engine with stencils that enable efficient structure relaxation and property characterization and post-processing codes; ii) a UQ framework to orchestrate the molecular simulations and propagate uncertainties in input parameters to predictions and compare the predictions to experimental values; iii) databases of force fields and molecular structures as well as predicted and experimental properties. The simulation framework will be deployed via NSF's nanoHUB where users will be able to run online simulations without downloading or installing any software while expert users will have the option to download, modify and contribute to the infrastructure. Usage of and contributions to the software framework will be facilitated and encouraged via online and in-person user guides, learning modules and research tutorials."
"1450372","Collaborative Research: SI2-SSI: ELSI-Infrastructure for Scalable Electronic Structure Theory","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","06/15/2015","06/11/2015","Lin Lin","CA","University of California-Berkeley","Standard Grant","Bogdan Mihaila","05/31/2020","$504,016.00","","linlin@math.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","1253, 1712, 8004","7433, 8009, 8084, 9216","$0.00","Predictive, so-called ab initio electronic structure calculations, particularly those based on the Kohn-Sham density functional theory (DFT) are now a widely used scientific workhorse with applications in virtually all sciences, and increasingly in engineering and industry. In materials science, they enable the computational (""in silico"") design of new materials with improved properties. In biological or pharmacological research, they provide molecular-level insights into the function of macromolecules or drugs. In the search for new energy solutions, they give molecular-level insights into new solar cell designs, catalytic processes, and many others. A key bottleneck in many applications and calculations is the ""cubic scaling wall"" of the so-called Kohn-Sham eigenvalue problem with system size (i.e., the effort increases by a factor of 1,000 if the model size increases by a factor of 10). This project will establish an open source software infrastructure ""ELSI"" that offers a common, practical interface to initially three complementary solution strategies to alleviate or overcome the difficulty associated with solving the Kohn-Sham eigenvalue problem. ELSI will enable a broad range of end user communities, centered around different codes with, often, unique features that tie a specialized group of scientists to that particular solution, to easily incorporate state-of-the-art solution strategies for a key problem they all share. By providing these effective, accessible solution strategies, we will open up major areas for electronic structure theory where DFT based predictive methodologies are not applicable today. This will in turn open doors for new development in materials science, chemistry, and all related areas. Commitments to support ELSI exist from some of the most important electronic structure developer communities, as well as from industry and government leaders in high-performance computing. Thus, we will create a strong U.S. based infrastructure that leverages the large user and developer base from a globally active community developing DFT methods for materials research.<br/><br/>ELSI will support and enhance three state-of-the-art approaches, each best suited for a specific problem range: (i) The ELPA (EigensoLvers for Petascale Applications) library, a leading library for efficient, massively parallel solution of eigenvalue problems (for small- and mid-sized problems up to several 1,000s of atoms), (ii) the OMM (Orbital Minimization Method) in a recent re-implementation, which circumvents the eigenvalue problem by focusing on a reduced, auxiliary problem (for systems in the several 1,000s of atoms range), and (iii) the PEXSI (Pole EXpansion and Selective Inversion) library, a proven reduced scaling (at most quadratic scaling) solution for general systems (for problems with 1,000s of atoms and beyond). By establishing standardized interfaces in a style already familiar to many electronic structure developers, ELSI will enable production electronic structure codes that use it to significantly reduce the ""scaling wall"" of the eigenvalue problem. First, ELSI will help them make efficient use of the most powerful computational platforms available. The target platforms are current massively parallel computers and multicore architectures, GPU based systems and future manycore processors. Second, the project will make targeted methodological improvements to ELPA, OMM, and PEXSI, e.g., a more effective use of matrix sparsity towards very large systems. The focus on similar computational architectures and similar methodological enhancements will lead to significant cross-fertilization and synergy between these approaches."
"1450273","SI2-SSI: Collaborative Research: A Sustainable Infrastructure for Perfomance, Security, and Correctness Tools","OAC","Software Institutes","08/01/2015","07/20/2015","John Mellor-Crummey","TX","William Marsh Rice University","Standard Grant","Bogdan Mihaila","07/31/2020","$1,500,000.00","","johnmc@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","8004","7433, 8009","$0.00","Software has become indispensable to society, used by computational scientists for science and engineering, by analysts mining big data for value, and to connect society over the Internet. However, the properties of software systems for any of these purposes cannot be understood without accounting for code transformations applied by optimizing compilers used to compose algorithm and data structure templates, and libraries available only in binary form. To address this need, this project will overhaul, integrate, and enhance static binary analysis and runtime technologies to produce components that provide a foundation for performance, correctness, and security tools. The project will build upon three successful and widely adopted open source software packages: the DynInst library for analysis and transformation of application binaries, the MRNet infrastructure for control of large-scale parallel executions and data analysis of their results, and the HPCToolkit performance analysis tools. The project team will engage the community to participate in the design and evaluation of the emerging components, as well as to adopt its components. <br/><br/>This project will have a wide range of impacts. First, software components built by the project will enable the development of sophisticated, high-quality, end-user performance, correctness, and security tools built by the project team, as well as others in academia, government, and industry. Software developed by the project team will help researchers and developers tackle testing, debugging, monitoring, analysis, and tuning of applications for systems at all scales. Second, end-user tools produced by the project have a natural place in the classroom to help students write efficient, correct, and secure programs. Third, components produced by the project will lower the barrier for new researchers to enter the field and build tools that have impact on production applications without years of investment. Fourth, the project will provide training for graduate students and interns in the area of software for performance, correctness, and security. Finally, through workshops and tutorials, the project will disseminate project results, provide training to enable others to leverage project software, and grow a community of tool researchers who depend on project components and thus have a strong motivation to help sustain project software into the future.<br/><br/>Modernizing open-source software components and tools for binary analysis will enable static analysis of application characteristics at the level of executable machine code, transformation of binaries to inject monitoring code, measurement to capture a detailed record of application?s interactions with all facets of a target platform, analysis of recorded data in parallel, and attribution of analysis results back to application source code in meaningful ways. Providing innovative, software components that support development of robust performance, correctness, and security tools will accelerate innovation by tools researchers and help them grapple with the increasing complexity of modern software. Of particular note, helping tools researchers and computational scientists grapple with the challenges of software for modern parallel systems and producing training materials that help people use this software, addresses several of the needs identified in the NSF Vision for Cyberinfrastructure for the 21st Century."
"1440665","SI2-SSE: High-Performance Software for Large-Scale Modeling of Binding Equilibria","OAC","Software Institutes","08/01/2014","06/17/2014","Emilio Gallicchio","NY","CUNY Brooklyn College","Standard Grant","Rajiv Ramnath","07/31/2016","$141,135.00","","egallicchio@brooklyn.cuny.edu","Office of Research & Sponsored P","Brooklyn","NY","112102889","7189515622","CSE","8004","7433, 8005, 9216","$0.00","Living organisms are regulated by specific interactions among proteins and other macromolecules. Cancer and genetic diseases are due to altered interactions caused by mutations. Cellular organelles and viruses spontaneously build themselves from the ordered assembly of component molecules. While a complete understanding remains elusive, these and many other phenomena fundamentally rest on the ability of molecules to recognize and bind specifically to other molecules. The general principles of molecular recognition are being employed to design new drugs, chemical catalysts, and advanced materials. Computer models offer an important mean to disentangle and analyze molecular interactions and to produce quantitative predictions. The main objective of the project is to develop novel algorithms to model molecular recognition processes at atomic resolution on modern parallel computer architectures. This research seeks to increase the speed of the calculations and expand hardware support to enable the screening of larger sets of drug candidates and the study of multiple protein mutations under various conditions. The increased accuracy and availability of modeling software technologies by a larger community will lead to new ideas and research approaches, and ultimately to new discoveries in medicine, chemistry and material science. This effort will contribute to the establishment of in silico means to evaluate environmental and clinical claims. For example computational evidences on toxicity of substances can inform public policy in the same way that, for instance, atmospheric models are currently used for global climate projections.<br/><br/>The project targets the Binding Energy Distribution Analysis Method (BEDAM, for short), an accurate model of molecular binding, currently limited by computational performance. Outcomes of this research include deployment the BEDAM model for the first time on General Purpose Graphical Processing Units (GPGPU) and Many Integrated Core (MIC) massively parallel architectures. To this end, the mathematical formulation of the model will be tuned to best utilize the features of these modern computing architectures. Specialized recursive computational geometry algorithms will be developed to extract from the parallel hardware near optimal performance. Robust automated tools for the processing of molecular models and their analysis will be put in place for large scale applications. Accessible user interfaces will be implemented to ensure wide applicability and adoption of the software. These software applications will be distributed under an open source license to promote sharing and community contributions. Student research assistants form an integral part of the research team. While contributing meaningfully to scientific research, students from challenging socioeconomic backgrounds will acquire computer programming and software maintenance skills useful to enter the high tech job market."
"1450195","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","OAC","Physical & Dynamic Meteorology, Software Institutes, EarthCube","08/01/2015","08/04/2015","Robert Fovell","NY","SUNY at Albany","Standard Grant","Bogdan Mihaila","07/31/2020","$246,111.00","","rfovell@albany.edu","1400 Washington Ave MSC 312","Albany","NY","122220100","5184374974","CSE","1525, 8004, 8074","4444, 7433, 8009","$0.00","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities."
"1440800","SI2-SSE: Synthesizing Self-Contained Scientific Software","OAC","Special Projects - CCF, Software Institutes","10/01/2014","04/05/2019","Ashish Gehani","CA","SRI International","Standard Grant","Bogdan Mihaila","09/30/2020","$499,919.00","","ashish.gehani@sri.com","333 RAVENSWOOD AVE","Menlo Park","CA","940253493","7032478529","CSE","2878, 8004","2878, 7433, 8004, 8005","$0.00","Computational aspects of scientific experiments have been growing steadily. This creates an increasing need to be able to reproduce the results. Science is also increasingly performed by exploring diverse sets of data. Unsurprisingly, there is a demand for being able to easily repeat the numerous transformations performed. Software packaged with tools from this project will allow scientists to publish their code in a form that can be utilized by others with minimal effort. By eliminating many of the challenges of building, configuring, and running software, it will allow members of the scientific community to more easily reproduce each others' computational results.<br/><br/>Increasingly, entire virtual machines are published to ensure that a recipient does not have to replicate the compute environment, retrieve data and code dependencies, or invest effort into configuring the system. However, this approach scales poorly with the growth in size of the included data sets, the extraneous functionality in applications that utilize versatile software libraries, and the irrelevant code in stock operating system distributions. This project will design, develop, and evaluate a toolchain that allows scientists to transform their software into specialized applications with all the necessary environmental conditions and portions of required data sets built directly into the code. The resulting scientific appliances can be distributed for others to explore and verify results without the overhead of shipping extraneous data and code."
"1440607","SI2-SSE: Collaborative Proposal: Symbolic-Numeric Approaches to Polynomials","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS","09/01/2014","08/01/2014","Andrew Sommese","IN","University of Notre Dame","Standard Grant","Rajiv Ramnath","08/31/2018","$199,847.00","Bei Hu, Charles Wampler","sommese@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","1253, 8004, 8069","7433, 8005, 8251","$0.00","Polynomial systems arise naturally in many areas of human endeavor. These include the modeling of tumor growth; the design of robotic devices; chemical systems arising in areas ranging from combustion to blood clotting; assorted problems in physics; plus many areas with mathematics. The solution of the polynomial systems answers questions critical to these endeavors. This research will be devoted to developing the next generation of Bertini, an open source software package, which has been used successfully by many researchers on many problems, which include all those mentioned above.<br/><br/>Bertini will be rewritten in C++ to be scriptable and modular, which will allow it to be interfaced transparently with symbolic software. The new Bertini will include tools allowing the user to construct and manipulate homotopies based on the output of Bertini. A major focus of the research will be given to systems of polynomials arising from the discretization of systems of differential equations. The great challenge of these very large systems of polynomials is balanced by the great potential impact new efficient and robust methods of solution will have."
"1148523","SI2-SSI: Next-Generation Volunteer Computing","OAC","CI-TEAM, Software Institutes","04/15/2012","04/22/2015","David Anderson","CA","University of California-Berkeley","Standard Grant","Daniel Katz","09/30/2015","$1,100,000.00","","davea@ssl.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7477, 8004","7433, 8004, 8009","$0.00","By 2015, the global consumer digital infrastructure will include two resource pools of importance to scientific computing: a billion GPU-equipped desktop and laptop computers with a capability of roughly 100 ExaFLOPS, and 10 billion mobile devices with a capacity of about 20 ExaFLOPS and energy efficiency about 20 times better than larger devices.  Volunteer computing is a proven approach by which these resources can be used for scientific computing.  BOINC (Berkeley Open Infrastructure for Network Computing) is the dominant middleware for volunteer computing.  This project will extend BOINC to create the technology for the next generation of volunteer computing, focusing on the following areas:<br/><br/>1) Computing on mobile devices: support for scientific computing on smart phones and other mobile devices based on Android;<br/><br/>2) Virtual-machine applications: support for applications that run in virtual machines.  This reduces heterogeneity issues and, by providing a strong form of sandboxing, allows untrusted applications to be run on volunteer computers;<br/><br/>3) Multi-user projects: server-side features that allow the resources of a BOINC-based project to be shared fairly among many scientists.  This will allow BOINC to be used effectively by science portals.<br/><br/>In addition we will add other new capabilities to BOINC, and will add support for new operating system versions, GPU types and models, and language systems such as OpenCL.<br/><br/>By providing scientists with access to huge and essentially free computing power, this will enable new research in fields such as nanotechnology, proteomics, genomics, climate modeling, epidemiology, cancer care, bio-fuels, battery technology for electric vehicles, earthquake engineering, volcanic activity, pharmaceutical engineering, microelectromechanical systems, and modeling of biological ecosystems, to name a few.  In addition, by attracting millions of ?citizen scientists? who volunteer their computing resources, we will increase public awareness of and interest in science, and will expand the public outreach and education channel provided by volunteer computing"
"1148362","SI2-SSE: SCIFIO: An Extensible Framework for Scientific Image Interoperability","OAC","ADVANCES IN BIO INFORMATICS, Software Institutes","07/01/2012","06/29/2012","Kevin Eliceiri","WI","University of Wisconsin-Madison","Standard Grant","Rajiv Ramnath","06/30/2015","$499,845.00","","eliceiri@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","1165, 8004","8004, 8005","$0.00","Digital imaging is one of the most commonly used tools in all of science. In a wide array of disciplines ranging from astronomy and geology to environmental science and biology, digital imaging approaches are used to capture dynamic processes in great detail. In the biological sciences, digital light microscopy has transformed the field with unprecedented levels of accessibility, functionality and performance with fewer compromises. Not only have there been great advances in the hardware and software needed to collect digital images, but also in the software tools for analysis, interpretation, storage and dissemination of the images. The accessibility of digital microscopy and imaging in general has resulted not only in widespread use and adoption but in increased innovation as well, with scientist pushing the envelope in developing core technologies in new directions such as hybrid and multimodal developments. Despite the great technological advances occurring in the field by commercial and academic researchers alike, there still is a fundamental barrier in imaging research that nearly every scientist encounters in their imaging workflow: the Proprietary File Format (PFF). PFFs are what the majority of software programs use to record images and any subsequent image analysis. While there are implemented efforts to create better open microscopy formats to help reduce the number of PFFs, it is clear that one universal format for everyone is not practical. Rather, the greatest practical need facing the community is not a universal scientific imaging format, but rather a universal scientific imaging format converter. With such a system, any current or future imaging format can be supported, including conversion from any PFF to any open standard. A universal imaging converter would enable a scientist to open a PFF from any imaging system and fully parse and analyze the full image contents without the need for any proprietary software. Such a converter would not only be of great utility to biologists but also of great benefit to instrument developers, who are equally limited by the lack of transparency and access of PFFs. We propose to develop a robust scientific software element for imaging file format interoperability. This effort that we dub ""SCIFIO"" for ""Scientific Image Format Input and Output"" would build on our current successful ""Bio-Formats"" efforts to make a file converter for light microscopy in our research domain and ""harden"" these efforts to make a robust interchange library for all of biological microscopy. The system will be generalizable, extensible and adaptable to new emerging microscopy types. It will also serve as a model for adaptation to other scientific imaging types.<br/><br/>A great practical barrier to collaborative work in imaging is the issue of proprietary file formats. One of the most fundamental needs in imaging is being able to open and freely share the original pixel information and associated text information with others using any processing software or workflow desired. We are developing ""Scientific Image Format Input and Output"" (SCIFIO) a robust software package that can read and convert any proprietary image file format. By harnessing the power of a reusable software project like SCIFIO, the community will be able to freely share all content collected on any imaging system both for visualization and quantitative analysis. SCIFIO will be developed as a robust software element, both as a library anyone can utilize and as a full software tool kit that any developer can easily use to add converter support to their application freely. This is important as our target---and thus our impact---is not only on the research scientist, but the wider community including researchers from other fields, academic software developers, commercial software developers, and educators."
"1147802","Collaborative Research: SI2-SSE: Sage-Combinat: Developing and Sharing Open Source Software for Algebraic Combinatorics","OAC","OFFICE OF MULTIDISCIPLINARY AC, DYNAMICAL SYSTEMS, Software Institutes, CDS&E-MSS","06/01/2012","06/19/2012","William Stein","WA","University of Washington","Standard Grant","Daniel Katz","05/31/2015","$97,114.00","","wstein@math.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","1253, 7478, 8004, 8069","1253, 5514, 7433, 7478, 7683, 8004, 8005","$0.00","Sage is an open source general purpose mathematical software system that has developed explosively within the last six years. Sage-Combinat is a subproject whose mission is ""to improve Sage as an extensible toolbox for computer exploration in (algebraic) combinatorics, and foster code sharing between researchers in this area"".  There is a long tradition of software packages for algebraic combinatorics. These have been crucial in the development of combinatorics since the 1960s.  The originality of the Sage-Combinat project lies in successfully addressing the following simultaneous objectives. It offers a wide variety of interoperable and extensible tools, integrated in a general purpose mathematical software package, as needed for daily computer exploration in algebraic combinatorics; it is developed by a community of researchers spread around the world and across institutions; and it is open source and depends only on open source software. Among the proposers, Stein is founder and lead developer of Sage while Bump, Musiker, and Schilling are strong contributors to Sage-Combinat. Hivert and Thiery (Paris-Sud, Orsay), founders and lead developers of Sage-Combinat, are both strongly affiliated with this project. Some of the research areas addressed in this project include symmetric functions, in particular Macdonald polynomials for arbitrary Cartan types and their nonsymmetric analogues, crystals, rigged configurations and combinatorial R-matrices, affine Weyl groups and Hecke algebras, cluster algebras, and posets.<br/> <br/>The project will develop Sage-Combinat in areas relevant to the ongoing research of the participants, together with relevant underlying infrastructure. The project will include three Sage Days workshops, and will be affiliated with a third scheduled workshop at ICERM. These workshops include a strong outreach component and have been a potent tool for connecting researchers and recruiting Sage users and developers. The grant will also fund a dedicated software development and computation server for Sage-Combinat, to be hosted in the Sage computation farm in Seattle. Emphasis will be placed on the development of thematic tutorials that will make the code accessible to new users. The proposal will also fund graduate student RA support, curriculum development, and other mentoring."
"1550221","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)","OAC","COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","06/27/2019","Rainer Fries","TX","Texas A&M University","Continuing Grant","Bogdan Mihaila","06/30/2021","$284,067.00","","rjfries@comp.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","CSE","7244, 8004","026Z, 7433, 7569, 8009, 8084","$0.00","Microseconds after the Big Bang, the universe was filled with an extremely hot fluid called the Quark-Gluon Plasma. As the universe expanded, this plasma cooled and condensed into the building blocks of ordinary matter around us: protons, neutrons, and atomic nuclei. Droplets of this fluid, which exists only at temperatures above 2 trillion Kelvin, are generated and studied in the laboratory today using collisions of high-energy heavy ions at Brookhaven National Laboratory and CERN. A key method to study the Quark-Gluon Plasma is the generation of high-energy quarks and gluons in the collision, which interact with the hot plasma and emerge as ""jets"" of particles that are measured by experiments. These jets provide powerful tools to study the internal structure of the plasma, analogous to tomography in medical imaging. However, interpretation of jet measurements requires sophisticated numerical modeling and simulation, and comparison of theory calculations with experimental data demands advanced statistical tools. The JETSCAPE Collaboration, an interdisciplinary team of physicists, computer scientists, and statisticians, will develop a comprehensive software framework that will provide a systematic, rigorous approach to meet this challenge. Training programs, workshops, summer schools and MOOCs, will disseminate the expertise needed to modify and maintain this framework.<br/><br/>The JETSCAPE Collaboration will develop a scalable and portable open source software package to replace a variety of existing codes. The modular integrated software framework will consist of interacting generators to simulate (i) wave functions of the incoming nuclei, (ii) viscous fluid dynamical evolution of the hot plasma, and (iii) transport and modification of jets in the plasma. Integrated advanced statistical analysis tools will provide non-expert users with quantitative methods to validate novel theoretical descriptions of jet modification, by comparison with the complete set of current experimental data. To improve the efficiency of this computationally intensive task, the collaboration will develop trainable emulators that can accurately predict experimental observables by interpolation between full model runs, and employ accelerators such as Graphics Processing Units (GPUs) for both the fluid dynamical simulations and the modification of jets. The collaboration will create this framework with a user-friendly envelope that allows for continuous modifications, updates and improvements of each of its components. The effort will serve as a template for other fields that involve complex dynamical modeling and comparison with large data sets. It will open a new era for high-precision extraction of the internal structure of the Quark-Gluon Plasma with quantifiable uncertainties. <br/><br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. <br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences."
"1216747","Collaborative Research - SI2-S2I2: High-Performance Computational Science with Structured Meshes and Particles (HPCS-SMP)","OAC","OFFICE OF MULTIDISCIPLINARY AC, CFS-Combustion & Fire Systems, PMP-Particul&MultiphaseProcess, FD-Fluid Dynamics, PHYSICS AT THE INFO FRONTIER, Integrat & Collab Ed & Rsearch, Software Institutes","09/01/2012","08/28/2012","John Mellor-Crummey","TX","William Marsh Rice University","Standard Grant","Daniel Katz","08/31/2014","$62,035.00","Vivek Sarkar","johnmc@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","1253, 1407, 1415, 1443, 7553, 7699, 8004","056E, 058E, 1253, 1407, 1415, 1443, 7433, 7483, 7553, 7699, 8004, 8211, 9216, 9263","$0.00","The starting point for this proposal is a view of scientific simulation articulated in the conclusions of the 2008 National Academy of Sciences Study, The Potential Impact of High-End Capability Computing on Four Illustrative Fields of Science and Engineering: ""Advanced computational science and engineering is a complex enterprise that requires models, algorithms, software, hardware, facilities, education and training, and a community of researchers attuned to its special needs."" (p. 122)<br/><br/>Over the last few years, the design of computer and software systems, particularly as they relate to simulation in the physical sciences, has been organized around a collection of algorithmic patterns / motifs. These patterns have been very productive because they are a natural ""common language"" in which application scientists can express their computations, and for which computer scientists can provide optimized libraries, domain specific languages, compilers, and other software tools.<br/><br/>This project will design an institute focused on a subset of these patterns --- structured grid discretizations of partial differential equations and particle methods, along with the linear and nonlinear solvers that enable their effective use --- with the specific goals of providing simulation capabilities for a set of scientific domains that make heavy use of these patterns. Two major components are envisioned to this proposed institute, called the Institute for High-Performance Computational Science with Structured Meshes and Particles (HPCS-SMP). The first component is a software infrastructure development activity that will be performed by a team whose expertise spans the design and development of mathematical algorithms and software frameworks, as well as the design and development of compilers, runtime systems, and tools that enable one to obtain high performance from emerging multicore and heterogeneous architectures. The second component is an outreach activity, in which algorithms, libraries, and software frameworks developed by the institute will be customized and integrated into simulation codes for stakeholder application domains. At the heart of this activity will be collaborations and partnerships, in which the institute will provide one or more software developers to collaborate with application scientists over a period of months to years to develop a new simulation capability or enhance an existing one.<br/><br/>The design of this institute will be carried out through a series of workshops, each focused on one of five stakeholder science domains that have been identified as using these motifs and that play a central role in various NSF Grand Challenge problems, with participation of both representatives of the science domain and the the relevant mathematics and computer science communities. In addition, there will be a final workshop that will bring together the relevant mathematics and computer science experts to identify cross-cutting themes. These information obtained from these workshops will be used by the project to develop the final conceptual design of the institute, in the form of a document that includes the input from all of the workshops and our analysis of how this leads to a  design of a software institute."
"1450323","Collaborative Research: SI2-SSI: Data-Intensive Analysis for High Energy Physics (DIANA/HEP)","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","05/01/2015","08/15/2019","Brian Bockelman","NE","University of Nebraska-Lincoln","Continuing Grant","Bogdan Mihaila","10/31/2020","$1,001,324.00","","bbockelman@morgridge.org","2200 VINE ST BOX 830861","LINCOLN","NE","685032427","4024723171","CSE","1253, 7244, 8004","7433, 8009, 8084, 9150","$0.00","Advanced software plays a fundamental role for large scientific projects. The primary goal of DIANA/HEP (Data Intensive ANAlysis for High Energy Physics) is developing state-of-the-art tools for experiments that acquire, reduce, and analyze petabytes of data. Improving performance, interoperability, and collaborative tools through modifications and additions to packages broadly used by the community will allow users to more fully exploit the data being acquired at CERN's Large Hadron Collider (LHC) and other facilities. These experiments are addressing questions at the heart of physics: What are the underlying constituents of matter? And how do they interact? With the discovery of the Higgs boson in 2012, the Standard Model of particle physics is complete. It provides an excellent description of known particles and forces.  However, the most interesting questions remain open: What is the dark matter which pervades the universe? Does space-time have additional symmetries or extend beyond the 3 spatial dimensions we know? What is the mechanism stabilizing the Higgs boson mass from enormous quantum corrections? The next generation of experiments will collect exabyte-scale data samples to provide answers. Analyzing this data will require new and better tools. <br/><br/>First, the project will provide the CPU and IO performance needed to reduce the iteration time so crucial to explore new ideas. It will develop software to effectively exploit emerging many- and multi-core hardware. It will establish infrastructure for a higher-level of collaborative analysis, building on the successful patterns used for the Higgs boson discovery and enabling a deeper communication between the theoretical community and the experimental community. DIANA?s products will sit in the ROOT framework, already used by the HEP community of more than 10000 particle and nuclear physicists. By improving interoperability with the larger scientific software ecosystem, DIANA will incorporate best practices and algorithms from other disciplines into HEP. Similarly, the project will make its computing insights, tools, and novel ideas related to collaborative analysis, standards for data preservation, and best practices for treating software as a research product available to the larger scientific community.  Finally, to improve the quality of the next generation of software engineers in HEP, DIANA will host an annual workshop on analysis tools and establish a fellowship program."
"1148291","SI2-SSE: A GPU-Enabled Toolbox for Solving Hamilton-Jacobi and Level Set Equations on Unstructured Meshes","OAC","OFFICE OF MULTIDISCIPLINARY AC, DYNAMICAL SYSTEMS, Software Institutes, CDS&E-MSS","06/01/2012","03/31/2014","Robert Kirby","UT","University of Utah","Standard Grant","Rajiv Ramnath","05/31/2016","$531,999.00","Ross Whitaker","kirby@cs.utah.edu","201 PRESIDENTS CIR","SALT LAKE CITY","UT","841129049","8015816903","CSE","1253, 7478, 8004, 8069","1253, 7433, 7478, 8004, 8005, 9150, 9251","$0.00","A variety of application domains from geophysics to biomedicine employ some form of Hamilton-Jacobi (H-J) mathematical models. These models are a natural way to express conservation properties, and the two most prevalent H-J models seen in the literature are the Eikonal equation (a static H-J model based upon Fermat's Principle for determining minimal paths) and the Level-Set equations (a time-dependent H-J model used for addressing moving interface problems). The goal of this<br/>effort is to develop, test, document and distribute a collection of software tools for efficiently solving several classes of equations of H-J type -- in particular, Eikonal (minimal path) equations and Level-set equations -- on unstructured (triangular and tetrahedral) meshes using commodity streaming architectures. The PIs have previously demonstrated the feasibility of efficiently solving H-J equations on GPUs; this effort seeks to both scientific extend previous work as well as solidify the software into a publicly available tool suite.<br/> <br/>The intellectual merit of this effort is the development of efficient algorithmic strategies for mapping numerical methods for solving H-J equations on unstructured meshes to commodity streaming architectures. The proposed work will tackle several important technical challenges. One challenge is maintaining sufficient computational density on the parallel computational units (blocks), especially as we move to 3D unstructured meshes. A second technical challenge is the loss in efficiency that comes with communication between blocks. The solutions to these challenges will allow us to exploit currently available commodity streaming architectures that promising to provide teraflop performance on the desktop, which will be a boon for a variety of communities that rely on computationally expensive, simulation-based  experiments. By overcoming the tedious and non-trivial step of developing and distributing software for solving H-J equations on unstructured meshes using commodity streaming architectures, the impact of this work has both longevity and ubiquity in a wide range of applications in diverse fields such as basic science, medicine, and engineering."
"1535032","SI2-SSE: Scalable Multifaceted Graphical Processing Unit (GPU) Program Debugging","OAC","Information Technology Researc, Special Projects - CCF, Software Institutes","09/01/2015","09/14/2016","Ganesh Gopalakrishnan","UT","University of Utah","Standard Grant","Bogdan Mihaila","03/31/2019","$435,482.00","","ganesh@cs.utah.edu","201 PRESIDENTS CIR","SALT LAKE CITY","UT","841129049","8015816903","CSE","1640, 2878, 8004","019Z, 7433, 8004, 8005, 9150, 9179, 9251","$0.00","Modern scientific research crucially depends on software simulations that help model scientific phenomena, and accelerate the process of discoveries, and communal result sharing. With the availability of affordable computational accelerators known as GPUs, the scientific community has begun migrating their existing CPU codes as well as creating new codes targeting GPUs. Unfortunately, this has resulted in a situation where the generated scientific results do not often agree across CPUs and GPUs. This exacerbates the danger of drawing wrong conclusions in crucial areas such as physics, weather simulations, drug discovery, and engineering computations. This project offers a combination of existing and new techniques in dissecting scientific experiments conducted through simulations, obtaining believable results, finding the root causes of varying results, and developing best practices to ensure higher result fidelity. Its techniques have special emphasis on GPUs, given their often poorly specified and evolving nature.<br/><br/>Result variability has many causes, including evolving, incorrect, or ambiguous specifications of computer hardware and software, racing data accesses, varying floating point precision standards, and incorrect result association within compound computational steps. This project develops methods that help a scientist systematically search through and eliminate these causes, thus accelerating the process of debugging result variability. The produced tools and exemplars of known erroneous behaviors allow a scientist to avoid the use of incorrect specifications, isolate and eliminate data races, and isolate and eliminate unreliable numerical steps. It also develops methods that help a scientist maintain focus on their basic scientific pursuits while still keeping up with technology evolution. It trains students in critical software engineering techniques that help the nation build the talent pool necessary for the extreme scale computing era.<br/><br/>The project will combine six research thrusts (GPU concurrency; challenge problems and develop user interfaces; pedagogy for domain scientists; improved GPU concurrency debugging tool support; more reproducible simulation results; and evolving and scaling tools with standards) to build and deliver open source software that incorporates proven stress-testing methods into tools; builds challenge problems, supports formalization support, and designs the user interface; delivers demos, books, and tutorials that help illustrate concurrency nuances; exploits symbolic analysis for input generation in mixed formal and GPU runs; develops stress testing inputs for round-off errors and separable verification to root-cause roundoff; and componentizes the symbolic verifier to enable parallelism, targeting from new APIs."
"2006409","Organizing CSSI PI Meeting - Towards a National Cyberinfrastructure Ecosystem","OAC","Software Institutes","04/01/2020","05/22/2020","Haiying (Helen) Shen","VA","University of Virginia Main Campus","Standard Grant","Amy Walton","03/31/2021","$65,480.00","Upulee Kanewala, Sandra Gesing, Xiaohui Carol Song, Ritu Ritu","hs6ms@virginia.edu","1001 N EMMET ST","CHARLOTTESVILLE","VA","229034833","4349244270","CSE","8004","026Z, 7556, 8004","$0.00","This project will host a 2-day workshop in Seattle, WA, which will bring together the community of Cyberinfrastructure for Sustained Scientific Innovation (CSSI) awardees (with the goal of involving at least one principal investigator (PI) from each Elements, Frameworks, Institute Conceptualizations, and Scientific Software Innovation Institutes project, many of which are collaborative awards) from approximately 250 awards. This will be the first CSSI PI meeting. In addition, PIs from the prior connected solicitations (such as Software Infrastructure for Sustained Innovation (SI2) and Data Infrastructure Building Blocks (DIBBS)) will be invited, as well as PIs on NSF awards in which CSSI seed investments were made (Venture funded PIs as well as CSSI Early Concept Grants for Exploratory Research (EAGER) awardees). In addition, the proximity to Society for Industrial and Applied Mathematics (SIAM) Conference on Parallel Processing for Scientific Computing (PP20) will encourage participation by non-PI community to further inform the academic community of CSSI goals and projects. Goals of this workshop include: (1) Serve as a focused forum for PIs to share technical information with each other, community, and NSF Program Officers; (2) Explore innovative topics emerging within software and data infrastructure communities; (3) Discuss emerging best practices across the supported software and data infrastructure projects; (4) Stimulate thinking on new ways of achieving software and data sustainability; (5) Gather the shared experiences in an online web portal. The workshop is expected to host close to 250 CSSI and other awardees, other speakers and panelists. <br/><br/>The proposed workshop will support the exchange of ideas among the current cyberinfrastructure development projects. It will provide guidance on issues related to the development of robust software and data artifacts and to the problem of their sustainability. Involvement of program officers across NSF is expected to help the interdisciplinary CSSI awardees understand the relevance and impact of cyberinfrastructure throughout the NSF. The participation of these awardees, program officers, and other researchers in a common forum will help ensure that the cyberinfrastructure developed as part of CSSI projects will be relevant and broadly applicable to most science and engineering domains. The results of this workshop thus have the potential to guide cyberinfrastructure development and cyberinfrastructure driven research for both the participating projects and for the wider cyberinfrastructure development community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1440700","SI2-SSE: Enhancing the PReconditioned Iterative MultiMethod Eigensolver Software with New Methods and Functionality for Eigenvalue and Singular Value Decomposition (SVD) Problems","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","09/01/2014","05/08/2018","Andreas Stathopoulos","VA","College of William and Mary","Standard Grant","Alan Sussman","08/31/2018","$451,884.00","","andreas@cs.wm.edu","1314 S MOUNT VERNON AVE","WILLIAMSBURG","VA","231852817","7572213965","CSE","1253, 1712, 8004","062Z, 7433, 8004, 8005, 8400, 9216, 9251","$0.00","The numerical solution of large, sparse Hermitian Eigenvalue Problems (HEP) and Generalized HEP (GHEP) for a few extreme eigenvalues is one of the most important but also computationally intensive tasks in a variety of applications. Examples abound in spectral graph partitioning, large scale spectral graph analysis, structural engineering, electromagnetics, lattice Quantum Chromodynamics, and electronic structure applications from atomic scale physics to molecular scale materials science. Closely related is the problem of computing a partial Singular Value Decomposition (SVD) of a matrix, which finds everyday use in numerous applications including data mining and machine learning. The importance of the problem is evidenced by the significant resources that have been invested over the last decade in developing high quality eigenvalue software packages. However, these packages still do not include the near-optimal methods that have made the package PRIMME the software to beat. PRIMME, or PReconditioned Iterative MultiMethod Eigensolver, is a software package developed in 2005 for the solution of HEP.  PRIMME brings state-of-the-art preconditioned iterative methods from ""bleeding edge"" to production, with a flexible, yet highly usable interface.  Yet, it is its focus on numerical robustness and computational efficiency that has gained PRIMME the recognition as one of the best eigenvalue packages. This success calls for a new effort to extend PRIMME with some long awaited functionality but also to include new algorithms to address some outstanding problems in eigenvalue computations. This work is critical to many groups whose research depends on the lattice QCD and materials science software packages that PRIMME will improve through collaborations. PRIMME already has a PETSc interface, and with the proposed development of Hypre and Trilinos interfaces, it will be accessible by a far wider community of users. The most requested feature, however, has been a MATLAB interface. This will unleash the power of an ""industrial strength"" software to end users. Last but not least, this project will educate and train two graduate and several undergraduate students in the art of high performance numerical software.<br/><br/>Specific goals for this projects include: PRIMME extension to GHEP, with special attention to ill conditioned mass matrices; PRIMME extension to SVD, with special attention to obtaining results at high accuracy (the solution must include not only PRIMME's robust components but a combination of known and new methods, as well as a dynamic way to choose between them); implementation of new methods and techniques for the solution of highly interior eigenvalue problems and for the computation of a large number of eigenvalues; interoperability with DOE libraries and MATLAB, and improved means of dissemination. As a numerical linear algebra kernel, PRIMME has a large potential audience in the computational sciences community. However, two specific collaborations will provide real-world, challenging problems and serve as a stress-test evaluator of the resulting methods and software. One involves the lattice QCD group at the DOE's Jefferson Lab, and the other involves the high performance computing and materials science group at IBM, Zurich."
"1664022","Collaborative Research: SI2-SSI: Expanding Volunteer Computing","OAC","Software Institutes","05/15/2017","04/16/2018","Lucas Wilson","TX","University of Texas at Austin","Standard Grant","Stefan Robila","07/31/2020","$500,000.00","","jnet@tacc.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","CSE","8004","7433, 8004, 8009","$0.00","Volunteer computing (VC) uses donated computing time consumer devices such as home computers and smartphones to do scientific computing. It has been shown that VC can provide greater computing power, at lower cost, than conventional approaches such as organizational computing centers and commercial clouds. BOINC is the most common software framework for VC. Essentially, donors of computing time simply have to load BOINC on their computer or smartphone, and then register to donate at the BOINC web site. VC provides ""high throughput computing"": handling lots of independent jobs, with performance goals based on the rate of job completion rather than completion time for individual jobs. This type of computing (all known as high-throughput computing) is in great demand in most areas of science. Until now, the adoption of VC has been limited by its structure. For example, VC projects (such as Einstein@home and Rosetta@home) are operated by individual research groups, and volunteers must browse and choose from among many such projects. As a result, there are relatively few VC projects, and volunteers are mostly tech-savvy computer enthusiasts.  This project aims to solve these problems using two complementary development efforts: First, it will add BOINC-based VC conduits to two major high-performance computing providers: (a) the Texas Advanced Computing Center, a supercomputer center, and (b) nanoHUB, a web portal for nano science that provides computing capabilities.Also, a unified control interface to VC will be developed, tentatively called Science United, where donors can register.  The project  will benefit thousands of scientists who use these facilities, and it will create technology that makes it easy for other HPC providers to add their own VC back ends. Also, Science United will provide a simpler interface to BOINC volunteers where they will register to support scientific areas, rather than specific projects. Science United will also serve as an allocator of computing power among projects. Thus, new projects will no longer have to do their own marketing and publicity to recruit volunteers. Finally, the creation of a single VC ""brand"" (i.e Science United) will allow coherent marketing of VC to the public. By creating a huge pool of low-cost computing power that will benefit thousands of scientists, and increasing public awareness of and interest in science, the project plans to establish VC as a central and long-term part of the U.S. scientific cyber infrastructure.<br/><br/>Adding VC to an existing HPC facility involves several technical issues, which will be addressed as follows: (1) Packaging science applications (which typically run on Linux cluster nodes) to run on home computers (mostly Windows, some Mac and Linux): the team is developing an approach using VirtualBox and Docker, in which the application and its environment (Linux distribution, libraries, executables) are represented as a set of layers comprising a Docker image, which is then run as a container within a Linux virtual machine on the volunteer device. This has numerous advantages: it reduces the work of packaging applications to near zero; it minimizes network traffic because a given Docker layer is downloaded to a host only once; and it provides a strong security sandbox so that volunteer computers are protected from buggy or malicious applications, (2) File management: Input and output files must be moved between existing private servers and public-facing servers that are accessible to the outside Internet. A file management system will be developed, based on Web RPCs, for this purpose. This system will use content-based naming so that a given file is transferred and stored only once. It also maintains job/file associations so that files can be automatically deleted from the public server when they are no longer needed. (3) Submitting and monitoring jobs:  BOINC provides a web interface for efficiently submitting and monitoring large batches of jobs. These were originally developed as part of a system to migrate HTCondor jobs to BOINC. This project is extending it to support the additional requirements of TACC and nanoHUB. Note that these new capabilities are not specific to TACC or nanoHUB: they provide the glue needed to easily add BOINC-based VC to any existing HTC facility. The team is also developing RPC bindings in several languages (Python, C++, PHP). The other component of the project, Science United, is a database-driven web site and an associated web service for the BOINC clients. Science United will control volunteer hosts (i.e. tell them which projects to work for) using BOINC's ""Account Manager"" mechanism, in which the BOINC client on each host periodically contacts Science United and is told what projects to run. Project servers, not Science United, will distribute jobs and files. Science United will define a set of ""keywords"" for science areas (physics, biomedicine, environment, etc.) and for location (country, institution). Projects will be labelled with appropriate keywords. Volunteers will have a yes/no/maybe interface for specifying the types of jobs they want to run. Science United will thus provide a mechanism in which a fraction of total computing capacity can be allocated to a project for a given period. Because total capacity changes slowly over time, this allows near-certain guaranteed allocations. Science United will embody a scheduling system that attempts to enforce allocations, honor volunteer preferences, and maximize throughput. Finally, Science United will do detailed accounting of computing. Volunteer hosts will tell Science United how much work (measured by CPU time and FLOPs, GPU time and FLOPs, and number of jobs) they have done for each project. Science United will maintain historical records of this data for volunteers and projects, and current totals with finer granularity (e.g. for each host/project combination). Finally, Science United will provide web interfaces letting volunteers see their contribution status and history, and letting administrators add projects, control allocations, and view accounting data."
"1550551","SI2-SSI: Collaborative Research: Einstein Toolkit Community Integration and Data Exploration","OAC","COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","05/27/2021","Frank Loffler","LA","Louisiana State University","Continuing Grant","Varun Chandola","08/31/2022","$449,470.00","Steven Brandt, Peter Diener","knarf@cct.lsu.edu","202 HIMES HALL","BATON ROUGE","LA","708030001","2255782760","CSE","7244, 8004","7433, 7569, 8009, 8084, 9150","$0.00","A new astronomy has arrived with the recent detection of gravitational waves. Modeling of sources of gravitational radiation is more than ever a critical necessity in order to interpret the observations. The project Einstein Toolkit has as overarching mission to provide the scientific community with a sustainable software platform of core computational tools for research focused on astrophysical systems endowed with complex multi-scale/multi-physics properties which are governed by Einstein's equations of General Relativity. The central premise of the project Einstein Toolkit is to create a broad and vibrant community of users, a community where interdisciplinary collaborations are the norm and not the exception, a community driving advances in the next generation of high-performance computing cyberinfrastructure. The main objectives of the project Einstein Toolkit are: developing software tools for a radical increase in scientific productivity, achieving sustainability of the software ecosystem, addressing software engineering challenges, and the curation of data from general relativistic numerical simulations. <br/><br/>This project will achieve its goals through two major activity areas. Regarding the software ecosystem and its sustainability, the scheduler that handles the flow of tasks in a problem will be redesigned to be more versatile and to improve its performance. In addition new software modules will be developed to broaden the choices of  initial data and matter sources, as well as modules for problems requiring a high degree of experimentation with equations and numerical methodologies.  A new general relativistic magneto-hydrodynamics code will also be integrated in the Einstein Toolkit. The second activity area involves building  a simulation data repository. The repository will allows user to compare results, contribute data,  test  innovative ideas and algorithms for gravitational wave data analysis, and to explore or discover new phenomena in sources of gravitational radiation. The broader impact effort in the project Einstein Toolkit will be organized in two major activity areas. The first involves community integration. The project will support a program of ease-of-use on-line tutorials and a workshops/tutorial series. The program will  help small groups or individual investigators familiarizing with the codes and modules in the toolkit as well as pathways to become a developer. Regarding outreach and education, the project Einstein Toolkit will enable interdisciplinary training of students and postdocs in numerical relativity, computational astrophysics and computer science. The effort will includes developing a teaching resources bank for educational activities involving computational topics applied to gravitational physics and astrophysics. The educational resources will be suitable for computational courses in general relativity and astrophysics at both the graduate and undergraduate level.<br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division in the Directorate of Mathematical and Physical Sciences."
"1440769","SI2-SSE:  AMASS - An Automated Monitoring AnalySis Service for Cyberinfrastructure","OAC","Software Institutes","09/01/2014","08/15/2016","Shava Smallen","CA","University of California-San Diego","Standard Grant","Alan Sussman","08/31/2018","$515,205.00","Lawrence Saul, Sameer Tilak","ssmallen@sdsc.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","CSE","8004","7433, 8005, 9251","$0.00","A science gateway is a community-developed set of tools, applications, and data collections that are integrated via a portal or a suite of applications. It provides easy, typically browser-based, access to supercomputers, software tools, and data repositories to allow researchers to focus on their scientific goals and less on the cyberinfrastructure. These gateways are fostering collaboration and exchange of ideas among thousands of researchers from multiple communities ranging from atmospheric science, astrophysics, chemistry, biophysics, biochemistry, earthquake engineering, geophysics, to neuroscience, and biology. However due to limited development and administrative personnel resources, science gateways often leverage only a small subset of the NSF-funded CI to mitigate the complexities involved with using multiple resource and services at scale in part due to software and hardware failures. Since many successful science gateways have had unprecedented growth in their user base and ever increasing datasets, increasing their usage of CI resources without introducing additional complexity would help them meet this demand.<br/><br/>In response to this need, an Automated Monitoring AnalySis Service (AMASS) will be built to provide a flexible and extensible service for automated analysis of monitoring data initially focused on science gateways.  AMASS will be based on data mining and machine learning techniques and emerging big data technologies to analyze monitoring data for improving the reliability and operational efficiency of CI as well as progress on fundamental questions in systematic and population biology, computational neuroscience, and biophysics communities.  Along with AMASS, a simulation framework will be built for testing automated analysis algorithms and adaptive execution techniques.  An intuitive query API will be provided for science gateway software to use and will be integrated into the following three target science gateways that  will drive the project's research and development: the Cyberinfrastructure for Phylogenetic Research (CIPRES), the Neuroscience Gateway (NSG), and UltraScan. The proposed approach does not require any changes to the end user applications, and the software developments will significantly enhance the science productivity and user satisfaction of science gateways by integrating monitoring data into their infrastructure to enable adaptive execution of their applications, allowing scientists to answer more sophisticated questions without having to understand the complexities of a large-scale distributed environment. The developed software products will be available as open source products under an Apache License and will be integrated into the NSF-funded SciGap project in order to impact a broader range of science gateways."
"1440412","SI2-SSE: Wavelet Enabled Progressive Data Access and Storage Protocol (WASP)","OAC","ADVANCES IN BIO INFORMATICS, Physical & Dynamic Meteorology, Software Institutes, EarthCube","10/01/2014","04/10/2017","Lawrence Frank","CA","University of California-San Diego","Standard Grant","Rajiv Ramnath","04/30/2018","$599,999.00","","lfrank@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","CSE","1165, 1525, 8004, 8074","4444, 7433, 8004, 8005, 8009","$0.00","Advances in digital imaging methods are revolutionizing a wide range of scientific disciplines by facilitating the acquisition of huge amounts of data that allow the visualization and analysis of complex, multidimensional images. Concurrently, modern computing technologies enable numerical modeling of a broad gamut of scientific phenomena, resulting in vast quantities of numerical data, which are just the starting point for the scientific exploration that modern computational and visualization methods enable. This is particularly true in the biological and geosciences, two seemingly very different disciplines. These capabilities come with a cost: increasing data size and complexity require more sophisticated methods for data analysis and visualization. This project will conduct research that will lead to a common software framework for supporting a multi scale progressive data refinement method based upon the representation of the data as a wavelet expansion, and enabling interactive exploration of large data sets for the bio  and geoscience communities.  The development of a general toolkit for wavelet based representations of data will have broad impact, allowing the multi scale analysis, storage, and visualization for data collected in a wide range of fields and on a multitude of platforms, from high end computing facilities to laptop computers used by students, field biologists, and others.<br/><br/>Analysis and visualization of large data sets play an important role in scientific discovery. Efficient, and broadly available tools to accomplish these tasks are crucial for a wide range of scientific and educational fields. However, efficient analysis and visualization is a non trivial problem as the size and complexity of data increases. This research addresses this challenge through a general progressive access, multi scale data representation for efficient handling of structured data sets across a range of science domains. The development is based upon a wavelet enabled data representation developed by NCAR for geoscience applications. The tools will utilize the very flexible and open source standard NetCDF format, and the methods will be documented as a set of conventions and a toolkit developed that incorporates and integrates these components for dissemination. In addition to an open source toolkit, these tools will be integrated into the VAPOR (NCAR) and STK (CSCI) platforms, thus expanding the capabilities and efficiencies of these platforms for the geo  and bio sciences communities, respectively.  Advancements generated by this project will be openly disseminated to the user community through an open source toolkit."
"1550404","SI2-SSI: Collaborative Research: A Robust High-Throughput Ab Initio Computation and Analysis Software Framework for Interface Materials Science","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes, DMREF","09/01/2016","08/31/2016","Kesong Yang","CA","University of California-San Diego","Standard Grant","Rob Beverly","08/31/2020","$600,000.00","Shyue Ping Ong","kesong@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","CSE","1253, 1712, 8004, 8292","7237, 7433, 7569, 8004, 8009, 8400, 9216","$0.00","A set of techniques scientists call ""ab initio"" methods, which are derived from the fundamental laws of physics with minimal assumptions and approximations, has become a critical tool in the study and design of materials. With computing advances and software innovations, the automation of high-throughput ab initio calculations has, in particular, heralded an explosion of computed data for a large variety of materials. However, these high-throughput efforts are limited to specific properties. In contrast, materials interfaces, one of the fastest growing research areas in materials science and engineering, are showing an increasing relevance in many areas of materials applications such as catalysis and electronics. This project will develop a software framework that enables novel high-throughput interface materials investigations and design. The developed software platform will expand the genome of materials by including the computed interfacial properties of interface materials. This community-based software can potentially become a critical component of the Materials Genome Initiative and serve not just the large and diverse materials research community, but also the physics and chemistry communities. Besides featuring heavily in existing and planned courses taught by the Principal Investigators in their home institutions, the proposed framework will facilitate the training of undergraduates and graduates in the ab initio methodologies in other institutions as well. This project will also conduct public outreach activities to increase awareness of the importance of sustainable software development for data-driven interface materials science. <br/><br/>The project will develop necessary workflow management, error correction schemes, and systematic analysis tools to support ab initio studies of thermodynamics, kinetics, diffusion, and electronic property of interface materials including hetero-structures and grain boundary. It targets developmental efforts on three key focus areas of great interest to interface materials science: (i) ab initio thermodynamics of surfaces and interfaces; ii) advanced methods for materials kinetics and diffusion at materials interfaces; and iii) automated algorithms for structural construction of grain boundary and post data-processing and analysis. In doing so, this project will greatly expand the suite of interfacial materials properties that are amenable to a high-throughput ab initio treatment, paving the way for materials investigations and design in a broad spectrum of technological applications, including energy generation and storage, catalysis and electronics. In addition, by interfacing with classical-mechanics simulation codes, this framework will bridge the gap between the ab initio and classical force-field approach, which is expected to significantly advance the high-throughput simulations of materials interfaces. <br/><br/>This award by the Advanced Cyberinfrastructure Division is jointly supported by the NSF Directorate for Mathematical and Physical Sciences (Division of Materials Research and Office of Multidisciplinary Activities)."
"2114580","SI2-SSI: Collaborative Research: Einstein Toolkit Community Integration and Data Exploration","OAC","COMPUTATIONAL PHYSICS","10/01/2020","01/04/2021","Pablo Laguna","TX","University of Texas at Austin","Continuing Grant","Amy Walton","08/31/2021","$56,473.00","","pablo.laguna@austin.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","CSE","7244","7433, 7569, 8004, 8084","$0.00","A new astronomy has arrived with the recent detection of gravitational waves. Modeling of sources of gravitational radiation is more than ever a critical necessity in order to interpret the observations. The project Einstein Toolkit has as overarching mission to provide the scientific community with a sustainable software platform of core computational tools for research focused on astrophysical systems endowed with complex multi-scale/multi-physics properties which are governed by Einstein's equations of General Relativity. The central premise of the project Einstein Toolkit is to create a broad and vibrant community of users, a community where interdisciplinary collaborations are the norm and not the exception, a community driving advances in the next generation of high-performance computing cyberinfrastructure. The main objectives of the project Einstein Toolkit are: developing software tools for a radical increase in scientific productivity, achieving sustainability of the software ecosystem, addressing software engineering challenges, and the curation of data from general relativistic numerical simulations. <br/><br/>This project will achieve its goals through two major activity areas. Regarding the software ecosystem and its sustainability, the scheduler that handles the flow of tasks in a problem will be redesigned to be more versatile and to improve its performance. In addition new software modules will be developed to broaden the choices of  initial data and matter sources, as well as modules for problems requiring a high degree of experimentation with equations and numerical methodologies.  A new general relativistic magneto-hydrodynamics code will also be integrated in the Einstein Toolkit. The second activity area involves building  a simulation data repository. The repository will allows user to compare results, contribute data,  test  innovative ideas and algorithms for gravitational wave data analysis, and to explore or discover new phenomena in sources of gravitational radiation. The broader impact effort in the project Einstein Toolkit will be organized in two major activity areas. The first involves community integration. The project will support a program of ease-of-use on-line tutorials and a workshops/tutorial series. The program will  help small groups or individual investigators familiarizing with the codes and modules in the toolkit as well as pathways to become a developer. Regarding outreach and education, the project Einstein Toolkit will enable interdisciplinary training of students and postdocs in numerical relativity, computational astrophysics and computer science. The effort will includes developing a teaching resources bank for educational activities involving computational topics applied to gravitational physics and astrophysics. The educational resources will be suitable for computational courses in general relativity and astrophysics at both the graduate and undergraduate level.<br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division in the Directorate of Mathematical and Physical Sciences."
"1743191","Collaborative Research: NISC SI2-S2I2 Conceptualization of CFDSI: Model, Data, and Analysis Integration for End-to-End Support of Fluid Dynamics Discovery and Innovation","OAC","FD-Fluid Dynamics, Special Initiatives, Software Institutes, CDS&E","03/01/2018","06/27/2018","Robert Moser","TX","University of Texas at Austin","Continuing Grant","Stefan Robila","08/31/2020","$45,000.00","","rmoser@ices.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","CSE","1443, 1642, 8004, 8084","026Z, 8004, 8005, 9263","$0.00","Fluid dynamics is a broad field spanning a large number of science and engineering problem domains that are critical to a wide variety of important applications including manufacturing, climate analysis, environment, health, transportation, propulsion, and power generation. To support the fluid dynamics research and applications community, this project seeks to engage the community in order to conceptualize a future institute, the Computational Fluid Dynamics Software Infrastructure (CFDSI), that will broadly develop, share, and apply computational tools for the generation and analysis of fluid dynamics data from both experimental and computational sources.  After its conceptualization, CFDSI will create and extend tools for problem definition, solution, and analysis of both computational and experimental investigations. The primary objective is to facilitate the sharing of computational tools and data resources through a rich and extensible set of software components that can be integrated into a wide range of existing fluid dynamics analysis tools. By improving the efficiency of tools and their ease of use, the ability for scientists and engineers to accurately predict and understand how complex fluid flows behave will be enhanced, having a significant impact on design, innovation, and discovery across the vast range of applications where fluid dynamics plays a role. CFDSI even has the potential to impact on K-12, undergraduate and graduate education by making a wide variety of resources available to students for fluid dynamics investigations.<br/><br/>The conceptualized institute will make a wide variety of powerful simulation, data, and analysis resources available to the fluid dynamics research community by lowering or eliminating barriers associated with the adoption and use of these resources. The software infrastructure will have a number of positive impacts on the fluid dynamics research community. To do so, CFDSI will connect the best research in fluid dynamics to the best research in data science/analytics within a highly sustainable software development environment. Specifically, CFDSI will: 1) enhance the dissemination of fluid dynamics data resources and advances in CFD modeling, 2) facilitate collaboration in fluid dynamics research, especially between computational and experimental researchers, 3) enable detailed comparisons between different data sources and detailed validation of computational models, 4) ease the use of advanced CFD models, methods, and codes in new and complex applications, 5) facilitate advanced analytics, such as uncertainty quantification, data compression, and optimization, 6) provide students access to advanced CFD methods and data resources, both computational and experimental, to enhance both graduate and undergraduate education in fluid dynamics, 7) improve the sustainability of current and future CFD software, and 8) facilitate the management of the growing body of fluid dynamics data sets. These outcomes will greatly enhance the effectiveness and productivity of research in fluid dynamics. In particular, they will transform the conduct of fluid dynamics research by: 1) making it more collaborative, 2) enhancing the credibility of research results, 3) enabling discovery, 4) reducing the cost of pursuing new research questions, and 5) diversifying and widening the fluid dynamics community through lowering the barriers associated with accessing and adopting CFD codes and large data sets. Software components will be designed for both analysis of experimental and computational databases as well as direct integration into CFD codes. The latter will enable in situ data analytics to address the growing chasm between data creation rate (solver performance) and data storage rate/volume (I/O resources). After conceptualization and implementation, CFDSI will enable more effective fluid dynamics research and thus impact the wide variety of application domains in which fluid dynamics is critical including manufacturing climate, environment, health, transportation, propulsion, and power generation (including conventional, alternative, and nuclear sources) which will, in turn, strongly impact our economy. Additionally, CFDSI will provide the capability for immersive simulations and experiments that will close the loop on idea, insight, discovery, and design through establishing links to in situ data analytics and problem redefinition during ongoing simulations or experiments. Finally, CFDSI will impact other problem domains governed by partial differential equations (e.g. solid mechanics) by serving as a model and starting point for similar domain-specific software infrastructures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740288","SI2-SSE: Abaco - Flexible, Scalable, and Usable Functions-As-A-Service via the Actor Model","OAC","Software Institutes","10/01/2017","08/30/2017","Joseph Stubbs","TX","University of Texas at Austin","Standard Grant","Seung-Jong Park","09/30/2021","$418,593.00","Matthew Vaughn","jstubbs@tacc.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","CSE","8004","026Z, 7433, 8004, 8005, 9102","$0.00","Researchers in virtually every field of science rely heavily on computing to perform their research.  In addition, increased availability of data has enabled entirely new kinds of analyses which have yielded answers to many important questions. However, these analyses are complex, and  require advanced computer science expertise.  This project seeks to simplify the manner in which researchers can create analysis tools that also scale better and more reliable. This project, Abaco, adopts and adapts the ""Actor"" model, which is a technique for designing software systems as a collection of simple functions, which can then be provided as a cloud-based capability on high performance computing environments. Doing this will significantly simplify the way scientific software is developed and used. Scientific software developers will find it much easier to design and implement a system. Further, scientists and researchers that use software will be able to easily compose together collections of actors with pre-determined functionality in order to get the computation and data they need. <br/><br/>The project will combine technologies and techniques from cloud computing, including Linux Containers and the ""functions-as-a-service"" paradigm with the proven Actor theoretical model for computing. Each Actor is implemented as a Linux container, and provides a single function on a cloud. The resulting system allows for small, lightweight programs to be run on virtually any system. This project will also extend Abaco's ability to implement data capabilities, such as data federation and discoverability. Abaco programs can be used to be used, for example, to build federated datasets consisting of separate datasets from all over the internet. By reducing the barriers to developing and using such services, this project will boost the productivity of scientists and engineers working on the problems of today, and better prepare them to tackle the new problems of tomorrow. Abaco has broad applicability across science domains, from biology to engineering to environmental studies. Further, the Abaco team will conduct substantial training and support activities aimed at empowering researchers to benefit from this approach.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1339708","Collaborative Research: SI2-SSE: UT Wrangler: Understanding the Software Needs of High End Computer Users","OAC","Software Institutes","10/01/2013","09/13/2013","Robert McLay","TX","University of Texas at Austin","Standard Grant","Rajiv Ramnath","09/30/2016","$233,046.00","","mclay@tacc.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","CSE","8004","7433, 8005","$0.00","This research addresses two important questions: what software do researchers actually use on high-end computers, and how successful are they in their efforts to use it? It is a plan to improve our understanding of individual users' software needs, then leverage that understanding to help stakeholders conduct business in a more efficient, effective, and systematic way. The signature product, UTWrangler, builds on work that is already improving the user experience and enhancing support programs for thousands of users on twelve supercomputers across the United States and Europe.   For the first time, complete, accurate, detailed, and continuous ground truth information about software needs, trends, and issues at the level of the individual job are being delivered.<br/> <br/>UTWrangler will instrument, monitor, and analyze individual jobs on high-end computers to generate a picture of the compilers, libraries, and other software that users need to run their jobs successfully. It will highlight the products our researchers need and do not need, and alert users and support staff to the root causes of software configuration issues as soon as the problems occur. UTWrangler's prototypes prove its value and future impact: simplifying end users' workflows; improving support, training and documentation; saving money; and helping administrators prioritize maintenance of their large base of installed software.  UTWrangler will build on the capabilities of its prototypes, providing a robust, sustainable, second generation mechanism that will help the computational research community make the most effective use of limited computing cycles and labor hours.  And UTWrangler will mitigate the difficulties new users encounter, reporting configuration problems as soon as jobs begin, and identifying opportunities to improve documentation, education and outreach programs."
"1642398","SI2-SSE: Collaborative Research: Extending the Practicality and Scalability of LibMesh-Based Unstructured, Adaptive Finite Element Computations","OAC","Software Institutes","09/01/2016","08/22/2016","Roy Stogner","TX","University of Texas at Austin","Standard Grant","Seung-Jong Park","08/31/2019","$144,815.00","","roystgnr@ices.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","CSE","8004","026Z, 7433, 8004, 8005","$0.00","The development and deployment of cyberinfrastructure focused on scientific and engineering simulation has been, and continues to be, essential to the progress of science and engineering in the U.S. This is particularly true for software used in large scale supercomputing environments. Thus, for the U.S. to continue leadership and advancement in scientific computing, it is crucial that software infrastructure advance to enable modern computational and software engineering strategies for simulating complex scientific and engineering systems. Once such piece of software is the libMesh finite element library. libMesh is used by hundreds of research groups in the U.S. and around the world. Critically, libMesh can utilize large scale supercomputing infrastructure for simulating scientific and engineering systems. This work will update the libMesh software library to use state-of-the-art algorithms that will enable robust simulations on the largest supercomputers in the world and further advance the complexity of systems that can be successfully modeled using libMesh. Furthermore, the library will be enhanced to support user applications to leverage modern computer architectures, including emerging many-core architectures. This will enable the continued use of libMesh as both a fundamental tool of scientific and engineering simulation and as an educational tool for computational algorithms.<br/><br/>The libMesh finite element library is a prominent example of an open-source tool supporting adaptive mesh refinement, interfaces to preeminent solver packages, and solutions on large parallel supercomputers of complex finite element models. libMesh supports hundreds of users and many applications in solving partial differential equations across a variety of disciplines including solid mechanics, fluids mechanics, magnetohydrodynamics, hypersonics, nuclear engineering, combustion, and acoustics, to name a few examples. Following over a decade of successful collaborative open-source development, the library is poised to maintain its place as a prominent open-source finite element package. To do so, libMesh must be made to support emerging many core architectures, leverage the most advanced scalable algorithms, and interface with geometry underlying the complex meshes used in engineering analysis. The work addresses these issues directly by extending and enhancing the libMesh finite element library. The extensions will seamlessly make available modern solution algorithms through interfaces to world class solver libraries, facilitate the interaction with underlying geometric representations using openly available software libraries, and efficiently utilize modern computing hardware through cutting-edge software engineering principles and designs. Simultaneously, the developed interfaces will allow for flexibility of development of modeling kernels and maintain the low the barrier of entry that libMesh has always had for both the libMesh community as well as the scientific community in general. Such lofty goals will be attained by designing usable interfaces that hide the complexity of the underlying algorithms and extensive testing on modern computing architectures to ensure performance and scalability is delivered to the libMesh community."
"1148125","Collaborative Research: SI2-SSI: A Linear Algebra Software Infrastructure for Sustained Innovation in Computational Chemistry and other Sciences","OAC","OFFICE OF MULTIDISCIPLINARY AC, Information Technology Researc, DMR SHORT TERM SUPPORT, CHEMISTRY PROJECTS, Software Institutes","06/01/2012","05/17/2013","Robert van de Geijn","TX","University of Texas at Austin","Standard Grant","Evelyn Goldfield","05/31/2016","$1,701,189.00","John Stanton, Don Batory, Margaret Myers, Victor Eijkhout","rvdg@cs.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","CSE","1253, 1640, 1712, 1991, 8004","1253, 1640, 1712, 1991, 7433, 7569, 8004, 8009, 9216, 9263","$0.00","Linear algebra is a branch of mathematics that provides the foundation for a significant fraction of computations in science and engineering. Historically, the importance of linear algebra is such that highly specialized codes written by computer scientists have been used by the community of scientific programmers as a vital part of their application programs.   With the rapid changes in computer architecture during the last several years, it would seem that corresponding modifications in linear algebra routines would be warranted. However, such progress is not in evidence; the development of such routines has been just incremental, involving successive rewrites of routines that had their genesis in the last quarter of the last century.   Correspondingly, there is something<br/>of a disconnect between the current `state-of-the-art' linear algebra libraries, modern computer architectures, and applications that utilize the libraries.<br/> <br/>The new project will create a new, vertically integrated framework and implementation that revisits every layer of software, from low-level kernels to higher level functionality.  The vertical integration is completed with a new generation of software for computational<br/>chemistry applications, guaranteeing that the developed software, to be freely available to the public, supports sustained innovation in that domain and other sciences.  The development builds on the FLAME project, which has been funded by NSF and industry for more than a decade."
"1550593","Collaborative Research: SI2-SSI: Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion","OAC","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, Software Institutes, CDS&E-MSS","09/01/2016","11/18/2019","Omar Ghattas","TX","University of Texas at Austin","Standard Grant","Seung-Jong Park","08/31/2020","$350,885.00","Umberto Villa","omar@ices.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","CSE","1266, 1271, 8004, 8069","4444, 7433, 8004, 8009, 8251, 9263","$0.00","Scientists often use mathematical models to predict the behavior of natural and engineered systems. These models are therefore fundamental to scientific and engineering progress and hence relevant to NSF's science mission. Most models of realistic physical systems  use complex formulae (such as, partial differential equations) involving many variables. When using such a model for predicting the future behavior of a system, a scientist has to provide initial values for all the variables.  This can be difficult because input values may not be directly measureable. Thus, scientists often must use ""inverse"" computations to calculate the initial input values of the variables of a system model based on external observations of the real world. In other words, scientists seek to infer inputs to a computer model of a physical process from real observational data of the outputs. There are many examples of inverse computations, ranging from computing the important dimensions of an organ from its CAT scan, reconstructing the source of a sound by measuring its volume and frequency at various places, calculating the density of the Earth from measurements of its gravity field, or calculating the initial condition of the atmosphere (temperature, pressure, etc.) from satellite and weather station observations over a time interval. Inverse problems are ubiquitous across all of science and engineering (and beyond). Many solutions exist for inverse problems, i.e. solutions that fit the data to the observations. However, there are variations in the solutions identified. That is, the solutions of an inverse problem are subject to uncertainty. Bayesian inferencing provides a systematic mathematical framework for characterizing this uncertainty. However, the Bayesian solution of inverse problems for large-scale complex models require enormous computational power. Only recently have algorithms begun to emerge that are computationally tractable. However, these algorithms have remained out of the reach of the mainstream of scientists who solve inverse problems, due to their complexity and the need for deeper information from the forward model. This project aims to develop, distribute, and support open-source software that encodes state-of-the-art algorithms for the solution of large-scale complex Bayesian inverse problems and is robust, scalable, flexible, modular, widely accessible, and easy to use.<br/><br/>The project builds heavily on two complementary open-source software libraries the team has been developing: MUQ at MIT, and hIPPYlib at UT-Austin/UC-Merced. MUQ provides a spectrum of powerful Bayesian inversion models and algorithms, but expects forward models to come equipped with gradients/Hessians to permit large-scale solution. hIPPYlib implements powerful large-scale gradient/Hessian-based inverse solvers in an environment that can automatically generate needed derivatives, but it lacks full Bayesian capabilities. By integrating these two complementary libraries, the project will result in a robust, scalable, and efficient software framework that realizes the benefits of each to tackle complex large-scale Bayesian inverse problems across a broad spectrum of scientific and engineering disciplines. The resulting software, that will be distributed under an open-source license, will provide an environment for rapid development of inverse models equipped with gradient/Hessian information; benchmark problems for evaluation and comparison of algorithms; and tutorial problems for training and testing purposes."
"1203182","Collaborative Research: SI2-SSE: Software for integral equation solvers on manycore and heterogeneous architectures","OAC","OFFICE OF MULTIDISCIPLINARY AC, DYNAMICAL SYSTEMS, COFFES, Software Institutes","08/18/2012","12/20/2012","George Biros","TX","University of Texas at Austin","Standard Grant","Thomas F. Russell","06/30/2013","$250,000.00","","gbiros@gmail.com","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","CSE","1253, 7478, 7552, 8004","","$0.00","We propose to develop and deploy mathematical software for boundary-value problems in three-dimensional complex geometries.  The algorithms in the library will be based on integral equation formulations.  The library will be designed to scale on novel computing platforms that comprise special accelerators and manycore architectures. <br/><br/>Integral equations can be used to conduct simulations on many problems in science and engineering with significant societal impact.  Three example applications on which the proposed simulation technologies will have an impact in this project are microfluidic chips, biomolecular electrostatics, and plasma physics.  First, microfluidic chips are submillimeter-sized devices used for medical diagnosis and drug design.  Optimizing the function of such devices at low cost requires efficient computer simulation tools, such as the ones we propose to develop.  Second, understanding the structure and function of biomolecules such as DNA and proteins is crucial in biotechnology.  The proposed technologies can be used to resolve bimolecular electrostatic interactions.  Third, plasma physics, which is related to fusion nuclear reactors, includes electrostatic interactions in complex geometries, and the proposed work will enable large-scale three-dimensional simulations. <br/><br/>The key features of the proposed software are: (1) parallel fast multipole methods, (2) efficient geometric modeling techniques for complex geometries, (3) simple library interfaces that allow use of the proposed software by non-experts, and (4) scalability on heterogeneous architectures.<br/><br/>Along with our research activities, an educational and dissemination program will be designed to communicate the results of this work to students and researchers.  Several postdoctoral, graduate, and undergraduate students will be involved with the project.  Additional educational activities will include research experiences for undergraduates, leveraging ongoing programs such as NSF REUs.  We will encourage participation by women, minorities, and underrepresented groups."
"1664142","Collaborative Research: SI2-SSI: EVOLVE: Enhancing the Open MPI Software for Next Generation Architectures and Applications","OAC","Software Institutes","06/01/2017","05/26/2017","George Bosilca","TN","University of Tennessee Knoxville","Standard Grant","Seung-Jong Park","05/31/2022","$1,566,215.00","Thomas Herault, Aurelien Bouteiller","bosilca@icl.utk.edu","201 ANDY HOLT TOWER","KNOXVILLE","TN","379960001","8659743466","CSE","8004","026Z, 7433, 7942, 8004, 8009","$0.00","For nearly two decades, the Message Passing Interface (MPI) has been an essential part of the High-Performance Computing ecosystem and consequently a key enabler for important scientific breakthroughs. It is a fundamental building block for most large-scale simulations from physics, chemistry, biology, material sciences as engineering.  Open MPI is an open source implementation of the MPI specification, widely used and adopted by the research community as well as industry. The Open MPI library is jointly developed and maintained by a consortium of academic institutions, national labs and industrial partners. It is installed on virtually all large-scale computer systems in the US as well as in the rest of the world. The goal of this project is to enhance and modernize the Open MPI library in the context of the ongoing evolution of modern computer systems, and to ensure its future operability on all upcoming architectures. We aim at implementing fundamental software techniques that can be used in many-core systems to execute MPI-based parallel applications more efficiently, and to tolerate process and memory failures at all scales, from current systems, up to the extreme scales expected before the end of the decade.<br/><br/>Open MPI is an open source implementation of the Message Passing Interface (MPI) specification. The MPI API is currently being extended to consider the needs of application developers in terms of efficiency, productivity and resilience. The project will also support academic involvement in the design, development and evaluation of the Open MPI software, and ensure academic presence in the MPI Forum. The goal of this proposal is to enhance the Open MPI software library, focusing on two aspects: (1) Extend Open MPI to support new features of the MPI specification. Open MPI will continue to support all new features of current and upcoming MPI specifications. The two most significant areas within the context of this proposal are (a) extensions to better support hybrid programming models and (b) support for fault tolerance in MPI applications. To improve support for hybrid programming models, the MPI Forum is currently considering introducing the notion of MPI Endpoints, which could be used by different threads of an MPI rank to instantiate multiple separate communication contexts. The goal within this project is to develop an implementation of endpoints to support effective hybrid programming model, and to extend the concept to other aspects of parallel applications such as File I/O operations. One of the project partners (UTK) leads the current proposal in the MPI Forum to expose failures and ensure the continuation of the execution of MPI applications. In the context of this SSI proposal, the goal is to harden, improve, and expand the support of the existing ULFM implementation in Open MPI and thus enable end-users to design application-specific resilience approaches for future platforms. (2) Enhance the Open MPI core to support new architectures and improve scalability. While Open MPI has demonstrated very good scalability in the past, there is significant work to be done to ensure similarly good performance on future architectures. Specifically, we propose a groundbreaking rework of the startup environment that will improve process launch scalability, increase support for asynchronous progress of operations, enable support for accelerators, and reduce sensitivity to system noise. The project would also enhance the support for File I/O operations as part of the Open MPI package by expanding our work on highly scalable collective I/O operations through delegation and exploring the utilization of burst buffers as temporary storage."
"1740142","NSCI SI2-SSE: The N-Jettiness Software Framework for Precision Perturbative QCD Calculations in Particle and Nuclear Physics","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","09/01/2017","08/29/2017","Francis Petriello","IL","Northwestern University","Standard Grant","Rob Beverly","08/31/2021","$476,976.00","","f-petriello@northwestern.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","CSE","1253, 7244, 8004","026Z, 7433, 7569, 8004, 8005, 8084","$0.00","This project will develop computational tools needed to interpret increasingly precise instrument data from particle accelerators such as the world's largest and most powerful, the Large Hadron Collider (LHC).  These computations will advance our knowledge of physics at the smallest scales and may potentially reveal deviations between measurements and theory (the Standard Model of particle physics).  The detailed scrutiny of the recently-discovered Higgs boson and searches for deviations from the Standard Model will guide the physics community for the coming decades.  The success of this program will rely upon increasingly intricate and precise theoretical calculations.  In the words of the U.S. Particle Physics Project Prioritization Panel (P5) report which describes the next decade of high energy physics: ""The full discovery potential of the Higgs will be unleashed by percent-level precision studies of the Higgs properties.""  The difficulty in achieving predictions at this precision is an enormous theoretical and computational challenge.  The project members have developed a novel approach to the necessary calculations that is especially adapted to run on the nation's largest high-performance computing systems.  This method has made previously unobtainable results possible, and there is great promise for similar future rapid progress.  The software development in this project will provide the tools needed to answer some of the most outstanding issues facing fundamental physics: What is the underlying origin of the Higgs boson?  Can we discover dark matter at the LHC?  What is the microscopic mechanism which gives the proton its observed spin?  Through the involvement of junior scientists in answering these questions the younger generation will be trained in applying cutting-edge computing knowledge to answer future scientific questions.<br/><br/><br/>The primary goal of this project is the development and deployment of codes incorporating the N-jettiness subtraction approach to perturbative QCD calculations in order to address the ever-increasing precision needs of collider experiments in  particle and nuclear physics.  This theoretical framework very effectively uses previous community investments in software development by extending publicly-available next-to-leading-order (NLO) codes to next-to-next-to-leading order (NNLO), where the expansion parameter is the strong coupling constant.  This advance improves their achievable theoretical precision by an order of magnitude, while maintaining the interface familiar to the user community.  The specific objectives of this project are as follows: the public release of NNLO corrections for jet production processes at the LHC into a public simulation code that is both fast and user-friendly; the expansion of the functionality of DISTRESS, a new code designed for precision simulations for RHIC and a future electron-ion-collider; the preparation of these precision simulation tools for future multi-core computing architectures that feature smaller memory per core.  The N-jettiness subtraction approach is optimized for the massively-parallel computing architectures in which the United States government has invested heavily, and therefore advances the goals of the National Strategic Computing Initiative.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering, the Physics Division and Office of Multidisciplinary Activities in the Directorate of Mathematical and Physical Sciences."
"1450374","SI2-SSI: Collaborative Research:  Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS, CDS&E","08/01/2015","08/11/2015","Neelesh Patankar","IL","Northwestern University","Standard Grant","Rob Beverly","07/31/2021","$512,966.00","","n-patankar@northwestern.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","CSE","1253, 8004, 8069, 8084","7433, 8004, 8009, 8084","$0.00","Many biological and biomedical systems involve the interaction of a flexible structure and a fluid. These systems range from the writhing and coiling of DNA, to the beating and pumping of cilia and flagella, to the flow of blood in the body, to the locomotion of fish, insects, and birds. This project aims to develop advanced software infrastructure for performing dynamic computer simulations of such biological and biomedical systems. To facilitate the deployment of this software in a range of scientific and engineering applications, this project will develop new software capabilities in concert with new computer models that use the software. Specific application domains to be advanced in this project include models of aquatic locomotion that can be used to understand the neural control of movement and ultimately to develop new treatments for neurological pathologies such as spinal cord injuries, and models that simulate the interaction between the electrophysiology of the heart and the contractions of the heart that pump blood throughout the body, which could lead to improved approaches to treating heart disease. The software to be developed within the project is freely available online and is used by a number of independent research groups in a variety of scientific and engineering domains. It is being actively used in projects that model different aspects of cardiovascular dynamics, such as platelet aggregation and the dynamics of natural and prosthetic heart valves, and in projects that study other biological problems, including cancer dynamics, insect flight, aquatic locomotion, and the dynamics of phytoplankton. The software is also being applied to non-biological problems, including nanoscale models of colloidal suspensions and models of active particles. The improved methods and software to be developed in this project will thereby have a broad and sustained impact on a large number of ongoing research efforts in the biological and biomedical sciences and other scientific and engineering disciplines.<br/><br/>The immersed boundary (IB) method is a broadly applicable framework for modeling and simulating fluid-structure interaction (FSI). The IB method was introduced to model the fluid dynamics of heart valves, and subsequent development initially focused on simulating cardiac fluid dynamics. This methodology is broadly useful, however, and has been applied to a variety of problems in which a fluid flow interacts with immersed structures, including elastic bodies, bodies with known or prescribed deformational kinematics, and rigid bodies. Extensions of the IB method have also been developed to model electrophysiological systems and systems with chemically active structures. To improve the efficiency of the IB method, the PI has developed adaptive versions of the IB method that employ structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where needed. These methods have been implemented within the IBAMR software framework, which provides parallel implementations of the IB method and its extensions that leverage high-quality computational libraries including SAMRAI, PETSc, and libMesh. This project will further extend the IBAMR software by implementing modeling and discretization technologies required by the research applications of current and prospective users of the software, by developing improved solver infrastructure facilitated by the implementation of native support for structured AMR discretizations in the PETSc library, and by integrating with existing high-quality software tools for model development, deployment, and analysis. IBAMR is freely distributed online and is used within a number of independent research groups both to the further development of the IB method and also to its application to simulate diverse problems in fluid dynamics and FSI. By enhancing IBAMR, this project will also enhance the ability of these and other researchers to construct detailed models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations. This project will also develop general-purpose support for AMR discretizations in PETSc, a software library with thousands of active users, ~400 downloads per month, and numerous applications. The work of this project will help to grow the IBAMR user community of students and researchers by developing UI tools for building models, running simulations, and analyzing results. Students will be actively engaged in all aspects of the project, including code, method, and model development."
"1663887","Collaborative Research: SI2-SSI: EVOLVE: Enhancing the Open MPI Software for Next Generation Architectures and Applications","OAC","Software Institutes","06/01/2017","05/26/2017","Edgar Gabriel","TX","University of Houston","Standard Grant","Seung-Jong Park","05/31/2022","$308,785.00","","gabriel@cs.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","CSE","8004","026Z, 7433, 7942, 8004, 8009","$0.00","For nearly two decades, the Message Passing Interface (MPI) has been an essential part of the High-Performance Computing ecosystem and consequently a key enabler for important scientific breakthroughs. It is a fundamental building block for most large-scale simulations from physics, chemistry, biology, material sciences as engineering.  Open MPI is an open source implementation of the MPI specification, widely used and adopted by the research community as well as industry. The Open MPI library is jointly developed and maintained by a consortium of academic institutions, national labs and industrial partners. It is installed on virtually all large-scale computer systems in the US as well as in the rest of the world. The goal of this project is to enhance and modernize the Open MPI library in the context of the ongoing evolution of modern computer systems, and to ensure its future operability on all upcoming architectures. We aim at implementing fundamental software techniques that can be used in many-core systems to execute MPI-based parallel applications more efficiently, and to tolerate process and memory failures at all scales, from current systems, up to the extreme scales expected before the end of the decade.<br/><br/>Open MPI is an open source implementation of the Message Passing Interface (MPI) specification. The MPI API is currently being extended to consider the needs of application developers in terms of efficiency, productivity and resilience. The project will also support academic involvement in the design, development and evaluation of the Open MPI software, and ensure academic presence in the MPI Forum. The goal of this proposal is to enhance the Open MPI software library, focusing on two aspects: (1) Extend Open MPI to support new features of the MPI specification. Open MPI will continue to support all new features of current and upcoming MPI specifications. The two most significant areas within the context of this proposal are (a) extensions to better support hybrid programming models and (b) support for fault tolerance in MPI applications. To improve support for hybrid programming models, the MPI Forum is currently considering introducing the notion of MPI Endpoints, which could be used by different threads of an MPI rank to instantiate multiple separate communication contexts. The goal within this project is to develop an implementation of endpoints to support effective hybrid programming model, and to extend the concept to other aspects of parallel applications such as File I/O operations. One of the project partners (UTK) leads the current proposal in the MPI Forum to expose failures and ensure the continuation of the execution of MPI applications. In the context of this SSI proposal, the goal is to harden, improve, and expand the support of the existing ULFM implementation in Open MPI and thus enable end-users to design application-specific resilience approaches for future platforms. (2) Enhance the Open MPI core to support new architectures and improve scalability. While Open MPI has demonstrated very good scalability in the past, there is significant work to be done to ensure similarly good performance on future architectures. Specifically, we propose a groundbreaking rework of the startup environment that will improve process launch scalability, increase support for asynchronous progress of operations, enable support for accelerators, and reduce sensitivity to system noise. The project would also enhance the support for File I/O operations as part of the Open MPI package by expanding our work on highly scalable collective I/O operations through delegation and exploring the utilization of burst buffers as temporary storage."
"1148052","SI2-SSI:  Collaborative Research:   A Glass Box Approach to Enabling Open, Deep Interactions in the HPC Toolchain","OAC","Information Technology Researc, Software Institutes","06/01/2012","09/09/2016","Edgar Gabriel","TX","University of Houston","Standard Grant","Rajiv Ramnath","12/31/2016","$926,666.00","Edgar Gabriel","gabriel@cs.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","CSE","1640, 8004","7433, 8004, 8009","$0.00","Parallel computing has entered the mainstream with increasingly large multicore processors and powerful accelerator devices. These compute engines, coupled with tighter integration of faster interconnection fabrics, are drivers for the next-generation high end computing (HEC) machines. However, the computing potential of HEC machines is delivered only through productive parallel program development and efficient parallel execution. This project enables application developers to improve performance on future HEC machines for their scientific and engineering processes. This project challenges the current model for parallel application development via ""black box"" tools and services. Instead, the project offers an open, transparent software infrastructure -- a Glass Box system -- for creating and tuning large-scale, parallel applications.  `Opening up' the tools and services used to create and evaluate peta- and exa-scale codes involves developing interfaces and methods that make tool-internal information and available for new performance management services that improve developer productivity and code efficiency.<br/><br/>The project will explore the information that can be shared 'across the software stack'.  Methods will be developed for analyzing program information, performance data and tool knowledge. The resulting Glass Box system will allow developers to better assess the performance of their parallel codes.  Tool creators can use the performance data to create new analysis and optimization techniques. System developers can also better manage multicore and machine resources at runtime, using JIT compilation and binary code editing to exploit the evolving hardware.  Working with the `Keeneland' NSF Track II machine and our industry partners, the project will create new performance monitoring tools, compiler methods and system-level resource management techniques. The effort is driven by the large-scale codes running on today's petascale machines.  Its broader impact is derived from the interactions with technology developers and application scientists as well as from its base in three universities with diverse student populations."
"1550481","Collaborative Research: SI2-SSI: Software Framework for Electronic Structure of Molecules and Solids","OAC","Software Institutes","08/01/2016","07/21/2016","Toru Shiozaki","IL","Northwestern University","Standard Grant","Amy Walton","07/31/2020","$600,000.00","","shiozaki@qsimulate.com","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","CSE","8004","7433, 8009, 9216","$0.00","Many traditionally experimental disciplines such as chemistry and materials science are rapidly changing due to our increasing ability to predict properties of molecules and materials purely by simulation. This is particularly true when molecules meet solid surfaces - due to the particular challenges of experiments in such a setting. Yet the molecule-on-surface frontier encompasses a vast class of problems of tremendous practical importance:  industrial applications facilitated by surface processes are estimated to produce globally more than than 15 trillion USD worth of goods and products.  This research will improve our ability to simulate the physics and chemistry of molecules on surfaces by extending the advanced simulation methodologies that were originally developed by for modeling electrons in molecules. This project will not only advance our fundamental understanding of the surface science but also open a road to technological applications relevant to producing and storing clean energy and in designing improved catalysts. The research may result in a new computer software framework for simulating electrons in molecules and materials. This software will be a unique contribution to the U.S. cyberinfrastructure and spur further innovation by other researchers in the US and worldwide, who will be able to access its source code for free. The software framework will also serve as an education platform for training computational chemists and materials scientists.<br/><br/>A frontier simulation challenge lies at the intersection of the two domains of chemistry and materials science - namely to determine, with predictive accuracy, the properties and chemistry of molecules on solid surfaces. The molecule-on-surface frontier encompasses a vast class of problems of tremendous practical importance: heterogeneous catalysis, photovoltaics, and emerging electronic materials. Yet, from a simulation perspective, it is not currently possible to efficiently combine the recent advances in highly accurate many-body molecular and periodic condensed phase methodologies in these problems, due to a significant gap between how the electronic structure theories of molecules and materials are formulated, as reflected in distinct algorithms and disjoint codebases. The goal of this project is to reduce and/or completely eliminate the gap between molecular and solid-state electronic structure methodologies, in theory, algorithms, and in usable community software implementations. This will be achieved by building an ambitious Electronic structure for Molecules and Solids (EMOS) software framework that will permit accurate computation of the first-principles electronic structure of both molecules and solids on an equivalent footing - and with the high efficiency necessary for high-throughput screening or ab initio molecular dynamics. These efforts build on the leading track-record of the principal investigators in developing open-source quantum chemistry software as well as automated computer implementation and high-performance parallel libraries. The project will allow the advances from molecular electronic structure - embedding, reduced-scaling many-body methodology, accurate excited-state electronic structure, and others - to be applied routinely to molecules, materials, and combinations of the two as relevant to surface chemistry. This has great potential to advance the state-of-the-art in treatment of electronic structure and open new lines of theoretical inquiry. The resulting open-source production-quality toolkit will be validated against experimental data for a host of surface phenomena, from exciton dynamics to surface spectroscopy and catalysis. An open-source US-based advanced materials code is a long-standing omission in U.S. cyberinfrastructure. As a high-performance framework for simulation of electronic structure of molecules, solids, and their interfaces with unprecedented accuracy, EMOS will be a significant contribution to this effort. Further, the modular component based structure will be able to be integrated with other major electronic structure packages through the reuse of the modules. This project will provide invaluable training opportunities to the students and postdocs who will develop the software framework under the direct supervision of principal investigators. In addition, each project site will contribute to the development of a stakeholder network for EMOS by hosting, each summer, visiting students and faculty representing the broader theoretical community, to train them on the use of EMOS in research and education. The project team will also use EMOS in teaching classes and summer schools, building on already established efforts in this area; these efforts will also be extended to an online setting."
"1664172","SI2-SSI: LIMPID: Large-Scale IMage Processing Infrastructure Development","OAC","DMR SHORT TERM SUPPORT, CYBERINFRASTRUCTURE, Software Institutes","10/01/2017","09/08/2019","Bangalore Manjunath","CA","University of California-Santa Barbara","Standard Grant","Rob Beverly","09/30/2023","$3,436,422.00","Nirav Merchant, Tresa Pollock, Robert Miller, Amit Roy Chowdhury","manj@ece.ucsb.edu","3227 CHEADLE HALL","SANTA BARBARA","CA","931060001","8058934188","CSE","1712, 7231, 8004","026Z, 054Z, 075Z, 7433, 8004, 8009, 9216, 9251","$0.00","Scientific imaging is ubiquitous: From materials science, biology, neuroscience and brain connectomics, marine science and remote sensing, to medicine, much of the big data science is image centric. Currently,interpretation of images is usually performed within isolated research groups either manually or as workflows over narrowly defined conditions with specific datasets. This LIMPID (Large-scale IMage Processing Infrastructure Development) project will have a transformative impact on such discipline-centric workflows through the creation of an extensive and unique resource for the curation, distribution and sharing of scientific image analysis methods. The project will create an image processing marketplace for use by a diverse community of researchers, enabling them to discover, test, verify and refine image analysis methods within a shared infrastructure.As a freely available, cloud-based resource, LIMPID will facilitate participation of underrepresented groups and minority-serving institutions, as well as international scientists, allowing them to addressquestions that would otherwise require expensive software. The potential impacts of the projectare significant: from wide dissemination of novel processing methods, to development of automaticmethods that can leverage data and human feedback from large datasets for software training andvalidation.For the broader scientific community, this immediately provides a resource for joint data and methods publication, with provenance control and security. This in turn will facilitate faster development and deployment of tools and foster new collaborations between computer scientists developing methods and scientific users. The project will prepare a diverse cadre of students and researchers, including women and members of under-represented groups, to tackle complex problems in an interdisciplinary environment. Through workshops, participation at scientific meetings, and summer undergraduate research internships, a broad community of users will be engaged to actively contribute to all aspects of research, development, and training during the course of this project.<br/>    <br/><br/>The primary goal is to create alarge scale distributed image processing infrastructure, the LIMPID, though a broad,interdisciplinary collaboration of researchers in databases, image analysis, and sciences. In order to create a resource ofbroad appeal, the focus will be on three types of image processing: simple detection and labelling of objects based on detection of significant features and leveraging recent advances in deep learning, semi-custom pipelines and workflows based onpopular image processing tools, and finally fully customizable analysis routines. Popular image processing pipeline tools will be leveraged to allow users to create or customize existing pipeline workflows and easily test these on large-scale cloud infrastructure from their desktop or mobile devices. In addition, a core cloud-based platform will be created where custom image processing can be created,shared, modified, and executed on large-scale datasets and applynovel methods to minimize data movement. Usage test cases will be created for three specific user communities: materials science, marine science and neuroscience. An industry supported consortium will be established at the beginning of the projecttowards achieving long-term sustainability of the LIMPID infrastructure.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering and the Division of Materials Research in the Directorate for Mathematical and Physical Sciences."
"1450356","SI2-SSI: Collaborative Research: Bringing End-to-End Provenance to Scientists","OAC","Special Projects - CCF, Software Institutes","06/01/2015","06/01/2015","Barbara Lerner","MA","Mount Holyoke College","Standard Grant","Bogdan Mihaila","05/31/2020","$422,997.00","","blerner@mtholyoke.edu","50 COLLEGE ST","SOUTH HADLEY","MA","010751423","4135382000","CSE","2878, 8004","7433, 8009","$0.00","Reproducability is the cornerstone of scientific progress. Historically, scientists make their work reproducible by including a formulaic description of the experimental methodology used in an experiment. In an age of computational science, such descriptions no longer adequately describe scientific methodology. Instead, scientific reproducibility relies on a precise and actionable description of the data and programs used to conduct the research. Provenance is the name given to the description of how a digital artifact came to be in its present state. Provenance includes a precise specification of an experiment's input data and the programs or procedures applied to that data. Most computational platforms do not record such data provenance, making it difficult to ensure reproducability. This project addresses this problem through the development of tools that transparently and automatically capture data provenance as part of a scientist's normal computational workflow.<br/><br/>An interdisciplinary team of computer scientists and ecologists have come together to develop tools to facilitate the capture, management, and query of data provenance -- the history of how a digital artifact came to be in its present state.  Such data provenance improves the transparency, reliability, and reproducibility of scientific results. Most existing provenance systems require users to learn specialized tools and jargon and are unable to integrate provenance from different sources; these are serious obstacles to adoption by domain scientists. This project includes the design, development, deployment, and evaluation of an end-to-end system (eeProv) that encompasses the range of activity from original data analysis by domain scientists to management and analysis of the resulting provenance in a common framework with common tools. This project leverages and integrates development efforts on (1) an emerging system for generating provenance from a computing environment that scientists actually use (the R statistical language) with (2) an emerging system that utilizes a library of language and database adapters to store and manage provenance from virtually any source. Accomplishing the goals of this proposal requires fundamental research in resolving the semantic gap between provenance collected in different environments, capturing detailed provenance at the level of a programming language, defining precisely aspects of provenance required for different use cases, and making provenance accessible to scientists."
"1147944","SI2-SSI: The Language Application Grid: A Framework for Rapid Adaptation and Reuse","OAC","Linguistics, Methodology, Measuremt & Stats, Special Projects - CNS, International Research Collab, IIS Special Projects, Software & Hardware Foundation, Software Institutes, ","08/01/2012","05/31/2018","Nancy Ide","NY","Vassar College","Standard Grant","Bogdan Mihaila","07/31/2019","$994,057.00","James Pustejovsky, Eric Nyberg, Christopher Cieri","ide@brandeis.edu","124 RAYMOND AVE","POUGHKEEPSIE","NY","126040001","8454377092","CSE","1311, 1333, 1714, 7298, 7484, 7798, 8004, O422","1311, 1333, 5983, 7433, 7944, 8004, 8009, 9251","$0.00","The need for robust language processing capabilities across academic disciplines, education, and industry is without question of vital importance to national security, infrastructure development, and the competitiveness of American business. However, at this time a robust, interoperable software infrastructure to support natural language processing (NLP) research and development does not exist. To fill this gap, this project establishes an large international collaborative effort involving key international players to develop an open, web-based infrastructure through which massive and distributed language resources can be easily accessed, in whole or in part, and within which tailored language services can be efficiently composed, disseminated and consumed by researchers, developers, and students. <br/><br/>The goal of this project is to build a comprehensive network of web services and resources within the NLP community. This requires four specific activities: (1) Design, develop and promote a service-oriented architecture for NLP development that defines atomic and composite web services, along with support for service discovery, testing and reuse; (2) Construct a Language Application Grid (LAPPS Grid) based on Service Grid Software developed in Japan; (3) Provide an open advancement (OA) framework for component- and application-based evaluation that enables rapid identification of frequent error categories within modules, thus contributing to more effective investment of resources; (4) Actively promote adoption, use, and community involvement with the LAPPS Grid. <br/><br/>By providing access to cloud-based services and support for locally-run services, the LAPPS Grid will lead to the development of a massive global network of language data and processing capabilities that can be used by scientists and engineers from diverse disciplines, providing components that require no expertise in language processing to use. Research in sociology, psychology, economics, education, linguistics, digital media, and the humanities will be impacted by the ability to easily manipulate and process diverse language data sources in multiple languages."
"1739419","SI2-SSE: An Ecosystem of Reusable Image Analytics Pipelines","OAC","EXTRAGALACTIC ASTRON & COSMOLO, OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","09/01/2017","08/24/2017","Andrew Connolly","WA","University of Washington","Standard Grant","Rob Beverly","08/31/2022","$500,000.00","Magdalena Balazinska, Mario Juric, Alvin Cheung, Ariel Rokem","ajc@astro.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","1217, 1253, 8004","1206, 7433, 7569, 8004, 8005, 9102","$0.00","Astronomy has entered an era of massive data streams generated by telescopes and surveys that can scan tens of thousands of square degrees of the sky across many decades of the electromagnetic spectrum. The promise of these new experiments - characterizing the nature of dark energy and the composition of dark matter, discovering the most energetic events in the universe, tracking asteroids whose orbits may intersect with that of the Earth - will only be realized if we can address the challenge of how to process and analyze the tens of petabytes of images that these astronomical surveys will generate per year. With the increasing capacity for scientists to collect ever larger sets of data, often in the form of images, our potential for scientific discovery will soon be limited not by how we collect or store data, but rather how we extract the knowledge that these data contain (e.g. how we account for noise inherent within the data, and understand when we have detected fundamentally new classes and interesting events or physical phenomena). This project is to develop an open source scalable framework for the analysis of large imaging data sets. It is designed to operate as a cloud service, incorporate seamlessly new or legacy image processing algorithms, support and optimize complex analysis workflows, and scale analyses to thousands of processors without the need for an individual user to develop custom solutions for a specific computer platforms or architecture. This framework will be integrated with state-of-the-art image analysis algorithms developed for astronomical surveys to provide an image analytics platform that can be used by future telescopes and cameras and the astronomical community as a whole. Beyond astronomy, the framework will be extended to enable scientists from the physical and life sciences that make use of imaging data (e.g. neuroscience, oceanography, biology, seismology) to focus their work on developing scientific algorithms and analyses rather than the infrastructure required to process massive data sets<br/><br/>Over the last decade, there have been many advancements in astronomical image analysis algorithms and techniques; driven by new surveys and experiments. The complexity of these techniques and the systems that run them has, however, meant that the number of users who make use of these advancements is small; typically restricted to the experiments themselves or to a small group of expert users. Because of this, the community as a whole does not benefit from the significant investment in image analytics for astronomy. In this project, the PIs address these issues by developing and deploying a scalable framework for the analysis of small and large imaging datasets. This cloud-based system will be able to incorporate new and legacy image processing algorithms, support and optimize complex analysis workflows, scale applications to thousands of processors without users needing to develop custom code for specific platforms, and support efficient sharing of algorithms and analysis results among users. It will enable state-of-the-art image analysis algorithms (e.g. those developed for surveys such as the Large Synoptic Survey Telescope; LSST) to be used by the broad astronomical community and in so doing will leverage then tens of thousands of hours that has been invested in the development of these techniques. To accomplish this the team will extract key data analysis functions from the LSST data analysis pipeline into a standalone library, independent of the LSST software stack and data access mechanisms. They will integrate this library with the Myria big data management system. Myria is an elastically scalable big data management system that operates as a service in the Amazon cloud that wedeveloped at the University of Washington. Compared with other big data systems, Myria is especially attractive because it integrates PostgreSQL database instances within its storage layer and thus provides access to PostgreSQL's rich libraries of spatial functions, which are frequently used in astronomical data analysis pipelines. At the same time, it has rich support for new and legacy Python code and for complex analytics. By integrating the library of LSST image analytics functions with Myria, new image analytics pipelines will become significantly easier to write. The skeleton of the analysis pipeline will be expressed in the MyriaL declarative query language (i.e. SQL extended with constructs such as iterations and others). The core data processing functions will directly map to Python functions, enabling the reuse of legacy code and the easy addition of new functions. The resulting code will be amenable to optimization and efficient execution using the Myria service. By doing so, they intend to reduce barriers to adoption. Users will be able to express their analysis in Python without worrying about how data and computation will be distributed in a cluster. The image analysis framework developed as part of this proposal will be made publicly available as open-source software. The PIs will utilize the use case of neuroscience to demonstrate how their system, developed for astronomy, can be deployed across multiple domains.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering, the Astronomical Sciences Division and Office of Multidisciplinary Activities in the Directorate of Mathematical and Physical Sciences."
"1550228","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)","OAC","COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","02/01/2019","Barbara Jacak","CA","University of California-Berkeley","Standard Grant","Bogdan Mihaila","06/30/2022","$497,683.00","Peter Jacobs, Xin-Nian Wang","bvjacak@lbl.gov","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","CSE","7244, 8004","026Z, 7433, 7569, 8009, 8084","$0.00","Microseconds after the Big Bang, the universe was filled with an extremely hot fluid called the Quark-Gluon Plasma. As the universe expanded, this plasma cooled and condensed into the building blocks of ordinary matter around us: protons, neutrons, and atomic nuclei. Droplets of this fluid, which exists only at temperatures above 2 trillion Kelvin, are generated and studied in the laboratory today using collisions of high-energy heavy ions at Brookhaven National Laboratory and CERN. A key method to study the Quark-Gluon Plasma is the generation of high-energy quarks and gluons in the collision, which interact with the hot plasma and emerge as ""jets"" of particles that are measured by experiments. These jets provide powerful tools to study the internal structure of the plasma, analogous to tomography in medical imaging. However, interpretation of jet measurements requires sophisticated numerical modeling and simulation, and comparison of theory calculations with experimental data demands advanced statistical tools. The JETSCAPE Collaboration, an interdisciplinary team of physicists, computer scientists, and statisticians, will develop a comprehensive software framework that will provide a systematic, rigorous approach to meet this challenge. Training programs, workshops, summer schools and MOOCs, will disseminate the expertise needed to modify and maintain this framework.<br/><br/>The JETSCAPE Collaboration will develop a scalable and portable open source software package to replace a variety of existing codes. The modular integrated software framework will consist of interacting generators to simulate (i) wave functions of the incoming nuclei, (ii) viscous fluid dynamical evolution of the hot plasma, and (iii) transport and modification of jets in the plasma. Integrated advanced statistical analysis tools will provide non-expert users with quantitative methods to validate novel theoretical descriptions of jet modification, by comparison with the complete set of current experimental data. To improve the efficiency of this computationally intensive task, the collaboration will develop trainable emulators that can accurately predict experimental observables by interpolation between full model runs, and employ accelerators such as Graphics Processing Units (GPUs) for both the fluid dynamical simulations and the modification of jets. The collaboration will create this framework with a user-friendly envelope that allows for continuous modifications, updates and improvements of each of its components. The effort will serve as a template for other fields that involve complex dynamical modeling and comparison with large data sets. It will open a new era for high-precision extraction of the internal structure of the Quark-Gluon Plasma with quantifiable uncertainties. <br/><br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. <br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences."
"1450439","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","OAC","Physical & Dynamic Meteorology, Software Institutes, EarthCube","08/01/2015","08/04/2015","Allen Evans","WI","University of Wisconsin-Milwaukee","Standard Grant","Bogdan Mihaila","07/31/2019","$164,381.00","","evans36@uwm.edu","3203 N DOWNER AVE","MILWAUKEE","WI","532113153","4142294853","CSE","1525, 8004, 8074","4444, 7433, 8009","$0.00","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities."
"1147463","Collaborative Research: SI2-SSE: Sage-Combinat: Developing and Sharing Open Source Software for Algebraic Combinatorics","OAC","OFFICE OF MULTIDISCIPLINARY AC, DYNAMICAL SYSTEMS, Software Institutes, CDS&E-MSS","06/01/2012","06/19/2012","Daniel Bump","CA","Stanford University","Standard Grant","Rajiv Ramnath","05/31/2017","$143,700.00","","bump@math.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","CSE","1253, 7478, 8004, 8069","1253, 5514, 7433, 7478, 7683, 8004, 8005","$0.00","Sage is an open source general purpose mathematical software system that has developed explosively within the last six years. Sage-Combinat is a subproject whose mission is ""to improve Sage as an extensible toolbox for computer exploration in (algebraic) combinatorics, and foster code sharing between researchers in this area"".  There is a long tradition of software packages for algebraic combinatorics. These have been crucial in the development of combinatorics since the 1960s.  The originality of the Sage-Combinat project lies in successfully addressing the following simultaneous objectives. It offers a wide variety of interoperable and extensible tools, integrated in a general purpose mathematical software package, as needed for daily computer exploration in algebraic combinatorics; it is developed by a community of researchers spread around the world and across institutions; and it is open source and depends only on open source software. Among the proposers, Stein is founder and lead developer of Sage while Bump, Musiker, and Schilling are strong contributors to Sage-Combinat. Hivert and Thiery (Paris-Sud, Orsay), founders and lead developers of Sage-Combinat, are both strongly affiliated with this project. Some of the research areas addressed in this project include symmetric functions, in particular Macdonald polynomials for arbitrary Cartan types and their nonsymmetric analogues, crystals, rigged configurations and combinatorial R-matrices, affine Weyl groups and Hecke algebras, cluster algebras, and posets.<br/> <br/>The project will develop Sage-Combinat in areas relevant to the ongoing research of the participants, together with relevant underlying infrastructure. The project will include three Sage Days workshops, and will be affiliated with a third scheduled workshop at ICERM. These workshops include a strong outreach component and have been a potent tool for connecting researchers and recruiting Sage users and developers. The grant will also fund a dedicated software development and computation server for Sage-Combinat, to be hosted in the Sage computation farm in Seattle. Emphasis will be placed on the development of thematic tutorials that will make the code accessible to new users. The proposal will also fund graduate student RA support, curriculum development, and other mentoring."
"1550562","Collaborative Research: SI2-SSI: Swift/E: Integrating Parallel Scripted Workflow into the Scientific Software Ecosystem","OAC","Software Institutes","10/01/2016","09/13/2016","Gerrick Lindberg","AZ","Northern Arizona University","Standard Grant","Amy Walton","09/30/2020","$50,000.00","","gerrick.lindberg@nau.edu","601 S KNOLES DR","Flagstaff","AZ","860117034","9285230886","CSE","8004","7433, 8004, 8009, 9102","$0.00","Science and engineering research increasingly relies on repeated execution of a complex series of steps (i.e., workflows) to form hypotheses; conduct experiments; analyze results; and refine theory.   Computation is often essential throughout the workflow and in this case, software can improve productivity by managing the computational and data workflow.  Swift is one such open-source workflow system that has been developed and widely used in diverse areas ranging from materials simulations and climate modeling to neuroscience and genomics. This project extends the capabilities of Swift by integrating it with other software systems that enable collaboration, usability, maintainability, and productivity. The new ecosystem, Swift/E, will enable scientists and engineers to more productively create and run computational workflow campaigns of larger scale, and debug, execute, adapt, and disseminate them faster and easier than has been possible to date. These workflows embody and communicate the computational methods specific to each domain of scientific inquiry. Swift/E achieves community engagement and extensive productivity benefits for a large user community through an integrated program of research, education, and software dissemination. The project engages and serves science and engineering communities by creating patterns of practice for building and sharing reusable workflow libraries, and by training students, educators, and researchers in their use.  To advance the education of the next generation of computationally trained scientists, Swift/E powers a network of NSF-supported ""e-Labs"" that teach the concepts of collaborative parallel computational science at high school and undergraduate levels, reaching over a thousand students annually.<br/><br/>The open-source Swift/E ""ecosystem"" integrates Swift with several scientific software elements that play a major role in the national and global cyberinfrastructure of today. These elements are: Swift for the parallel scripting of scientific workflow; Globus for data cataloging, management, and high-speed wide-area transport; the Web-based Galaxy workflow portal for workflow composition, execution, and collaborative sharing; Jupyter for the interactive development, testing, debugging, and assembly of high level programming and workflow languages; Python and R for productively expressing high-level computational logic; and ""git"" and related tools and Web portals for revision control, code dissemination and sharing, and for the collaborative engagement of developers.  Swift's implicitly parallel programming language is minimal and compact.  Swift provides a facility for embedding other scripting languages (currently Python, R, Julia and Tcl) into its runtime environment.  This project merges newer extreme-scale ""Swift/T"" capabilities with the flexible and portable original ""Swift/K"" version to make the core Swift/E software element more powerful and flexible while lowering it?s ongoing support cost. Swift/E enhances usability by extending Swift's troubleshooting and inter-language integration facilities.  And with enhanced and innovative workflow sharing archives, new training materials, and a sustained program for user support and self-sustaining and expanding community engagement, the Swift/E project engages, supports, and sustains a large global science and engineering user base."
"1342076","SI2-SSI: REAL-TIME LARGE-SCALE PARALLEL INTELLIGENT CO2 DATA ASSIMILATION SYSTEM","OAC","ADVANCES IN BIO INFORMATICS, INTERFAC PROCESSES & THERMODYN, Integrat & Collab Ed & Rsearch, Software Institutes","09/01/2012","03/06/2015","Anna Michalak","DC","Carnegie Institution of Washington","Continuing Grant","Rajiv Ramnath","08/31/2017","$1,667,410.00","","michalak@stanford.edu","5241 BROAD BRANCH RD NW","WASHINGTON","DC","200151305","2023876400","CSE","1165, 1414, 7699, 8004","","$0.00","Intellectual Merit: This proposal seeks to address this need by creating a state-of-the-art autonomous software platform for real-time integration of in-situ and satellite-based atmospheric CO2 measurements within a Data Assimilation (DA) system for producing estimates of global land and oceanic CO2 exchange at weekly to bi-weekly intervals. The proposed software infrastructure will be capable of autonomous processing of large volumes of data through a multi-stage pipeline, without the delays conventionally associated with such processing. Within the DA component, we will provide options for multiple DA algorithms for estimating global CO2 exchange. Users will, for the first time, have the capability to use these multiple methods as part of a single system for comparing estimates of CO2 exchange, and to obtain an improved understanding of the relative advantages of the various DA methods. As part of the analysis component of the software, we will build a carbon-climate surveillance system by drawing from a range of techniques in pattern recognition and high-dimensional statistical inference. This system will be able to detect and analyze localized variations in CO2 exchange within any user-specified spatio-temporal window. In addition, summaries of the CO2 exchange will be provided at annual and monthly temporal scales for continents and countries.<br/><br/>Broader Impacts: This software can be used by researchers and governmental institutions for evaluating both the natural components of the carbon cycle and anthropogenic carbon emissions, as well as in the design of new satellites for improved monitoring of CO2. All data and software will be publicly available and open-source development platforms will be used whenever possible.  The algorithm prototypes developed as part of this project will be used in undergraduate and graduate courses at the University of Michigan, and will be made available online for educators at other institutions. Finally, the project will train three graduate students, with a focus on developing their cross-disciplinary skills in the field of Earth science, statistics, computer science, and atmospheric science."
"1450319","Collaborative Research: SI2-SSI: Data-Intensive Analysis for High Energy Physics (DIANA/HEP)","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","05/01/2015","04/12/2021","Michael Sokoloff","OH","University of Cincinnati Main Campus","Continuing Grant","Bogdan Mihaila","04/30/2022","$1,000,000.00","","mike.sokoloff@uc.edu","2600 CLIFTON AVE","CINCINNATI","OH","452202872","5135564358","CSE","1253, 7244, 8004","7433, 8009, 8084","$0.00","Advanced software plays a fundamental role for large scientific projects. The primary goal of DIANA/HEP (Data Intensive ANAlysis for High Energy Physics) is developing state-of-the-art tools for experiments that acquire, reduce, and analyze petabytes of data. Improving performance, interoperability, and collaborative tools through modifications and additions to packages broadly used by the community will allow users to more fully exploit the data being acquired at CERN's Large Hadron Collider (LHC) and other facilities. These experiments are addressing questions at the heart of physics: What are the underlying constituents of matter? And how do they interact? With the discovery of the Higgs boson in 2012, the Standard Model of particle physics is complete. It provides an excellent description of known particles and forces.  However, the most interesting questions remain open: What is the dark matter which pervades the universe? Does space-time have additional symmetries or extend beyond the 3 spatial dimensions we know? What is the mechanism stabilizing the Higgs boson mass from enormous quantum corrections? The next generation of experiments will collect exabyte-scale data samples to provide answers. Analyzing this data will require new and better tools. <br/><br/>First, the project will provide the CPU and IO performance needed to reduce the iteration time so crucial to explore new ideas. It will develop software to effectively exploit emerging many- and multi-core hardware. It will establish infrastructure for a higher-level of collaborative analysis, building on the successful patterns used for the Higgs boson discovery and enabling a deeper communication between the theoretical community and the experimental community. DIANA?s products will sit in the ROOT framework, already used by the HEP community of more than 10000 particle and nuclear physicists. By improving interoperability with the larger scientific software ecosystem, DIANA will incorporate best practices and algorithms from other disciplines into HEP. Similarly, the project will make its computing insights, tools, and novel ideas related to collaborative analysis, standards for data preservation, and best practices for treating software as a research product available to the larger scientific community.  Finally, to improve the quality of the next generation of software engineers in HEP, DIANA will host an annual workshop on analysis tools and establish a fellowship program."
"1642345","Collaborative Research: SI2-SSE: High-Performance Workflow Primitives for Image Registration and Segmentation","OAC","Software Institutes","10/01/2016","09/08/2016","Gregory Sharp","MA","Massachusetts General Hospital","Standard Grant","Seung-Jong Park","09/30/2020","$110,000.00","","gcsharp@partners.org","55 FRUIT ST","BOSTON","MA","021142621","8572821670","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Image registration and segmentation are vital enabling technologies for addressing many complex, data driven problems. Examples include individualized medical treatment where disease progression is monitored by analyzing MRI, CT, or ultrasound images over time; identifying anatomical structures in medical images; recognizing objects and people in video footage; and extracting imageable biometrics such as fingerprints, faces, and the iris. Images and videos can now be easily acquired at a rate that far surpasses our capacity to perform advanced image analysis.  For this reason, advanced registration and segmentation algorithms are not routinely used for many large-scale and time sensitive applications because they require more processing time than is available. This project will remedy this situation by developing a high-performance software package for image registration and segmentation, suitable to be run on massively parallel processors, and building a strong user and developer base around it. All software developed through the project will be open source and licensed under the MIT License. Improvements in processing speed achieved by the proposed platform will have significant impact in disciplines such as computer vision, digital forensics, and biomedical image analysis. Finally, the project team is committed to the diversity mission of Drexel University and will reach out to under-represented groups when recruiting graduate students for this project. Selected research tasks will be integrated within existing courses and curriculum will be developed for new experiential programs stemming from this effort.<br/><br/>The overall goal of this project is to develop a high-performance, many-core CPU and GPU accelerated algorithmic software package for attacking classes of problems that depend on solutions to data-dense inverse problems such as registration, segmentation, tomography, and parameter estimation. The specific technical approach involves developing algorithmic primitives required by a broad class of inference and analysis based workflows. Probabilistic primitives for building generative, discriminative, and conditional random field classification models will be implemented with emphasis on object segmentation.  Specialized registration operators will be developed for spline and voxel-driven algorithms. These primitives will be developed within the single instruction multiple data paradigm which utilizes many-core processing architectures via OpenMP, CUDA, and OpenCL. The workflow will be supplemented by a graphical user interface (GUI), providing a feature rich studio of tools that expose high-performance primitives to scientists visually and intuitively. The platform architecture will be designed as a distributed system service targeting locally administered scientific computing clusters where the number of compute nodes will be able to scale with load requirements. The GUI and the computational core may either run in a distributed client-server configuration or together locally on a single high performance workstation. Emphasis will be placed on documentation and video/written tutorials necessary for adoption. The project team will use an open software development model to build a strong user base comprising both novice users as well as researchers with the need to implement new algorithms on top of a stable software infrastructure. It is expected that the availability of this tool and its source code will catalyze an increase in quantitative image analysis spanning across research disciplines."
"1440753","SI2-SSE: Adding Research Accounts to the ASSISTments' Platform: Helping Researchers Do Randomized Controlled Studies with Thousands of Students","OAC","Project & Program Evaluation, Software Institutes","09/01/2014","09/06/2016","Neil Heffernan","MA","Worcester Polytechnic Institute","Standard Grant","Rajiv Ramnath","08/31/2017","$486,209.00","Joseph Williams","nth@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","CSE","7261, 8004","7433, 8005, 9177, SMET","$0.00","ASSISTments is a free, university-based platform created to perform controlled experiments  with the potential to help increase the quality, speed, and reliability of results related to K12 education. ASSISTments' mission is ""to improve education through scientific research while not compromising student learning time."" Each day, teachers assign problems to thousands of students (currently 50,000 students) in ASSISTments. These problem sets often contain controlled experiments. ASSISTments has used this platform to do controlled experiments that have resulted in 17 peer-reviewed publications. For a typical education researcher, developing relationships with schools is costly. ASSISTments has built relationships with teachers and researchers to run experiments to improve education without disrupting classrooms. This project will add researcher accounts to ASSISTments to better facilitate the research process. Researchers will create their own experiments, get approval from WPI for release to teachers, and get anonymized data. ASSISTments will reach out to its community of teachers who trust ASSISTments, to invite them to run the study in their classrooms.   The intellectual merit of this work will be the contribution of the studies that this system would support. ASSISTments' ten-year goal is to have a community of hundreds of scientists that use this tool to do their studies. <br/><br/>Psychologists tend to study human learning in lab studies; researchers in education and learning sciences point out that it's not clear if those studies generalize to K12. These communities need to work together, but are lacking common ground.  Thousands of researchers in psychology, mathematics education, and learning sciences care about using science to better understand human learning. Some researchers study how to help students with motivational messages, spaced retesting, or comparing feedback. Many researchers have used thousands of psychology undergraduates as subjects, but want their ideas tested and validated in authentic K12 settings. Everyone understands physicists need a shared scientific instrument to do their work, but so do educational psychologists.  The broader impact of this work will be as a demonstration, showing how a tool could be built that helps many researchers conduct controlled experiments.  This will include showing how the project can increase the efficiency of the scientists? work."
"1339893","SI2-SSI: Particle-In-Cell and Kinetic Simulation Center","OAC","OFFICE OF MULTIDISCIPLINARY AC, PHYSICS AT THE INFO FRONTIER, Software Institutes","09/01/2013","07/22/2017","Warren Mori","CA","University of California-Los Angeles","Continuing Grant","Bogdan Mihaila","08/31/2019","$4,000,000.00","Russel Caflisch, Viktor Decyk, Frank Tsung, Michail Tzoufras","mori@physics.ucla.edu","10889 WILSHIRE BLVD STE 700","LOS ANGELES","CA","900244201","3107940102","CSE","1253, 7553, 8004","7433, 7483, 8009, 8084","$0.00","Computer simulations using the particle-in-cell (PIC) method are widely used for basic and applied research involving plasma physics applications. For example, simulations that calculate the self-consistent interaction of charged particles aid in the development of new accelerator technologies, new radiation sources, are used in magnetic and inertial fusion research, and help understand the physics of the solar wind. The Particle-in-Cell and Kinetic Simulation Software Center (PICKSC) at UCLA will aim to significantly broaden the impact of PIC simulations by making available and documenting illustrative software programs for different computing hardware, a flexible Framework for rapid construction of parallelized PIC programs, and several distinct production programs. This project will also include activities on developing and comparing different PIC algorithms and documenting best practices for developing and using PIC programs. The activities fostered by this project will bring together an interdisciplinary team of faculty, research staff, post-doctoral scholars, and graduate students. Important goals of this project include also the development of educational software for undergraduate and graduate courses in plasma physics and computer science and will, to build a large community of users through outreach and an annual workshop. <br/><br/>The broader impact of the activities fostered by this project will be significant. A well-documented set of components and example codes for running on large computers and a set of basic production codes will allow students and researchers from all levels and many disciplines to understand the inner workings of optimized PIC and kinetic simulation software used to model plasmas. It will allow them to build their own software, or to make independent comparisons against commercially available codes, against their own codes, and against published simulation data. The availability of production codes to more researchers will increase the rate of scientific discovery. The software will allow computer scientists who are developing tools that allow existing software to use next generation hardware to compare their performances against highly optimized codes, and will provide new code developers a test-bed of parallelized and optimized software for performance comparison. Furthermore, this projet will make state-of-the-art research software available for education, both for physics and computer science courses, will help train the next generation of plasma physicists (in many sub disciplines) and computational scientists. Interactive tools based on simpler skeleton codes will also be useful for undergraduate and high school education. Documenting examples of best practices will save graduate students and new researchers significant time in learning how to best employ PIC simulations."
"1440587","Collaborative: SI2-SSE - A Plug-and-Play Software Platform of Robotics-Inspired Algorithms for Modeling Biomolecular Structures and Motions","OAC","Special Projects - CCF, Software Institutes, CDS&E","02/01/2015","08/08/2014","Erion Plaku","DC","Catholic University of America","Standard Grant","Bogdan Mihaila","01/31/2019","$215,476.00","","plaku@cua.edu","620 MICHIGAN AVE NE","WASHINGTON","DC","200640001","2026355000","CSE","2878, 8004, 8084","2878, 7433, 8004, 8005, 8084, 9216","$0.00","This project aims to develop a novel plug-and-play platform of open-source software elements to advance algorithmic research in molecular biology. The focus is on addressing the algorithmic impasse faced by computational chemists and biophysicists in structure-function related problems involving dynamic biomolecules central to our biology. The software platform resulting from this project provides the critical software infrastructure to support transformative research in molecular biology and computer science that benefits society at large by advancing our modeling capabilities and in turn our understanding of the role of biomolecules in critical mechanisms in a living and diseased cell.<br/><br/>The project addresses the current impasse on the length and time scales that can be afforded in biomolecular modeling and simulation. It does so by integrating cutting-edge knowledge from two different research communities, computational chemists and biophysicists focused on detailed physics-based simulations, and AI researchers focused on efficient search and optimization algorithms. The software elements integrate sophisticated energetic models and molecular representations with powerful search and optimization algorithms for complex modular systems inspired from robot motion planning. The plug-and-play feature of the software platform supports putting together novel algorithms, such as wrapping Molecular Dynamics or Monte Carlo as local search operators within larger robotics-inspired exploration frameworks, and adding emerging biomolecular representations, models, and search techniques even beyond the timeline of this project."
"1535108","SI2-SSE: Analyze Visual Data from Worldwide Network Cameras","OAC","Information Technology Researc, Software Institutes","08/01/2015","01/11/2017","Yung-Hsiang Lu","IN","Purdue University","Standard Grant","Stefan Robila","07/31/2020","$599,987.00","","yunglu@purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","CSE","1640, 8004","019Z, 1504, 7433, 8004, 8005, 9179, 9251","$0.00","Many network cameras have been deployed for a wide range of purposes, such as monitoring traffic, evaluating air pollution, observing wildlife, and watching landmarks. The data from these cameras can provide rich information about the natural environment and human activities. To extract valuable information from this network of cameras, complex computer programs are needed to retrieve data from the geographically distributed cameras and to analyze the data. This project creates a open source software infrastructure by solving many problems common to different types of analysis programs.  By using this infrastructure, researchers can focus on scientific discovery, not writing computer programs. This project can improve efficiency and thus reduce the cost for running programs analyzing large amounts of data. This infrastructure promotes education because students can obtain an instantaneous view of the network cameras and use the visual information to understand the world. Better understanding of the world may encourage innovative solutions for many pressing issues, such as better urban planning and lower air pollution. This project can enhance diversity through multiple established programs that encourage underrepresented minorities to pursue careers in science and engineering.   <br/><br/>This project will combine: (1) the ability to retrieve data from many heterogeneous and distributed cameras, (2) the management of computational and storage resources using cloud computing, and (3) improved performance by reducing data movement, balancing loads among multiple cloud instances, and enhancing data-level parallelism. The project provides an application programming interface (API) that hides the underlying sophisticated infrastructure. This infrastructure will handle both real-time streaming data and archival data in a uniform way, so that the same analysis programs can be reused.  This project has four major components: (1) a web-based user interface, (2) a database that stores the details about the network cameras, (3) a resource manager that allocates cloud instances, and (4) a computational engine that execute the programs written by users. The service-oriented architecture will allow new functions to be integrated more easily by the research community."
"1642424","SI2-SSE: 3DSIM: A Unified Framework for 3D CPU Co-Simulation","OAC","Software Institutes","01/01/2017","01/11/2022","Ankur Srivastava","MD","University of Maryland, College Park","Standard Grant","Rob Beverly","09/30/2022","$500,000.00","Bruce Jacob","ankurs@eng.umd.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","CSE","8004","026Z, 7433, 7942, 8004, 8005","$0.00","Performance enhancements and increased energy efficiency that could be obtained by reducing the dimensions of transistors is becoming difficult. Thus, Moore's law no longer holds true for conventional approaches to chip design. Three-dimensional (3D) integration of chip components has emerged as an innovative packaging alternative to conventional approaches where multiple layers of silicon are stacked and interconnected using directly through the silicon layers (this technique is known as ""Through Silicon Via"" or TSV). Using TSVs and 3D packaging enables significant benefits to the performance, functionality and energy efficiency of future CPUs. However, 3D integration results in new types of interaction patterns between computing cores and between core and memory components. In addition, the close proximity between cores and memory causes their physical attributes, such as their temperature, noise of power delivery, and reliability to become uniquely interdependent. If innovations in 3D integration are to continue, substantial investment in frameworks that can simulate and evaluate 3D computer architectures are necessary. This project seeks to develop such a simulation framework and make it available to the computer architecture design community.<br/><br/>The objective of this project is to develop a full system simulator for 3D CPUs while accounting for the architectural and physical interactions between the cores and memory components thereby allowing the co-simulation of power, performance and reliability characteristics. The framework supports a wide array of 3D CPU configurations including intricate specifications of cores, core counts, network on chip protocols, on-chip/off-chip caches, main memory and off-chip secondary storage (built using diverse set of devices including SRAM, DRAM, non volatile devices). The project is a substantial addition to the repertoire of 3D integrated circuit design and simulation frameworks and shall play a vital role in future innovations in 3D CPU architectures."
"1550417","SI2-SSI: Collaborative Research: ParaTreet: Parallel Software for Spatial Trees in Simulation and Analysis","OAC","Software Institutes","09/01/2016","08/19/2016","Derek Richardson","MD","University of Maryland, College Park","Standard Grant","Rob Beverly","08/31/2017","$54,999.00","Wolfgang Losert","dcr@astro.umd.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","CSE","8004","7433, 8004, 8009","$0.00","Many scientific and visualization methods involve organizing the data they are processing into a hierarchy (also known as a ""tree"").   These applications and methods include: astronomical simulations of particles moving under the influence of gravity, analysis of spatial data (that is, data that describes objects with respect to their relative position in space), photorealistic rendering of virtual environments,reconstruction of surfaces from laser scans, collision detection when simulating the movement of physical objects, and many others.   Tree data structures, and the algorithms used to work on these structures, are heavily used in these applications because they help to make these applications run much faster on supercomputers. However, implementing tree-based algorithms can require a significant effort, particularly on modern highly parallel computers.  This project will create ParaTreet, a software toolkit for parallel trees, that will enable rapid development of such applications.  Details of the parallel aspects will be hidden from the programmer, who will be able to quickly evaluate the relative merits of different trees and algorithms even when applied to large datasets and very computation-intensive applications. The combination of such an abstract and extensible framework with a portable adaptive runtime system will allow scientists to effectively use parallel hardware ranging from small clusters to petascale-class machines, for a wide variety of tree-based applications. This project will demonstrate the feasibility of such an approach as well as generate evidence of community adoption of this technology. If successful, this project will enable NSF-supported researchers to solve science problems faster as well as to tackle more complex problems, thus serving NSF's science mission.<br/><br/><br/>This project builds upon an existing collaboration on Computational Astronomy and the resultant software base in the ChaNGa (Charm N-body GrAvity solver) code. ChaNGa is a software package that performs collisionless N-body simulations, and can perform cosmological simulations with periodic boundary conditions in co-moving coordinates or simulations of isolated stellar systems. This project will extend ChaNGa with a parallel tree toolkit called ParaTreet and associated applications, that will allow scientists to effectively utilize small clusters as well as very large supercomputers for parallel tree-based calculations.  The key data structure in ParaTreet is an asynchronous software-based tree data cache, which maintains a writeback local copy of remote tree data. We plan to support a variety of spatial decomposition methods and the associated trees, including Oct-trees, KD-trees, inside-outside trees, ball trees, R-trees, and their combinations. Different trees are useful in different application circumstances, and the software will allow their relative merits to be evaluated with relative ease. The framework will support a variety of parallel work decomposition methods, including those based on space filling curves, and support dynamic rearrangement of parallel work at runtime. The algorithms supported will range from Barnes-Hut with various multipole expansions, data clustering, collision detection, surface reconstruction, ray intersection, etc. The software includes a collection of dynamic load balancing strategies in the Charm++ framework that can be tuned for specific problem structures. It also includes support for clusters of accelerators, such as GPGPUs. This project will demonstrate the feasibility of such an approach as well as generate evidence of community adoption of this technology."
"1740151","SI2-SSE: Deep Forge: a Machine Learning Gateway for Scientific Workflow Design","OAC","Software Institutes","09/01/2017","10/17/2019","Akos Ledeczi","TN","Vanderbilt University","Standard Grant","Seung-Jong Park","08/31/2021","$400,000.00","Peter Volgyesi, Brian Broll","akos.ledeczi@vanderbilt.edu","110 21ST AVE S","NASHVILLE","TN","372032416","6153222631","CSE","8004","7433, 8004, 8005","$0.00","Recent advances in machine learning have already had a transformative impact on our lives. However, astonishing successes in diverse domains, such as image classification, speech recognition, self-driving cars and natural language processing, have mostly been driven by commercial forces, and these techniques have not yet been widely transitioned into various science domains. The field is ripe for innovation since many science fields have readily available large-scale datasets, as well as access to public or private compute infrastructure capable of executing computationally expensive artificial neural network (ANN) training workflows. The main roadblocks seem to be the steep learning curve of the ANN tools, the accidental complexities of setting up and executing machine learning workflows, and the fact that finding the right deep neural network architecture requires significant experience and lots of experimentation. DeepForge overcomes these obstacles by providing an intuitive visual interface, a large library of reusable components and architectures as well as automatic software generation enabling domain scientist to experiment with ANNs in their own field. There is unmet high demand of talent in machine learning, exactly because it has so much potential in a wide variety of application areas. Therefore, any tool that helps scientists apply machine learning in their own domains will have a broad impact. The promise of DeepForge is to flatten the learning curve, hide low level unimportant details and provide components that are reusable within and across disciplines. Therefore, DeepForge will have transformative impact on a number of fields.<br/><br/>DeepForge, a web- and cloud-based software infrastructure raises the abstraction of creating ANN workflows via an intuitive visual interface and by managing training artifacts. Hence, it enables domain scientists to leverage recent advances in machine learning. DeepForge will also integrate with existing cyberinfrastructure, including private and commercial compute clusters, cloud services (e.g. Amazon EC2), public supercomputing resources, and online repositories of scientific datasets. The DeepForge visual language for designing ANN architectures and workflows is powerful enough to capture the concepts related to common deep learning tasks, yet it provides a high level of abstraction that shields the users from the underlying complexity at the same time. DeepForge will provide a facility that allows for sharing design artifacts across a wide interdisciplinary user community. Curating a rich library of reusable components, integrating with a wide variety of existing cyberinfrastructure resources from data sources to compute platform and providing data provenance in a seamless manner are other advantages of the project. DeepForge will promote ""data as product,"" ""model as product,"" and ""service as product"" concepts through integration with the Digital Object Identifier (DOI) infrastructure. DeepForge will enable scientist to assign DOIs to their shared assets providing data provenance enabling citing and publicly reproducing research results by executing the referenced ANN workflows with the linked data artifacts.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1440756","SI2-SSE: An Active File System for Processing and Sharing Petascale Multi-Dimentional Datasets","OAC","ADVANCES IN BIO INFORMATICS, Special Projects - CCF, Software Institutes","09/01/2014","08/08/2014","Arthur Wetzel","PA","Carnegie-Mellon University","Standard Grant","Varun Chandola","08/31/2018","$500,000.00","","awetzel@psc.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","CSE","1165, 2878, 8004","1165, 2878, 7433, 8004, 8005","$0.00","Data sets in diverse areas such as biology, engineering, or astronomy routinely reach terabyte scales and are expected to grow to petabytes within the next few years. Typical examples include time series measurement or multidimensional volumetric data sets. Due to their rapidly increasing size, these data present severe limitations for data storage, transmission, and processing, and will thus become serious bottlenecks for analysis pipelines and collaborative data analysis.  New approaches and frameworks are needed to enable the timely and cost effictive analyses at next generation data scales. The Active Data Processing and Transformation File System (ADAPT FS) combines efficient storage of original data with on-the-fly processing to enable collaborative processing and sharing of scientific datasets at the largest scales with minimal data duplication and latency. The remote access will leverage PSC's SLASH2 distributed filesystem and be extended to provide visualization. ADAPT-FS processing will enable easy access to leading-edge scientific datasets for teaching and training at institutions of all sizes.<br/><br/>ADAPT-FS provides a FUSE based file system interface allowing seamless use from programs or web servers. The guiding principles behind ADAPT-FS are to: 1) eliminate unwanted data duplication; 2) minimize data transfer by working directly from original data when possible; 3) minimize delays between data capture and end-user analyses; and 4) provide a flexible workflow which incorporates active computation. The latter aspect enables improved parallel I/O performance by exploiting untapped CPU and GPU power on the nodes of large data servers. We will provide ADAPT-FS as open source, including a flexible and well-documented API and a plug-in framework enabling users to insert their own GPU and CPU codes into the data pipeline to extend and customize its data processing capabilities.  Thus, ADAPT-FS will provide a critical technology to tackle the next generation of massive data intensive processing, allowing efficient and rapid analysis of petabytes size data sets with minimal data duplication in a collaborative multi-site framework."
"1550225","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)","OAC","OFFICE OF MULTIDISCIPLINARY AC, APPLIED MATHEMATICS, COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","06/25/2019","Steffen Bass","NC","Duke University","Continuing Grant","Bogdan Mihaila","06/30/2021","$574,428.00","Robert Wolpert","bass@phy.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","CSE","1253, 1266, 7244, 8004","026Z, 4444, 7433, 7569, 8009, 8084, 8251","$0.00","Microseconds after the Big Bang, the universe was filled with an extremely hot fluid called the Quark-Gluon Plasma. As the universe expanded, this plasma cooled and condensed into the building blocks of ordinary matter around us: protons, neutrons, and atomic nuclei. Droplets of this fluid, which exists only at temperatures above 2 trillion Kelvin, are generated and studied in the laboratory today using collisions of high-energy heavy ions at Brookhaven National Laboratory and CERN. A key method to study the Quark-Gluon Plasma is the generation of high-energy quarks and gluons in the collision, which interact with the hot plasma and emerge as ""jets"" of particles that are measured by experiments. These jets provide powerful tools to study the internal structure of the plasma, analogous to tomography in medical imaging. However, interpretation of jet measurements requires sophisticated numerical modeling and simulation, and comparison of theory calculations with experimental data demands advanced statistical tools. The JETSCAPE Collaboration, an interdisciplinary team of physicists, computer scientists, and statisticians, will develop a comprehensive software framework that will provide a systematic, rigorous approach to meet this challenge. Training programs, workshops, summer schools and MOOCs, will disseminate the expertise needed to modify and maintain this framework.<br/><br/>The JETSCAPE Collaboration will develop a scalable and portable open source software package to replace a variety of existing codes. The modular integrated software framework will consist of interacting generators to simulate (i) wave functions of the incoming nuclei, (ii) viscous fluid dynamical evolution of the hot plasma, and (iii) transport and modification of jets in the plasma. Integrated advanced statistical analysis tools will provide non-expert users with quantitative methods to validate novel theoretical descriptions of jet modification, by comparison with the complete set of current experimental data. To improve the efficiency of this computationally intensive task, the collaboration will develop trainable emulators that can accurately predict experimental observables by interpolation between full model runs, and employ accelerators such as Graphics Processing Units (GPUs) for both the fluid dynamical simulations and the modification of jets. The collaboration will create this framework with a user-friendly envelope that allows for continuous modifications, updates and improvements of each of its components. The effort will serve as a template for other fields that involve complex dynamical modeling and comparison with large data sets. It will open a new era for high-precision extraction of the internal structure of the Quark-Gluon Plasma with quantifiable uncertainties. <br/><br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. <br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences."
"1912444","SI2-SSE:   GenApp - A Transformative Generalized Application Cyberinfrastructure","OAC","Software Institutes","08/31/2018","02/11/2019","Emre Brookes","MT","University of Montana","Standard Grant","Seung-Jong Park","09/30/2020","$197,151.00","","emre.brookes@umontana.edu","32 CAMPUS DR","MISSOULA","MT","598120003","4062436670","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Scientific computing and computational analysis are becoming integral aspects of virtually any field of science, be it exact sciences like Physics, Chemistry and Biology, or social sciences. Efforts of many research laboratories are focused on creation of scientific codes for data generation, analysis and interpretation. However, publicly funded and often hard won scientific codes developed in a typical research laboratory too frequently become unsustainable  beyond the lifetime of funding or shortly after staff rotation. Projects that are funded to afford expensive computer science expertise simply to maintain and update existing software divert scarce resources from the lab's primary goals and often translates the problem without resolving it.  Only a select number of researchers receive sufficient funding to maintain and update software, limiting the dissemination of new ideas and techniques. The diversity and continually changing nature of software environments compounds the issues.  Enabling user utilization presents hurdles in deployment, access and training.  These issues also create barriers to the implementation of new ideas embodied in new codes.  The GenApp project's goals are to address these issues. To begin with, GenApp enables the rapid dissemination of scientific codes to researchers with minimal software expertise. As more researchers use these codes, more of them become vested in the codes, which helps their sustainability. <br/><br/>The fundamental goal of this project is to advance the GenApp framework into a transformative tool to broadly benefit the scientific software developer community. GenApp is a generalized application generation framework intended for rapid deployment of scientific codes, which can generate both science gateways and stand-alone applications. Among the main unique features of GenApp are the minimal technical expertise requirement for the end user and an open-end design ensuring sustainability of generated applications. To produce fully functional applications, GenApp weaves libraries of fragments and user defined modules as directed by simple definition files, created from a uniform, logical, and simple-to-encode general interface definition file provided by GenApp.  This general definition file and the underlying software can be reused indefinitely to produce applications in a variety of existing and yet-to-be defined software environments. Preserving such simplicity with GenApp's maturation is one of the main developmental strategies. To achieve the goal of GenApp four focus Aims have been proposed. The first is infrastructure development, which includes general enhancements to the capabilities of GenApp. The second is documentation, training, dissemination, outreach and sustainability - all important aspects to produce a software product that is useful to the community. The third is simply feedback, since user and developer feedback will help drive the first two Aims.  The final Aim includes two structural biology domain science applications that will adopt and drive GenApp development.  GenApp will see its primary practical utilization in making highly demanding novel computational and analysis tools accessible to experimentalists and theoreticians working in the nuclear magnetic resonance (NMR) and small-angle scattering (SAS) domains of structural biology. The GenApp framework will serve as a platform for applications utilizing advanced tools requiring efficient use of HPC resources, tools for modeling SAS data with molecular simulations, and a large software suite for a combined analysis of NMR and SAS measurements coupled to computational modeling. Easy access to these powerful tools will enable hitherto impossible studies of a number of fundamental biological problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1450451","SI2-SSI: Community Software for Extreme-Scale Computing in Earthquake System Science","OAC","Geophysics, GEOINFORMATICS, Software Institutes, EarthCube, CDS&E","09/01/2015","03/21/2019","Yehuda Ben-Zion","CA","University of Southern California","Standard Grant","Stefan Robila","08/31/2019","$2,200,000.00","Thomas Jordan, Kim Olsen, Yifeng Cui, Ricardo Taborda, Eric Daub","benzion@usc.edu","3720 S FLOWER ST","LOS ANGELES","CA","900894304","2137407762","CSE","1574, 7255, 8004, 8074, 8084","7433, 8004, 8009","$0.00","The Software Environment for Integrated Seismic Modeling (SEISM) Project of the Southern California Earthquake Center (SCEC) will develop advanced earthquake simulation software capable of using high-performance computing to produce new information about earthquakes and the hazards they present. SCEC's SEISM project is developing an integrated, sustainable community software framework for earthquake system science to serve diverse communities of earthquake scientists and engineers, computer scientists, and at-risk stakeholders. The SEISM project is a collaboration among several diverse user communities with shared interests in reducing seismic risk and enhancing seismic resilience. SCEC SEISM researchers are addressing scientific problems that limit the accuracy and scale in current numerical representations of earthquake processes. SEISM computational improvements in seismic hazard calculations will benefit earthquake system science worldwide. The SCEC SEISM project will educate a diverse STEM workforce from the undergraduate to early-career level, and it will cross-train scientists and engineers in a challenging high-performance environment. As one application of SEISM, the researchers will develop new simulations for the Great California ShakeOut, which is engaging millions of people in earthquake preparedness exercises.<br/><br/>Earthquake simulations at the spatiotemporal scales required for probabilistic seismic hazard analysis present some of the toughest computational challenges in geoscience, requiring extreme-scale computing. The Southern California Earthquake Center is creating a Software Environment for Integrated Seismic Modeling (SEISM) that will provide the extreme-scale simulation capability needed to transform probabilistic seismic hazard analysis into a physics-based science. This project will advance SEISM through a user-driven research and development agenda that will push validated SEISM capabilities to higher seismic frequencies and towards extreme-scale computing. It will develop an integrated, sustainable community software framework for earthquake system science to serve diverse communities of earthquake scientists and engineers, computer scientists and at-risk stakeholders. A new SEISM-T framework will support both in-situ and post-hoc data processing to make efficient use of available heterogeneous architectures. The main goal of the project is to increase the 4D outer-scale/inner-scale ratio of simulations at constant time-to-solution by two orders of magnitude above current capabilities. The software development plan will use an agile process of test-driven development, continuous software integration, automated acceptance test suites for each application, frequent software releases, and attention to user feedback. The researchers will take advantage of the SCEC Implementation Interface to develop a dialog among user communities regarding the application of SEISM to the reduction of seismic risk and enhancement of seismic resilience. This research will address fundamental scientific problems that limit the accuracy and scale range in current numerical representations of earthquake processes, which will benefit earthquake system science worldwide. This project will educate a diverse STEM workforce from the undergraduate to early-career level, and it will cross-train scientists and engineers in a challenging high-performance environment. As one application of SEISM, the project team will develop new simulations for the Great California ShakeOut, which is engaging millions of people in earthquake preparedness exercises."
"1535232","SI2-SSE: EASE: Improving Research Accountability through Artifact Evaluation","OAC","Special Projects - CCF, Software Institutes","09/01/2015","06/16/2015","Bruce Childers","PA","University of Pittsburgh","Standard Grant","Bogdan Mihaila","08/31/2019","$499,515.00","Daniel Mosse'","childers@cs.pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","CSE","2878, 8004","7433, 8005","$0.00","Research in computer systems, particularly in the first stages of creating a new innovation, relies almost exclusively on software prototypes, simulators, benchmarks, and data sets to understand the benefits and costs of new ideas in computers, ranging from consumer devices to exascale systems. These artifacts are used to evaluate new capabilities, algorithms, bottlenecks and trade-offs. Empirical study is behind the rapid pace of innovation in creating faster, lower energy and more reliable systems. This experimental approach lies at the core of development that fuels the nation's information economy.  Given the critical importance of experimental study to developing new computer systems, several efforts are underway to curate experimental results through accountable research.  One effort, Artifact Evaluation (AE), is being adopted to promote high quality artifacts  and experimentation, including making public the experimental information necessary for reproducibility. However, the rapid adoption of AE is hampered by technical challenges that create a high barrier to the process: there is no consistent or simple environment, or mechanism, to package and reproduce experiments for AE. Authors rely on their own approaches, leading to much time consumed, as well as considerable variability in the ways materials are prepared and evaluated, unnecessarily obstructing the AE process.  <br/><br/>To overcome the technical challenges with AE, and to more broadly encourage adoption of AE in computer science and engineering research, this project is developing a software infrastructure, Experiment and Artifact System for Evaluation (EASE), to create and run experiments specifically for AE, in which authors create, conduct and share artifacts and experiments. It allows for repeating,  modifying, and extending experiments. Authors may also use EASE to  package and upload their experiments for archival storage in a digital library. EASE is being developed and deployed for two use cases, namely compilers and real-time systems, keeping the project tractable to address specific needs. These communities have overlapping but also distinct requirements, helping to ensure EASE can also be extended and<br/>used by other computer systems research communities as well.<br/><br/>EASE will be release as open source software, based on an Experiment Management System (EMS) previously developed by the project investigator in a project call Open Curation for Computer Architecture Modeling (OCCAM), used to define and conduct experiments using computer architecture simulators. Using EMS as a starting point, EASE will provide AE support, by: 1) separating EMS from OCCAM's repository and hardware services, transforming the EMS infrastructure into EASE, a fully standalone, sustainable, and extensible platform for AE; 2) supporting record and replay (for repeating and reproducing results, as well as provenance) of artifacts and experiments as part of normal development and experimental practice to ease participation in AE by authors and evaluators; 3) supporting artifacts,workflows of artifacts and experiments that run directly on a machine, including specialized hardware and software, and run indirectly on a simulator or emulator; 4) allowing both user-level (artifacts and experiments as user processes) and system-level (artifacts and experiments involving kernel changes) innovations; 5) providing consistent/uniform access, whether locally or remotely, to artifacts and experiments; 6) simplifying viewing, running, modifying, and comparing experiments by innovators (i.e., during innovation development), artifact evaluators (during AE), and archive users (after publication); 7) enabling indexing (object locators and search tags) and packaging of artifacts and experiments for AE and for archival deployment (e.g., to ACM?s or IEEE?s Digital Library); and 8) refining, expanding, generalizing, and documenting EASE to ensure it is robust, maintainable and extensible, and that it can be used and sustained by different CSR communities (starting with real-time and compilers, given their different artifacts, data and methods)."
"1663696","Collaborative Research: SI2-SSI: Modules for Experiments in Stellar Astrophysics","OAC","STELLAR ASTRONOMY & ASTROPHYSC, OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","05/01/2017","04/27/2017","Richard Townsend","WI","University of Wisconsin-Madison","Standard Grant","Rob Beverly","04/30/2023","$683,452.00","","townsend@astro.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","CSE","1215, 1253, 7244, 8004","1206, 7433, 7569, 8004, 8009, 8084","$0.00","Stars are the most commonly observed celestial objects, and remain at the forefront of astronomical research.  The goal of this project is to sustain the MESA software framework as a key piece of software infrastructure for the astronomy community while building new innovative scientific capabilities and educational networks. MESA (Modules for Experiments in Stellar Astrophysics) is a software instrument that solves the equations governing the evolution of stars.  The MESA project 1) has attracted over 900 registered users world-wide; 2) has over 10,000 downloads of the source code; 3) has received over 12,000 archived and searchable posts about community discussions of MESA; 4) has been cited about 1000 times and has a current citation rate of about 300/year; 5) papers that cite MESA have themselves generated over 10,000 citations; 6) provides a Software Development Kit to build MESA across a variety of platforms; 7) delivers an annual Summer School program that now has over 150 graduates; 8) hosts a web-portal for the astronomy community to share knowledge and tools; 9) offers a prototype of a cloud resource for education, MESA-Web.  These metrics provide evidence that MESA is becoming standard software for understanding evolving stars.  <br/><br/>This project supports the MESA software framework and its user community. MESA provides solvers for one-dimensional fully coupled structures and composition equations governing stellar evolution. It is based on an implicit finite volume scheme with adaptive mesh refinement and sophisticated time-step controls; state-of-the-art modules provide equation of state, opacity, nuclear reaction rates, element diffu- sion, boundary conditions, and changes to the mass of the star. MESA is an open source library that employs contemporary numerical approaches, supports shared memory parallelism based on OpenMP, and is designed with present and future multi-core and multi-thread architectures in mind. MESA combines composable, interoperable, robust, efficient, thread-safe numerical and physics modules for provenance-capable simulations of a wide range of stellar evolution scenarios ranging from giant planets to low mass single stars to massive star binaries. MESA?s domain of applicability continues to grow, with recent extensions enabling users to model oscillations, rota- tion, binary stars, and explosions. Recent technical innovations allow for user plugins and provide bit-for-bit consistent results across all supported platforms.  The products of the MESA project has driven and will continue to drive significant innovation in the stellar, gravitational wave, nuclear, exoplanet, galactic, and cosmological communities."
"1663954","Collaborative Research: SI2-SSI: Inquiry-Focused Volumetric Data Analysis Across Scientific Domains: Sustaining and Expanding the yt Community","OAC","Leadership-Class Computing, Software Institutes","10/01/2017","04/09/2019","Leigh Orf","WI","University of Wisconsin-Madison","Standard Grant","Seung-Jong Park","09/30/2023","$285,059.00","","leigh.orf@ssec.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","CSE","7781, 8004","7433, 7781, 8004, 8009","$0.00","Scientific discovery across the physical sciences is increasingly dependent on the analysis of volumetric - or three-dimensional - data, that may come from a supercomputer simulation, direct measurement, or mathematical models. Researchers typically seek to extract meaningful insights from this data by visualizing and analyzing it in various ways. The ways in which scientists process volumetric data are actually quite similar across domains, but cross-disciplinary knowledge transfer and tool development is blocked by barriers of terminology. This project seeks to enhance an analysis and visualization toolkit named yt that is currently primarily used for astrophysical simulations. yt allows scientists to access and analyze data at several different levels by providing an interface that is designed to answer questions motivated by the underlying scientific problem, while worrying less about details such data formats, specific analysis techniques etc. yt's utilization in computational astrophysics has dramatically increased access to advanced algorithms for both visualization and analysis, and fostered the growth of a community of researchers sharing techniques and results. This project seeks to make yt available and adopted by scientists in other domains of science thus reproducing its success in astrophysics in these other science domains. This project will expand the yt community beyond theoretical astrophysics and enable and promote collaboration and advanced data analysis in the fields of meteorology, seismology and global tomography, observational astronomy, hydrology and oceanography, and plasma physics. <br/><br/>Improvements to the yt project will proceed along four principal technical avenues. The first is to develop a system that adapts the way yt presents data via a set of domain contexts that encode the ontology, domain-specific vocabulary, and common analysis tasks for a given field of study. This will include creating a domain context system as well as a set of five pilot domain contexts developed in collaboration with domain practitioners. The second is to overhaul the yt field system, adding more versatility and enabling significant optimizations. Thirdly, the project team will implement non-spatial indexing schemes, providing methods for accessing and analyzing data that is not organized according to the standard spatial axes. The final improvement will be the development of a non-local analysis system, allowing generalized path traversal as well as domain convolutions. To ensure wide dissemination and use of these improved capabilities, the team will design domain-specific documentation and training materials, and organize outreach and training events for early-career researchers. This will consist of both hands-on technical workshops and curricula developed in collaboration with Data Carpentry for utilization at other institutions. This combination of technical developments and social investments has been designed to ensure both readiness of the software and engagement of the targeted research communities."
"1147912","SI2-SSI: The Language Application Grid: A Framework for Rapid Adaptation and Reuse","OAC","Special Projects - CNS, Special Projects - CCF, International Research Collab, CSR-Computer Systems Research, IIS Special Projects, Robust Intelligence, Software & Hardware Foundation, Software Institutes, ","08/01/2012","08/24/2018","James Pustejovsky","MA","Brandeis University","Standard Grant","Bogdan Mihaila","07/31/2019","$1,764,929.00","Marc Verhagen, Eric Nyberg, Christopher Cieri","pustejovsky@gmail.com","415 SOUTH ST","WALTHAM","MA","024532728","7817362121","CSE","1714, 2878, 7298, 7354, 7484, 7495, 7798, 8004, O422","1714, 2878, 5983, 7433, 7484, 7944, 8004, 8009","$0.00","The need for robust language processing capabilities across academic disciplines, education, and industry is without question of vital importance to national security, infrastructure development, and the competitiveness of American business. However, at this time a robust, interoperable software infrastructure to support natural language processing (NLP) research and development does not exist. To fill this gap, this project establishes an large international collaborative effort involving key international players to develop an open, web-based infrastructure through which massive and distributed language resources can be easily accessed, in whole or in part, and within which tailored language services can be efficiently composed, disseminated and consumed by researchers, developers, and students. <br/><br/>The goal of this project is to build a comprehensive network of web services and resources within the NLP community. This requires four specific activities: (1) Design, develop and promote a service-oriented architecture for NLP development that defines atomic and composite web services, along with support for service discovery, testing and reuse; (2) Construct a Language Application Grid (LAPPS Grid) based on Service Grid Software developed in Japan; (3) Provide an open advancement (OA) framework for component- and application-based evaluation that enables rapid identification of frequent error categories within modules, thus contributing to more effective investment of resources; (4) Actively promote adoption, use, and community involvement with the LAPPS Grid. <br/><br/>By providing access to cloud-based services and support for locally-run services, the LAPPS Grid will lead to the development of a massive global network of language data and processing capabilities that can be used by scientists and engineers from diverse disciplines, providing components that require no expertise in language processing to use. Research in sociology, psychology, economics, education, linguistics, digital media, and the humanities will be impacted by the ability to easily manipulate and process diverse language data sources in multiple languages."
"1642439","SI2-SSE: GraphPack: Unified Graph Processing with Parallel Boost Graph Library, GraphBLAS, and High-Level Generic Algorithm Interfaces","OAC","Software Institutes","10/01/2016","08/24/2016","Andrew Lumsdaine","IN","Indiana University","Standard Grant","Vipin Chaudhary","03/31/2017","$499,386.00","Marcin Zalewski","al75@uw.edu","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","CSE","8004","026Z, 7433, 7942, 8004, 8005","$0.00","Modeling and simulating physical phenomena with computers is now an important tool for development and discovery in almost all fields of science and engineering.  Joining theory and experimentation, computation is now recognized as the ""third pillar"" of scientific research.  More recently, data analytics (the science of examining raw data with the purpose of drawing conclusions about that information) has emerged as an important computational tool for scientific discovery - a tool that is likely to be as important as, if not more important than, modeling and simulation.  Within the broad domain of data analytics, the use of graphs is a powerful conceptual tool that describes relationships between discrete objects.  Because of the growing importance of data analytics, many research groups have turned their attention to developing new approaches for solving large-scale graph problems.  While the research results in this area have been valuable, the software products that have been produced tend to be limited in scope and/or not of sufficient quality to be reused. This project will address this problem by creation of GraphPack, a comprehensive unified graph library with a coherent user interface and support for multiple state-of-the-art compute platforms. This work will have broad impacts in scientific and engineering application areas, larger social and economic areas depending on graph analytics, and in education. GraphPack will improve the ease of use and broaden the applicability of graph algorithms. Application areas include such diverse areas as knowledge discovery, genomics, proteomics, electronic design automation, forest management, Internet routing, power grid management, and many more.<br/><br/>GraphPack will be a reliable and comprehensive toolkit applicable across a wide variety of problems and architectures that will unleash the capabilities of the community by making the current state of the art readily available. GraphPack will develop a consistent and comprehensive set of abstractions necessary to express a wide variety of (generic) graph algorithms and data structures in the context of unimodal as well as hybrid parallelism. These abstractions will be incorporated as abstract concepts, and selected concrete efficient implementations will be provided. While genericity is an important goal, GraphPack will also provide a simplified user interface for graph algorithms for the situations where simplicity is more important than fully tuned performance. GraphPack will also provide a GraphBLAS interface based on the recent efforts to provide a standardized set of graph operations based on the concepts of linear algebra. By providing multiple interfaces with efficient parallel implementations, GraphPack will enable a wide variety of applications to take advantage of high-performance graph algorithms."
"1550471","Collaborative Research: SI2-SSI: Software Framework for Electronic Structure of Molecules and Solids","OAC","Software Institutes","08/01/2016","07/21/2016","Garnet Chan","NJ","Princeton University","Standard Grant","Rob Beverly","10/31/2016","$600,000.00","","garnetc@caltech.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","CSE","8004","7433, 8009, 9216","$0.00","Many traditionally experimental disciplines such as chemistry and materials science are rapidly changing due to our increasing ability to predict properties of molecules and materials purely by simulation. This is particularly true when molecules meet solid surfaces - due to the particular challenges of experiments in such a setting. Yet the molecule-on-surface frontier encompasses a vast class of problems of tremendous practical importance:  industrial applications facilitated by surface processes are estimated to produce globally more than than 15 trillion USD worth of goods and products.  This research will improve our ability to simulate the physics and chemistry of molecules on surfaces by extending the advanced simulation methodologies that were originally developed by for modeling electrons in molecules. This project will not only advance our fundamental understanding of the surface science but also open a road to technological applications relevant to producing and storing clean energy and in designing improved catalysts. The research may result in a new computer software framework for simulating electrons in molecules and materials. This software will be a unique contribution to the U.S. cyberinfrastructure and spur further innovation by other researchers in the US and worldwide, who will be able to access its source code for free. The software framework will also serve as an education platform for training computational chemists and materials scientists.<br/><br/>A frontier simulation challenge lies at the intersection of the two domains of chemistry and materials science - namely to determine, with predictive accuracy, the properties and chemistry of molecules on solid surfaces. The molecule-on-surface frontier encompasses a vast class of problems of tremendous practical importance: heterogeneous catalysis, photovoltaics, and emerging electronic materials. Yet, from a simulation perspective, it is not currently possible to efficiently combine the recent advances in highly accurate many-body molecular and periodic condensed phase methodologies in these problems, due to a significant gap between how the electronic structure theories of molecules and materials are formulated, as reflected in distinct algorithms and disjoint codebases. The goal of this project is to reduce and/or completely eliminate the gap between molecular and solid-state electronic structure methodologies, in theory, algorithms, and in usable community software implementations. This will be achieved by building an ambitious Electronic structure for Molecules and Solids (EMOS) software framework that will permit accurate computation of the first-principles electronic structure of both molecules and solids on an equivalent footing - and with the high efficiency necessary for high-throughput screening or ab initio molecular dynamics. These efforts build on the leading track-record of the principal investigators in developing open-source quantum chemistry software as well as automated computer implementation and high-performance parallel libraries. The project will allow the advances from molecular electronic structure - embedding, reduced-scaling many-body methodology, accurate excited-state electronic structure, and others - to be applied routinely to molecules, materials, and combinations of the two as relevant to surface chemistry. This has great potential to advance the state-of-the-art in treatment of electronic structure and open new lines of theoretical inquiry. The resulting open-source production-quality toolkit will be validated against experimental data for a host of surface phenomena, from exciton dynamics to surface spectroscopy and catalysis. An open-source US-based advanced materials code is a long-standing omission in U.S. cyberinfrastructure. As a high-performance framework for simulation of electronic structure of molecules, solids, and their interfaces with unprecedented accuracy, EMOS will be a significant contribution to this effort. Further, the modular component based structure will be able to be integrated with other major electronic structure packages through the reuse of the modules. This project will provide invaluable training opportunities to the students and postdocs who will develop the software framework under the direct supervision of principal investigators. In addition, each project site will contribute to the development of a stakeholder network for EMOS by hosting, each summer, visiting students and faculty representing the broader theoretical community, to train them on the use of EMOS in research and education. The project team will also use EMOS in teaching classes and summer schools, building on already established efforts in this area; these efforts will also be extended to an online setting."
"1265278","SI2-SSE: General Tensor Software Elements for Quantum Chemistry, Tensor Network Theories, and Beyond","OAC","OFFICE OF MULTIDISCIPLINARY AC, CHEMISTRY PROJECTS, Software Institutes","06/26/2012","09/24/2012","Garnet Chan","NJ","Princeton University","Standard Grant","Daniel Katz","05/31/2015","$386,818.00","","garnetc@caltech.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","CSE","1253, 1991, 8004","1253, 1991, 7433, 7683, 8004, 8005, 9216, 9263","$0.00","The PI will develop reusable software components to accelerate innovation in science that relies on high-dimensional tensor computations. While the components are informed by use-cases taken from quantum chemistry, the challenges of tensor computation are universal, and span diverse areas of science and engineering. These problems range from simulations of nuclear spin spectra, to quantum chemical calculations on molecules, to psychometric analysis, to numerical general relativity. The overarching aim of the funded work is to develop reusable tensor software elements, based on modern sustainable software practices, to benefit tensor algorithmic development in the scientific community at large. In addition, beyond the broad scientific impacts of the software, our education and outreach agenda comprises a multi-tiered effort to uplift the ability of the science and engineering community to reason about high-dimensional tensor and matrix computations.<br/><br/>Important aspects of the software elements will include (i) expressive programming interfaces for rapid prototyping of tensor based theories (ii) layered tensor libraries for dense, block-sparse, and out-of-core tensors that provide peak-performance implementations of the above interfaces, (iii) a multi-linear algebra package for general high dimensional computations, based on the matrix product state approach, and (iv) tensor virtual machine technology that abstracts algorithm development from hardware, and which provides a framework for optimizing compiler transformations to adapt algorithms to the memory access, communication networks, and processor characteristics of modern computer architectures."
"1642441","SI2-SSE: BONSAI: An Open Software Infrastructure for Parallel Autotuning of Computational Kernels","OAC","Software Institutes","11/01/2016","09/12/2016","Jakub Kurzak","TN","University of Tennessee Knoxville","Standard Grant","Rob Beverly","10/31/2019","$499,977.00","Piotr Luszczek","kurzak@icl.utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","8004","7433, 8004, 8005","$0.00","Most supercomputers today accelerate the computations by using processors with many cores to solve important problems in science and engineering. Although this reduces the cost of the hardware system, it greatly increases the complexity of writing and optimizing (""tuning"") software. This project extends a previously funded NSF project:  Benchtesting Environment for Automated Software Tuning (BEAST) program to create a software toolkit that allows for semi-automatic optimization of software, thereby reducing the programming overhead. This project, BEAST OpeN Software Autotuning Infrastructure (BONSAI) will greatly increase the efficiency of scientists and engineers to develop fast and efficient programs to solve their problems. BONSAI has tremendous support from various computer processor manufacturing companies and academic institutions. The BONSAI system will be available as open-source software for academic and commercial use and many students will be trained in using the software.<br/><br/>The emergence and growing dominance of hybrid systems that incorporate accelerator processors, such as GPUs and coprocessors, have made it far more difficult to optimize the performance of the different computational kernels that do the majority of the work in most research applications. The BONSAI project aims to create a transformative solution to this problem by developing a software infrastructure that uses parallel hybrid machines to enable large autotuning sweeps on computational kernels for GPU accelerators and many-core coprocessors. The system will go beyond just measuring runtimes, allowing for collection and analysis of non-trivial amount of data from hardware performance counters and power meters. The system will have a modular architecture and rely on standard data formats and interfaces to easily connect with mainstream tools for data analytics and visualization. The BONSAI project will leverage the experiences of the BEAST project, which established a successful autotuning methodology and validated an autotuning workflow. BONSAI will equip the community with a software environment for applying parallel resources to the tuning and performance analysis of computational kernels. Specifically, the work will be organized around the following objectives: (1) Harden and extend the programming language called BeastLang, which was created during prior research as a way of defining the search space that the autotuning infrastructure generates and explores. BeastLang enables users to create parameterized kernel specifications that encode the interplay between the kernels themselves, the compilation tools, and the target hardware. It will be integrated with the other components of BONSAI, have its Python syntax enhanced and extended, its compiler improved, and be supplemented with a runtime that supports it with multi-way parallelism for the autotuning process. (2) Develop and test a benchtesting engine for making large scale parallel tuning sweeps, using large numbers of GPU accelerators or many-core coprocessors. This engine will support both parallel compilation and parallel tests of the resulting kernels, using many distributed memory nodes and multithreading within each node, with dynamic load balancing. It will produce an extensive collection of performance information from hardware counters, and possibly energy meters, as well as collection of information about the saturation of the compiled code with different classes of instructions. (3) Develop and test a software infrastructure for collecting, preprocessing, and analyzing BONSAI performance data. The system will a) simplify the task of instrumenting the kernel and provide a simple interface for selecting the metrics to be collected with sensible defaults; b) simplify the process of collecting hardware counters and performance data from various open source and vendor specific libraries; and c) provide tools that allow the user to quickly and efficiently transform output data to a format that can be easily read and analyzed using mainstream tools such as R and Python. (4) Document and illustrate the process of using BONSAI to tune various different types of kernels. These model case studies will include discussions of how BeastLang was applied to create the parameterized kernel stencil, how the parallel benchtesting engine is invoked to generate and explore the search space, and how the data collected from the operation of the engine can be analyzed and visualized to gain insights that can correct or refine the process for another iteration. The BONSAI project has the potential to fundamentally transform autotuning research by: 1) Making autotuning accessible to a broad audience of developers from a broad range of computing disciplines, as opposed to a few selected individuals with the wizardry to set up a successful experiment within the confines of serial execution. 2) Changing the general perception of autotuning as not just the means of producing fast code, but as a general technique for performance analysis and reasoning about the complex software and hardware interactions, and positioning the technique as one of primary tools for hardware-software co-design. 3) Boosting interest in exploring neglected avenues of computing, such as exploration of unorthodox data layouts, and challenge the status quo of legacy software interfaces. BONSAI has the potential to bring autotuning to the forefront of software development and to help position autotuning as a pillar of software engineering."
"1743184","SI2-S2I2 Conceptualization: Geospatial Software Institute","OAC","Methodology, Measuremt & Stats, Software Institutes","10/01/2017","08/12/2021","Shaowen Wang","IL","University of Illinois at Urbana-Champaign","Standard Grant","Rob Beverly","09/30/2021","$599,997.00","Donna Cox, Daniel Katz, Paul Morin, Margaret Palmer","shaowen@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","1333, 8004","026Z, 7433, 8004, 8211, 9102","$0.00","Throughout the globe, changes and concerns such as emergency management, population growth, and rapid urbanization are creating scientific and societal challenges that are both localized and interdependent across space and time. Data related to location (i.e. geospatial data) collected and used for academic, governmental, and industrial purposes urgently needs innovative geospatial software to allow such data to be transformed into valuable insights and significant scientific knowledge. Fields such as agriculture, ecology, emergency management, environmental engineering and sciences, hydrology, geography and spatial sciences, geosciences, national security, public health, and social sciences all require geospatial data and software to make important advances.  This project seeks to conceptualize a Geospatial Software Institute (GSI) as a long-term hub of excellence in software infrastructure that will bring together the diverse research communities that use advanced geospatial analysis software to do their research. This project will develop a strategic plan for such a Geospatial Software Institute, that is, develop the vision and roadmap of the Institute by mobilizing the relevant communities and stakeholders. The central goal of this project is to understand how to structure and implement a GSI as a long-term hub of excellence in advanced geospatial software infrastructure, that can bring together and serve the advanced and diverse geospatial software and science research and education communities. The project will bring together the perspectives of diverse academic, governmental, and industrial institutions as well as international partners.to better understand the requirements for a GSI, identify potential software contributors, and then develop the mission, vision and plan for the GSI.  A community report will additionally be produced, that will assess critical science and engineering needs as well as promising solutions for high-performance geospatial software. Finally, this project will make useful contributions to the strategic objectives of the National Strategic Computing Initiative (NSCI). <br/><br/>The project will conduct a number of activities, consisting of three workshops as well as a number of engagement and outreach activities. These activities will consist of an initial, in depth survey of science drivers and user needs to be completed within the first month of the project, with the goal of identifying a preliminary list of community needs and requirements. This survey will help focus the three workshops; the project team will use the results of the survey to develop the key foci, agenda, and desired outcomes of each workshop. This information will also be used to advertise the workshops and solicit applications for participation. The project will reach out to underrepresented groups, target a small set of minority institutions (e.g. Harris-Stowe State College in St. Louis) and communities (e.g. social sciences) by leveraging existing activities by at the National Center for Supercomputing Applications (NCSA) and the National Socio-Environmental Synthesis Center (SESYNC), engage national museum programs, and collaborate with professional organizations (e.g., IEEE TCPP and UCGIS Body of Knowledge Committee) to understand the needs of education and workforce development, as well as develop effective mechanisms for leveraging these organizations to ensure that the findings and recommendations of the project related to curriculum and education materials are disseminated to broad communities. Finally, this project is well-aligned with the strategic objectives of the National Strategic Computing Initiative (NSCI) and will contribute to increasing coherence between technology for modeling and simulation and data analytics, increasing the capacity and capability of an enduring national HPC ecosystem, and to to developing U.S. government, industry, and academic collaborations.<br/> <br/>This award, supported by the Office of Advanced Cyberinfrastructure and the Division of Social and Economic Sciences, reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740111","SI2-SSE: Collaborative Research: Software Framework for Strongly Correlated Materials: from DFT to DMFT","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2017","11/06/2019","Aldo Romero","WV","West Virginia University Research Corporation","Standard Grant","Seung-Jong Park","05/31/2021","$249,947.00","","Aldo.Romero@mail.wvu.edu","886 CHESTNUT RIDGE ROAD","MORGANTOWN","WV","265052742","3042933998","CSE","1712, 8004","026Z, 054Z, 7433, 8004, 8005, 9102, 9150, 9216","$0.00","The main objective of this project is to develop advanced computational (ab-initio) tools that bridge the gap between the existing complex theories that describe the behavior of strongly-correlated electron materials, and the scientists working in other diverse fields who want to investigate the physical properties of the strongly correlated materials using modernstate-of-the-art computational methodologies. These strongly-correlated materials show a large set of interesting properties that can impact different fields as in opto-catalysis, magneto-optics, magneto-transport, high temperature superconductivity and magneto-electricity. The intriguing properties of such strongly-correlated materials includes unconventional superconductivity, complex charge spin and orbital ordering, metal-to-insulator transitions, and excellent thermoelectricity that have promising applications in modern technology. The existence of strong electron-electron interactions limits the use of existing Density Functional Theory (DFT) to understand the electronic structure of the strongly-correlated materials. However, recent developments of a new theory, named Dynamical Mean Field Theory (DMFT), has enabled researchers to correctly describe the electronic structure of the strongly correlated materials. In this project, the PIs will develop advanced Python-based computational research tools that will enable the researchers from diverse fields to investigate the properties of the strongly-correlated materials using DMFT. The specific applications include -- correct prediction of the electronic structure, vibrational properties and elastic properties of the strongly-correlated materials. The developed software tools will be freely available and open source and a user-manual will be made available for training purposes.<br/><br/><br/>The main goal of this project is to provide end users of various electronic structure codes with a flexible Python-based interface that does not rely on the extensive user experience or specific parameters to perform calculations for strongly-correlated materials and to develop new software to calculate electronic, vibrational, and elastic properties of strongly-correlated materials by using Dynamical Mean Field Theory (DMFT) methods starting from a Density Functional Theory (DFT) calculation. The developed software tools will be powerful enough to allow scientists in different fields to calculate the diverse electronic properties of a wide range of strongly-correlated materials with the state-of-the-art computational methodologies. Furthermore, these software packages will allow the correct electronic structure calculations in a minimal set of parameters, by offering to the end user the possibility of using three different methodologies to describe basic physics of strongly-correlated materials.All the developed computer software will be designed to enable the non-expert materials scientists and engineers to investigate the novel properties of the strongly-correlated materials. The scientific aim of this project also concerns the evolution of electronic correlations for several complex oxinitrides and Heusler alloys, in particular the dependence of several physical observables with respect to external fields such as pressure and strain. Targeted physical properties include electronic, vibrational, and elastic. The technical goal consists of the development of an open-source software that will address the scientific issues raised by the research on calculating properties of the strongly-correlated materials.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences."
"1339834","Collaborative Research: SI2-SSI: The Community-Driven BiG CZ Software System for Integration and Analysis of Bio- and Geoscience Data in the Critical Zone","OAC","ECOSYSTEM STUDIES, EAR-Earth Sciences Research, Geobiology & Low-Temp Geochem, CZO-Critical Zone Obsrvatories, Software Institutes, EarthCube","12/01/2013","09/20/2013","Anthony Aufdenkampe","PA","Stroud Water Research Center","Standard Grant","Rajiv Ramnath","11/30/2017","$1,366,089.00","Kerstin Lehnert, Robert Cheetham, Jeffery Horsburgh, Emilio Mayorga","aaufdenkampe@limno.com","970 SPENCER RD","AVONDALE","PA","193119514","6102682153","CSE","1181, 6898, 7295, 7693, 8004, 8074","7433, 8009","$0.00","The Critical Zone (CZ) science community takes as its charge the effort to integrate theory, models and data from the multitude of disciplines collectively studying processes on the Earth's surface. The Critical Zone is Earth's permeable near-surface layer - from the atmosphere at the vegetation's canopy to the lower boundary of actively circulating groundwaters. The Critical Zone was a term coined by the National Research Council's Basic Research Opportunities in the Earth Sciences (BROES) Report (2001) to highlight the imperative for a new approach to thoroughly multi-disciplinary research on the zone of the Earth?s surface that is critical to sustaining terrestrial life on our planet. In January 2013, 103 members of the CZ community met for the CZ-EarthCube Domain Workshop (NSF Award #1252238) to prioritize the CZ community's key science drivers, key computational and information technology (""cyber"") challenges and key cyber needs. They identified that the central scientific challenge of the critical zone science community is to develop a ""grand unifying theory"" of the critical zone through a theory-model-data fusion approach.  Work participants unanimously described that the key missing need of this approach was a future cyberinfrastructure for seamless 4D visual exploration of the integrated knowledge (data, model outputs and interpolations) from all the bio and geoscience disciplines relevant to critical zone structure and function, similar to today?s ability to easily explore historical satellite imagery and photographs of the earth's surface using Google Earth.  This project takes the first ""BiG"" steps toward answering that need.<br/><br/>The overall goal of this project is to co-develop with the CZ science and broader community, including natural resource managers and stakeholders, a web-based integration and visualization environment for joint analysis of cross-scale bio and geoscience processes in the critical zone (BiG CZ), spanning experimental and observational designs. Our Project Objectives are to: (1) Engage the CZ and broader community to co-develop and deploy the BiG CZ software stack; (2) Develop the BiG CZ Portal web application for intuitive, high-performance map-based discovery, visualization, access and publication of data by scientists, resource managers, educators and the general public; (3) Develop the BiG CZ Toolbox to enable cyber-savvy CZ scientists to access BiG CZ Application Programming Interfaces (APIs); and (4) Develop the BiG CZ Central software stack to bridge data systems developed for multiple critical zone domains into a single metadata catalog. The entire BiG CZ Software system will be developed on public repositories as a modular suite of fully open source software projects.  It will be built around a new Observations Data Model Version 2.0 (ODM2) that is being developed by members of the BiG CZ project team, with community input, under separate funding (NSF Award #1224638)."
"1533581","SI2-SSE: Fast Dynamic Load Balancing Tools for Extreme Scale Systems","OAC","Software Institutes","10/01/2015","07/08/2015","Mark Shephard","NY","Rensselaer Polytechnic Institute","Standard Grant","Seung-Jong Park","09/30/2020","$500,000.00","Cameron Smith","shephard@rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","8004","7433, 8005","$0.00","Massively parallel computing combined with scalable simulation workflows that can reliably model systems of interest are central to the continued quest of scientists, engineers, and other practitioners to address advances in scientific discovery, engineering design, and medical treatment. However, to meet their potential, these methods must be able to operate efficiently and scale on massively parallel computers executing millions of processes. Reaching the goal of millions of parallel processes requires new methods in which the computational workload is extremely well balanced and interprocessor communications overheads are minimized. Attaining such parallel performance is greatly complicated in realistic simulation workflows where the models and their discrete computer representation must evolve to ensure simulation reliability, or to account for changing input streams. To address the need to obtain workload balance with controlled communications, various algorithms and associated software, referred to as load balancing procedures, have been, and continue to be, developed. To be effective in the execution of simulation workflows in which the workload evolves, the load balancing procedures must be applied dynamically at multiple points in the simulation. Current load balancing techniques demonstrate two deficiencies when applied as dynamic load balancing procedures at very large numbers of compute cores (e.g., greater than 100,000 cores): They become a major fraction of the total parallel computation (in some cases never finishing within an allocation) and they do not maintain good load balance for simulation steps that must balance based on multiple criteria.  Building on initial efforts to improve dynamic load balancing methods for adaptive unstructured mesh applications, the goal of the proposed research is to develop fast multicriteria dynamic load balancing methods that are capable of quickly producing well balanced computations, with well controlled communications, for a wide variety of applications. <br/><br/>An important characteristic of the dynamic load balancing procedures to be developed is generalizing the graph to account for multiple types of computational entities and interactions. The initial ideas for supporting multiple entity types came from consideration balancing finite element calculations that must consider multiple orders of mesh entities. These concepts will be refined and generalized to support multiple applications areas. An additional development will be fast hybrid dynamic load balancing methods that are combinations of ""geometric"", standard graph, and multicriteria graph methods in which the individual methods can be executed globally of at a more local level (such as at the node level). The dynamic load balancing method to be developed will be demonstrated on three applications in which the workload, and its distribution, is changing as the simulation proceeds. The applications will be adaptive mesh simulations, adaptive multiscale modeling, and massive scale free graphs. These applications will be carried out on available massively parallel computers where examples on >1 million cores will be demonstrated. A goal of the dynamic load balancing methods to be developed will be to attain scalability, and do so with controlled data movement such that the wall clock time and energy used is substantially less than that required for an equivalent accuracy non-adaptive calculation.<br/><br/>The software produced by this project will be made available as open source components. These developments coupled with efforts to support users in applying them in the development of new simulation tools will impact many research communities. Based on past and present efforts, the PIs fully expect that technologies developed in this project will also be integrated into future industrial software systems."
"1450277","SI2-SSI: Collaborative Research: Bringing End-to-End Provenance to Scientists","OAC","Special Projects - CCF, Software Institutes","06/01/2015","05/16/2019","Margo Seltzer","MA","Harvard University","Standard Grant","Bogdan Mihaila","05/31/2020","$1,422,728.00","Aaron Ellison, Emery Boose","margo@eecs.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","CSE","2878, 8004","7433, 8009","$0.00","Reproducability is the cornerstone of scientific progress. Historically, scientists make their work reproducible by including a formulaic description of the experimental methodology used in an experiment. In an age of computational science, such descriptions no longer adequately describe scientific methodology. Instead, scientific reproducibility relies on a precise and actionable description of the data and programs used to conduct the research. Provenance is the name given to the description of how a digital artifact came to be in its present state. Provenance includes a precise specification of an experiment's input data and the programs or procedures applied to that data. Most computational platforms do not record such data provenance, making it difficult to ensure reproducability. This project addresses this problem through the development of tools that transparently and automatically capture data provenance as part of a scientist's normal computational workflow.<br/><br/>An interdisciplinary team of computer scientists and ecologists have come together to develop tools to facilitate the capture, management, and query of data provenance -- the history of how a digital artifact came to be in its present state.  Such data provenance improves the transparency, reliability, and reproducibility of scientific results. Most existing provenance systems require users to learn specialized tools and jargon and are unable to integrate provenance from different sources; these are serious obstacles to adoption by domain scientists. This project includes the design, development, deployment, and evaluation of an end-to-end system (eeProv) that encompasses the range of activity from original data analysis by domain scientists to management and analysis of the resulting provenance in a common framework with common tools. This project leverages and integrates development efforts on (1) an emerging system for generating provenance from a computing environment that scientists actually use (the R statistical language) with (2) an emerging system that utilizes a library of language and database adapters to store and manage provenance from virtually any source. Accomplishing the goals of this proposal requires fundamental research in resolving the semantic gap between provenance collected in different environments, capturing detailed provenance at the level of a programming language, defining precisely aspects of provenance required for different use cases, and making provenance accessible to scientists."
"1148085","Collaborative Research SI2 SSE: Pipeline Framework for Ensemble Runs on Clouds","OAC","Software Institutes","05/01/2012","05/07/2012","Craig Mattocks","FL","University of Miami","Standard Grant","Daniel Katz","04/30/2014","$199,725.00","Brian Soden","cmattock@rsmas.miami.edu","1251 MEMORIAL DR","CORAL GABLES","FL","33146","3054214089","CSE","8004","8004, 8005","$0.00","Cloud computing is an attractive computational resource for e-Science because of the ease with which cores can be accessed on demand, and because the virtual machine implementation that underlies cloud computing reduces the cost of porting a numeric or analysis code to a new platform. It is difficult to use cloud computing resources for large-scale, high throughput ensemble jobs however. Additionally, the computationally oriented researcher is increasingly encouraged to make data sets available to the broader community. For the latter to be achieved, using capture tools during experimentation to harvest metadata and provenance reduces the manual burden of marking up results. Better automatic capture of metadata and provenance is the only means by which sharing of scientific data can scale to meet the burgeoning explosion of data.<br/><br/>This project develops a pipeline framework for running ensemble simulations on the cloud; the framework has two key components: ensemble deployment and metadata harvest. Regarding the former, on commercial cloud platforms typically a much smaller number of jobs than desired can be started at any one time. An ensemble run will need to be pipelined to a cloud resource, that is, executed in well-controlled batches over a period of time. We will use platform features of Azure, and employ machine learning techniques to continuously refine the pipeline submission strategy and workflow strategies for ensemble parameter specification, pipelined deployment, and metadata capture. Regarding the latter key component, we expect to reduce the burden of sharing scientific datasets resulting from the use of cloud resources through automatic metadata and provenance capture and representation that aligns the metadata with emerging best practices in data sharing and discovery. Ensemble simulations result in complex data sets, whose reuse could be increased by expressive, granule and collection level metadata, including the lineage of the resulting products, to contribute towards trust.<br/><br/>In this project we focus on a compelling and timely application from climate research: One of the more immediate and dangerous impacts of climate change could be a change in the strength of storms that form over the oceans. In addition, as sea level rises due to global warming and melting of the polar ice caps, coastal communities will become increasingly vulnerable to storm surge. There have already been indications that even modest changes in ocean surface temperature can have a disproportionate effect on hurricane strength and the damage inflicted by these storms. In an effort to understand these impacts, modelers turn to predictions generated by hydrodynamic coastal ocean models such as the Sea, Lake and Overland Surges from Hurricanes (SLOSH) model. The proposed research advances the knowledge and understanding of probabilistic storm surge products by enhancements to the SLOSH model itself and through mechanisms that take advantage of commercial cloud resources. This knowledge is expected to have application in research, the classroom, and in operational settings.<br/><br/>The broader significance of the project is several-fold. Cloud computing is an important economic driver but it remains difficult for use in computationally driven scientific research. This project lowers the barriers to conducting e-Science research that utilizes cloud resources, specifically Azure. It will contribute tools to help researchers share, preserve, and publicize the scientific data sets that result from their research. Because we focus on and improve an application that predicts storm surge in response to sea level changes and severe storms, our work contributes to societal responses and adaptations to climate change, including planning and building the sustainable, hazard-resilient coastal communities of the future."
"1642397","SI2 - SSE: A Next-Generation Decision Diagram Library","OAC","Software Institutes","01/01/2017","09/08/2016","Andrew Miner","IA","Iowa State University","Standard Grant","Seung-Jong Park","06/30/2021","$498,672.00","Gianfranco Ciardo","asminer@iastate.edu","515 MORRILL RD, 1350 BEARDSHEAR","AMES","IA","500112105","5152945225","CSE","8004","026Z, 7433, 7942, 8004, 8005","$0.00","There are a variety of scientific problems whose solution is made difficult because of the extremely large number of possibilities that must be considered and evaluated. Often, the difficulty is caused by a large number of combinations of interacting components, even though the individual components are relatively simple. Relevant practical problems are measuring the reliability of a communication network where links may fail (made difficult by the number of different communication paths in the network), or determining that an automobile's brakes will always work (made difficult by the number of combinations of the interacting software and hardware components in an automobile), or determining that the failure of one power generator will not cause a cascading failure that affects a large portion of the nation's power grid. This class of problem is conceptually similar to finding an optimal solution for Rubik's cube (which is made difficult by the large number of different possible configurations) or, in chess, determining if there is a sequence of moves in chess such that white can always force a win (made difficult by the huge number of different possible chess games). The goal of this project is to develop a software library called Meddly that various applications can use to build and represent solutions to these types of combinatoric problems. The underlying technology of Meddly is decision diagrams, a mechanism for organizing data in such a way that repeated patterns or subpatterns are automatically discovered and exploited during computation. The project will add to the capabilities of decision diagram technology, help to advance the understanding of this technology as well as apply it to more types of problems. Several researchers from around the world have expressed interest in Meddly, and as part of this project, developers will assist those researchers to integrate Meddly into existing tools, which will then be applied to real problems. The project also has educational goals, through the engagement of students in the project, and the incorporation of this work in existing courses.<br/><br/>Many computer-based scientific or engineering applications need to store, analyze, and manipulate large data. Often, this data has enough structure that specialized data structures and algorithms can have dramatically smaller memory and time requirements than explicit approaches. An important such case is symbolic verification of hardware and software, where, traditionally, binary decision diagrams (BDDs) have been successfully employed to study systems with enormous state spaces. Several software libraries for BDDs have arisen to support these operations, and BDDs have been applied to diverse applications as a means to exploit structure that is often hidden. However, in the past decade, decision diagram theory has continued to advance, by generalizing BDDs to variants with multi-way decisions (MDDs), multi-way or numerical outcomes, or edge values to encode real-valued data, and by proposing a variety of reduction rules to change (and often shrink) the decision diagram shape, as well as many important algorithmic improvements. Unfortunately, decision diagram libraries have not kept up with these theoretical advances. The proposed work seeks to fill this gap, by merging and expanding two existing prototype libraries developed by the two invesigators, Meddly and TEDDY, into a powerful, next-generation decision diagram library that supports a more general theory of decision diagrams. The new library will encompass (1) non-binary variables, including a-priori unbounded discrete domains and even infinite domains under certain restrictions, (2) non-boolean function values, attached either to terminal nodes or to the edges of the decision diagram, and (3) a more general definition of canonicitythat includes a wide spectrum of reduction rules. Several proposed activities will help smooth the learning curve for users adopting this library, from proven methods such as user manuals, tutorials, examples, wikis, and user groups, to novel ones such as the development of visualization techniques to aid the debugging and understanding of decision diagrams. The proposed software will push decision diagram library support far beyond the capabilities of today's typical BDD libraries, allowing exciting new applications to emerge in diverse fields well beyond classic ones such as symbolic model checking. Additionally, the proposed research activities will improve our understanding of decision diagram technology, providing deep new insights into the nature of structured functions and their representations.  This has the potential to advance the state of the art both in fields that currently utilize decision diagrams, as improved library support can lead to the ability to tackle problem instances of unprecedented size, and in fields where the availability of a library implementing the proposed decision diagram variants will allow researchers to tackle classic problems with novel approaches based on decision diagrams. A next-generation decision diagram library will positively impact disciplines ranging from engineering to computer science theory to biology, via improved software applications that manage large and structured data. Letters of support attest to the many research groups worldwide eager to include more general and powerful decision diagram capabilities in their tools. The anticipated educational impact includes development of publicly available online tutorials; research, implementation, and experimentation opportunities for both undergraduate and graduate students; and integration of the developed techniques and software into existing courses via lectures, assignments, and projects. The underlying theory and developed software resulting from the proposed activities will reinforce concepts that students will retain and apply during their careers."
"1450169","Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","08/01/2015","06/24/2015","Thomas Crawford","VA","Virginia Polytechnic Institute and State University","Standard Grant","Stefan Robila","07/31/2019","$600,000.00","","crawdad@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","CSE","1253, 8004","7433, 8009","$0.00","Computational chemistry has become ubiquitous throughout the fields of science and engineering. Among the many uses of computational chemistry codes are to aid in the design of new materials with specific properties, to understand protein folding dynamics related to the human body, to understand the mechanisms of enzyme catalysis to produce biofuels, and to identify and characterize gaseous species in the atmosphere. Because computational chemistry simulations can require large amounts of computer, memory and disk space, one focus of this research is to reduce the demands on computer resources by exploiting methods that can efficiently fragment large molecules into smaller pieces and at the same time take advantage of computers that have many thousands of computer cores. The principal investigators are developers of several computational chemistry programs, each of which has unique functionalities that have taken multiple person-years to develop and implement. In order to minimize further development efforts and to maximize the utility of the unique features, a second key focus of this research will be to develop the ability of each program to access the unique features and data of the other programs. A major bottleneck in the effort to construct high performance computers with more and more compute cores is the power that is required to drive such systems. One way that the research team will address the power issue is to explore the utility of low power architectures, such as graphical processing units for computational chemistry calculations. All of the newly developed codes will be made available to the user community by web download at no cost.<br/><br/>This project presents an integrated computational science approach to very high quality electronic structure and dynamics calculations that will (a) be broadly accessible to both the development and applications communities, (b) have the capability to address problems of great interest, such as the properties of liquids and solvent effects, and photochemical/photobiological dynamics with high accuracy, (c) provide interoperability and sustainability into the foreseeable future, and (d) solve bottlenecks including power consumption using accelerators. A primary goal is to provide to the broad community new, accurate approaches that may be easily used, with a clear path forward to further code improvement and development. As part of the proposed effort, in addition to the usual outlets of journal articles and public presentations, the investigators will organize workshops at prominent national meetings, so that the expertise and software developed will be available to as broad a group of users as possible.  All of the developed codes will be available on the web for easy downloads. The proposed research will develop new paradigms for interoperability, with a focus on the highly popular program suites GAMESS, NWChem, PSI4  and the AIMS dynamics code. Also included will be a cloud-based client-server model, a common quantum chemistry driver and novel data management approaches. The integration of the codes will be accomplished by making use of the combined expertise of the PIs in developing interoperable methods and data interfaces in computational science. In addition, several new methods will be developed, including novel explicit (R12) correlation methods that will be integrated with the most accurate levels of theory: multi-reference and the most accurate and novel coupled cluster methods, as well as the derivation and implementation of analytic derivatives. Applicability to large molecular systems will be made feasible by drawing upon novel fragmentation methods that scale nearly perfectly to the petascale."
"1550456","Collaborative Research: SI2-SSI: Software Framework for Electronic Structure of Molecules and Solids","OAC","Software Institutes","08/01/2016","07/21/2016","Edward Valeev","VA","Virginia Polytechnic Institute and State University","Standard Grant","Varun Chandola","07/31/2021","$600,000.00","","evaleev@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","CSE","8004","7433, 8009, 9216","$0.00","Many traditionally experimental disciplines such as chemistry and materials science are rapidly changing due to our increasing ability to predict properties of molecules and materials purely by simulation. This is particularly true when molecules meet solid surfaces - due to the particular challenges of experiments in such a setting. Yet the molecule-on-surface frontier encompasses a vast class of problems of tremendous practical importance:  industrial applications facilitated by surface processes are estimated to produce globally more than than 15 trillion USD worth of goods and products.  This research will improve our ability to simulate the physics and chemistry of molecules on surfaces by extending the advanced simulation methodologies that were originally developed by for modeling electrons in molecules. This project will not only advance our fundamental understanding of the surface science but also open a road to technological applications relevant to producing and storing clean energy and in designing improved catalysts. The research may result in a new computer software framework for simulating electrons in molecules and materials. This software will be a unique contribution to the U.S. cyberinfrastructure and spur further innovation by other researchers in the US and worldwide, who will be able to access its source code for free. The software framework will also serve as an education platform for training computational chemists and materials scientists.<br/><br/>A frontier simulation challenge lies at the intersection of the two domains of chemistry and materials science - namely to determine, with predictive accuracy, the properties and chemistry of molecules on solid surfaces. The molecule-on-surface frontier encompasses a vast class of problems of tremendous practical importance: heterogeneous catalysis, photovoltaics, and emerging electronic materials. Yet, from a simulation perspective, it is not currently possible to efficiently combine the recent advances in highly accurate many-body molecular and periodic condensed phase methodologies in these problems, due to a significant gap between how the electronic structure theories of molecules and materials are formulated, as reflected in distinct algorithms and disjoint codebases. The goal of this project is to reduce and/or completely eliminate the gap between molecular and solid-state electronic structure methodologies, in theory, algorithms, and in usable community software implementations. This will be achieved by building an ambitious Electronic structure for Molecules and Solids (EMOS) software framework that will permit accurate computation of the first-principles electronic structure of both molecules and solids on an equivalent footing - and with the high efficiency necessary for high-throughput screening or ab initio molecular dynamics. These efforts build on the leading track-record of the principal investigators in developing open-source quantum chemistry software as well as automated computer implementation and high-performance parallel libraries. The project will allow the advances from molecular electronic structure - embedding, reduced-scaling many-body methodology, accurate excited-state electronic structure, and others - to be applied routinely to molecules, materials, and combinations of the two as relevant to surface chemistry. This has great potential to advance the state-of-the-art in treatment of electronic structure and open new lines of theoretical inquiry. The resulting open-source production-quality toolkit will be validated against experimental data for a host of surface phenomena, from exciton dynamics to surface spectroscopy and catalysis. An open-source US-based advanced materials code is a long-standing omission in U.S. cyberinfrastructure. As a high-performance framework for simulation of electronic structure of molecules, solids, and their interfaces with unprecedented accuracy, EMOS will be a significant contribution to this effort. Further, the modular component based structure will be able to be integrated with other major electronic structure packages through the reuse of the modules. This project will provide invaluable training opportunities to the students and postdocs who will develop the software framework under the direct supervision of principal investigators. In addition, each project site will contribute to the development of a stakeholder network for EMOS by hosting, each summer, visiting students and faculty representing the broader theoretical community, to train them on the use of EMOS in research and education. The project team will also use EMOS in teaching classes and summer schools, building on already established efforts in this area; these efforts will also be extended to an online setting."
"1047961","SI2-SSE: Collaborative: Extensible Languages for Sustainable Development of High Performance Software in Materials Science","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, CSR-Computer Systems Research, Software Institutes","09/15/2010","09/07/2010","Eric Van Wyk","MN","University of Minnesota-Twin Cities","Standard Grant","Daryl Hess","08/31/2015","$300,000.00","Yousef Saad","evw@umn.edu","200 OAK ST SE # 224","MINNEAPOLIS","MN","554552009","6126245599","CSE","1253, 1712, 7354, 8004","1253, 1712, 1765, 7237, 7354, 7569","$0.00","This award is part of the Software Infrastructure for Sustained Innovation. The Office of Cyberinfrastructure, the Division of Computer and Network Systems, and the Division of Materials Research contribute funds to this award. <br/><br/>Developing large computational codes such as those used to perform simulations in quantum  mechanics to calculate properties of materials, or to predict the  aerodynamics forces around airplanes, still typically require several human-years.  However the pace of research and industrial product development demands much more rapid software tool development to make progress and to remain competitive. This award contributes to developing the capability to rapidly create high performance large scale codes. <br/><br/>The PIs will augment a computer programming language with a very high level language that is interactive in the sense that the developer will enter language commands and get instantaneous interpreted answers, instead of  processing the whole code.  This approach of creating an interactive extensible language framework will provide a way to help speed development of large scale computer software.  Efforts will be specifically targeted at software for materials science applications. This will enable progress in large scale computational research that aims to predict properties of materials starting from a knowledge of the constituent atoms and the way they are arranged in the material. <br/><br/>This award contributes to the education of knowledgeable specialists capable of developing large and complex computational codes.  The PIs will design new graduate level courses outside of the current curriculum to increase the number of students who receive training in effective development of software for materials research and scientific computing in general. <br/><br/>This award also supports the research team's efforts to broaden participation of underrepresented groups through the existing Alice in Wonderland Program, which aims to recruit members of underrepresented groups at the high school level, and to attract female high school students to science and engineering by  involving them in research over the summer before they make decisions  about colleges. They will also revive the Summer Undergraduate Interns program to recruit undergraduate students interested in high performance computing for summer internships."
"1642410","SI2-SSE: Collaborative Research: High Performance Low Rank Approximation for Scalable Data Analytics","OAC","CYBERINFRASTRUCTURE, Software & Hardware Foundation, Software Institutes","11/01/2016","09/08/2019","Haesun Park","GA","Georgia Tech Research Corporation","Standard Grant","Amy Walton","10/31/2020","$387,281.00","Barry Drake","hpark@cc.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","CSE","7231, 7798, 8004","026Z, 7433, 7942, 8004, 8005","$0.00","Big Data analytics is at the core of discovery covering vast areas such as medical informatics, business analytics, national security, and materials sciences. This project aims to model some of the key data analytics problems and design, verify, and deploy scalable methods for knowledge extraction.  The algorithms developed will be able to handle data sets of extreme sizes and will be deployable on advanced computer hardware. The goal is to realize orders-of-magnitude improvements over existing data analytics technologies, developing algorithms that are robust to incompleteness, noise, ambiguity, and high dimension in the data.  Particular focus will be parallel and distributed algorithms that can efficiently solve large problems and produce accurate solutions.  The proposed research and software development will allow domain experts to tackle Big Data sets requiring large parallel systems.  The improved performance will enable fast and scalable data analysis across applications, from social network analysis to study citizens' attitudes toward sustainability-related issues to computational marketing techniques that refine customers' shopping experiences.  The proposed work will help bridge the gap between computational science and data analytics ecosystems, two fields that stand to make great advancements from cross-fertilization.  The education and outreach plan includes graduate course creation, engagement of under-represented groups via both undergraduate and graduate research experiences, and community-building efforts by workshop and mini-symposium organization.<br/><br/>With the advent of internet-scale data, the data mining and machine learning community has adopted Nonnegative Matrix Factorization (NMF) for performing numerous tasks such as topic modeling, background separation from video data, hyper-spectral imaging, web-scale clustering, and community detection.  The goals of this proposal are to develop efficient parallel algorithms for computing nonnegative matrix and tensor factorizations (NMF and NTF) and their variants using a unified framework, and to produce a software package called Parallel Low-rank Approximation with Nonnegative Constraints (PLANCK) that delivers the high performance, flexibility, and scalability necessary to tackle the ever-growing size of today's data sets. The algorithms will be generalized to NTF problems and extend the class of algorithms we can efficiently parallelize; our software framework will allow end-users to use and extend our techniques.  Rather than developing separate software for each problem domain and mathematical technique, flexibility will be achieved by characterizing nearly all of the current NMF and NTF algorithms in the context of a block coordinate descent framework. Using this framework the shared computational kernels can be separated, which usually extend run times, from the algorithm-specific computations. Finally, the usability and practicality of the proposed software will be maintained by being application driven, establishing collaborations with early end-users, and by incrementally generalizing the framework in terms of both algorithms and problems."
"1148310","SI2-SSI: Collaborative Research: A Glass Box Approach to Enabling Open, Deep Interactions in the HPC Toolchain","OAC","Information Technology Researc, Software Institutes","06/01/2012","12/16/2015","Karsten Schwan","GA","Georgia Tech Research Corporation","Standard Grant","Rajiv Ramnath","05/31/2016","$926,666.00","Greg Eisenhauer, Matthew Wolf, Sudhakar Yalamanchili","schwan@cc.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","CSE","1640, 8004","7433, 8004, 8009","$0.00","Parallel computing has entered the mainstream with increasingly large multicore processors and powerful accelerator devices. These compute engines, coupled with tighter integration of faster interconnection fabrics, are drivers for the next-generation high end computing (HEC) machines. However, the computing potential of HEC machines is delivered only through productive parallel program development and efficient parallel execution. This project enables application developers to improve performance on future HEC machines for their scientific and engineering processes. This project challenges the current model for parallel application development via ""black box"" tools and services. Instead, the project offers an open, transparent software infrastructure -- a Glass Box system -- for creating and tuning large-scale, parallel applications.  `Opening up' the tools and services used to create and evaluate peta- and exa-scale codes involves developing interfaces and methods that make tool-internal information and available for new performance management services that improve developer productivity and code efficiency.<br/><br/>The project will explore the information that can be shared 'across the software stack'.  Methods will be developed for analyzing program information, performance data and tool knowledge. The resulting Glass Box system will allow developers to better assess the performance of their parallel codes.  Tool creators can use the performance data to create new analysis and optimization techniques. System developers can also better manage multicore and machine resources at runtime, using JIT compilation and binary code editing to exploit the evolving hardware.  Working with the `Keeneland' NSF Track II machine and our industry partners, the project will create new performance monitoring tools, compiler methods and system-level resource management techniques. The effort is driven by the large-scale codes running on today's petascale machines.  Its broader impact is derived from the interactions with technology developers and application scientists as well as from its base in three universities with diverse student populations."
"1450122","SI2-SSI: Collaborative Proposal: Performance Application Programming Interface for Extreme-Scale Environments (PAPI-EX)","OAC","Software Institutes","09/01/2015","06/29/2020","Vincent Weaver","ME","University of Maine","Standard Grant","Seung-Jong Park","08/31/2021","$282,828.00","","vincent.weaver@maine.edu","5717 CORBETT HALL RM 422","ORONO","ME","044695717","2075811484","CSE","8004","7433, 8004, 8009, 9150, 9251","$0.00","Modern High Performance Computing (HPC) systems continue to increase in size and complexity. Tools to measure application performance in these increasingly complex environments must also increase the richness of their measurements to provide insights into the increasingly intricate ways in which software and hardware interact. The PAPI performance-monitoring library has provided a clear, portable interface to the hardware performance counters available on all modern CPUs and some other components of interest (scattered across the chip and system). Widely deployed and widely used, PAPI has established itself as fundamental software infrastructure, not only for the scientific computing community, but for many industry users of HPC as well.  But the radical changes in processor and system design that have occurred over the past several years pose new challenges to PAPI and the HPC software infrastructure as a whole.  The PAPI-EX project integrates critical PAPI enhancements that flow from both governmental and industry research investments, focusing on processor and system design changes that are expected to be present in every extreme scale platform on the path to exascale computing.<br/><br/>The primary impact of PAPI-EX is a direct function of the importance of the PAPI library. PAPI has been in predominant use by tool developers, major national HPC centers, system vendors, and application developers for over 15 years. PAPI-EX builds on that foundation. As important research infrastructure, the PAPI-EX project allows PAPI to continue to play its essential role in the face of the revolutionary changes in the design and scale of new systems. In terms of enhancing discovery and education, the list of partners working with PAPI-EX includes NSF computing centers, major tool developers, major system vendors, and individual community leaders, and this diverse group will help facilitate training sessions, targeted workshops, and mini-symposia at national and international meetings. Finally, the active promotion of PAPI by many major system vendors means that PAPI, and therefore PAPI-EX, will continue to deliver major benefits for government and industry in many domains.<br/><br/>PAPI-EX addresses a hardware environment in which the cores of current and future multicore CPUs share various performance-critical resources (a.k.a., 'inter-core' resources), including power management, on-chip networks, the memory hierarchy, and memory controllers between cores. Failure to manage contention for these 'inter-core' resources has already become a major drag on overall application performance. Consequently, the lack of ability to reveal the actual behavior of these resources at a low level, has become very problematic for the users of the many performance tools (e.g., TAU, HPCToolkit, Open|SpeedShop, Vampir, Scalasca, CrayPat, Active Harmony, etc.). PAPI-EX enhances and extends PAPI to solve this critical problem and prepare it to play its well-established role in HPC performance optimization. Accordingly, PAPI-EX targets the following objectives: (1) Develop shared hardware counter support that includes system-wide and inter-core measurements; (2) Provide support for data-flow based runtime systems; (3) Create a sampling interface to record streams of performance data with relevant context; (4) Combine an easy-to-use tool for text-based application performance analysis with updates to PAPI?s high-level API to create a basic, ?out of the box? instrumentation API."
"1450409","Collaborative Research: SI2-SSI: Landlab: A Flexible, Open-Source Modeling Framework for Earth-Surface Dynamics","OAC","Geomorphology & Land-use Dynam, Software Institutes, EarthCube","08/01/2015","01/21/2021","Gregory Tucker","CO","University of Colorado at Boulder","Standard Grant","Seung-Jong Park","07/31/2021","$789,777.00","Daniel Hobley","gtucker@colorado.edu","3100 MARINE ST STE 481 572 UCB","BOULDER","CO","803090001","3034926221","CSE","7458, 8004, 8074","7433, 8009","$0.00","Earth scientists discover and predict the behavior of the natural world in part through the use of computer models. Models are used to study a wide range of phenomena, such as soil erosion, flooding, plant growth, landslide occurrence, and many other processes. Models can be used to develop scientific understanding by comparing model calculations with the real world. They can also be used to make predictions about how nature will behave under certain conditions. In the areas of geosciences that deal with the earth's surface, one bottleneck in the development and use of models is the effort required to build, test, and debug the necessary software. This project contributes to scientific research and discovery by creating open source software tools that scientists can use to more efficiently create, modify, or combine computer models that represent portions of earth's surface and the processes occurring thereon. The project combines software development with user training and web-based resources, so that the products will be openly accessible, well documented, and widely disseminated within the relevant scientific communities. The project also contributes to K-12, undergraduate, and graduate-level education in computational modeling and earth-surface processes.<br/><br/>This project catalyzes research in earth-surface dynamics by developing a software framework that enables rapid creation, refinement, and reuse of two-dimensional (2D) numerical models. The phrase earth-surface dynamics refers to a remarkably diverse group of science and engineering fields that deal with our planet's surface and near-surface environment: its processes, its management, and its responses to natural and human-made perturbations. Scientists who want to use an earth-surface model often build their own unique model from the ground up, re-coding the basic building blocks of their model rather than taking advantage of codes that have already been written. Whereas the end result may be novel software programs, many person-hours are lost rewriting existing code, and the resulting software is often idiosyncratic, poorly documented, and unable to interact with other software programs in the same scientific community and beyond, leading to lost opportunities for exploring an even wider array of scientific questions than those that can be addressed using a single model. The Landlab model framework seeks to eliminate these redundancies and lost opportunities, and simultaneously lower the bar for entry into numerical modeling, by creating a user- and developer-friendly software library that provides scientists with the fundamental building blocks needed for modeling earth-surface dynamics. The framework takes advantage of the fact that nearly all surface-dynamics models share a set of common software elements, despite the wide range of processes and scales that they encompass. Providing these elements in the context of a popular scientific programming environment, with strong user support and community engagement, contributes to accelerating progress in the diverse sciences of the earth's surface.<br/><br/>The Landlab modeling framework is designed so that grid creation, data storage, and sharing of data among process components is done for the user. The framework is generic enough so that the coupling of two process components, whether they are squarely within a geoscience subdiscipline, or they cross subdisciplines, is the same. This architecture makes it easy to explore a wide range of questions in the geosciences without the need for much coding. Further, the model code is primarily written in Python, a language that is relatively easy for casual programmers to learn. Because of these attributes, the Landlab modeling framework has the potential to add computational modeling to the toolbox of a wide array of geoscientists, and to clear a path for trans-disciplinary earth-surface modeling that explores societally relevant topics, such as land-cover changes in response to climate change, as well as modeling of topics that currently require the coupling of sophisticated but harder-to-use models, such as sediment source-to-sink dynamics. The project will generate several proof-of-concept studies that are interesting in their own right, and demonstrate the capabilities of the modeling framework. Community engagement is fostered by presenting clinics and demonstrations at professional venues aimed at hydrologists, sedimentologists, critical zone scientists, and the broader earth and environmental sciences community. The team also supports visits by individual scientists for on-site training beyond these clinics. Input/output tools allow compatibility with data from relevant NSF-supported research. The project supports four graduate students and three postdoctoral researchers, and also includes modeling workshops for fifth to seventh grade girls in a community that has a greater than 50% minority population."
"1440749","SI2-SSE: Improving Vectorization","OAC","Software Institutes","09/01/2014","07/01/2014","Ponnuswamy Sadayappan","OH","Ohio State University","Standard Grant","Rob Beverly","08/31/2018","$500,000.00","","saday@cs.utah.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","CSE","8004","7433, 8005","$0.00","The increasing width of vector instruction sets in processors and accelerators raises the importance of effective vectorization.  Although the topic of automatic vectorization by compilers has received significant attention over the last few decades, current vectorizing compilers can typically realize only a small fraction of a processor's peak performance. This project will explore several compiler optimization approaches for generating high-performance vectorized code. Advanced vectorization techniques will be incorporated in the open-source LLVM/Clang compiler through the Polly/LLVM module. A benchmark suite will also be developed, aimed at testing the effectiveness of vectorizing compilers.<br/><br/>Production compilers limit their optimization search space in order to control the time taken to compile programs. This is because the majority of users expect rapid compile times. However, the developers of high-performance applications are generally very willing to tolerate a much longer wait for program compilation, in return for a boost in the performance of the compiled code. A significant focus of this project will be the development of vectorization approaches for such users who prioritize high application performance over short compile times. The project will investigate semantically-driven pattern-based approaches to vector optimization, a vectorization-friendly approach to tiling, and aggressive vector instruction scheduling approaches that promise higher performance at the expense of possibly high compile times."
"1148502","SI2-SSE: Developing and Deploying Path-Integral Quantum Simulation Tools for a Broad Research Community","OAC","OFFICE OF MULTIDISCIPLINARY AC, CONDENSED MATTER & MAT THEORY, Software Institutes","06/01/2012","09/03/2014","John Shumway","AZ","Arizona State University","Standard Grant","Rajiv Ramnath","05/31/2014","$160,896.00","","john.shumway@asu.edu","660 S MILL AVE STE 312","TEMPE","AZ","852813670","4809655479","CSE","1253, 1765, 8004","6863, 7237, 7433, 7569, 8004, 8005, 9162, AMPP","$0.00","This award supports research and education activities that will make path integral Monte Carlo codes for simulation of atomic and molecular systems, and materials at the quantum mechanical level accessible to students and non-experts. This will stimulate new researchers to contribute fresh ideas and help build a larger base of users. The PI will pursue a strategy based on three principles: (i) On-going development of open-source software following software engineering practices must emphasize features that have a timely impact on science, encourage intuitive understanding, and assist non-expert users. (ii) Education and outreach in the form of tutorials, documentation, and workshops must dramatically lower the barrier for the use of path integral Monte Carlo by non-specialists. (iii) Development and execution of a set of high-impact projects -- in warm, dense matter, cold atoms, nano-electronics, and quantum chemistry -- by a network of collaborators which will demonstrate the research capabilities of the open-source path integral Monte Carlo code.<br/><br/>The code under development already embodies many software design principles, including object oriented design, XML input, HDF5 output, GNU GPL open-source distribution, and code management in subversion. Under this project the PI will: (i) complete documentation, including tutorials with explanations of the scientific concepts; (ii) develop a set of benchmarks and unit tests, to document standard applications and verify proper execution of the code; (iii) coordinate a set of high-impact projects across a network of collaborators, (iv) develop a timeline and milestones for formal releases, with executables for UNIX, Macintosh, and Windows, including the pyQt4 graphical user interface; and (v) integrate these improvements into existing nanoHUB port and other software infrastructures. New computer code will be distributed under the General Public License, and analysis tools will use open-source python libraries.<br/><br/>This award supports research and education activities to develop well engineered computer codes for the high accuracy simulation of electrons in materials and tiny structures made of a small number of atoms, and systems of atoms at the level of quantum mechanics. Developed computer codes will be made available to the broader research community through existing software centers, such as nanoHUB in a well documented form. This will enable students and non-experts to access and use the codes to perform simulations using path-integral techniques and to generate fresh ideas to advance the computational method. <br/><br/>Path-integral methods have been applied so far to various problems where quantum mechanics is important, including electronic systems and devices on the nanoscale, cold atoms trapped by lasers, and some atoms and small molecules. The technique has high accuracy and has advantages over existing methods, but it has so far not been adequately developed. It also has great pedagogical value for teaching the principles of quantum mechanics.<br/><br/>Students and junior scientists will be trained in the application of advanced computational methods to problems in nanotechnology."
"1836797","SI2-SSI: Collaborative Research:  Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS, CDS&E","08/01/2017","09/19/2018","Matthew Knepley","NY","SUNY at Buffalo","Standard Grant","Stefan Robila","07/31/2020","$215,715.00","","knepley@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","1253, 8004, 8069, 8084","7433, 8004, 8009, 8084","$0.00","Many biological and biomedical systems involve the interaction of a flexible structure and a fluid. These systems range from the writhing and coiling of DNA, to the beating and pumping of cilia and flagella, to the flow of blood in the body, to the locomotion of fish, insects, and birds. This project aims to develop advanced software infrastructure for performing dynamic computer simulations of such biological and biomedical systems. To facilitate the deployment of this software in a range of scientific and engineering applications, this project will develop new software capabilities in concert with new computer models that use the software. Specific application domains to be advanced in this project include models of aquatic locomotion that can be used to understand the neural control of movement and ultimately to develop new treatments for neurological pathologies such as spinal cord injuries, and models that simulate the interaction between the electrophysiology of the heart and the contractions of the heart that pump blood throughout the body, which could lead to improved approaches to treating heart disease. The software to be developed within the project is freely available online and is used by a number of independent research groups in a variety of scientific and engineering domains. It is being actively used in projects that model different aspects of cardiovascular dynamics, such as platelet aggregation and the dynamics of natural and prosthetic heart valves, and in projects that study other biological problems, including cancer dynamics, insect flight, aquatic locomotion, and the dynamics of phytoplankton. The software is also being applied to non-biological problems, including nanoscale models of colloidal suspensions and models of active particles. The improved methods and software to be developed in this project will thereby have a broad and sustained impact on a large number of ongoing research efforts in the biological and biomedical sciences and other scientific and engineering disciplines.<br/><br/>The immersed boundary (IB) method is a broadly applicable framework for modeling and simulating fluid-structure interaction (FSI). The IB method was introduced to model the fluid dynamics of heart valves, and subsequent development initially focused on simulating cardiac fluid dynamics. This methodology is broadly useful, however, and has been applied to a variety of problems in which a fluid flow interacts with immersed structures, including elastic bodies, bodies with known or prescribed deformational kinematics, and rigid bodies. Extensions of the IB method have also been developed to model electrophysiological systems and systems with chemically active structures. To improve the efficiency of the IB method, the PI has developed adaptive versions of the IB method that employ structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where needed. These methods have been implemented within the IBAMR software framework, which provides parallel implementations of the IB method and its extensions that leverage high-quality computational libraries including SAMRAI, PETSc, and libMesh. This project will further extend the IBAMR software by implementing modeling and discretization technologies required by the research applications of current and prospective users of the software, by developing improved solver infrastructure facilitated by the implementation of native support for structured AMR discretizations in the PETSc library, and by integrating with existing high-quality software tools for model development, deployment, and analysis. IBAMR is freely distributed online and is used within a number of independent research groups both to the further development of the IB method and also to its application to simulate diverse problems in fluid dynamics and FSI. By enhancing IBAMR, this project will also enhance the ability of these and other researchers to construct detailed models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations. This project will also develop general-purpose support for AMR discretizations in PETSc, a software library with thousands of active users, ~400 downloads per month, and numerous applications. The work of this project will help to grow the IBAMR user community of students and researchers by developing UI tools for building models, running simulations, and analyzing results. Students will be actively engaged in all aspects of the project, including code, method, and model development."
"1743179","Collaborative Research: NISC SI2-S2I2 Conceptualization of CFDSI: Model, Data, and Analysis Integration for End-to-End Support of Fluid Dynamics Discovery and Innovation","OAC","FD-Fluid Dynamics, Special Initiatives, Software Institutes, CDS&E","03/01/2018","06/27/2018","Charles Meneveau","MD","Johns Hopkins University","Continuing Grant","Stefan Robila","08/31/2020","$22,821.00","","meneveau@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","1443, 1642, 8004, 8084","026Z, 8004, 8005, 9263","$0.00","Fluid dynamics is a broad field spanning a large number of science and engineering problem domains that are critical to a wide variety of important applications including manufacturing, climate analysis, environment, health, transportation, propulsion, and power generation. To support the fluid dynamics research and applications community, this project seeks to engage the community in order to conceptualize a future institute, the Computational Fluid Dynamics Software Infrastructure (CFDSI), that will broadly develop, share, and apply computational tools for the generation and analysis of fluid dynamics data from both experimental and computational sources.  After its conceptualization, CFDSI will create and extend tools for problem definition, solution, and analysis of both computational and experimental investigations. The primary objective is to facilitate the sharing of computational tools and data resources through a rich and extensible set of software components that can be integrated into a wide range of existing fluid dynamics analysis tools. By improving the efficiency of tools and their ease of use, the ability for scientists and engineers to accurately predict and understand how complex fluid flows behave will be enhanced, having a significant impact on design, innovation, and discovery across the vast range of applications where fluid dynamics plays a role. CFDSI even has the potential to impact on K-12, undergraduate and graduate education by making a wide variety of resources available to students for fluid dynamics investigations.<br/><br/>The conceptualized institute will make a wide variety of powerful simulation, data, and analysis resources available to the fluid dynamics research community by lowering or eliminating barriers associated with the adoption and use of these resources. The software infrastructure will have a number of positive impacts on the fluid dynamics research community. To do so, CFDSI will connect the best research in fluid dynamics to the best research in data science/analytics within a highly sustainable software development environment. Specifically, CFDSI will: 1) enhance the dissemination of fluid dynamics data resources and advances in CFD modeling, 2) facilitate collaboration in fluid dynamics research, especially between computational and experimental researchers, 3) enable detailed comparisons between different data sources and detailed validation of computational models, 4) ease the use of advanced CFD models, methods, and codes in new and complex applications, 5) facilitate advanced analytics, such as uncertainty quantification, data compression, and optimization, 6) provide students access to advanced CFD methods and data resources, both computational and experimental, to enhance both graduate and undergraduate education in fluid dynamics, 7) improve the sustainability of current and future CFD software, and 8) facilitate the management of the growing body of fluid dynamics data sets. These outcomes will greatly enhance the effectiveness and productivity of research in fluid dynamics. In particular, they will transform the conduct of fluid dynamics research by: 1) making it more collaborative, 2) enhancing the credibility of research results, 3) enabling discovery, 4) reducing the cost of pursuing new research questions, and 5) diversifying and widening the fluid dynamics community through lowering the barriers associated with accessing and adopting CFD codes and large data sets. Software components will be designed for both analysis of experimental and computational databases as well as direct integration into CFD codes. The latter will enable in situ data analytics to address the growing chasm between data creation rate (solver performance) and data storage rate/volume (I/O resources). After conceptualization and implementation, CFDSI will enable more effective fluid dynamics research and thus impact the wide variety of application domains in which fluid dynamics is critical including manufacturing climate, environment, health, transportation, propulsion, and power generation (including conventional, alternative, and nuclear sources) which will, in turn, strongly impact our economy. Additionally, CFDSI will provide the capability for immersive simulations and experiments that will close the loop on idea, insight, discovery, and design through establishing links to in situ data analytics and problem redefinition during ongoing simulations or experiments. Finally, CFDSI will impact other problem domains governed by partial differential equations (e.g. solid mechanics) by serving as a model and starting point for similar domain-specific software infrastructures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1047734","SI2-SSE: Parallel and Adaptive Simulation Infrastructure for Biological Fluid-Structure Interaction","OAC","INTERFAC PROCESSES & THERMODYN, Mechanics of Materials and Str, DYNAMICAL SYSTEMS, Software Institutes","09/15/2010","09/07/2010","Boyce Griffith","NY","New York University Medical Center","Standard Grant","Daniel Katz","10/31/2014","$499,996.00","Charles Peskin, David McQueen","boyceg@email.unc.edu","One Park Avenue, 6th FL","New York","NY","100165800","2122638822","CSE","1414, 1630, 7478, 8004","1414, 1630, 7478","$0.00","The immersed boundary (IB) method is both a mathematical formulation and a numerical approach to problems of fluid-structure interaction, treating the specific case in which an elastic structure is immersed in a viscous incompressible fluid.  The IB method was introduced to describe the fluid dynamics of heart valves, but this methodology has also been applied to a wide range of problems in biological and non-biological fluid dynamics.  The IB method typically requires high spatial resolution to resolve the viscous boundary layers at fluid-structure interfaces and, at higher Reynolds numbers, to resolve vortices shed from such interfaces.  To improve the efficiency of the IB method, the principal investigator has developed an adaptive version of the IB method that employs block-structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where it is needed.  IBAMR software is a distributed-memory parallel implementation of this adaptive scheme.  The key goal of this project is to make IBAMR the unifying software framework for users of the IB method, thereby establishing a community of researchers who employ a common software infrastructure for biofluids model development and simulation.  The project aims to enhance IBAMR substantially by (1) developing and implementing implicit IB schemes that will allow for the efficient use of large numerical timesteps; (2) developing and implementing extensions of the basic IB methodology, including a new variable-viscosity version of the IB method, and an existing stochastic version for microscale and nanoscale problems in which Brownian motion is important; (3) optimizing IBAMR for use with modern as well as projected-future high performance computing systems comprised of multi-core compute nodes interconnected by a high-speed network; and (4) developing front-end tools for model construction, validation, and execution, thereby facilitating the adoption and use of IBAMR, especially by students and researchers with limited computational experience.<br/><br/>From the writhing and coiling of DNA, to the beating and pumping motions of cilia and flagella, to the flow of blood in the heart and throughout the circulation, coupled fluid-structure systems are ubiquitous in biology and physiology.  This project aims to enhance significantly the IBAMR software developed by the principal investigator.  IBAMR is a framework for performing computer simulations of biological fluid mechanics, and this project seeks to establish IBAMR as a unifying software infrastructure that will serve as a common ""language"" for developing and exchanging such models.  IBAMR is already being actively used within several independent research projects that aim to model different aspects of cardiovascular dynamics, such as platelet aggregation and the fluid dynamics of natural and prosthetic heart valves.  Such simulations promise ultimately to improve the efficacy of devices and procedures for treating cardiovascular disease.  This software also is being used within projects that study other problems in biofluid mechanics, including insect flight, aquatic locomotion, and the dynamics of phytoplankton.  By enhancing IBAMR, this project will also enhance significantly the ability of these and other research groups to construct detailed biofluids models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations.  This project will enhance the IBAMR software substantially, extending the range of problems to which it may be applied, and improving the methods implemented within the software as well as the efficiency of the implementation.  The work of this project will extend greatly the community of students and researchers who are able to use IBAMR to model biological fluid-structure interaction, in part by implementing graphical software tools for building IB models and running IB simulations."
"1047963","SI2-SSE: Collaborative Research: Lagrangian Coherent Structures for Accurate Flow Structure Analysis","OAC","OFFICE OF MULTIDISCIPLINARY AC, Mechanics of Materials and Str, DYNAMICAL SYSTEMS, COFFES, Software Institutes","09/15/2010","09/07/2010","Shawn Shadden","IL","Illinois Institute of Technology","Standard Grant","Daniel Katz","10/31/2013","$248,356.00","","shadden@berkeley.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","CSE","1253, 1630, 7478, 7552, 8004","1253, 1630, 7478, 7552","$0.00","The Lagrangian Coherent Structures (LCS) software elements developed by this project will provide a valuable tool set for fluid mechanics research to extract new discoveries from the vast and growing body of computational and experimental fluid mechanics data. The computation of LCS enables a systematic approach to accurately characterize transport phenomena in complex systems that pose insurmountable challenges to traditional Eulerian approaches. Prior, ah hoc implementations of LCS have already helped in important, real-world challenges including, tracking pollutants in the ocean, developing novel diagnoses and therapies for cardiovascular disease, and helping airplanes to avoid turbulence. We will produce an open LCS software system to provide a modular, extensible and flexible infrastructure to broaden the community of scientists and engineers that benefit from LCS, in problems ranging from fluid dynamics to general dynamical systems. Prof. Shadden will lead the LCS algorithm design and numerical analysis, and Prof. Hart will oversee the package's architectural design and the efficient parallel implementation of its elements."
"1047719","SI2-SSE: Adaptive Software for Quantum Chemistry","OAC","OFFICE OF MULTIDISCIPLINARY AC, PROJECTS, Software Institutes","09/15/2010","09/07/2010","So Hirata","FL","University of Florida","Standard Grant","Evelyn Goldfield","01/31/2011","$391,079.00","","sohirata@illinois.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","1253, 1978, 8004","1253, 1978, 9216, 9263","$0.00","The goal of this project is to establish a new paradigm of scientific software, electing quantum chemistry as the domain science. The new software does not have a static, compiled code, but instead consists of an expert system and code generator. At every execution, it analyzes the hardware and application parameters, determines(parallel) algorithms, and implements them for one-time use. This strategy not only allows unprecedented flexibility in algorithm optimization but can also realize ideas that are impossible otherwise. Since the approach makes no assumption about hardware or application, it is more extensible, maintainable, and portable. It is particularly well suited for chemistry, where a variety of molecules and reactions is infinite.<br/><br/>The expected long-term impact of this project is a change in the way scientific and engineering computing software is developed and defined. It promises novel software technology, which simultaneously achieves development efficiency, high product quality, and increased ability to optimize the code and enhance the methodological capabilities, by having no fixed source code. This project also offers unique, interdisciplinary education for chemistry graduate students, which places exceptionally large focus on computing, the field that has been a driving force of the 21st century economy.<br/><br/>This is an award within the solicitation of Software Infrastructure for Sustained Innovation. The award is co-funded by the Office of Cyberinfrastructure, the Division of Chemistry and the Office of Multidisciplinary Activities."
"1339785","SI2-SSE: Development of Cassandra, A General, Efficient and Parallel Monte Carlo Multiscale Modeling Software Platform for Materials Research","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","10/01/2013","03/11/2015","Edward Maginn","IN","University of Notre Dame","Standard Grant","Rajiv Ramnath","05/31/2018","$395,133.00","Jindal Shah","ed@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","1253, 1712, 8004","7433, 7569, 8005, 9216, 9263","$0.00","The properties of materials are the result of the interactions between the atoms that make up these materials. These properties can now be predicted with great accuracy, even for materials that have not yet existed in nature, by using advanced computational methods to study how the constituent atoms of the materials interact with one another and their environment. This field relies upon the existence of sophisticated software packages that enable researchers to conduct these simulations. There are two general approaches for simulating bulk materials: molecular dynamics and Monte Carlo, each of which is appropriate for certain problems. There are many molecular dynamics software packages available but almost no general purpose Monte Carlo codes. This project seeks to develop an efficient, general-purpose open source Monte Carlo code called Cassandra.<br/><br/>To do this, the academic Monte Carlo code developed in the PI's group will be extended and enhanced. The code will be capable of simulating any type of molecule in bulk and heterogeneous environments. The code will contain a wide range of advanced features, making it useful for a range of problems. By providing a general purpose code to the research community and establishing a mechanism whereby users can add their own features and extend the code, this project will have a broad impact on the research community. It will enable non-experts to use Monte Carlo simulations to study new problems. It will enable experienced molecular modelers to utilize and contribute features to a single optimized and validated code, thereby alleviating the time and expense associated with developing specialized codes for individual applications. Because the code will be used in teaching and workshops, materials will be made available to educators to use the code in the classroom when teaching courses such as thermodynamics, molecular modeling and statistical mechanics."
"1339820","SI2-SSE: Collaborative Research: ADAPT: Next Generation Message Passing Interface (MPI) Library - Open MPI","OAC","Special Projects - CCF, Software Institutes","09/01/2013","08/29/2013","George Bosilca","TN","University of Tennessee Knoxville","Standard Grant","Rajiv Ramnath","08/31/2016","$347,216.00","","bosilca@icl.utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","2878, 8004","7433, 8005, 9150","$0.00","High-performance computing has reshaped science and industry in many areas. However, the rapid evolution at the hardware level over the last few years have been unmatched by corresponding changes at the programming paradigm level. According to the consensus of several major studies, the degree of parallelism on large systems is expected to increase by several orders of magnitude. As a result, the Message Passing Interface (MPI), which has been the de-facto standard message passing paradigm, lacks an efficient and portable way of handling today's architectures. To efficiently handle such systems, MPI implementations must adopt more asynchronous and thread-friendly behaviors to perform better than they do on today?s systems. Maintaining and further enhancing MPI, one of the most widely-used communication libraries in high-performance computing, will have a far-reaching impact beyond the scientific community, and represents a critical building block for continued advances in all areas of science and engineering.<br/>The ADAPT project enhances, hardens and modernizes the Open MPI library in the context of this ongoing revolution in processor architecture and system design. It creates a viable foundation for a new generation of Open MPI components, enabling the rapid exploration of new physical capabilities, providing greatly improved performance portability, and working toward full interoperability between classes of components. More specifically, ADAPT implements fundamental software techniques that can be used in many-core systems to efficiently execute MPI-based applications and to tolerate fail-stop process failures, at scales ranging from current large systems to the extreme scale systems that are coming soon. To improve the efficiency of Open MPI, ADAPT integrates, as a core component, knowledge about the hardware architecture, and allows all layers of the software stack full access to this information. Process placement, distributed topologies, file accesses, point-to-point and collective communications can then adapt to such topological information, providing more portability. The ADAPT team is also updating the current collective communication layer to allow for a task-based collective description contained at a group-level, which in turn adjusts to the intra and inter-node topology. Planned expansion of the current code with resilient capabilities allows Open MPI to efficiently survive hard and soft error types of failures. These capabilities can be used as building blocks for all currently active fault tolerance proposals in the MPI standard body.<br/>MPI is already one of the most relevant parallel programming models, the most important brick of most parallel applications, and one of the most critical communication pieces of most other programing models. Thus, the experience of the research team and emerging capabilities can benefit all future users of these programming standards, tools, and libraries--regardless of discipline. Any improvement in the performance and capabilities of a major MPI library such as Open MPI, has tremendous potential for an immediate and dramatic impact on the application communities. In addition to improving the time to solution for their applications, it has the potential to decrease the energy usage and maximize the performance delivered by the existing execution platforms. The scale at which the Open MPI library is used in government research institutions (including universities and national laboratories), as well as in the private sector, is a major vector for a quick impact on all scientific and engineering communities."
"1339841","Collaborative Research: SI2-SSI: A Sustainable Open Source Software Pipeline for Patient Specific Blood Flow Simulation and Analysis","OAC","Software Institutes, CDS&E","10/01/2013","09/20/2013","Shawn Shadden","IL","Illinois Institute of Technology","Standard Grant","Daniel Katz","12/31/2013","$306,276.00","","shadden@berkeley.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","CSE","8004, 8084","7433, 8009, 8084","$0.00","The SimVascular package is a crucial research tool for cardiovascular modeling and simulation, and has contributed to numerous advances in personalized medicine, surgical planning and medical device design. SimVascular is currently the only comprehensive software package that provides a complete pipeline from medical image data segmentation to patient specific blood flow simulation and analysis. This software now forms the backbone in cardiovascular simulation research in a small but active group of domestic and international academic labs. However, since its original release there have been several critical barriers preventing wider adoption by new users, application to large-scale research studies, and educational access. These include 1) the cost and complications associated with embedded commercial components, 2) the need for more efficient geometric model construction tools, 3) lack of sustainable architecture and infrastructure, and 4) a lack of organized maintenance. <br/><br/>This project is addressing the above roadblocks through the following aims:  1) create a sustainable and modular open source SimVascular 2.0 project housed at Stanford Simbios? simtk.org, with documentation, benchmarking and test suites, 2) provide alternatives to all commercial components in the first truly open source release of SimVascular, 3) improve the image segmentation methods and efficiency of model construction to enable high-throughput studies, and 4) enhance functionality by merging state of the art research in optimization, flow analysis, and multiscale modeling. The project leverages existing resources and infrastructure at simtk.org, and builds upon the significant previous investment that enabled the initial open source release of SimVascular. Access is further enhanced by cross-linking with the NIH funded Vascular Model Repository. This project will increase the user base and build a sustainable software platform supported by an active open source community.   Releasing the first fully open source version of SimVascular will enable greater advances in cardiovascular medicine, provide open access to state of the art simulation tools for educational purposes, and facilitate training of young investigators. These efforts will also further promote diversity and attract students to science and engineering by leveraging this software to enable high school field trips to the UCSD StarCAVE to view simulation data using virtual reality."
"1339649","Collaborative Research: SI2-SSI: Open Gateway Computing Environments Science Gateways Platform as a Service (OGCE SciGaP)","OAC","Cross-BIO Activities, Software Institutes","10/01/2013","08/29/2013","Borries Demeler","TX","University of Texas Health Science Center San Antonio","Standard Grant","Rajiv Ramnath","09/30/2018","$600,065.00","","borries.demeler@umontana.edu","7703 FLOYD CURL DR","San Antonio","TX","782293901","2105672340","CSE","7275, 8004","7433, 8009","$0.00","Science Gateways are virtual environments that dramatically accelerate scientific discovery by enabling scientific communities to utilize distributed computational and data resources (that is, cyberinfrastructure). Successful Science Gateways provide access to sophisticated and powerful resources, while shielding their users from the resources' complexities. Given Science Gateways' demonstrated impact on progress in many scientific fields, it is important to remove barriers to the creation of new gateways and make it easier to sustain them. The Science Gateway Platform (SciGaP) project will create a set of hosted infrastructure services that can be easily adopted by gateway providers to build new gateways based on robust and reliable open source tools. The proposed work will transform the way Science Gateways are constructed by significantly lowering the development overhead for communities requiring access to cyberinfrastructure, and support the efficient utilization of shared resources.<br/><br/>SciGaP will transform access to large scale computing and data resources by reducing development time of new gateways and by accelerating scientific research for communities in need of access to large-scale resources. SciGaP's adherence to open community and open governance principles of the Apache Software Foundation will assure open source software access and open operation of its services. This will give all project stakeholders a voice in the software and will clear the proprietary fog that surrounds cyberinfrastructure services. The benefits of SciGaP services are not restricted to scientific fields, but can be used to accelerate progress in any field of endeavor that is limited by access to computational resources. SciGaP services will be usable by a community of any size, whether it is an individual, a lab group, a department, an institution, or an international community. SciGaP will help train a new generation of cyberinfrastructure developers in open source development, providing these early career developers with the ability to make publicly documented contributions to gateway software and to bridge the gap between academic and non-academic development."
"1339606","Collaborative Research: SI2-SSE: Modules for Experiments in Stellar Astrophysics","OAC","STELLAR ASTRONOMY & ASTROPHYSC, OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","01/01/2014","08/29/2013","Richard Townsend","WI","University of Wisconsin-Madison","Standard Grant","Rajiv Ramnath","12/31/2017","$75,888.00","","townsend@astro.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","1215, 1253, 8004","1206, 7433, 8005","$0.00","As the most commonly observed objects, stars remain at the forefront of astrophysical research. Technical advances in detectors, computer processing power, networks and data storage have enabled new sky surveys. Many of these search for transient events at optical wavelengths, such as the Palomar Transient Factory and Pan-STARRS1 that probe ever-larger areas of the sky and ever-fainter sources, opening up the vast discovery space of ""time domain astronomy"". The recent Kepler and COROT space missions achieved nearly continuous monitoring of more than 100,000 stars. The stellar discoveries from these surveys include revelations about stellar evolution, rare stars, unusual explosion outcomes, and remarkably complex binary star systems. The immediate future holds tremendous promise, as both the space-based survey Gaia and the ground based Large Synoptic Survey Telescope come to fruition. This tsunami of data has created a new demand for a reliable and publicly available research and education tool in computational stellar astrophysics that will reap the full scientific benefits of these discoveries while also creating a collaborative environment where theory, computation and interpretation can come together to address critical scientific issues. This demand by the stellar community led to our release of the Modules for Experiments in Stellar Astrophysics (MESA) software project in 2011.  MESA has driven, and will continue to drive with support from this award, innovation in the stellar community as well as the exoplanet, galactic, and cosmological communities. Educators have widely deployed MESA in their undergraduate and graduate stellar evolution courses because MESA is a community platform with an active support network for leading-edge scientific investigations. Stellar astrophysics research, and all the communities that rely on stellar astrophysics, will be significantly enhanced by sustaining innovative development of MESA.<br/><br/>This award supports the Modules for Experiments in Stellar Astrophysics (MESA) software project and user community.  MESA solves the 1D fully coupled structure and composition equations governing stellar evolution. It is based on an implicit finite difference scheme with adaptive mesh refinement and sophisticated timestep controls; state-of-the-art modules provide equation of state, opacity, nuclear reaction rates, element diffusion, boundary conditions, and changes to the mass of the star. MESA is an open source library that employs contemporary numerical approaches, supports shared memory parallelism based on OpenMP, and is written with present and future multi-core and multi-thread architectures in mind. MESA combines the robust, efficient, thread-safe numerical and physics modules for simulations of a wide range of stellar evolution scenarios ranging from very-low mass to massive stars. Innovations in MESA and its domain of applicability continues to grow, just recently extended to include giant planets, oscillations, and rotation.  This project will sustain MESA as a key piece of software infrastructure for stellar astrophysics while building new scientific and educational networks."
"1450300","Collaborative Research: SI2-SSI:Task-Based Environment for Scientific Simulation at Extreme Scale (TESSE)","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","05/15/2015","05/20/2015","George Bosilca","TN","University of Tennessee Knoxville","Standard Grant","Bogdan Mihaila","04/30/2019","$1,178,068.00","Thomas Herault","bosilca@icl.utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","1253, 1712, 8004","7433, 8009, 8084, 9150, 9216","$0.00","This project, TESSE, is developing a new-generation programming environment that uniquely address the needs of emerging computational models in chemistry and other fields, and allowing these models to reap the benefits of the computer hardware of tomorrow. The project thus is bringing closer the ability to predict properties of matter and design matter transformations of importance to the technological leadership and energy security of the US.  TESSE's impact has scientific, cyberinfrastructure, and interdisciplinary educational aspects.  TESSE helps carry widely used scientific applications (NWChem, MADNESS, MPQC, etc.) into a whole new generation of many-core and accelerated architectures, while at the same time dramatically improving programmer productivity. In terms of cyberinfrastructure, TESSE's PaRSEC runtime system delivers capabilities that many libraries and advanced applications will require for high performance on large scale and hybridized systems.<br/><br/>The goals of this project, Task-based Environment for Scientific Simulation at Extreme Scale (TESSE), are to design and demonstrate via substantial scientific simulations within chemistry and other disciplines a prototype software framework that provides a groundbreaking response to the twin problems of portable performance and programmer productivity for advanced scientific applications on emerging massively-parallel, hybrid, many-core systems. TESSE will create a viable foundation for a new generation of science codes, one which enables even more rapid exploration of new physical models, provides greatly enhanced performance portability through directed acyclic graph (DAG) scheduling and auto-tuned kernels, and works towards full interoperability between major chemistry packages through compatible runtimes and data structures. TESSE will mature to become a standard, widely available, community-based and broadly-applicable parallel programming environment complementing and rivaling MPI/OpenMP. This is needed due to the widely appreciated shortfalls of existing mainstream programming models and the already huge successes of the existing DAG-based runtimes that are the foundation of the next generation of NSF- and DOE-supported (Sca)LAPACK high-performance linear algebra libraries."
"1440467","SI2-SSE: Collaborative Proposal: Symbolic-Numeric Approaches to Polynomials","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS","09/01/2014","08/01/2014","Daniel Bates","CO","Colorado State University","Standard Grant","Rajiv Ramnath","08/31/2018","$149,346.00","","bates@math.colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","1253, 8004, 8069","7433, 8005, 8251","$0.00","Polynomial systems arise naturally in many areas of human endeavor. These include the modeling of tumor growth; the design of robotic devices; chemical systems arising in areas ranging from combustion to blood clotting; assorted problems in physics; plus many areas with mathematics. The solution of the polynomial systems answers questions critical to these endeavors. This research will be devoted to developing the next generation of Bertini, an open source software package, which has been used successfully by many researchers on many problems, which include all those mentioned above.<br/><br/>Bertini will be rewritten in C++ to be scriptable and modular, which will allow it to be interfaced transparently with symbolic software. The new Bertini will include tools allowing the user to construct and manipulate homotopies based on the output of Bertini. A major focus of the research will be given to systems of polynomials arising from the discretization of systems of differential equations. The great challenge of these very large systems of polynomials is balanced by the great potential impact new efficient and robust methods of solution will have."
"1440547","SI2-SSE: Genetic Algorithm Software Package for Prediction of Novel Two-Dimensional Materials and Surface Reconstructions","OAC","DMR SHORT TERM SUPPORT, Software Institutes","01/01/2015","07/24/2014","Richard Hennig","NY","Cornell University","Standard Grant","Bogdan Mihaila","12/31/2018","$344,696.00","","rhennig@ufl.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","1712, 8004","7237, 7433, 8005, 8400, 9216","$0.00","The ability to control structure and composition at the nanoscale has introduced exciting scientific and technological opportunities. Advances in the creation of nanomaterials such as single-layer materials and nanocrystals have led to improved understanding of basic structure-property relationships that, in turn, have enabled impressive progress in a broad range of nanotechnologies with applications for energy storage, catalysis and electronic devices. Yet, significant knowledge gaps persist in what single-layer materials could be synthesized and in our understanding of the nature of the surfaces of nanocrystals, particularly in the complex environment of solvents and ligands. The discovery of potentially stable novel single-layer materials and the prediction of nanocrystal surface structures are arguably among the most critical aspects of nanoscale materials. This research will provide the computational tools for the detailed prediction of the structure of two-dimensional materials and nanostructure surfaces in complex environments. This will impact the development and the design of novel nanomaterials with properties optimized for applications ranging from catalyst for chemical reactions, to energy conversion materials, to low-power and high-speed electronic devices.<br/> <br/>Progress in the field requires better computational methods for structure prediction. This project will (i) transform the Genetic Algorithm for Structure Prediction (GASP) software package developed by the PI into a sustainable scientific tool, (ii) extend its functionality to 2D materials and materials interfaces, and (iii) increase its performance by coupling to surrogate energy models that are optimized on the fly. These complementary goals will be achieved through expansion of the developer and user base, transition to portable software interfaces and data structures, and the addition of modular algorithms for functionality and performance enhancements. To enhance the functionality, the GASP algorithms will be extended to two two-dimensional materials and materials surfaces with adsorbates and ligands. To enhance the performance of the genetic algorithm, the optimization approach will be coupled to surrogate energy models such as machine-learning techniques and empirical energy models that are optimized on the fly. The publication of user tutorials, and documentation on the data structures and software interfaces will enhance the GASP codes overall utility, increase the user and developer base, and enable further extension to other data-mining and structure prediction approaches. The students involved in this project will receive extensive training and experience in algorithm development, scientific computation, and structure/property determination of complex nanomaterials. As part of the education and outreach component of the project, the PI will develop a course module on Materials Structure Predictions and widely distribute it. A weeklong workshop for students and postdocs in the third year of the project on Materials Discovery and Design will broaden the research?s impact beyond the creation of new software and the discovery of novel single-layer materials and nanocrystal surface and ligand configurations."
"1440788","SI2-SSE: Enhancement and Support of DMTCP for Adaptive, Extensible Checkpoint-Restart","OAC","Special Projects - CCF, Software Institutes","09/01/2014","11/17/2014","Gene Cooperman","MA","Northeastern University","Standard Grant","Alan Sussman","08/31/2018","$514,427.00","","gene@ccs.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","2878, 8004","2878, 7433, 8004, 8005, 9251","$0.00","Society's increasingly complex cyberinfrastructure creates a concern for software robustness and reliability.  Yet, this same complex infrastructure is threatening the continued use of fault tolerance. Consider when a single application or hardware device crashes.  Today, in order to resume that application from the point where it crashed, one must also consider the complex subsystem to which it belongs. While in the past, many developers would write application-specific code to support fault tolerance for a single application, this strategy is no longer feasible when restarting the many inter-connected applications of a complex subsystem.  This project will support a plugin architecture for transparent checkpoint-restart.  Transparency implies that the software developer does not need to write any application-specific code. The plugin architecture implies that each software developer writes the necessary plugins only once.  Each plugin takes responsibility for resuming any interrupted sessions for just one particular component. At a higher level, the checkpoint-restart system employs an ensemble of autonomous plugins operating on all of the applications of a complex subsystem, without any need for application-specific code.<br/><br/>The plugin architecture is part of a more general approach called process virtualization, in which all subsystems external to a process are virtualized.  It will be built on top of the DMTCP checkpoint-restart system.  One simple example of process virtualization is virtualization of ids.  A plugin maintains a virtualization table and arranges for the application code of the process to see only virtual ids, while the outside world sees the real id.  Any system calls and library calls using this real id are extended to translate between real and virtual id. On restart, the real ids are updated with the latest value, and the process memory remains unmodified, since it contains only virtual ids. Other techniques employing process virtualization include shadow device drivers, record-replay logs, and protocol virtualization.  Some targets of the research include transparent checkpoint-restart support for the InfiniBand network, for programmable GPUs (including shaders), for networks of virtual machines, for big data systems such as Hadoop, and for mobile computing platforms such as Android."
"1449918","SI2-SSI: Collaborative Research: A Sustainable Infrastructure for Performance, Security, and Correctness Tools","OAC","Software Institutes","08/01/2015","07/20/2015","Barton Miller","WI","University of Wisconsin-Madison","Standard Grant","Bogdan Mihaila","07/31/2021","$1,500,000.00","","bart@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","8004","7433, 8009","$0.00","Software has become indispensable to society, used by computational scientists for science and engineering, by analysts mining big data for value, and to connect society over the Internet. However, the properties of software systems for any of these purposes cannot be understood without accounting for code transformations applied by optimizing compilers used to compose algorithm and data structure templates, and libraries available only in binary form. To address this need, this project will overhaul, integrate, and enhance static binary analysis and runtime technologies to produce components that provide a foundation for performance, correctness, and security tools. The project will build upon three successful and widely adopted open source software packages: the DynInst library for analysis and transformation of application binaries, the MRNet infrastructure for control of large-scale parallel executions and data analysis of their results, and the HPCToolkit performance analysis tools. The project team will engage the community to participate in the design and evaluation of the emerging components, as well as to adopt its components. <br/><br/>This project will have a wide range of impacts. First, software components built by the project will enable the development of sophisticated, high-quality, end-user performance, correctness, and security tools built by the project team, as well as others in academia, government, and industry. Software developed by the project team will help researchers and developers tackle testing, debugging, monitoring, analysis, and tuning of applications for systems at all scales. Second, end-user tools produced by the project have a natural place in the classroom to help students write efficient, correct, and secure programs. Third, components produced by the project will lower the barrier for new researchers to enter the field and build tools that have impact on production applications without years of investment. Fourth, the project will provide training for graduate students and interns in the area of software for performance, correctness, and security. Finally, through workshops and tutorials, the project will disseminate project results, provide training to enable others to leverage project software, and grow a community of tool researchers who depend on project components and thus have a strong motivation to help sustain project software into the future.<br/><br/>Modernizing open-source software components and tools for binary analysis will enable static analysis of application characteristics at the level of executable machine code, transformation of binaries to inject monitoring code, measurement to capture a detailed record of application?s interactions with all facets of a target platform, analysis of recorded data in parallel, and attribution of analysis results back to application source code in meaningful ways. Providing innovative, software components that support development of robust performance, correctness, and security tools will accelerate innovation by tools researchers and help them grapple with the increasing complexity of modern software. Of particular note, helping tools researchers and computational scientists grapple with the challenges of software for modern parallel systems and producing training materials that help people use this software, addresses several of the needs identified in the NSF Vision for Cyberinfrastructure for the 21st Century."
"1148305","Collaborative Research: SI2-SSE: Component-Based Software Architecture for Computational Landscape Modeling","OAC","Geomorphology & Land-use Dynam, Software Institutes","06/01/2012","05/06/2014","Erkan Istanbulluoglu","WA","University of Washington","Continuing Grant","Daniel Katz","05/31/2015","$180,289.00","","erkani@u.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7458, 8004","7433, 7458, 8004, 8005","$0.00","Presently there are no widely adopted software conventions for holistic computational landscape models. To fill this important gap this project adapts and enhances existing landscape modeling codes by introducing a component-based approach to software development.  This project adapts and enhances an existing model -- CHILD (Channel-Hill slope Integrated Landscape Development) -- to provide a set of interoperable, independent modeling components that provide flexible and modular approaches to landscape modeling which are fully compatible with the NSF-funded Community Surface Dynamics Modeling System CSDMS) infrastructure.  In accord with the CSDMS architecture, the software to be developed will also employ the standards and tools of the Common Component Architecture (CCA) software architecture.  Included is the design of an interface for communication with and between the developed components.  The end result will be a set of independent, interoperable C++ software modeling modules that are compatible with the CSDMS modeling toolkit as well as the standards and tools of the CCA structure.  The software will be tested against data on post-wildfire erosion.<br/><br/>This approach was selected to provide maximum flexibility to users by allowing them to plug-and-play, seamlessly linking together selected computing modules to enable custom combinations of components to support modeling for a wide variety of research topics.  Work will include the development of a gridding engine to handle both regular and unstructured meshes and an interface for space-time rainfall input, as well as a surface hydrology component, a sediment erosion-deposition component, a vegetation component, and a simulation driver. <br/><br/>If successful this project will impact many branches of the Earth and environmental sciences by building a new modeling platform and creating essential software infrastructure for science with applications that span hydrology, soil erosion, tectonics, geomorphology, vegetation ecology, stratigraphy, and planetary science.  Broader impacts of the work include creation of classroom modeling exercises for both undergraduate and high schools students, and support of a PI whose gender is under-represented in the STEM fields and who is employed at an institution in an EPSCoR state.  It also includes workforce-training in computational geoscience for graduate students and a postdoc as well as minority undergraduates from Xavier University, a Historically Black University in the New Orleans area."
"1148255","Collaborative Research: SI2-SSI:  Empowering the Scientific Community with Streaming Data Middleware: Software Integration into Complex Science Environments","OAC","ADVANCES IN BIO INFORMATICS, ECOSYSTEM STUDIES, Software Institutes, Cybersecurity Innovation","08/01/2012","03/12/2013","Shirley Dyke","IN","Purdue University","Standard Grant","Rajiv Ramnath","07/31/2016","$42,000.00","","sdyke@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","1165, 1181, 8004, 8027","1165, 1181, 7434, 8004, 8009, 8027, 9251","$0.00","This project will catalyze a new generation of sensor-based streaming data applications by building on the foundation of the Open Source DataTurbine (OSDT) middleware (www.dataturbine.org). This will involve strategic software development, systems integration and testing, science experiments, and education and outreach. The software developments will include (A) tools for configuring, testing, and controlling distributed sensor systems, (B) software interfaces that are compliant with the Open Geospatial Consortium Sensor Web Enablement standards, (C) a new OSDT-Matlab interface, and (D) OSDT cybersecurity enhancements. These software products will be integrated into the production research infrastructures at Purdue, the University of Connecticut, the North Temperate Lake LTER Network site, and the Monterey Bay Aquarium Research Institute. The software will be professionally developed and managed as community resources. The intellectual merit of this project will be advances in software engineering, interoperability standards, and systems management for complex real-time sensor systems as well as advances on important open questions in civil engineering, marine science, and ecology. This project will yield broader impacts by enabling new science applications, enhancing productivity, and accelerating innovation across NSF directorates. To ensure that these broader impacts are realized, the team will provide student research opportunities, develop curriculum materials and training courses, and cultivate a community of software users."
"1147454","Collaborative Research: SI2-SSE: Component-Based Software Architecture for Computational Landscape Modeling","OAC","ECOSYSTEM STUDIES, Geomorphology & Land-use Dynam, Software Institutes","06/01/2012","04/25/2014","Gregory Tucker","CO","University of Colorado at Boulder","Continuing Grant","Daniel Katz","05/31/2015","$224,124.00","","gtucker@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","1181, 7458, 8004","1181, 7433, 7458, 8004, 8005","$0.00","Presently there are no widely adopted software conventions for holistic computational landscape models. To fill this important gap this project adapts and enhances existing landscape modeling codes by introducing a component-based approach to software development.  This project adapts and enhances an existing model -- CHILD (Channel-Hill slope Integrated Landscape Development) -- to provide a set of interoperable, independent modeling components that provide flexible and modular approaches to landscape modeling which are fully compatible with the NSF-funded Community Surface Dynamics Modeling System CSDMS) infrastructure.  In accord with the CSDMS architecture, the software to be developed will also employ the standards and tools of the Common Component Architecture (CCA) software architecture.  Included is the design of an interface for communication with and between the developed components.  The end result will be a set of independent, interoperable C++ software modeling modules that are compatible with the CSDMS modeling toolkit as well as the standards and tools of the CCA structure.  The software will be tested against data on post-wildfire erosion.<br/><br/>This approach was selected to provide maximum flexibility to users by allowing them to plug-and-play, seamlessly linking together selected computing modules to enable custom combinations of components to support modeling for a wide variety of research topics.  Work will include the development of a gridding engine to handle both regular and unstructured meshes and an interface for space-time rainfall input, as well as a surface hydrology component, a sediment erosion-deposition component, a vegetation component, and a simulation driver. <br/><br/>If successful this project will impact many branches of the Earth and environmental sciences by building a new modeling platform and creating essential software infrastructure for science with applications that span hydrology, soil erosion, tectonics, geomorphology, vegetation ecology, stratigraphy, and planetary science.  Broader impacts of the work include creation of classroom modeling exercises for both undergraduate and high schools students, and support of a PI whose gender is under-represented in the STEM fields and who is employed at an institution in an EPSCoR state.  It also includes workforce-training in computational geoscience for graduate students and a postdoc as well as minority undergraduates from Xavier University, a Historically Black University in the New Orleans area."
"1148011","SI2-SSI: Collaborative Research:  A Computational Materials Data and Design Environment","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, CHEMISTRY PROJECTS, Software Institutes","10/01/2012","09/20/2012","Dane Morgan","WI","University of Wisconsin-Madison","Standard Grant","Alan Sussman","09/30/2018","$1,050,000.00","","ddmorgan@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","1253, 1712, 1991, 8004","1253, 1712, 1982, 1991, 7237, 7433, 7569, 7644, 8004, 8009, 9216, 9263","$0.00","TECHNICAL SUMMARY<br/>The Office of Cyberinfrastructure, Division of Materials Research, and Chemistry Division contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports development of new theory and tools to enable rapid and efficient calculation of atomic level material properties.  The incredible advances in computing power and tools of atomic scale simulation have now made it possible to predict critical properties for existing and new materials without experimental input.  However, present simulation approaches typically require researchers to perform many steps by hand, which is both slow and error prone compared to what a computer can do.  Through computer codes that automate the tasks in first principles modeling human bottlenecks can be removed and predictive capabilities of first principles simulation techniques can be accelerated by orders of magnitude.  Such a high-throughput computing approach will enable generation of critical materials data on an unprecedented scale and open new doors for material science.<br/><br/>The team will develop tools for the specific challenges of predicting point defect properties, atomic diffusion, and surface stability, with a focus on automating steps to enable computations on a massive scale.  The PIs will use state-of-the-art first principles quantum mechanical methods.   Best practices for treating the multiple issues of charged defect calculations, for example convergence with cell size and band gap errors, will be refined and automated for rapid execution.  Similarly, tools to identify diffusion pathways and determine their barriers will be streamlined to allow users to quickly identify transport properties of new systems.  New theoretical approaches to modeling charged surfaces will be developed to enable simulation of surfaces in more realistic environments.  This award will support prediction of properties that play a critical role in advancing a wide range of technologies, from improving semiconductors for next generation computers to better fuel cells for more efficient energy conversion.  Software tools and data produced by this effort will enable researchers to predict properties for thousands of materials with almost no human effort, accelerating the pace at which researchers can develop new materials technologies.<br/><br/>Software and data developed from this award will be shared with academic and industrial researchers through modules on the web, scientific journals and presentations at national and international conferences.  This award supports two workshops to educate researchers about the latest opportunities to use high-throughput computing of atomic scale properties for materials development.  Students will be trained to work at the critical interface of the computer and physical sciences, supporting a generation of scientists who use modern computers to their fullest potential to develop new understanding and technology.<br/><br/>NON-TECHNICAL SUMMARY<br/>The Office of Cyberinfrastructure, Division of Materials Research, and Chemistry Division contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports development of new theory and tools to enable rapid and efficient calculation of atomic level material properties.  The incredible advances in computing power and tools of atomic scale simulation have now made it possible to predict critical properties for existing and new materials without experimental input.  However, present simulation approaches typically require researchers to perform many steps by hand, which is both slow and error prone compared to what a computer can do.  Through computer codes that automate the tasks in first-principles modeling human bottlenecks can be removed and predictive capabilities of first principles simulation techniques can be accelerated by orders of magnitude.  Such a high-throughput computing approach will enable generation of critical materials data on an unprecedented scale and open new doors for material science.<br/><br/>The team will develop tools for the specific challenges of predicting point defect properties, atomic diffusion, and surface stability, with a focus on automating steps to enable computations on a massive scale.  These properties play a critical role in advancing a wide range of technologies, from improving semiconductors for next generation computers to better fuel cells for more efficient energy conversion.  Software tools and data produced by this effort will enable researchers to predict properties for thousands of materials with almost no human effort, accelerating the pace at which researchers can develop new materials technologies.<br/><br/>Software and data developed from this award will be shared with academic and industrial researchers through modules on the web, scientific journals and presentations at national and international conferences.  In particular, this award will support two workshops to educate researchers about the latest opportunities to use high-throughput computing of atomic scale properties for materials development.  This award will train students to work at the critical interface of the computer and physical sciences, supporting a generation of scientists who use modern computers to their fullest potential to develop new understanding and technology."
"1550337","SI2-SSI: Collaborative Research: ENKI: Software infrastructure that ENables Knowledge Integration for Modeling Coupled Geochemical and Geodynamical Processes","OAC","Software Institutes, EarthCube","09/01/2016","08/19/2016","Marc Spiegelman","NY","Columbia University","Standard Grant","Stefan Robila","08/31/2019","$245,093.00","","mspieg@ldeo.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","8004, 8074","7433, 8004, 8009","$0.00","Earth scientists seek to understand the mechanisms of planetary evolution from a process perspective in order to promote the progress of science.  They model the chemistry of melting of the interiors of planets as a result of heat flow within the body.  They calculate the flows of energy and mass from the interior to the surface. They model the interaction of fluids and rocks, which drives chemical weathering and the formation of ore deposits.  They seek to understand the synthesis and stabilities of organic compounds and their economic and biological roles.  They study the interactions of atmosphere, oceans, biosphere and land as a dynamically coupled evolving chemical system. To achieve this level of understanding of planetary evolution, Earth scientists use software tools that encode two fundamentally different types of models: (1) thermodynamic models of naturally occurring materials, and (2) models of transport that track physical flows of both fluids and solids.  Much of the fundamental science of planetary evolution lies in understanding coupled thermodynamic and transport models. This grant funds development of a software infrastructure that supports this coupled modeling of the chemical evolution of planetary bodies.   It is their aim to establish an essential and active community resource that will engage a large number of researchers, especially early career scientists, in the exercise of model building and customization.  <br/><br/>This is a project to create ENKI, a collaborative model configuration and testing portal that will transform research and education in the fields of geochemistry, petrology and geophysics.  ENKI will provide software tools in computational thermodynamics and fluid dynamics.  It will support development and access to thermochemical models of Earth materials, and establish a standard infrastructure of web services and libraries that permit these models to be integrated into fluid dynamical transport codes. This infrastructure will allow scientific questions to be answered by quantitative simulations that are presently difficult to impossible because of the lack of interoperable software frameworks.  ENKI, via the adoption of state-of-the-art model interfacing (OpenMI) and deployment environments (HubZero), will modernize how thermodynamic and fluid dynamic models are used by the Earth science community in five fundamental ways: (1) provenance tracking will enable automatic documentation of model development and execution workflows, (2) new tools will assist users in updating thermochemical models as new data become available, with the ability to merge these data and models into existing repositories and frameworks, (3) automated code generation will eliminate the need for users to manually code web services and library modules, (4) visualization tools and standard test suites will facilitate validation of model outcomes against observational data, (5) collaborative groups will be able to share and archive models and modeling workflows with associated provenance for publication. With these tools we seek to transform the large community of model users, who currently depend on a small group of dedicated and experienced researchers for model development and maintenance, into an empowered ensemble of model developers who take ownership of the process and bring their own expertise, intuition and perspective to shaping the software tools they use in daily research.  ENKI development will be community driven.  Participation of a dedicated and diverse group of early career professionals will guide us in user interface development - insuring portal capabilities are responsive to user needs, and in development of a rich set of documentation, tutorials and examples.  All software associated with this project will be released as open source."
"1550281","SI2-SSI: Collaborative Research: ENKI: Software infrastructure that ENables Knowledge Integration for Modelling Coupled Geochemical and Geodynamical Processes","OAC","Software Institutes, EarthCube","09/01/2016","08/19/2016","Peter Fox","NY","Rensselaer Polytechnic Institute","Standard Grant","Stefan Robila","08/31/2019","$450,001.00","","foxp@rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","8004, 8074","7433, 8004","$0.00","Earth scientists seek to understand the mechanisms of planetary evolution from a process perspective in order to promote the progress of science.  They model the chemistry of melting of the interiors of planets as a result of heat flow within the body.  They calculate the flows of energy and mass from the interior to the surface. They model the interaction of fluids and rocks, which drives chemical weathering and the formation of ore deposits.  They seek to understand the synthesis and stabilities of organic compounds and their economic and biological roles.  They study the interactions of atmosphere, oceans, biosphere and land as a dynamically coupled evolving chemical system. To achieve this level of understanding of planetary evolution, Earth scientists use software tools that encode two fundamentally different types of models: (1) thermodynamic models of naturally occurring materials, and (2) models of transport that track physical flows of both fluids and solids.  Much of the fundamental science of planetary evolution lies in understanding coupled thermodynamic and transport models. This grant funds development of a software infrastructure that supports this coupled modeling of the chemical evolution of planetary bodies.   It is their aim to establish an essential and active community resource that will engage a large number of researchers, especially early career scientists, in the exercise of model building and customization.  <br/><br/>This is a project to create ENKI, a collaborative model configuration and testing portal that will transform research and education in the fields of geochemistry, petrology and geophysics.  ENKI will provide software tools in computational thermodynamics and fluid dynamics.  It will support development and access to thermochemical models of Earth materials, and establish a standard infrastructure of web services and libraries that permit these models to be integrated into fluid dynamical transport codes. This infrastructure will allow scientific questions to be answered by quantitative simulations that are presently difficult to impossible because of the lack of interoperable software frameworks.  ENKI, via the adoption of state-of-the-art model interfacing (OpenMI) and deployment environments (HubZero), will modernize how thermodynamic and fluid dynamic models are used by the Earth science community in five fundamental ways: (1) provenance tracking will enable automatic documentation of model development and execution workflows, (2) new tools will assist users in updating thermochemical models as new data become available, with the ability to merge these data and models into existing repositories and frameworks, (3) automated code generation will eliminate the need for users to manually code web services and library modules, (4) visualization tools and standard test suites will facilitate validation of model outcomes against observational data, (5) collaborative groups will be able to share and archive models and modeling workflows with associated provenance for publication. With these tools we seek to transform the large community of model users, who currently depend on a small group of dedicated and experienced researchers for model development and maintenance, into an empowered ensemble of model developers who take ownership of the process and bring their own expertise, intuition and perspective to shaping the software tools they use in daily research.  ENKI development will be community driven.  Participation of a dedicated and diverse group of early career professionals will guide us in user interface development - insuring portal capabilities are responsive to user needs, and in development of a rich set of documentation, tutorials and examples.  All software associated with this project will be released as open source."
"1550601","Collaborative Research: SI2-SSI: Adding Volunteer Computing to the Research Cyberinfrastructure","OAC","Software Institutes","08/01/2016","08/02/2016","David Anderson","CA","University of California-Berkeley","Standard Grant","Rajiv Ramnath","07/31/2017","$259,999.00","","davea@ssl.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","8004","7433, 8004, 8009","$0.00","The aggregate computing power of consumer devices - desktop and laptop computers, tablets, smartphones - far exceeds that of institutional computing resources.  ""Volunteer computing"" uses these consumer devices, volunteered by their owners, to do scientific computing. In addition to providing additional, much-needed computational resources to scientists, volunteer computing publicizes scientific research and engages citizens in science. BOINC is the primary software system for volunteer computing.  It was developed at UC Berkeley with NSF support starting in 2002. Until now, BOINC has been based on a model of independent competing projects.  Scientists set up their own BOINC servers, port their applications to run on BOINC, and publicize their projects to attract volunteers.  There are about 40 such projects, in many areas of science: examples include Einstein@home, CERN, and SETI@home (astrophysics), Rosetta@home and GPUGrid.net (biomedicine), Climateprediction.net (climate study), and IBM World Community Grid (multiple applications).  Together these projects have about 400,000 active volunteers and 12 PetaFLOPS of computing throughput. This model, while successful to an extent, has reached a limit.  The number of projects and volunteers has stagnated.  Volunteer computing is supplying lots of computing power, but only to a few research projects.  For other scientists, there are two major barriers.  First, creating a BOINC project has significant overhead: learning a new technology, creating a public web site, generating publicity, and so on.  Second, volunteer computing is risky and uncertain; there is no guarantee that a new project will attract volunteers. This project aims to break this barrier, and to make volunteer computing available to all scientists doing high-throughput computing, by replacing the competing-projects model with a new ""central broker"" model. The new model has two related parts: 1) the integration of BOINC with existing high-throughput computing facilities such as supercomputing centers and science portals. Jobs currently run on cluster nodes will be transparently offloaded to volunteer computers. Scientists using these facilities will see faster turnaround times; they'll benefit from volunteer computing without even knowing it's there. 2) The project will change the volunteer interface so that participants sign up for scientific areas and goals rather then for particular projects. For example, a participant might sign up to contribute to cancer research. A central broker, to be developed as part of this project, would dynamically assign their computing resources to projects doing that type of research. This project mobilizes public support for and interest in scientific research by encouraging ""volunteer computing"" and engaging citizens in the conduct of the research itself. It simultaneously advances NSF's mission to advance science while broadening citizen engagement.<br/><br/>The first year of this project will prototype each of these parts, and will integrate BOINC with TACC and nanoHub. Integrating BOINC with existing HTC systems involves several subtasks: 1) Job routing: modifying existing job processing systems used by TACC and nanoHub (Launcher and Rappture respectively) to decide when a group of jobs should be offloaded to BOINC. This decision might involve the estimated runtime of the jobs, input and output file sizes, data sensitivity, the deadline or priority of the jobs, and the identity of the job submitter. 2) Job format conversion: mapping job descriptions (input/output file specifications, resource and timing requirements) to their BOINC equivalents. 3) Application packaging: adapting existing applications (such as nanoHub's simulation tools and TACC's Autodock) to run under BOINC. We will use BOINC's virtual machine facility, which packages an application as a virtual machine image (VirtualBox or Docker) and a program to be run within the VM.  This allows existing Linux applications to run on consumer desktop platforms such as Windows and Mac, as well as providing a strong security sandbox and an efficient application-independent checkpoint/restart mechanism. 4) File handling: moving input and output files between existing storage systems (typically inaccessible from outside firewalls) to Internet-visible servers.  This will use existing BOINC components that manage files based on hashes to eliminate duplicate transfer and storage of files. 5) Job monitoring and control: adapting existing web- or command-line based tools for monitoring the progress of batches of jobs, and for aborting jobs, to work with BOINC. This will use existing Web RPCs provided by BOINC for these purposes. This project will carry out these tasks by designing and implementing new software as needed, testing for correctness, performance, and scalability, and deploying it in a production environment. The second part of the project - a brokering system for allocating computing power based on volunteer scientific preferences - will be designed and prototyped.  This involves several subtasks: 1) Designing a schema for volunteer preferences, including scientific areas and sub-areas, project nationality and institutions, specific projects and applications, inclusions/exclusions, and so on. 2) Designing a schema for assigning attributes to job streams (e.g. their area, sub-area, institution, etc.), and for assigning quotas or priorities to job streams. 3) Designing a relational database for storing the above information. 4) Designing and implementing policies for assigning volunteer resources to job streams in a way that respects volunteer preferences and optimizes quota, fairness, and throughput criteria.  This will be implemented as a BOINC ""account manager"" so that volunteers see a single interface rather than lots of separate projects and web sites."
"1550172","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)","OAC","COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","07/06/2016","Gunther Roland","MA","Massachusetts Institute of Technology","Standard Grant","Bogdan Mihaila","06/30/2021","$189,500.00","","Gunther.Roland@cern.ch","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7244, 8004","026Z, 7433, 7569, 8009, 8084","$0.00","Microseconds after the Big Bang, the universe was filled with an extremely hot fluid called the Quark-Gluon Plasma. As the universe expanded, this plasma cooled and condensed into the building blocks of ordinary matter around us: protons, neutrons, and atomic nuclei. Droplets of this fluid, which exists only at temperatures above 2 trillion Kelvin, are generated and studied in the laboratory today using collisions of high-energy heavy ions at Brookhaven National Laboratory and CERN. A key method to study the Quark-Gluon Plasma is the generation of high-energy quarks and gluons in the collision, which interact with the hot plasma and emerge as ""jets"" of particles that are measured by experiments. These jets provide powerful tools to study the internal structure of the plasma, analogous to tomography in medical imaging. However, interpretation of jet measurements requires sophisticated numerical modeling and simulation, and comparison of theory calculations with experimental data demands advanced statistical tools. The JETSCAPE Collaboration, an interdisciplinary team of physicists, computer scientists, and statisticians, will develop a comprehensive software framework that will provide a systematic, rigorous approach to meet this challenge. Training programs, workshops, summer schools and MOOCs, will disseminate the expertise needed to modify and maintain this framework.<br/><br/>The JETSCAPE Collaboration will develop a scalable and portable open source software package to replace a variety of existing codes. The modular integrated software framework will consist of interacting generators to simulate (i) wave functions of the incoming nuclei, (ii) viscous fluid dynamical evolution of the hot plasma, and (iii) transport and modification of jets in the plasma. Integrated advanced statistical analysis tools will provide non-expert users with quantitative methods to validate novel theoretical descriptions of jet modification, by comparison with the complete set of current experimental data. To improve the efficiency of this computationally intensive task, the collaboration will develop trainable emulators that can accurately predict experimental observables by interpolation between full model runs, and employ accelerators such as Graphics Processing Units (GPUs) for both the fluid dynamical simulations and the modification of jets. The collaboration will create this framework with a user-friendly envelope that allows for continuous modifications, updates and improvements of each of its components. The effort will serve as a template for other fields that involve complex dynamical modeling and comparison with large data sets. It will open a new era for high-precision extraction of the internal structure of the Quark-Gluon Plasma with quantifiable uncertainties. <br/><br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. <br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences."
"1535081","SI2-SSE: Collaborative Research: TrajAnalytics: A Cloud-Based Visual Analytics Software System to Advance Transportation Studies Using Emerging Urban Trajectory Data","OAC","Software Institutes","09/01/2015","08/05/2015","Jing Yang","NC","University of North Carolina at Charlotte","Standard Grant","Bogdan Mihaila","08/31/2019","$200,950.00","","Jing.Yang@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","CSE","8004","7433, 8005","$0.00","Advanced technologies in sensing and computing have created urban trajectory datasets of humans and vehicles travelling over urban networks. Understanding and analyzing the large-scale, complex data reflecting city dynamics is of great importance to enhance both human lives and urban environments. Domain practitioners, researchers, and decision-makers need to manage, query and visualize such big and dynamics data to conduct both exploratory and analytical tasks. To support these tasks, this project develops a open source software tool, named TrajAnalytics, which integrates computational techniques of scalable trajectory database, intuitive and interactive visualization, and high-end computers, to explicitly facilitate interactive data analytics of the emerging trajectory data.<br/><br/>The software provides a new platform for researchers in transportation assessment and planning to directly study moving populations in urban spaces. Researchers in social, economic and behavior sciences can use the software to understand the complicated mechanisms of urban security, economic activities, behavior trends, and so on. Specifically, this software advances the research activities in the community of transportation studies. The software also acts as an outreach platform allowing government agencies to communicate more effectively with the public with the real-world dynamics of city traffic, vehicles, and networks. The integration of research and education prepares the next generation workforce, where students across multi-disciplines can benefit through their participation in the development and use of the software.<br/><br/>In order to lay the foundations for effective analysis of urban trajectory datasets, this software (1) facilitates easy access and unprecedented capability for researchers and analysts with a cloud-based storage and computing infrastructure; (2) develops a parallel graph based data model, named TrajGraph, where massive trajectories are managed on a large-scale graph created from urban networks which is naturally amenable for studying urban dynamics; and (3) provides a visualization interface, named TrajVis, which supports interactive visual analytics tasks with a set of visualization tools and interaction functions. A variety of transportation-related researchers from geography, civil engineering, computer science participate in the design, evaluation, and use of the software. This robust and easy-to-use software enables the users to conduct iterative, evolving information foraging and sense making."
"1535191","SI2-SSE: Algorithms and Tools for Data-Driven Executable Biology","OAC","ADVANCES IN BIO INFORMATICS, Software Institutes","10/01/2015","11/24/2017","Rastislav Bodik","WA","University of Washington","Standard Grant","Stefan Robila","09/30/2019","$515,784.00","Aditya Virendra Thakur","bodik@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","1165, 8004","026Z, 7433, 8004, 8005, 9251","$0.00","This project seeks to understand the signaling mechanisms that control cellular activities such as  cell division, cell growth, and cell differentiation.  Errors in cellular signaling cause diseases such as cancer, autoimmunity, and diabetes.  Accurate models of cellular signalling are thus necessary for rational drug design and other applications central to national health.  This project focuses on inferring models from experimental data. Specifically, it is interested in models of protein signalling because proteins control and mediate the vast majority of biological processes in a living cell. The project follows of the approach of executable biology: models of cell signalling are computer programs, which allows executing the models on the computer and comparing the model behavior against the behavior of the living cell observed in the lab setting.  Most importantly for this project, viewing models as programs will allow the team to harness the recent advances in automatic synthesis of computer programs for synthesis of models from experimental measurements of cells. <br/><br/>The goal of the project is to provide biologists with a tool that synthesizes a variety of executable models from varied types of experimental data.  To facilitate synthesis of mechanistic models from experimental data, the project will develop a family of modeling languages that will capture complex behaviors of biological systems, such as time and concurrency.  The languages will be instances of the more general Boolean-Networks language.  The team will investigate how to adjust the modeling abstraction based on the nature of available experimental data; the abstractions will be instantiated as suitably chosen languages from their language family.  The modeling framework will be built by leveraging techniques from programming languages and formal methods such as meta-programming and constraint solving."
"1265624","Collaborative Research: SI2-CHE:Developing First Principles Monte Carlo Methods for Reactive Phase and Sorption Equilibria in the CP2k Software Suite","CHE","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","04/15/2013","04/02/2013","Troy Van Voorhis","MA","Massachusetts Institute of Technology","Standard Grant","Evelyn Goldfield","03/31/2016","$137,775.00","","tvan@MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1253, 8004","5918, 5946, 5950, 7433, 8009, 8650","$0.00","An international research team consisting of Ilja Siepmann, Ben Lynch (University of Minnesota), Neeraj Rai (Mississippi State University), Troy Van Voorhis (Massachusetts Institute of Technology), Ben Slater (University College London), Michiel Sprik (University of Cambridge), Adam Carter (Edinburgh Parallel Computing Centre), Jrg Hutter (University of Zurich), I-Feng Kuo (Lawrence Livermore National Laboratory), Christopher Mundy (Pacific Northwest National Laboratory), Joost VandeVondele (ETH Zurich), and Rodolphe Vuilleumier (University Pierre & Marie Curie Paris) is collaborating to develop and implement new theoretical methods in the CP2K computational chemistry software suite.  These new methodologies enable the predictive modeling of reactive multi-phase systems, including free energy landscapes and product yields, where the system interactions are described by Kohn-Sham density functional theory with van der Waals and hybrid functionals.  Markov chain Monte Carlo approaches utilizing smart moves with asymmetric underlying matrices, such as the aggregation-volume-bias and configurational-bias Monte Carlo methods, and the Gibbs ensemble framework are employed for efficient exploration of the phase space for reactive single- and multi-phase equilibria in bulk and in confinement.  The U.S. based research team is supported jointly by the Chemistry Division in MPS and the Office of Cyberinfrastucture. Funds for the UK based research team are provided by the EPSRC.<br/><br/>The software infrastructure is advanced by the development of efficient and accurate methodologies for reactive phase and sorption equilibria that are applicable to chemical processes in diverse science and engineering applications.  The extensively validated methodologies will be incorporated into the open-source CP2K software suite to make them available to a large user base.  The new software can be used to identify optimal reaction conditions and separation processes for sustainable chemistry.  The collaborative research team plans to use the new software for the investigation of reactive processes that address critical needs of society (fertilizers for food supply, fuels from renewable sources, and environmentally benign chemical processes)."
"1265872","SI2-CHE: Collaborative Research: Developing First Principles Monte Carlo Methods for Reactive Phase and Sorption Equilibria in CP2K Software Suite","CHE","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","04/15/2013","07/06/2014","Neeraj Rai","MS","Mississippi State University","Standard Grant","Evelyn Goldfield","03/31/2017","$230,558.00","","neerajrai@che.msstate.edu","PO Box 6156","MISSISSIPPI STATE","MS","397629662","6623257404","MPS","1253, 8004","5918, 5946, 5950, 7433, 8009, 8650, 9150, 9216, 9251","$0.00","An international research team consisting of Ilja Siepmann, Ben Lynch (University of Minnesota), Neeraj Rai (Mississippi State University), Troy Van Voorhis (Massachusetts Institute of Technology), Ben Slater (University College London), Michiel Sprik (University of Cambridge), Adam Carter (Edinburgh Parallel Computing Centre), Jrg Hutter (University of Zurich), I-Feng Kuo (Lawrence Livermore National Laboratory), Christopher Mundy (Pacific Northwest National Laboratory), Joost VandeVondele (ETH Zurich), and Rodolphe Vuilleumier (University Pierre & Marie Curie Paris) is collaborating to develop and implement new theoretical methods in the CP2K computational chemistry software suite.  These new methodologies enable the predictive modeling of reactive multi-phase systems, including free energy landscapes and product yields, where the system interactions are described by Kohn-Sham density functional theory with van der Waals and hybrid functionals.  Markov chain Monte Carlo approaches utilizing smart moves with asymmetric underlying matrices, such as the aggregation-volume-bias and configurational-bias Monte Carlo methods, and the Gibbs ensemble framework are employed for efficient exploration of the phase space for reactive single- and multi-phase equilibria in bulk and in confinement.  The U.S. based research team is supported jointly by the Chemistry Division in MPS and the Office of Cyberinfrastucture. Funds for the UK based research team are provided by the EPSRC.<br/><br/>The software infrastructure is advanced by the development of efficient and accurate methodologies for reactive phase and sorption equilibria that are applicable to chemical processes in diverse science and engineering applications.  The extensively validated methodologies will be incorporated into the open-source CP2K software suite to make them available to a large user base.  The new software can be used to identify optimal reaction conditions and separation processes for sustainable chemistry.  The collaborative research team plans to use the new software for the investigation of reactive processes that address critical needs of society (fertilizers for food supply, fuels from renewable sources, and environmentally benign chemical processes)."
"1740250","SI2:SSE: MAtrix, TEnsor, and Deep-Learning Optimized Routines (MATEDOR)","OAC","Software Institutes","09/01/2017","09/25/2018","Azzam Haidar","TN","University of Tennessee Knoxville","Standard Grant","Seung-Jong Park","08/31/2022","$400,000.00","Stanimire Tomov","haidar@icl.utk.edu","201 ANDY HOLT TOWER","KNOXVILLE","TN","379960001","8659743466","CSE","8004","7433, 8004, 8005","$0.00","A number of scientific software applications from important fields, including applications in deep learning, data mining, astrophysics, image and signal processing, hydrodynamics, and more, do many computations on small matrices (also known as ""tensors"") and using widely available standard linear-algebra software libraries. Scientists are trying to make these applications run faster by running them on advanced high performance computing (HPC) systems, that are heterogeneous systems that use processors of many different types, such as ""accelerators"" - that use of specialized computer hardware to perform some functions more efficiently than standard, general-purpose processors - and ""co-processors"" - that can run certain specialized functions in parallel with the central processor. However, standard linear algebra software libraries cannot make use of these specialized hardware components, and so the scientific applications mentioned above do not become much faster. Many existing linear algebra libraries, including libraries supplied by commercial vendors of computing technology have been tried to no avail. This issue is now critical because advancements in science from important fields are being held back due to the lack of progress in speeding up software. This project will address this through research and development that will create efficient software that can repetitively execute tensor operations grouped together in ""batches"" and which can be written to run very efficiently and quickly on the types of hardware components that exist in HPC systems. In addition to the research and development, several students will be engaged in the project, thus helping develop a critically needed component of the U.S. workforce.<br/><br/>The trend in high performance computing (HPC) toward large-scale, heterogeneous systems with GPU accelerators and coprocessors has made the near total absence of linear algebra software for small matrix or tensor operations especially noticeable. Given the fundamental importance of numerical libraries to science and engineering applications of all types, the need for libraries that can perform batched operations on small matrices or tensors has become acute. This MAtrix, TEnsor, and Deep-learning Optimized Routines (MATEDOR) project seeks to provide a solution to this problem by developing a sustainable and portable library for such small computations. Future releases of MATEDOR are expected to have a significant impact on application areas that use small matrices and tensors and need to exploit the power of advanced computing architectures. Such application areas include deep-learning, data mining, metabolic networks, computational fluid dynamics, direct and multi-frontal solvers, image and signal processing, and many more. This team has a proven record of providing software infrastructure that is widely adopted and used, that supports ongoing community contributions, and that becomes incorporated in vendor libraries (e.g., Intel's MKL and NVIDIA's CUBLAS) and other software tools and frameworks (e.g., MATLAB and R). Students will be regularly integrated into the project activities, and this group of PIs has an exceptionally strong record of community outreach, having given numerous performance optimization and software tutorials at conferences and Users Group meetings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1534688","SI2-SSE: Collaborative Research: An Intelligent and Adaptive Parallel CPU/GPU Co-Processing Software Library for Accelerating Reactive-Flow Simulations","OAC","CFS-Combustion & Fire Systems, Software Institutes","09/01/2015","08/03/2015","Chih-Jen Sung","CT","University of Connecticut","Standard Grant","Bogdan Mihaila","08/31/2019","$214,357.00","","cjsung@engr.uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","CSE","1407, 8004","148E, 7433, 8005","$0.00","In order to develop the next generation of clean and efficient vehicle engines and power-generating combustors, engineers need the next generation of computational modeling tools. Accurately describing the chemistry of conventional and alternative liquid transportation fuels is vital to predict harmful emission levels and other important quantities, but the high computational cost of detailed models for chemistry poses a significant barrier to use by designers. In order to use such accurate models, software is needed that can efficiently handle chemistry in practical simulations. This collaborative project aims to develop such tools, employing the computational power of modern parallelized central processing units (CPUs) and graphics processing units (GPUs). In addition to helping designers create clean and efficient engine technology, the advances made in this project are widely applicable to other computational modeling problems including astrophysics, nuclear reactions, atmospheric chemistry, biochemical networks, and even cardiac electrophysiology.<br/><br/>The objective of the proposed effort is to develop software elements specifically targeted at co-processing on GPUs, CPUs, and other many-core accelerator devices to reduce the computational cost of using detailed chemistry and enable high-fidelity yet affordable reactive-flow simulations. This will be achieved by (1) developing and comparing chemical kinetics integration algorithms for parallel operation on CPUs and GPUs/accelerators, (2) developing a method for detecting local stiffness due to chemical kinetics and adaptively selecting the most efficient solver based on available hardware, (3) implementing a computational cell clustering strategy to group similar spatial locations, (4) demonstrating the improved performance offered by these software elements using commercial and open-source computational fluid dynamics codes for modeling reactive flows, and (5) designing a portable and sustainable software library based on the above software elements, including building a community of users. The result of this program will be an open source software library that significantly decreases the cost of using detailed, accurate chemistry in realistic combustion simulations; the success of the program will be determined based on achieving order-of-magnitude performance improvement or better."
"1740219","Collaborative Research: NSCI: SI2-SSE: Time Stepping and Exchange-Correlation Modules for Massively Parallel Real-Time Time-Dependent DFT","OAC","DMR SHORT TERM SUPPORT, Software Institutes","09/01/2017","08/24/2017","Andre Schleife","IL","University of Illinois at Urbana-Champaign","Standard Grant","Rob Beverly","08/31/2022","$249,940.00","","schleife@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","1712, 8004","026Z, 054Z, 7433, 8004, 8005, 9216","$0.00","Recent advances in high-performance (HPC) computing allow simulations of quantum dynamics of electrons in complex materials, and such simulations are central to advancing various medical and semiconductor technologies, ranging from proton beam cancer therapy to fabricating faster and smaller electronics. At the same time, the increasing scale and complexity of modern high-performance computers exposed a need for development of scientific software that is tailored for computers with large numbers of processors so that simulations can efficiently take advantage of increasing computing power. This project advances scientific software for simulating quantum dynamics of electrons for high-performance computers with tens and hundreds of thousands of processors that are becoming widely available. This work builds the HPC academic research community around the proposed software by extending the existing software available for quantum dynamics simulation with better user-friendly features and analysis techniques. In the process, this project engages graduate students and early-career researchers to use and further develop scientific software for high-performance computers in general. Additionally, a summer school for hands-on training will be conducted. The open source software will be made available to the community on Github (public repository). <br/><br/>Real-time propagation in time-dependent density functional theory (RT-TDDFT) is becoming increasingly popular for studying non-equilibrium electronic dynamics both in the linear regime and beyond linear response. RT-TDDFT can be combined to study coupled dynamics of quantum-mechanical electrons with the movement of classical ions within Ehrenfest dynamics. In spite of its great promise, RT-TDDFT is computationally very demanding, especially for studying large condensed-matter systems. The large cost arises from small time steps of numerical integration of the electron dynamics, rendering accurate (hybrid) exchange-correlation (XC) functionals unfeasible, despite their clear benefits. In addition, while modern high-performance computing (HPC) helps tackling great scientific questions, massively parallel, hybrid-paradigm architectures present new challenges. Theoretical and algorithmic methods need to be developed in order to take full advantage of modern massively parallel HPC. This work builds new modules for the RT-TDDFT software component of the Qb@ll code, that enables a large community of researchers to perform advanced first-principles simulations of non-equilibrium electron dynamics in complex condensed-phase systems, using massively parallel HPC. This is done through developing (1) new modules for numerical integration that propagate the underlying non-linear partial differential equations in real time with high efficiency and accuracy, and (2) new modules for improved approximations of the underlying electronic structure, using a modern meta-generalized-gradient XC functional. Furthermore, the work builds the HPC academic research community around RT-TDDFT within the Qb@ll code through (1) development of user-friendly features that interface Qb@ll with other code and analysis techniques and (2) engagement of early-career scientists by incorporating hands-on training on RT-TDDFT using the Qb@ll code in TDDFT summer school.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering, the Materials Research Division and Chemistry Division in the Directorate of Mathematical and Physical Sciences."
"1339881","Collaborative Research: SI2-SSI: A Comprehensive Ray Tracing Framework for Visualization in Distributed-Memory Parallel Environments","OAC","Software Institutes","10/01/2013","09/13/2013","Charles Hansen","UT","University of Utah","Standard Grant","Rajiv Ramnath","09/30/2017","$282,225.00","","hansen@cs.utah.edu","201 PRESIDENTS CIR","SALT LAKE CITY","UT","841129049","8015816903","CSE","8004","7433, 8009, 9150","$0.00","Scientific visualization plays a large role in exploring the scientific simulations that run on supercomputers; new discoveries are often made by studying renderings generated through visualization of simulation results. The standard technique for rendering geometry is rasterization and the most commonly used library for performing this is OpenGL. Many visualization programs (VisIt, Ensight, VAPOR, ParaView, VTK) use OpenGL for rendering. However, recent architectural changes on supercomputers create significant opportunities for alternate rendering techniques. The computational power available on emerging many-core architectures, such as the Intel Xeon Phi processors on TACC?s Stampede system, enable ray-tracing, a higher quality technique. Further, as the amount of geometry per node rises, ray-tracing becomes increasingly cost effective, since its computational costs are proportional to the screen size, not the geometry size. Finally, the software implementation for OpenGL can not be easily mapped to non-GPU multi-core and many-core systems, creating a significant gap; if not closed, visualization will not be possible directly on large supercomputers. This confluence of new, more capable architectures, the increase in geometry per node, and concerns about the durability of the established rendering path all motivate this   work. <br/><br/>To address these trends, this research uses a two-pronged approach. First, the research will replace the OpenGL pathways that are commonly used for visualization with a high-performance, open-source ray tracing engine that can interactively render on both a CPU and on accelerator architectures. This new library will support the OpenGL API and will be usable immediately by any OpenGL-based visualization package without additional code modi&#64257;cation. Second, this research will provide a direct interface to a high-performance distributed ray tracing engine so that applications can take advantage of ray tracing capabilities not easily exposed through the standard OpenGL interface, such as participating media and global illumination simulation. These features will enable the open science community to easily create photo-realistic imagery with natural lighting cues to aid in analysis and discovery. It will further expand the capacity of existing cyberinfrastructure to provide interactive visualization on standard HPC resources. <br/><br/>This work has the potential to revolutionize in situ visualization capabilities by unifying the (potentially hybrid) architecture that efficiently run both simulation and visualization. Communicating with underrepresented groups will be a major component of outreach efforts through the PCARP, MITE and Women in Engineering programs. In addition, the project team will disseminate this work to the general public through NSF XD program, the VisIt visualization toolkit and by exhibiting at forums such as IEEE Visualization, IEEE High Performance Graphics and ACM Supercomputing."
"1740210","SI2-SSE: C11Tester: Scaling Testing of C/C++11 Atomics to Real-World Systems","OAC","Software Institutes","10/01/2017","06/11/2020","Brian Demsky","CA","University of California-Irvine","Standard Grant","Seung-Jong Park","09/30/2022","$415,999.00","","bdemsky@uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","CSE","8004","077Z, 7433, 8004, 8005, 9251","$0.00","We have long relied on increased raw computing power to drive technological progress. However, processors are now reaching their limits in terms of raw computing power, and continuing progress will require increased productivity in developing parallel software. Fully leveraging the performance of multi-core processors will in many cases require developers to make use of low-level ""atomic"" (or indivisible) operations such as those provided by the C11 and C++11 languages, so that can make very fine-grained optimizations to their code, and take full advantage of the computing power these processors offer them. Unfortunately, using C/C++ atomics is extremely difficult to do correctly and it is very easy to introduce subtle bugs in the use of these constructs. Testing for concurrency bugs in code that uses C/C++11 atomics can be extremely difficult as a bug can depend on the schedule, the state of the processor's memory subsystem, the specific processor, and the compiler. The C11Tester project will develop tools for testing concurrent code that makes use of C/C++11 atomics and make these tools available to both researchers and practitioners.<br/><br/>The C/C++11 standard introduced a relaxed memory model with atomic operations into the C and C++ languages. While C/C++11 atomics can provide significant performance benefits, using C/C++11 atomics correctly is extremely difficult. Existing tools such as CDSChecker can only find bugs in small unit tests of concurrent data structures.  Bugs can also arise due to the interaction of subtle memory model semantics and the composition of software components. The C11Tester project will develop new techniques for testing and debugging complete concurrent applications that make use of C/C++11 atomics. The C11Tester project will make the following contributions: (1) it will develop new approaches for testing the correctness of concurrent applications, (2) it will develop new approaches for debugging concurrent applications, and (3) it will develop and make available a robust implementation of the approach in the C11Tester tool."
"1339856","Collaborative Research: SI2-SSI: Open Gateway Computing Environments Science Gateways Platform as a Service (OGCE SciGaP)","OAC","Cross-BIO Activities, Software Institutes","10/01/2013","08/29/2013","Mark Miller","CA","University of California-San Diego","Standard Grant","Bogdan Mihaila","09/30/2019","$1,742,099.00","Amitava Majumdar","mmiller@sdsc.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","CSE","7275, 8004","7433, 8009","$0.00","Science Gateways are virtual environments that dramatically accelerate scientific discovery by enabling scientific communities to utilize distributed computational and data resources (that is, cyberinfrastructure). Successful Science Gateways provide access to sophisticated and powerful resources, while shielding their users from the resources' complexities. Given Science Gateways' demonstrated impact on progress in many scientific fields, it is important to remove barriers to the creation of new gateways and make it easier to sustain them. The Science Gateway Platform (SciGaP) project will create a set of hosted infrastructure services that can be easily adopted by gateway providers to build new gateways based on robust and reliable open source tools. The proposed work will transform the way Science Gateways are constructed by significantly lowering the development overhead for communities requiring access to cyberinfrastructure, and support the efficient utilization of shared resources.<br/><br/>SciGaP will transform access to large scale computing and data resources by reducing development time of new gateways and by accelerating scientific research for communities in need of access to large-scale resources. SciGaP's adherence to open community and open governance principles of the Apache Software Foundation will assure open source software access and open operation of its services. This will give all project stakeholders a voice in the software and will clear the proprietary fog that surrounds cyberinfrastructure services. The benefits of SciGaP services are not restricted to scientific fields, but can be used to accelerate progress in any field of endeavor that is limited by access to computational resources. SciGaP services will be usable by a community of any size, whether it is an individual, a lab group, a department, an institution, or an international community. SciGaP will help train a new generation of cyberinfrastructure developers in open source development, providing these early career developers with the ability to make publicly documented contributions to gateway software and to bridge the gap between academic and non-academic development."
"1339793","Collaborative Research: SI2-SSI: The Community-Driven Big-CZ Software System for Integration and Analysis of Bio- and Geoscience Data in the Critical Zone","OAC","Software Institutes","12/01/2013","08/09/2016","Ilya Zaslavsky","CA","University of California-San Diego","Standard Grant","Rajiv Ramnath","11/30/2017","$433,911.00","David Valentine","zaslavsk@sdsc.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","CSE","8004","7433, 8009","$0.00","The Critical Zone (CZ) science community takes as its charge the effort to integrate theory, models and data from the multitude of disciplines collectively studying processes on the Earth's surface. The Critical Zone is Earth's permeable near-surface layer - from the atmosphere at the vegetation's canopy to the lower boundary of actively circulating groundwaters. The Critical Zone was a term coined by the National Research Council's Basic Research Opportunities in the Earth Sciences (BROES) Report (2001) to highlight the imperative for a new approach to thoroughly multi-disciplinary research on the zone of the Earth?s surface that is critical to sustaining terrestrial life on our planet. In January 2013, 103 members of the CZ community met for the CZ-EarthCube Domain Workshop (NSF Award #1252238) to prioritize the CZ community's key science drivers, key computational and information technology (""cyber"") challenges and key cyber needs. They identified that the central scientific challenge of the critical zone science community is to develop a ""grand unifying theory"" of the critical zone through a theory-model-data fusion approach.  Work participants unanimously described that the key missing need of this approach was a future cyberinfrastructure for seamless 4D visual exploration of the integrated knowledge (data, model outputs and interpolations) from all the bio and geoscience disciplines relevant to critical zone structure and function, similar to today?s ability to easily explore historical satellite imagery and photographs of the earth's surface using Google Earth.  This project takes the first ""BiG"" steps toward answering that need.<br/><br/>The overall goal of this project is to co-develop with the CZ science and broader community, including natural resource managers and stakeholders, a web-based integration and visualization environment for joint analysis of cross-scale bio and geoscience processes in the critical zone (BiG CZ), spanning experimental and observational designs. Our Project Objectives are to: (1) Engage the CZ and broader community to co-develop and deploy the BiG CZ software stack; (2) Develop the BiG CZ Portal web application for intuitive, high-performance map-based discovery, visualization, access and publication of data by scientists, resource managers, educators and the general public; (3) Develop the BiG CZ Toolbox to enable cyber-savvy CZ scientists to access BiG CZ Application Programming Interfaces (APIs); and (4) Develop the BiG CZ Central software stack to bridge data systems developed for multiple critical zone domains into a single metadata catalog. The entire BiG CZ Software system will be developed on public repositories as a modular suite of fully open source software projects.  It will be built around a new Observations Data Model Version 2.0 (ODM2) that is being developed by members of the BiG CZ project team, with community input, under separate funding (NSF Award #1224638)."
"1339824","Collaborative Research: SI2-SSI: A Sustainable Open Source Software Pipeline for Patient Specific Blood Flow Simulation and Analysis","OAC","Software Institutes, CDS&E","10/01/2013","09/20/2013","Alison Marsden","CA","University of California-San Diego","Standard Grant","Daniel Katz","10/31/2015","$1,237,578.00","","amarsden@stanford.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","CSE","8004, 8084","7433, 8009, 8084","$0.00","The SimVascular package is a crucial research tool for cardiovascular modeling and simulation, and has contributed to numerous advances in personalized medicine, surgical planning and medical device design. SimVascular is currently the only comprehensive software package that provides a complete pipeline from medical image data segmentation to patient specific blood flow simulation and analysis. This software now forms the backbone in cardiovascular simulation research in a small but active group of domestic and international academic labs. However, since its original release there have been several critical barriers preventing wider adoption by new users, application to large-scale research studies, and educational access. These include 1) the cost and complications associated with embedded commercial components, 2) the need for more efficient geometric model construction tools, 3) lack of sustainable architecture and infrastructure, and 4) a lack of organized maintenance. <br/><br/>This project is addressing the above roadblocks through the following aims:  1) create a sustainable and modular open source SimVascular 2.0 project housed at Stanford Simbios? simtk.org, with documentation, benchmarking and test suites, 2) provide alternatives to all commercial components in the first truly open source release of SimVascular, 3) improve the image segmentation methods and efficiency of model construction to enable high-throughput studies, and 4) enhance functionality by merging state of the art research in optimization, flow analysis, and multiscale modeling. The project leverages existing resources and infrastructure at simtk.org, and builds upon the significant previous investment that enabled the initial open source release of SimVascular. Access is further enhanced by cross-linking with the NIH funded Vascular Model Repository. This project will increase the user base and build a sustainable software platform supported by an active open source community.   Releasing the first fully open source version of SimVascular will enable greater advances in cardiovascular medicine, provide open access to state of the art simulation tools for educational purposes, and facilitate training of young investigators. These efforts will also further promote diversity and attract students to science and engineering by leveraging this software to enable high school field trips to the UCSD StarCAVE to view simulation data using virtual reality."
"1440709","SI2-SSE: Petascale Enzo: Software Infrastructure Development and Community Engagement","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/01/2014","08/13/2014","Michael Norman","CA","University of California-San Diego","Standard Grant","Bogdan Mihaila","08/31/2018","$500,000.00","","mlnorman@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","CSE","1253, 1798, 8004","1206, 7433, 8005","$0.00","The purpose of this project is to develop an astrophysics and cosmology software application ""Enzo-P"", built on the highly scalable parallel adaptive mesh refinement (AMR) software framework ""Cello"" that is being developed concurrently.  The Enzo-P application will be capable of running extreme scale numerical simulations to investigate frontier questions in star formation, molecular cloud turbulence, interstellar medium dynamics, galaxy formation, intergalctic medium, formation of the first stars and galaxies, galaxy clusters, and cosmic reionization.  This new software will empower the current large and diverse Enzo user/developer community to take full advantage of current and future high performance computer (HPC) systems. The Cello AMR framework can be used independently of Enzo-P, thus enabling researchers in other diverse scientific fields to develop AMR applications capable of running on ""Petascale-and-beyond"" HPC platforms. <br/><br/>The novel approach used for Cello is to implement a ""forest-of-octree"" AMR scheme using the Charm++ parallel programming system.  Octree-based AMR has been shown to be among the highest scaling AMR approaches, with demonstrated scaling to over 200K CPU cores.  The Charm++ object-oriented parallel programming language supports data-driven asynchronous execution, is inherently latency-tolerant and automatically overlaps computation with communication, and provides support for developing Exascale applications, including in-memory distributed checkpointing and sophisticated dynamic load balancing schemes.  Enzo-P development will be directed by the vibrant Enzo open development community, who will migrate Enzo's self-gravity, cosmology, chemistry and cooling, MHD, and radiation hydrodynamics  capabilities to use the Cello scalable AMR framework."
"1642336","SI2-SSE: Enabling Chemical Accuracy in Computer Simulations: An Integrated Software Platform for Many-Body Molecular Dynamics","OAC","Software Institutes","04/01/2017","03/27/2017","Francesco Paesani","CA","University of California-San Diego","Standard Grant","Stefan Robila","03/31/2020","$499,918.00","Andreas Goetz, Andrea Zonca","fpaesani@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","CSE","8004","026Z, 7433, 8004, 8005, 9216, 9263","$0.00","This project is jointly funded by the Office of Advanced Cyberinfrastructure and and the Division of Chemistry within the Directorate of Mathematical and Physical Sciences. As attested by the 2013 Nobel Prize in Chemistry awarded to Martin Karplus, Michael Levitt, and Arieh Warshel, molecular-level computer simulations have become indispensable in many research areas, including chemistry, physics, materials science, and biochemistry, and often provide fundamental insights that are otherwise difficult to obtain. Nowadays, computer simulations are used to complement, guide, and sometimes replace experimental measurements, reducing the amount of time and money spent on research to bring ideas from the lab to practical applications. In the pharmaceutical industry computer simulations play a key role in structure-based drug design as demonstrated by the development of HIV protease inhibitors. In the chemical industry, computer modeling guides the design of new catalysts as well as novel materials for applications in more efficient batteries, and fuel and solar cells. More recently, there has been significant success in using of computer simulations to design more effective chemical processes as well as to provide information on safety issues. However, both the realism and the predictive power of a molecular-level computer simulation directly depend on the accuracy with which the interactions between molecules are described. To address the limitations of existing simulation approaches, the project group has recently developed a new theoretical/computational methodology that has been shown to display unprecedented accuracy when applied to a variety of molecular systems. The overarching goal of the proposed research is the implementation of this new methodology into an integrated and publicly available software platform that will allow the scientific community to address a broad range of problems through computer simulations. Potential applications include, but are not limited to, the rational design of new drugs as well as novel materials for water purification and the detection of toxic compounds and explosives, the virtual screening of catalysts for more efficient chemical processes, the development of new batteries, solar and fuel cells, and biomolecular structure prediction. A diverse group of high school, undergraduate, and graduate students will be directly involved in different aspects of the proposed research. The students will thus acquire critical knowledge about computer simulations and programming, which will significantly enhance their competitiveness in today's computer-driven job market. Given its multidisciplinary and multifaceted nature, the proposed research will promote scientific progress at different levels and contribute to the development of new technologies that will advance the national health, prosperity and welfare, as well as secure the national defense.<br/><br/>The proposed research focuses on the development and implementation of unique software elements that will enable computer simulations on both CPU and GPU architectures using the so-called many-body molecular dynamics (MB-MD) methodology developed by the Paesani group. These software elements will be made publicly available to the scientific community through an integrated platform. MB-MD is a new simulation methodology that has already been shown to provide unprecedented accuracy in molecular simulations of a variety of molecular systems from the gas to the condensed phase. The new software elements comprise three components integrated in a unique software platform: a suite of publicly available computational tools for the automated generation of many-body potential energy functions from electronic structure data; a client-server architecture for the calculation of the required electronic structure data through volunteer computing; independent CPU and GPU plugins for the OpenMM toolkit which will enable MB-MD simulations of generic molecular systems across different phases. In parallel with the proposed research and software engineering projects, outreach and mentoring activities to promote STEM disciplines among students from underprivileged and underrepresented minorities through the PI and Co-PI direct involvement in several outreach programs at UC San Diego and the San Diego Supercomputer Center. These activities are specifically designed to increase the involvement and advancement of women, minorities, and economically disadvantaged groups across different education levels, from high school to undergraduate and graduate students."
"1664162","SI2-SSI: Pegasus: Automating Compute and Data Intensive Science","OAC","Software Institutes","05/15/2017","05/08/2017","Ewa Deelman","CA","University of Southern California","Standard Grant","Seung-Jong Park","09/30/2023","$2,500,000.00","Miron Livny","deelman@isi.edu","3720 S FLOWER ST","LOS ANGELES","CA","900894304","2137407762","CSE","8004","7433, 8004, 8009","$0.00","This project addresses the ever-growing gap between the capabilities offered by on-campus and off-campus cyberinfrastructures (CI) and the ability of researchers to effectively harness these capabilities to advance scientific discovery. Faculty and students on campuses struggle to extract knowledge from data that does not fit on their laptops or cannot be processed by an Excel spreadsheet and they find it difficult to efficiently manage their computations. The project sustains and enhances the Pegasus Workflow Management System, which enables scientist to orchestrate and run data- and compute-intensive computations on diverse distributed computational resources. Enhancements focus on the automation capabilities provided by Pegasus to support workflows handling large data sets, as well as usability of Pegasus that lowers the barrier of its adoption. This effort expands the reach of the advanced capabilities provided by Pegasus to researchers from a broader spectrum of disciplines that range from gravitational-wave physics to bioinformatics, and from earth science to material science.<br/><br/>For more than 15 years the Pegasus Workflow Management System has been designed, implemented and supported to provide abstractions that enable scientists to focus on structuring their computations without worrying about the details of the target CI. To support these workflow abstractions Pegasus provides automation capabilities that seamlessly map workflows onto target resources, sparing scientists the overhead of managing the data flow, job scheduling, fault recovery and adaptation of their applications. Automation enables the delivery of services that consider criteria such as time-to-solution, as well as takes into account efficient use of resources, managing the throughput of tasks, and data transfer requests. The power of these abstractions was demonstrated in 2015 when Pegasus was used by an international collaboration to harness a diverse set of resources and to manage compute- and data- intensive workflows that confirmed the existence of gravitational waves, as predicted by Einstein's theory of relativity. Experience from working with diverse scientific domains - astronomy, bioinformatics, climate modeling, earthquake science, gravitational and material science - uncover opportunities for further automation of scientific workflows. This project addresses these opportunities through innovation in the following areas: automation methods to include resource provisioning ahead of and during workflow execution, data-aware job scheduling algorithms, and data sharing mechanisms in high-throughput environments. To support a broader group of ""long-tail"" scientists, effort is devoted to usability improvements as well as outreach, education, and training activities. The proposed work includes the implementation and evaluation of advanced frameworks, algorithms, and methods that enhance the power of automation in support of data-intensive science. These enhancements are delivers as dependable software tools integrated with Pegasus so that they can be evaluated in the context of real-life applications and computing environments. The data-aware focus targets new classes of applications executing in high-throughput and high-performance environments. Pegasus has been adopted by researchers from a broad spectrum of disciplines that range from gravitational-wave physics to bioinformatics, and from earth science to material science. It provides and enhances access to national CI such as OSG and XSEDE, and as part of this work it will be deployed within Chameleon and Jetstream to provide broader access to NSF's CI investments. Through usability improvements, engagement with CI and community platform providers such as HubZero and Cyverse, combined with educational, training, and tutorial activities, this project broadens the set of researchers that leverage automation for their work. Collaboration with the Gateways Institute assures that Pegasus interfaces are suitable for vertical integration within science gateways and seamlessly supports new scientific communities."
"1550300","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","05/25/2022","Abhijit Majumder","MI","Wayne State University","Continuing Grant","Bogdan Mihaila","06/30/2023","$2,241,626.00","Loren Schwiebert, Joern Putschke, Ron Soltz","abhijit.majumder@wayne.edu","5057 WOODWARD STE 13001","DETROIT","MI","482024050","3135772424","CSE","1253, 7244, 8004","026Z, 7433, 7569, 8009, 8084","$0.00","Microseconds after the Big Bang, the universe was filled with an extremely hot fluid called the Quark-Gluon Plasma. As the universe expanded, this plasma cooled and condensed into the building blocks of ordinary matter around us: protons, neutrons, and atomic nuclei. Droplets of this fluid, which exists only at temperatures above 2 trillion Kelvin, are generated and studied in the laboratory today using collisions of high-energy heavy ions at Brookhaven National Laboratory and CERN. A key method to study the Quark-Gluon Plasma is the generation of high-energy quarks and gluons in the collision, which interact with the hot plasma and emerge as ""jets"" of particles that are measured by experiments. These jets provide powerful tools to study the internal structure of the plasma, analogous to tomography in medical imaging. However, interpretation of jet measurements requires sophisticated numerical modeling and simulation, and comparison of theory calculations with experimental data demands advanced statistical tools. The JETSCAPE Collaboration, an interdisciplinary team of physicists, computer scientists, and statisticians, will develop a comprehensive software framework that will provide a systematic, rigorous approach to meet this challenge. Training programs, workshops, summer schools and MOOCs, will disseminate the expertise needed to modify and maintain this framework.<br/><br/>The JETSCAPE Collaboration will develop a scalable and portable open source software package to replace a variety of existing codes. The modular integrated software framework will consist of interacting generators to simulate (i) wave functions of the incoming nuclei, (ii) viscous fluid dynamical evolution of the hot plasma, and (iii) transport and modification of jets in the plasma. Integrated advanced statistical analysis tools will provide non-expert users with quantitative methods to validate novel theoretical descriptions of jet modification, by comparison with the complete set of current experimental data. To improve the efficiency of this computationally intensive task, the collaboration will develop trainable emulators that can accurately predict experimental observables by interpolation between full model runs, and employ accelerators such as Graphics Processing Units (GPUs) for both the fluid dynamical simulations and the modification of jets. The collaboration will create this framework with a user-friendly envelope that allows for continuous modifications, updates and improvements of each of its components. The effort will serve as a template for other fields that involve complex dynamical modeling and comparison with large data sets. It will open a new era for high-precision extraction of the internal structure of the Quark-Gluon Plasma with quantifiable uncertainties. <br/><br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. <br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences."
"1339801","SI2-SSI: Collaborative Research: STORM: A Scalable Toolkit for an Open Community Supporting Near Realtime High Resolution Coastal Modeling","OAC","OFFICE OF MULTIDISCIPLINARY AC, PHYSICAL OCEANOGRAPHY, Special Projects - CNS, Special Projects - CCF, Software Institutes, CDS&E-MSS, EarthCube","10/01/2014","02/27/2015","Clinton Dawson","TX","University of Texas at Austin","Standard Grant","Bogdan Mihaila","09/30/2019","$540,012.00","Craig Michoski","clint@ices.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","CSE","1253, 1610, 1714, 2878, 8004, 8069, 8074","7433, 8009, 8251","$0.00","The ADCIRC coastal circulation and storm surge model has a long standing track record of extremely high societal impact. It has been used to define risk (e.g., 100-yr, 500-yr coastal flood levels) for the FEMA National Flood Insurance Program in coastal states from New York to Texas, it has been used to design the multi-billion dollar Hurricane and Storm Damage Risk Reduction System around greater New Orleans and southern Louisiana by the Army Corps of Engineers, it is currently run operationally by NOAA/NWS National Center for Environmental Prediction to forecast storm surge, to name just a few of its current and recent applications. Thus there is a well-established user network in place to convert improvements in ADCIRC into significant broader impacts. The proposed research provides transformative intellectual contributions that focus on applying new parallelization schemes to enable major advances in the algorithms, implementation and utilization of the ADCIRC model. The broadening of ADCIRC to a multi-algorithmic framework and the resulting performance gains that are anticipated will help ensure ADCIRC's sustainability as a core community model for at least the next 20 years. In addition, the proposed collaboration will impact computer science by serving as a high impact use case to inform the design of new approaches to efficient scalable computing. Together, the advancements in coastal modeling and parallelization technology will make a significant contribution to the science of modeling and HPC. The results of the proposed research will be disseminated to a wider community through ongoing educational outreach activities at the participating organizations as well as through refereed conference and journal papers, and invited presentations. The involvement of graduate students and post-doctoral fellows will be crucial towards the success of this project. The PIs have a long history of training and mentoring students and post-docs in computational science and engineering, coastal engineering and marine science. The recruitment and involvement of underrepresented groups in these efforts has always been a high priority. In addition, aspects of the proposed research will be incorporated into the curricula of several courses taught by the PIs in the areas of finite element methods, scientific computation, hydrology and oceanography.<br/>The aim of this project is to broaden the ADCIRC coastal circulation and storm surge model from a successful, but somewhat static coastal modeling tool that is tied to a single solution algorithm and the MPI parallelization paradigm, to a dynamic computational platform that is comprised of multiple solution algorithms, that readily admits new solution algorithms and that is built on a transformational new parallelization scheme that will allow us to scale to at least 256k compute cores on modern high performance computing (HPC) systems. We will do this by creating a living, evolving coastal modeling framework that will continue to lead the community in merging physical science / engineering and high performance computing and we will make the framework available to the broader community as a sustainable long term solution for its coastal modeling needs. In addition we will utilize these advancements in the highly demanding coastal storm surge forecasting system that we presently operate to demonstrate both improved robustness and speed of the model solution. We expect this effort will shorten the time required to provide reliable forecasting results and improve our ability to provide highly resolved, accurate, and physically complete predictions on an unprecedented scale. Concurrently, it should enable the use of smaller resources for simulations of increased scale which improves the usability and widens the applicability of ADCIRC in a broader community. The development of tightly integrated web-oriented products like CERA (www.coastalemergency.org) will enable the wide and timely dissemination of forecast modeling results to reach a broad audience."
"1550350","Collaborative Research: Si2-SSI: Adding Volunteer Computing to the Research Cyberinfrastructure","OAC","Software Institutes","08/01/2016","08/02/2016","Lucas Wilson","TX","University of Texas at Austin","Standard Grant","Rajiv Ramnath","07/31/2017","$120,374.00","","jnet@tacc.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","CSE","8004","7433, 8004, 8009","$0.00","The aggregate computing power of consumer devices - desktop and laptop computers, tablets, smartphones - far exceeds that of institutional computing resources.  ""Volunteer computing"" uses these consumer devices, volunteered by their owners, to do scientific computing. In addition to providing additional, much-needed computational resources to scientists, volunteer computing publicizes scientific research and engages citizens in science. BOINC is the primary software system for volunteer computing.  It was developed at UC Berkeley with NSF support starting in 2002. Until now, BOINC has been based on a model of independent competing projects.  Scientists set up their own BOINC servers, port their applications to run on BOINC, and publicize their projects to attract volunteers.  There are about 40 such projects, in many areas of science: examples include Einstein@home, CERN, and SETI@home (astrophysics), Rosetta@home and GPUGrid.net (biomedicine), Climateprediction.net (climate study), and IBM World Community Grid (multiple applications).  Together these projects have about 400,000 active volunteers and 12 PetaFLOPS of computing throughput. This model, while successful to an extent, has reached a limit.  The number of projects and volunteers has stagnated.  Volunteer computing is supplying lots of computing power, but only to a few research projects.  For other scientists, there are two major barriers.  First, creating a BOINC project has significant overhead: learning a new technology, creating a public web site, generating publicity, and so on.  Second, volunteer computing is risky and uncertain; there is no guarantee that a new project will attract volunteers. This project aims to break this barrier, and to make volunteer computing available to all scientists doing high-throughput computing, by replacing the competing-projects model with a new ""central broker"" model. The new model has two related parts: 1) the integration of BOINC with existing high-throughput computing facilities such as supercomputing centers and science portals. Jobs currently run on cluster nodes will be transparently offloaded to volunteer computers. Scientists using these facilities will see faster turnaround times; they'll benefit from volunteer computing without even knowing it's there. 2) The project will change the volunteer interface so that participants sign up for scientific areas and goals rather then for particular projects. For example, a participant might sign up to contribute to cancer research. A central broker, to be developed as part of this project, would dynamically assign their computing resources to projects doing that type of research. This project mobilizes public support for and interest in scientific research by encouraging ""volunteer computing"" and engaging citizens in the conduct of the research itself. It simultaneously advances NSF's mission to advance science while broadening citizen engagement.<br/><br/>The first year of this project will prototype each of these parts, and will integrate BOINC with TACC and nanoHub. Integrating BOINC with existing HTC systems involves several subtasks: 1) Job routing: modifying existing job processing systems used by TACC and nanoHub (Launcher and Rappture respectively) to decide when a group of jobs should be offloaded to BOINC. This decision might involve the estimated runtime of the jobs, input and output file sizes, data sensitivity, the deadline or priority of the jobs, and the identity of the job submitter. 2) Job format conversion: mapping job descriptions (input/output file specifications, resource and timing requirements) to their BOINC equivalents. 3) Application packaging: adapting existing applications (such as nanoHub's simulation tools and TACC's Autodock) to run under BOINC. We will use BOINC's virtual machine facility, which packages an application as a virtual machine image (VirtualBox or Docker) and a program to be run within the VM.  This allows existing Linux applications to run on consumer desktop platforms such as Windows and Mac, as well as providing a strong security sandbox and an efficient application-independent checkpoint/restart mechanism. 4) File handling: moving input and output files between existing storage systems (typically inaccessible from outside firewalls) to Internet-visible servers.  This will use existing BOINC components that manage files based on hashes to eliminate duplicate transfer and storage of files. 5) Job monitoring and control: adapting existing web- or command-line based tools for monitoring the progress of batches of jobs, and for aborting jobs, to work with BOINC. This will use existing Web RPCs provided by BOINC for these purposes. This project will carry out these tasks by designing and implementing new software as needed, testing for correctness, performance, and scalability, and deploying it in a production environment. The second part of the project - a brokering system for allocating computing power based on volunteer scientific preferences - will be designed and prototyped.  This involves several subtasks: 1) Designing a schema for volunteer preferences, including scientific areas and sub-areas, project nationality and institutions, specific projects and applications, inclusions/exclusions, and so on. 2) Designing a schema for assigning attributes to job streams (e.g. their area, sub-area, institution, etc.), and for assigning quotas or priorities to job streams. 3) Designing a relational database for storing the above information. 4) Designing and implementing policies for assigning volunteer resources to job streams in a way that respects volunteer preferences and optimizes quota, fairness, and throughput criteria.  This will be implemented as a BOINC ""account manager"" so that volunteers see a single interface rather than lots of separate projects and web sites."
"1339863","Collaborative Research: SI2-SSI: A Comprehensive Ray Tracing Framework for Visualization in Distributed-Memory Parallel Environments","OAC","Software Institutes","10/01/2013","04/17/2017","Paul Navratil","TX","University of Texas at Austin","Standard Grant","Rob Beverly","09/30/2018","$1,398,111.00","","pnav@tacc.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","CSE","8004","7433, 8004, 8009","$0.00","Scientific visualization plays a large role in exploring the scientific simulations that run on supercomputers; new discoveries are often made by studying renderings generated through visualization of simulation results. The standard technique for rendering geometry is rasterization and the most commonly used library for performing this is OpenGL. Many visualization programs (VisIt, Ensight, VAPOR, ParaView, VTK) use OpenGL for rendering. However, recent architectural changes on supercomputers create significant opportunities for alternate rendering techniques. The computational power available on emerging many-core architectures, such as the Intel Xeon Phi processors on TACC?s Stampede system, enable ray-tracing, a higher quality technique. Further, as the amount of geometry per node rises, ray-tracing becomes increasingly cost effective, since its computational costs are proportional to the screen size, not the geometry size. Finally, the software implementation for OpenGL can not be easily mapped to non-GPU multi-core and many-core systems, creating a significant gap; if not closed, visualization will not be possible directly on large supercomputers. This confluence of new, more capable architectures, the increase in geometry per node, and concerns about the durability of the established rendering path all motivate this   work. <br/><br/>To address these trends, this research uses a two-pronged approach. First, the research will replace the OpenGL pathways that are commonly used for visualization with a high-performance, open-source ray tracing engine that can interactively render on both a CPU and on accelerator architectures. This new library will support the OpenGL API and will be usable immediately by any OpenGL-based visualization package without additional code modi&#64257;cation. Second, this research will provide a direct interface to a high-performance distributed ray tracing engine so that applications can take advantage of ray tracing capabilities not easily exposed through the standard OpenGL interface, such as participating media and global illumination simulation. These features will enable the open science community to easily create photo-realistic imagery with natural lighting cues to aid in analysis and discovery. It will further expand the capacity of existing cyberinfrastructure to provide interactive visualization on standard HPC resources. <br/><br/>This work has the potential to revolutionize in situ visualization capabilities by unifying the (potentially hybrid) architecture that efficiently run both simulation and visualization. Communicating with underrepresented groups will be a major component of outreach efforts through the PCARP, MITE and Women in Engineering programs. In addition, the project team will disseminate this work to the general public through NSF XD program, the VisIt visualization toolkit and by exhibiting at forums such as IEEE Visualization, IEEE High Performance Graphics and ACM Supercomputing."
"1664198","SI2-SSI: Lightweight Infrastructure for Land Atmosphere Coupling (LILAC): A Tool for Easy Integration of the Community Land Model into Multiple Modeling Systems","OAC","Software Institutes, CR, Earth System Models, EarthCube","10/01/2017","09/15/2017","A Denning","CO","Colorado State University","Standard Grant","Rob Beverly","09/30/2022","$1,600,000.00","L. Darrell Whitley","denning@atmos.colostate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","CSE","8004, 8012, 8074","026Z, 4444, 7433, 8004, 8009","$0.00","Everyone on Earth lives on the ground, so interactions between the atmosphere and the land surface are a critically important part of the climate system and understanding these interactions is important for weather prediction, agriculture, and urban water management. Earth System Models (ESMs) are complex software systems representing complex natural systems. They are critical tools for diagnosing, understanding, and predicting interactions and change in the atmosphere, oceans, and land ecosystems. This project will develop a new software system for coupling the land and atmosphere components of Earth System models, specifically for the most widely-used climate model in the world: the Community Earth System Model (CESM). The new system will be capable of simulating climates near the ground including exchanges of heat, water ,and carbon between vegetated land and the air as well as streamflow and soil moisture. As an officially-supported component of CESM, it will be used by thousands of scientists and students around the world. Unlike its predecessor, the new system will be able to simulate small areas at high resolution for important applications and testing. The project will also support a computer science graduate student as well as academics and scientists who will help develop and test the software. The PIs will engage a global community of software developers and users through a series of workshops and webinars as well as through professional societies and publications. The PIs recognize the chronic under-representation of women and ethnic minorities in both Computer Science and Atmospheric Science. To address this, they will host a computer science summer camp for middle school students from underrepresented groups (URGs), and close collaboration with existing climate courses for K-12 teachers, science outreach in K-12 schools, and a highly successful REU-Site operated by the PI. This project will dramatically improve the usability of the most widely-used climate model in existence. Hundreds of developers and thousands of users around the world will benefit from the far greater flexibility and use cases for this model. The Community Land Model will be coupled to a much greater range of atmospheric codes for weather prediction, air quality applications, and climate projections that enhance the quality of life for people everywhere. <br/>    <br/><br/><br/>The Community Earth System Model CESM) is a uniquely open ESM with a distributed community of hundreds of developers and thousands of users around the world. Compared to other ESMs, CESM is the ""Linux of climate models."" The land component of CESM, the Community Land Model (CLM), has its own vibrant and diverse user and development community, which has supported the construction of a particularly comprehensive terrestrial system model. It includes a rich array of processes that enable examination of the physical, biological, and chemical processes by which natural terrestrial ecosystems and human-managed land affect and are affected by climate and weather. CESM can only be run globally, but there is widespread interest in coupling CLM to alternative high-resolution atmosphere models to study the challenging scientific problems that exist at the interface between small and large spatial scales. A barrier to this research, however, is that CLM cannot readily be coupled to alternative atmosphere models. The CLM coupling interface only supports communication with the CESM coupling infrastructure (CPL7), which imposes strict requirements on how an atmospheric component can communicate with CLM. One key requirement is for the atmosphere model to complete a full time step before coupling can occur. This requirement necessitates significant refactoring of many atmospheric models in order for them to couple to CLM via CPL7. In addition, the tools to build and configure CLM are currently difficult to use outside of the CESM context. For all of these reasons, CLM has never been coupled to alternative atmospheric models in a sustainable fashion. Colorado State University (CSU), in partnership with the National Center for Atmospheric Research (NCAR), proposes to develop a Lightweight Infrastructure for Land-Atmosphere Coupling (LILAC) that will significantly simplify the coupling of CLM to alternative atmospheric models. The LILAC coupler and the associated proposed streamlining and simplification of the CLM tool chain will be developed and tested with a prototype high-resolution atmosphere model, the CSU SAM Cloud Resolving Model, and will be extended for use with any arbitrary Target Atmosphere Model. The development of LILAC and associated tools will enable numerous groups to couple CLM to their atmospheric models, and to quickly update to new state-of-the-art CLM model versions as they become available. This will open up new avenues of land-atmosphere research that can exploit the combined biogeophysical and biogeochemical capabilities of CLM with the strengths of high-resolution atmosphere models. It will enable research into land-atmosphere interactions across a variety of scales, ranging from turbulence-resolving simulations of tower and aircraft data to cloud-resolving simulations to study how small-scale land features such as hillslopes, valleys, lakes, rivers, urban areas and farms conspire to alter surface climate and atmospheric boundary layer characteristics to continental-scale simulations of the impact of land cover and land use change on weather and climate.<br/><br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering and the Office of Polar Programs and Division of Atmospheric and Geospace Science in the Directorate for Geosciences."
"1339782","SI2-SSI: Collaborative Research: STORM: A Scalable Toolkit for an Open Community Supporting Near Realtime High Resolution Coastal Modeling","OAC","PHYSICAL OCEANOGRAPHY, Special Projects - CNS, Special Projects - CCF, Software Institutes, EarthCube","10/01/2014","08/26/2014","Hartmut Kaiser","LA","Louisiana State University","Standard Grant","Bogdan Mihaila","09/30/2019","$970,835.00","Robert Twilley","hkaiser@cct.lsu.edu","202 HIMES HALL","BATON ROUGE","LA","708030001","2255782760","CSE","1610, 1714, 2878, 8004, 8074","7433, 8009, 9150","$0.00","The ADCIRC coastal circulation and storm surge model has a long standing track record of extremely high societal impact. It has been used to define risk (e.g., 100-yr, 500-yr coastal flood levels) for the FEMA National Flood Insurance Program in coastal states from New York to Texas, it has been used to design the multi-billion dollar Hurricane and Storm Damage Risk Reduction System around greater New Orleans and southern Louisiana by the Army Corps of Engineers, it is currently run operationally by NOAA/NWS National Center for Environmental Prediction to forecast storm surge, to name just a few of its current and recent applications. Thus there is a well-established user network in place to convert improvements in ADCIRC into significant broader impacts. The proposed research provides transformative intellectual contributions that focus on applying new parallelization schemes to enable major advances in the algorithms, implementation and utilization of the ADCIRC model. The broadening of ADCIRC to a multi-algorithmic framework and the resulting performance gains that are anticipated will help ensure ADCIRC's sustainability as a core community model for at least the next 20 years. In addition, the proposed collaboration will impact computer science by serving as a high impact use case to inform the design of new approaches to efficient scalable computing. Together, the advancements in coastal modeling and parallelization technology will make a significant contribution to the science of modeling and HPC. The results of the proposed research will be disseminated to a wider community through ongoing educational outreach activities at the participating organizations as well as through refereed conference and journal papers, and invited presentations. The involvement of graduate students and post-doctoral fellows will be crucial towards the success of this project. The PIs have a long history of training and mentoring students and post-docs in computational science and engineering, coastal engineering and marine science. The recruitment and involvement of underrepresented groups in these efforts has always been a high priority. In addition, aspects of the proposed research will be incorporated into the curricula of several courses taught by the PIs in the areas of finite element methods, scientific computation, hydrology and oceanography.<br/>The aim of this project is to broaden the ADCIRC coastal circulation and storm surge model from a successful, but somewhat static coastal modeling tool that is tied to a single solution algorithm and the MPI parallelization paradigm, to a dynamic computational platform that is comprised of multiple solution algorithms, that readily admits new solution algorithms and that is built on a transformational new parallelization scheme that will allow us to scale to at least 256k compute cores on modern high performance computing (HPC) systems. We will do this by creating a living, evolving coastal modeling framework that will continue to lead the community in merging physical science / engineering and high performance computing and we will make the framework available to the broader community as a sustainable long term solution for its coastal modeling needs. In addition we will utilize these advancements in the highly demanding coastal storm surge forecasting system that we presently operate to demonstrate both improved robustness and speed of the model solution. We expect this effort will shorten the time required to provide reliable forecasting results and improve our ability to provide highly resolved, accurate, and physically complete predictions on an unprecedented scale. Concurrently, it should enable the use of smaller resources for simulations of increased scale which improves the usability and widens the applicability of ADCIRC in a broader community. The development of tightly integrated web-oriented products like CERA (www.coastalemergency.org) will enable the wide and timely dissemination of forecast modeling results to reach a broad audience."
"1339763","SI2-SSE: Collaborative Research: ADAPT: Next Generation Message Passing Interface (MPI) Library - Open MPI","OAC","Software Institutes","09/01/2013","08/29/2013","Edgar Gabriel","TX","University of Houston","Standard Grant","Rajiv Ramnath","08/31/2016","$147,236.00","","gabriel@cs.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","CSE","8004","7433, 8005","$0.00","High-performance computing has reshaped science and industry in many areas. However, the rapid evolution at the hardware level over the last few years have been unmatched by corresponding changes at the programming paradigm level. According to the consensus of several major studies, the degree of parallelism on large systems is expected to increase by several orders of magnitude. As a result, the Message Passing Interface (MPI), which has been the de-facto standard message passing paradigm, lacks an efficient and portable way of handling today's architectures. To efficiently handle such systems, MPI implementations must adopt more asynchronous and thread-friendly behaviors to perform better than they do on today?s systems. Maintaining and further enhancing MPI, one of the most widely-used communication libraries in high-performance computing, will have a far-reaching impact beyond the scientific community, and represents a critical building block for continued advances in all areas of science and engineering.<br/>The ADAPT project enhances, hardens and modernizes the Open MPI library in the context of this ongoing revolution in processor architecture and system design. It creates a viable foundation for a new generation of Open MPI components, enabling the rapid exploration of new physical capabilities, providing greatly improved performance portability, and working toward full interoperability between classes of components. More specifically, ADAPT implements fundamental software techniques that can be used in many-core systems to efficiently execute MPI-based applications and to tolerate fail-stop process failures, at scales ranging from current large systems to the extreme scale systems that are coming soon. To improve the efficiency of Open MPI, ADAPT integrates, as a core component, knowledge about the hardware architecture, and allows all layers of the software stack full access to this information. Process placement, distributed topologies, file accesses, point-to-point and collective communications can then adapt to such topological information, providing more portability. The ADAPT team is also updating the current collective communication layer to allow for a task-based collective description contained at a group-level, which in turn adjusts to the intra and inter-node topology. Planned expansion of the current code with resilient capabilities allows Open MPI to efficiently survive hard and soft error types of failures. These capabilities can be used as building blocks for all currently active fault tolerance proposals in the MPI standard body.<br/>MPI is already one of the most relevant parallel programming models, the most important brick of most parallel applications, and one of the most critical communication pieces of most other programing models. Thus, the experience of the research team and emerging capabilities can benefit all future users of these programming standards, tools, and libraries--regardless of discipline. Any improvement in the performance and capabilities of a major MPI library such as Open MPI, has tremendous potential for an immediate and dramatic impact on the application communities. In addition to improving the time to solution for their applications, it has the potential to decrease the energy usage and maximize the performance delivered by the existing execution platforms. The scale at which the Open MPI library is used in government research institutions (including universities and national laboratories), as well as in the private sector, is a major vector for a quick impact on all scientific and engineering communities."
"1550436","SI2-SSI: Collaborative Research: Einstein Toolkit Community Integration and Data Exploration","OAC","COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","07/08/2019","Manuela Campanelli","NY","Rochester Institute of Tech","Continuing Grant","Amy Walton","06/30/2021","$437,521.00","Joshua Faber, Yosef Zlochower","manuela@astro.rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","7244, 8004","7433, 7569, 8009, 8084","$0.00","A new astronomy has arrived with the recent detection of gravitational waves. Modeling of sources of gravitational radiation is more than ever a critical necessity in order to interpret the observations. The project Einstein Toolkit has as overarching mission to provide the scientific community with a sustainable software platform of core computational tools for research focused on astrophysical systems endowed with complex multi-scale/multi-physics properties which are governed by Einstein's equations of General Relativity. The central premise of the project Einstein Toolkit is to create a broad and vibrant community of users, a community where interdisciplinary collaborations are the norm and not the exception, a community driving advances in the next generation of high-performance computing cyberinfrastructure. The main objectives of the project Einstein Toolkit are: developing software tools for a radical increase in scientific productivity, achieving sustainability of the software ecosystem, addressing software engineering challenges, and the curation of data from general relativistic numerical simulations. <br/><br/>This project will achieve its goals through two major activity areas. Regarding the software ecosystem and its sustainability, the scheduler that handles the flow of tasks in a problem will be redesigned to be more versatile and to improve its performance. In addition new software modules will be developed to broaden the choices of  initial data and matter sources, as well as modules for problems requiring a high degree of experimentation with equations and numerical methodologies.  A new general relativistic magneto-hydrodynamics code will also be integrated in the Einstein Toolkit. The second activity area involves building  a simulation data repository. The repository will allows user to compare results, contribute data,  test  innovative ideas and algorithms for gravitational wave data analysis, and to explore or discover new phenomena in sources of gravitational radiation. The broader impact effort in the project Einstein Toolkit will be organized in two major activity areas. The first involves community integration. The project will support a program of ease-of-use on-line tutorials and a workshops/tutorial series. The program will  help small groups or individual investigators familiarizing with the codes and modules in the toolkit as well as pathways to become a developer. Regarding outreach and education, the project Einstein Toolkit will enable interdisciplinary training of students and postdocs in numerical relativity, computational astrophysics and computer science. The effort will includes developing a teaching resources bank for educational activities involving computational topics applied to gravitational physics and astrophysics. The educational resources will be suitable for computational courses in general relativity and astrophysics at both the graduate and undergraduate level.<br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division in the Directorate of Mathematical and Physical Sciences."
"1740095","SI2-SSE: TLDS: Transactional Lock-Free Data Structures","OAC","CI REUSE, Software Institutes","09/01/2017","12/08/2021","Damian Dechev","FL","The University of Central Florida Board of Trustees","Standard Grant","Seung-Jong Park","08/31/2022","$427,472.00","","dechev@cs.ucf.edu","4000 CENTRAL FLORIDA BLVD","ORLANDO","FL","328168005","4078230387","CSE","6892, 8004","7433, 8004, 8005","$0.00","Exploiting the parallelism in multiprocessor systems is a major challenge in computer science. Multicore programming demands a change in the way we design and use fundamental data structures. Non-blocking data structures allow un-constrained access to shared data; however, care must be taken so that data is not overwritten incorrectly. A main obstacle to the design and use of non-blocking data structures is the lack of a generic methodology for allowing efficient transactional and composable operations. This project, supported by the Office of Advanced Cyberinfrastructure, and the Division of Computing and Communication Foundations, will explore and define the fundamental techniques for the design and use of transactional multiprocessor algorithms. The software elements produced in this work will significantly improve the functionality and reuse of the existing concurrent containers and will accelerate the performance of multiprocessor applications beyond what is possible with current programming libraries. The programming elements created in this project will be disseminated through the open-source release of a software library. The project will also help contribute to a workforce trained in systems programming.<br/><br/>To achieve the project's goals, the PI will explore solutions to overcome two key challenges for supporting high-performance data structure transactions: 1) how to efficiently buffer write operations so that their modifications are invisible to operations outside the transaction's scope, and 2) how to minimize the penalty of rollbacks when aborting partially executed transactions. A representative collection of six transactional lock-free containers will be created including: a linked-list, a set, a skiplist, a multi-dimensional list, a priority queue, and a dictionary. This work will substantially advance the knowledge and practice in multiprocessor software design. To the best of the PI's knowledge, lock-free transactional transformation is the first methodology that provides both lock-free progress and semantic conflict detection for data structure transactions. To achieve this, the PI will create new techniques for conflict detection and recovery in the execution of transactions. A node-based conflict detection scheme that does not rely on Software Transactional Memory nor require the use of an additional data structure will be introduced. A new and efficient conflict recovery strategy will be designed, based on the interpretation of the logical status of nodes instead of explicitly revoking executed operations in an aborted transaction. This research will introduce the first approach for the efficient execution of composable non- blocking transactions across multiple data structures. The software components and data structures engineered as part of this work will be significant research artifacts themselves."
"1664061","Collaborative Research: SI2-SSI: Cyberinfrastructure for Advancing Hydrologic Knowledge through Collaborative Integration of Data Science, Modeling and Analysis","OAC","Hydrologic Sciences, Special Initiatives, EAR-Earth Sciences Research, Software Institutes, EarthCube","10/01/2017","08/23/2018","David Tarboton","UT","Utah State University","Standard Grant","Seung-Jong Park","09/30/2022","$2,809,998.00","Martyn Clark, Alva Couch, Daniel Ames, Jeffery Horsburgh","davidtarboton@usu.edu","1000 OLD MAIN HILL","LOGAN","UT","843221000","4357971226","CSE","1579, 1642, 6898, 8004, 8074","026Z, 7433, 7556, 8004, 8009","$0.00","Researchers across the country and around the world expend tremendous resources to gather and analyze vast stores of hydrologic data and populate a myriad of models to better understand hydrologic phenomena and find solutions to vexing water problems. Each of those researchers has limited money, time, computational capacity, data storage, and ability to put that data to productive use. What if they could combine their efforts to make collaboration easier? What if those collected data sets and processed model outputs could be used collaboratively to help advance hydrologic understanding beyond their original purpose? HydroShare is a system to advance hydrologic science by enabling the scientific community to more easily and freely share products resulting from their research, not just the scientific publication summarizing a study, but also the data and models used to create the scientific publication. HydroShare supports the sharing and publication of hydrologic data and models. This capability is necessary for community model development, execution, and evaluation and to improve reproducibility and community trust in scientific findings through transparency. As a platform for collaboration and running models on advanced computational infrastructure, HydroShare enhances the capability for data intensive research in hydrology and other aligned sciences. HydroShare is designed to help researchers easily meet the sharing requirements of data management plans while at the same time providing value added functionality that makes metadata capture more effective and helps researchers improve their work productivity. This project will extend the capabilities of the HydroShare cyberinfrastructure to enhance support for scientific methods, advance the social capabilities of HydroShare to enable improved collaborative research, integrate with 3rd party consumer data storage systems to provide more flexible and sustainable data storage. and establish an application testing environment to empower researchers to develop their own computer programs to act on and work with data in HydroShare. Empowering HydroShare users with the ability to rapidly develop web application programs opens the door to unforeseen, innovative combinations of data and models. WRF-Hydro, the framework for the NOAA National Water Model, will be used as a use case for collaboration on model development. Since WRF-Hydro is used by NOAA as part of the National Water Model (NWM), this collaboration opens possibilities for transfer of research to operations. Collectively, this functionality will provide a computing framework for transforming the practice of broad science communities to leverage advances in data science and computation and accelerate discovery.<br/><br/>HydroShare is a system for sharing hydrologic data and models aimed at giving hydrologists the cyberinfrastructure needed to manage data, innovate and collaborate in research to solve water problems. It addresses the challenges of sharing data and hydrologic models to support collaboration and reproducible hydrologic science through the publication of hydrologic data and models. With HydroShare users can: (1) share data and models with colleagues; (2) manage who has access to shared content; (3) share, access, visualize and manipulate a broad set of hydrologic data types and models; (4) use the web services interface to program automated and client access; (5) publish data and models to meet the requirements of research project data management plans; (6) discover and access data and models published by others; and (7) use web apps to visualize, analyze, and run models on data. This project will extend the capabilities of HydroShare to: (1) enhance support for scientific methods enabling systematic data and model analysis and hypothesis testing; (2) advance the social capabilities of HydroShare to enable improved collaborative research; (3) integrate with 3rd party consumer data storage systems to provide more flexible and sustainable data storage; and (4) establish an application testing environment to empower researchers to develop their own computer programs to act on and work with data in HydroShare. Under development since 2012 and first released in 2014, HydroShare supports the sharing and publication of hydrologic data and models. This capability is necessary for community model development, execution, and evaluation. As a platform for collaboration and cloud based computation on network servers remote from the user, HydroShare enhances the capability for data intensive research in hydrology and other aligned sciences. HydroShare is innovative from a computer science and CI perspective in the way computation and data sharing are framed as a network computing platform that integrates data storage, organization, discovery, and programmable actions through web applications (web apps). Support for these three key elements of computation allows researchers to easily employ services beyond the desktop to make data storage and manipulation more reliable and scalable, while improving ability to collaborate and reproduce results. The generation of new understanding, through integration of information from multiple sources and reuse and collaborative enrichment of research data and models, will be enhanced. Structured and systematic model process intercomparisons and alternative hypothesis testing will be enabled, bringing, through user friendly CI, the latest thinking in advancing hydrologic modeling to a broad community of earth science researchers, thereby transforming research practices and the knowledge generated from this research. Interoperability with consumer cloud storage will greatly ease entry of content into HydroShare and support its sustainability. This meshing of the rigorous metadata model of HydroShare with consumer file sharing will enhance reproducibility as well as provide an innovative mechanism for sharing and collaboration. Empowering HydroShare users with the ability to rapidly develop web apps opens the door to unforeseen, innovative combinations of data and models. WRF- Hydro will be used as a use case for collaboration on model development. WRF-Hydro provides a reach-based high resolution representation of hydrologic processes, and offers the potential to bring together scientists working at scales from research catchments on the order of 1 to 100s of square kilometers as well as those working at regional to continental scales and cut across disciplines from environmental engineering to aquatic ecologists. Since WRF-Hydro is used by NOAA as part of the National Water Model (NWM), this collaboration opens possibilities for transfer of research to operations. This project will adapt current best practices in CI for interoperability and extensibility to serve this multidisciplinary community of scientists. HydroShare has already had a broader impact, with documented rapid growth in use and uptake by other projects including in EarthCube. It will become sustainable community CI through operation as part of the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI) Water Data Center (WDC) facility. The use of WRF- Hydro/NWM, as a driving use case, will advance CI for community based model improvement. Through the Summer Young Innovators Program at the National Water Center (NWC), supported by the National Weather Service (NWS) and operated by CUAHSI, a pathway already exists to translate research findings to the operational needs of federal agencies participating in the NWC. HydroShare already touches a broad and diverse community, with user base including Native American tribes, hydrologic science students, and faculty researchers across the U.S. This proposal builds on the success of HydroShare to extend its capabilities and broaden model hypothesis testing, collaborative data sharing, and open app development across earth science research and education."
"1440443","SI2-SSE: A Next-Generation Open-Source Computational Fluid Dynamic Code for Polydisperse Multiphase Flows in Science and Engineering","OAC","FD-Fluid Dynamics, Software Institutes","10/01/2014","08/21/2014","Alberto Passalacqua","IA","Iowa State University","Standard Grant","Bogdan Mihaila","09/30/2019","$499,551.00","Simanta Mitra, Rodney Fox","albertop@iastate.edu","515 MORRILL RD, 1350 BEARDSHEAR","AMES","IA","500112105","5152945225","CSE","1443, 8004","058E, 1443, 7433, 8005, 9150","$0.00","Many processes for the production of drugs, fuels and plastic materials, as well as energy from coal or biomasses, involve multiphase flows, which are composed by a combination of a fluid, either liquid or gas, and particles, droplets or bubbles. This type of flow is also naturally present in the environment. Examples are the formation of a mixture of air and solid particles due to volcanic eruptions, and particles of sand and other materials transported by the wind. Scientists and engineers use software to study how these flows behave in order to improve the yield of industrial processes, reduce their environmental impact, and energy consumption. The computer programs used to perform these studies solve complex mathematical problems, and require powerful computers to be able to obtain the results in a useful time. This project focuses on developing the next generation of computer software for the simulation of multiphase flows, enabling it to use the latest generation of computers which combine traditional and graphical processors for improved performance. This software will be released to the public and will enable, scientists and engineers from different research areas to tackle real-world problems by taking advantage of the latest developments in multiphase flow science, combined with the benefit of being able to use the software on powerful computer infrastructures. Students and educators will be able to use the software and learn about multiphase flows through the examples and the documentation that will be provided.<br/><br/>The objectives of the project will be achieved by first developing computational models to describe turbulent flows in the framework of quadrature-based moment methods, an efficient and accurate approach to describe this type of flows. These computational models will then be implemented, together with appropriate numerical methods that will ensure the accuracy of the computational codes, in the open-source framework OpenFOAM. Three representative problems of typical multiphase flows will be considered: a population balance equation for particles with negligible inertia, such as in the formation of nanoparticles in a fluid flow; the description of gas-liquid flows, where the bubble inertia is small but not zero; and the most complex case of gas-solid flows, where particle inertia is large. The possibility of using graphical processing units will be added to the OpenFOAM framework, to enable it to run on hybrid computational systems involving traditional processors and graphical processing units. The source code and its documentation will be made available to the public at an early stage of their development, under the GNU GPL 3 license, in order to disseminate the results of the research and gather feedback. Detailed code documentation, verification and validation cases, and tutorials will also be created to favor external contributions to the software."
"1663636","SI2-SSI: Sustainable Open-Source Quantum Dynamics and Spectroscopy Software","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","09/01/2017","05/08/2017","Xiaosong Li","WA","University of Washington","Standard Grant","Rob Beverly","08/31/2021","$1,500,000.00","Anne McCoy, Eitan Geva, Albert DePrince","xsli@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","1253, 8004","7433, 8004, 8009","$0.00","Advances in experimental techniques that use multiple light sources to probe chemical systems   provide unprecedented ability to understand the evolution of important and interesting chemical processes.   Theory and simulation are essential to extract the maximum amount of information from these experiments.   Thus, there is a strong need for user-friendly computer programs to model these processes and to provide physical insights to experimentalists.  To this end, this project is developing an innovative software package (ChronusQ) capable of modeling many types of experimental measurements that involve matter interacting with multiple incident light sources. The physical insights gleaned through application of ChronusQ  will be useful for the advancement of renewable energy and information technologies. In addition, given the nature of the project, students and postdocs will have unique opportunities to gain experience in high performance computing software development through direct collaborations with engineers from industrial partnerships <br/><br/>The overarching goal of the proposed activity is to develop an innovative software platform, namely Chronus Quantum (ChronusQ), which is capable of modeling different types of time resolved multidimensional spectral signals using quantum electronic and nuclear dynamics. ChronusQ performs quantum dynamic simulations of the same light and matter interactions that occur in time resolved multidimensional spectroscopies directly in the time domain.  The time correlated experimental observables required to model multidimensional spectra can then be extracted from these simulations. By providing a time dependent, state specific interpretation for the chemical dynamics encoded in multidimensional spectra, the proposed development will aid in the design of new molecules and materials that exhibit the desirable optical characteristics, with the potential for transformative impact in the broader scientific community and beyond. The ChronusQ software represents the next frontier for innovations in computational spectroscopy, which will have a far reaching impact on education and research in multidisciplinary scientific communities including chemistry, physics, nanoscience and surface science, and other fields relying on these cutting edge spectroscopic methods."
"1664119","Collaborative Research: SI2-SSI:  Cyberinfrastructure for Advancing Hydrologic Knowledge through Collaborative Integration of Data Science, Modeling and Analysis","OAC","Special Initiatives, EAR-Earth Sciences Research, XC-Crosscutting Activities Pro, Software Institutes, EarthCube","10/01/2017","09/14/2017","Shaowen Wang","IL","University of Illinois at Urbana-Champaign","Standard Grant","Seung-Jong Park","09/30/2022","$699,999.00","","shaowen@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","1642, 6898, 7222, 8004, 8074","026Z, 7433, 8004, 8009","$0.00","Researchers across the country and around the world expend tremendous resources to gather and analyze vast stores of hydrologic data and populate a myriad of models to better understand hydrologic phenomena and find solutions to vexing water problems. Each of those researchers has limited money, time, computational capacity, data storage, and ability to put that data to productive use. What if they could combine their efforts to make collaboration easier? What if those collected data sets and processed model outputs could be used collaboratively to help advance hydrologic understanding beyond their original purpose? HydroShare is a system to advance hydrologic science by enabling the scientific community to more easily and freely share products resulting from their research, not just the scientific publication summarizing a study, but also the data and models used to create the scientific publication. HydroShare supports the sharing and publication of hydrologic data and models. This capability is necessary for community model development, execution, and evaluation and to improve reproducibility and community trust in scientific findings through transparency. As a platform for collaboration and running models on advanced computational infrastructure, HydroShare enhances the capability for data intensive research in hydrology and other aligned sciences. HydroShare is designed to help researchers easily meet the sharing requirements of data management plans while at the same time providing value added functionality that makes metadata capture more effective and helps researchers improve their work productivity. This project will extend the capabilities of the HydroShare cyberinfrastructure to enhance support for scientific methods, advance the social capabilities of HydroShare to enable improved collaborative research, integrate with 3rd party consumer data storage systems to provide more flexible and sustainable data storage. and establish an application testing environment to empower researchers to develop their own computer programs to act on and work with data in HydroShare. Empowering HydroShare users with the ability to rapidly develop web application programs opens the door to unforeseen, innovative combinations of data and models. WRF-Hydro, the framework for the NOAA National Water Model, will be used as a use case for collaboration on model development. Since WRF-Hydro is used by NOAA as part of the National Water Model (NWM), this collaboration opens possibilities for transfer of research to operations. Collectively, this functionality will provide a computing framework for transforming the practice of broad science communities to leverage advances in data science and computation and accelerate discovery.<br/><br/>HydroShare is a system for sharing hydrologic data and models aimed at giving hydrologists the cyberinfrastructure needed to manage data, innovate and collaborate in research to solve water problems. It addresses the challenges of sharing data and hydrologic models to support collaboration and reproducible hydrologic science through the publication of hydrologic data and models. With HydroShare users can: (1) share data and models with colleagues; (2) manage who has access to shared content; (3) share, access, visualize and manipulate a broad set of hydrologic data types and models; (4) use the web services interface to program automated and client access; (5) publish data and models to meet the requirements of research project data management plans; (6) discover and access data and models published by others; and (7) use web apps to visualize, analyze, and run models on data. This project will extend the capabilities of HydroShare to: (1) enhance support for scientific methods enabling systematic data and model analysis and hypothesis testing; (2) advance the social capabilities of HydroShare to enable improved collaborative research; (3) integrate with 3rd party consumer data storage systems to provide more flexible and sustainable data storage; and (4) establish an application testing environment to empower researchers to develop their own computer programs to act on and work with data in HydroShare. Under development since 2012 and first released in 2014, HydroShare supports the sharing and publication of hydrologic data and models. This capability is necessary for community model development, execution, and evaluation. As a platform for collaboration and cloud based computation on network servers remote from the user, HydroShare enhances the capability for data intensive research in hydrology and other aligned sciences. HydroShare is innovative from a computer science and CI perspective in the way computation and data sharing are framed as a network computing platform that integrates data storage, organization, discovery, and programmable actions through web applications (web apps). Support for these three key elements of computation allows researchers to easily employ services beyond the desktop to make data storage and manipulation more reliable and scalable, while improving ability to collaborate and reproduce results. The generation of new understanding, through integration of information from multiple sources and reuse and collaborative enrichment of research data and models, will be enhanced. Structured and systematic model process intercomparisons and alternative hypothesis testing will be enabled, bringing, through user friendly CI, the latest thinking in advancing hydrologic modeling to a broad community of earth science researchers, thereby transforming research practices and the knowledge generated from this research. Interoperability with consumer cloud storage will greatly ease entry of content into HydroShare and support its sustainability. This meshing of the rigorous metadata model of HydroShare with consumer file sharing will enhance reproducibility as well as provide an innovative mechanism for sharing and collaboration. Empowering HydroShare users with the ability to rapidly develop web apps opens the door to unforeseen, innovative combinations of data and models. WRF- Hydro will be used as a use case for collaboration on model development. WRF-Hydro provides a reach-based high resolution representation of hydrologic processes, and offers the potential to bring together scientists working at scales from research catchments on the order of 1 to 100s of square kilometers as well as those working at regional to continental scales and cut across disciplines from environmental engineering to aquatic ecologists. Since WRF-Hydro is used by NOAA as part of the National Water Model (NWM), this collaboration opens possibilities for transfer of research to operations. This project will adapt current best practices in CI for interoperability and extensibility to serve this multidisciplinary community of scientists. HydroShare has already had a broader impact, with documented rapid growth in use and uptake by other projects including in EarthCube. It will become sustainable community CI through operation as part of the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI) Water Data Center (WDC) facility. The use of WRF- Hydro/NWM, as a driving use case, will advance CI for community based model improvement. Through the Summer Young Innovators Program at the National Water Center (NWC), supported by the National Weather Service (NWS) and operated by CUAHSI, a pathway already exists to translate research findings to the operational needs of federal agencies participating in the NWC. HydroShare already touches a broad and diverse community, with user base including Native American tribes, hydrologic science students, and faculty researchers across the U.S. This proposal builds on the success of HydroShare to extend its capabilities and broaden model hypothesis testing, collaborative data sharing, and open app development across earth science research and education."
"1550488","SI2-SSI: Integrating the NIMBLE Statistical Algorithm Platform with Advanced Computational Tools and Analysis Workflows","OAC","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, Software Institutes","10/01/2016","02/13/2017","Perry de Valpine","CA","University of California-Berkeley","Standard Grant","Rob Beverly","09/30/2022","$999,709.00","Duncan Temple Lang, Christopher Paciorek, Benjamin Shaby","pdevalpine@berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","CSE","1253, 1269, 8004","4444, 7433, 8004, 8009, 8251","$0.00","The software developed in this project will enable scientists to learn more from complex data and to share new analysis methods more easily.  Increasingly, scientists in many fields aim to draw sound conclusions from large and complex data sets.  Such fields include environmental biology, political science, education research, atmospheric and oceanic science, climate science, and many others.  Data may be complex because many related variables are measured and/or because some measurements are not independent from each other.  Non-independence can arise when some variables are measured repeatedly through time; or when measurements are made at nearby locations; or when measurements are made on groups of related individuals; or for a combination of those and other similar reasons.  For such cases, general statistical methods have been developed to allow researchers to tailor their analysis to each data set in order to account for the relationships among the data.  Such methods rely on computer algorithms to explore the range of possible conclusions given the uncertainties inherent in limited data. Within those general methods there are many varieties of specific approaches that have been and continue to be developed.  Thus, a major software gap has emerged: Many new and evolving methods are not easily available for application by a wide range of scientists because there has not been a software framework that makes them easy to program and disseminate.  This project will support continued development of the NIMBLE software to help fill that gap.   As a result, scientists will be able to use computational analysis methods more flexibly, to combine and compare different algorithms more easily, to integrate such algorithms into other software workflows, and to gain better computational performance.  This will enable more advanced and more routine use of some modern computational methods for analyzing complex data.<br/><br/>The existing NIMBLE framework for hierarchical statistical models and algorithms comprises a model specification language, a language for programming model-generic algorithms within the R statistical environment, and a compiler that generates, compiles and interfaces to model- and algorithm-specific C++ for efficient execution.  These enable general implementation and dissemination of methods such as Markov chain Monte Carlo, sequential Monte Carlo, and many related methods. In this project NIMBLE will be extended and generalized to be more powerful and flexible, enabling use in a variety of software workflows.  Extensions to NIMBLE's core capabilities will include harnessing automatic differentiation and parallelization in generated C++, enhancements to its existing linear algebra capabilities, more efficient implementation of large statistical models including those with structural uncertainty such as latent group membership, and extensions to the statistical modeling language.  Enhancements to facilitate integration of NIMBLE-generated models and algorithms with other software will include generation of stand-alone executables, generation of clearly defined application-programmer interfaces such as for use by Python, features to call user-provided libraries from algorithm code, features to load and save data via standard formats such as JSON and NetCDF, and separation of NIMBLE components into distinct packages. The project will include substantial outreach, training, and user community development.  These activities will include development of uses cases in fields such as population and ecosystem ecology, oceanography, climate science, political science, and education. They will also include workshops, user meetings, key-user visits, and training material. This award by the Advanced Cyberinfrastructure Division is jointly supported by the NSF Directorate for Mathematical and Physical Sciences (Division of Mathematical Sciences)."
"1740251","SI2-SSE: Software for Semiconductor and Electrochemical Interfaces (SSEI)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2017","09/11/2017","Richard Hennig","FL","University of Florida","Standard Grant","Rob Beverly","09/30/2022","$322,051.00","","rhennig@ufl.edu","1523 UNION RD RM 207","GAINESVILLE","FL","326111941","3523923516","CSE","1712, 8004","026Z, 054Z, 6863, 7433, 8004, 8005, 8396, 8399, 9216","$0.00","The importance of materials interfaces is apparent when considering key industrial segments such as the microelectronics, chemical, energy, and biomedical industries. Interfaces and surfaces often control the properties of materials and the interface structure and chemistry are one of the most important, yet least understood, aspects of materials synthesis and functionalization. Materials interfaces come in a broad variety, ranging from solid/solid interfaces in semiconductor hetero-structures to solid/liquid interfaces in electrochemistry and biomaterials, and solid/vapor interfaces in chemical vapor deposition and free-standing 2D materials. Despite their diversity, all interfaces present a common set of challenges for computational studies. This project will develop a sustainable software package that enables ab initio simulations of materials interfaces in various environments that extends the current state of the art by adding functionality and increasing performance. The software tools will impact the development and the design of novel materials that broadly benefit society in the fields of catalysis, electrochemistry, battery technologies, and electronic devices.<br/><br/><br/>The goal of this project is to develop a comprehensive and sustainable Software for Semiconductor and Electrochemical Interfaces (SSEI) that provides various continuum models and couples them to ab initio simulations. The SSEI package will enable an efficient and accurate description of a wide variety of materials interfaces that are important for application in energy technologies, corrosion research, electronic devices, and 2D materials. The SSEI is based on the VASPsol module developed by the PI and will be developed primarily for direct use as a module in VASP. For future extension of the software to other DFT codes, the project will provide a portable software interface and detailed documentation. The software design goals are to develop SSEI into (i) a sustainable scientific tool, (ii) significantly extend its functionality to nonlinear models and arbitrary electrostatic boundary conditions, and (iii) increase its performance. These complementary goals will be achieved through an expansion of the developer and user base, a transition to portable software interfaces and data structures, and the addition of modular algorithms for functionality and performance enhancements. All developments will make extensive use of object-oriented programming principles and software design patterns to speed up the development process and aid maintainability. The SSEI package provides the predictive tools for materials interfaces that have the potential to transform the use of ab initio methods to advance our fundamental knowledge and enable the design and optimization of materials interfaces for better catalysts, battery electrodes, electronic junctions, and corrosion resistance.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences."
"1663893","Collaborative Research:  SI2-SSI: Inquiry-Focused Volumetric Data Analysis Across Scientific Domains: Sustaining and Expanding the yt Community","OAC","Software Institutes","10/01/2017","07/28/2017","Benjamin Holtzman","NY","Columbia University","Standard Grant","Seung-Jong Park","09/30/2022","$261,310.00","","benh@ldeo.columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","CSE","8004","7433, 8004, 8009","$0.00","Scientific discovery across the physical sciences is increasingly dependent on the analysis of volumetric - or three-dimensional - data, that may come from a supercomputer simulation, direct measurement, or mathematical models. Researchers typically seek to extract meaningful insights from this data by visualizing and analyzing it in various ways. The ways in which scientists process volumetric data are actually quite similar across domains, but cross-disciplinary knowledge transfer and tool development is blocked by barriers of terminology. This project seeks to enhance an analysis and visualization toolkit named yt that is currently primarily used for astrophysical simulations. yt allows scientists to access and analyze data at several different levels by providing an interface that is designed to answer questions motivated by the underlying scientific problem, while worrying less about details such data formats, specific analysis techniques etc. yt's utilization in computational astrophysics has dramatically increased access to advanced algorithms for both visualization and analysis, and fostered the growth of a community of researchers sharing techniques and results. This project seeks to make yt available and adopted by scientists in other domains of science thus reproducing its success in astrophysics in these other science domains. This project will expand the yt community beyond theoretical astrophysics and enable and promote collaboration and advanced data analysis in the fields of meteorology, seismology and global tomography, observational astronomy, hydrology and oceanography, and plasma physics. <br/><br/>Improvements to the yt project will proceed along four principal technical avenues. The first is to develop a system that adapts the way yt presents data via a set of domain contexts that encode the ontology, domain-specific vocabulary, and common analysis tasks for a given field of study. This will include creating a domain context system as well as a set of five pilot domain contexts developed in collaboration with domain practitioners. The second is to overhaul the yt field system, adding more versatility and enabling significant optimizations. Thirdly, the project team will implement non-spatial indexing schemes, providing methods for accessing and analyzing data that is not organized according to the standard spatial axes. The final improvement will be the development of a non-local analysis system, allowing generalized path traversal as well as domain convolutions. To ensure wide dissemination and use of these improved capabilities, the team will design domain-specific documentation and training materials, and organize outreach and training events for early-career researchers. This will consist of both hands-on technical workshops and curricula developed in collaboration with Data Carpentry for utilization at other institutions. This combination of technical developments and social investments has been designed to ensure both readiness of the software and engagement of the targeted research communities."
"1740315","SI2-SSE: MetPy - A Python GEMPAK Replacement for Meteorological Data Analysis","OAC","Physical & Dynamic Meteorology, Software Institutes, EarthCube","09/01/2017","08/29/2017","Ryan May","CO","University Corporation For Atmospheric Res","Standard Grant","Seung-Jong Park","08/31/2021","$499,693.00","Kevin Goebbert, John Leeman","rmay@ucar.edu","3090 CENTER GREEN DR","BOULDER","CO","803012252","3034971000","CSE","1525, 8004, 8074","4444, 7433, 8004, 8005","$0.00","The MetPy project aims to make atmospheric science research and teaching easier and more reproducible by providing a set of well-tested and modern software tools. Meteorologists require many specialized calculations and maps in order to understand the weather and make reliable predictions. The tools they use must provide correct results, since lives and property depend on accurate forecasts and research. This project will port the bulk of the functionality from a widely used and trusted -- but aging and minimally supported -- software program called GEMPAK (the GEneral Meteorological PAcKage) into MetPy, developed using the Python programming language, and with a well-designed, new software architecture. Python has been selected as the language of choice because it has become very popular in many scientific communities. MetPy will be the meteorological community's entry into this growing scientific software ecosystem. In addition to making GEMPAK's functionality available in MetPy, this project will implement a better user-interface, which will help students and researchers get started more easily. The software team will use software development best practices in its development of MetPy, and ensure that it can work with all common meteorological data sources. Every relevant aspect of MetPy will be documented in an easy to digest way on the MetPy project webpage. The development team will work with university instructors to help revise their course materials to integrate MetPy. In addition, the team will teach MetPy and Python training workshops each year, allowing university professors, students, and professionals to get hands-on training on how to do their research in a faster and more robust way. <br/><br/>This project seeks to fill a need within the atmospheric science community by bringing key functional elements of a foundational software program, GEMPAK, to the innovation-rich Python ecosystem. By devoting software development resources to increasing the number of data types and file formats MetPy can work with, improving the underlying data model, and reaching feature parity with GEMPAK, MetPy can be positioned as a community-supported replacement for the older package. This effort leverages the entire Python ecosystem, and supports the movement (already well under way) of the atmospheric science community to Python-driven reproducible workflows. This transition will provide a number of community benefits. By bringing needed functionality from GEMPAK to the Python ecosystem, this project will allow atmospheric scientists to: simplify the process of exploratory analysis, have a cross-platform toolchain that can be carried from the classroom to the workforce, simplify the research workflow to make science easier and more reproducible, provide a tested library of domain-specific calculations with literature references, and create publication-quality data visualizations. Educators and researchers will be able to replace their use of legacy software, which is no longer being developed and is increasingly hard to maintain, with a modern toolkit that allows increased flexibility and reproducibility within atmospheric science research. Sustainability of the atmospheric science software workflow will be enhanced by the inclusion of modern automated software build-and-test tools, robust community-supported documentation and learning materials, and the ability to quickly incorporate new sources of environmental data. Finally, modernizing the atmospheric science toolchain opens the door to the use of innovations like web-based tools (Jupyter notebooks, for example) that would be difficult or impossible to take advantage of when using legacy software.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1642388","SI2-SSE: Collaborative Research: Extending the Practicality and Scalability of LibMesh-Based Unstructured, Adaptive Finite Element Computations","OAC","Software Institutes, CDS&E","09/01/2016","12/03/2021","Paul Bauman","NY","SUNY at Buffalo","Standard Grant","Seung-Jong Park","08/31/2022","$350,065.00","","pbauman@buffalo.edu","520 LEE ENTRANCE STE 211","AMHERST","NY","142282577","7166452634","CSE","8004, 8084","026Z, 7433, 8004, 8005, 9263","$0.00","The development and deployment of cyberinfrastructure focused on scientific and engineering simulation has been, and continues to be, essential to the progress of science and engineering in the U.S. This is particularly true for software used in large scale supercomputing environments. Thus, for the U.S. to continue leadership and advancement in scientific computing, it is crucial that software infrastructure advance to enable modern computational and software engineering strategies for simulating complex scientific and engineering systems. Once such piece of software is the libMesh finite element library. libMesh is used by hundreds of research groups in the U.S. and around the world. Critically, libMesh can utilize large scale supercomputing infrastructure for simulating scientific and engineering systems. This work will update the libMesh software library to use state-of-the-art algorithms that will enable robust simulations on the largest supercomputers in the world and further advance the complexity of systems that can be successfully modeled using libMesh. Furthermore, the library will be enhanced to support user applications to leverage modern computer architectures, including emerging many-core architectures. This will enable the continued use of libMesh as both a fundamental tool of scientific and engineering simulation and as an educational tool for computational algorithms.<br/><br/>The libMesh finite element library is a prominent example of an open-source tool supporting adaptive mesh refinement, interfaces to preeminent solver packages, and solutions on large parallel supercomputers of complex finite element models. libMesh supports hundreds of users and many applications in solving partial differential equations across a variety of disciplines including solid mechanics, fluids mechanics, magnetohydrodynamics, hypersonics, nuclear engineering, combustion, and acoustics, to name a few examples. Following over a decade of successful collaborative open-source development, the library is poised to maintain its place as a prominent open-source finite element package. To do so, libMesh must be made to support emerging many core architectures, leverage the most advanced scalable algorithms, and interface with geometry underlying the complex meshes used in engineering analysis. The work addresses these issues directly by extending and enhancing the libMesh finite element library. The extensions will seamlessly make available modern solution algorithms through interfaces to world class solver libraries, facilitate the interaction with underlying geometric representations using openly available software libraries, and efficiently utilize modern computing hardware through cutting-edge software engineering principles and designs. Simultaneously, the developed interfaces will allow for flexibility of development of modeling kernels and maintain the low the barrier of entry that libMesh has always had for both the libMesh community as well as the scientific community in general. Such lofty goals will be attained by designing usable interfaces that hide the complexity of the underlying algorithms and extensive testing on modern computing architectures to ensure performance and scalability is delivered to the libMesh community."
"1450179","Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","08/01/2015","06/24/2015","Todd Martinez","CA","Stanford University","Standard Grant","Stefan Robila","07/31/2019","$600,000.00","","Todd.Martinez@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","CSE","1253, 8004","7433, 8009","$0.00","Computational chemistry has become ubiquitous throughout the fields of science and engineering. Among the many uses of computational chemistry codes are to aid in the design of new materials with specific properties, to understand protein folding dynamics related to the human body, to understand the mechanisms of enzyme catalysis to produce biofuels, and to identify and characterize gaseous species in the atmosphere. Because computational chemistry simulations can require large amounts of computer, memory and disk space, one focus of this research is to reduce the demands on computer resources by exploiting methods that can efficiently fragment large molecules into smaller pieces and at the same time take advantage of computers that have many thousands of computer cores. The principal investigators are developers of several computational chemistry programs, each of which has unique functionalities that have taken multiple person-years to develop and implement. In order to minimize further development efforts and to maximize the utility of the unique features, a second key focus of this research will be to develop the ability of each program to access the unique features and data of the other programs. A major bottleneck in the effort to construct high performance computers with more and more compute cores is the power that is required to drive such systems. One way that the research team will address the power issue is to explore the utility of low power architectures, such as graphical processing units for computational chemistry calculations. All of the newly developed codes will be made available to the user community by web download at no cost.<br/><br/>This project presents an integrated computational science approach to very high quality electronic structure and dynamics calculations that will (a) be broadly accessible to both the development and applications communities, (b) have the capability to address problems of great interest, such as the properties of liquids and solvent effects, and photochemical/photobiological dynamics with high accuracy, (c) provide interoperability and sustainability into the foreseeable future, and (d) solve bottlenecks including power consumption using accelerators. A primary goal is to provide to the broad community new, accurate approaches that may be easily used, with a clear path forward to further code improvement and development. As part of the proposed effort, in addition to the usual outlets of journal articles and public presentations, the investigators will organize workshops at prominent national meetings, so that the expertise and software developed will be available to as broad a group of users as possible.  All of the developed codes will be available on the web for easy downloads. The proposed research will develop new paradigms for interoperability, with a focus on the highly popular program suites GAMESS, NWChem, PSI4  and the AIMS dynamics code. Also included will be a cloud-based client-server model, a common quantum chemistry driver and novel data management approaches. The integration of the codes will be accomplished by making use of the combined expertise of the PIs in developing interoperable methods and data interfaces in computational science. In addition, several new methods will be developed, including novel explicit (R12) correlation methods that will be integrated with the most accurate levels of theory: multi-reference and the most accurate and novel coupled cluster methods, as well as the derivation and implementation of analytic derivatives. Applicability to large molecular systems will be made feasible by drawing upon novel fragmentation methods that scale nearly perfectly to the petascale."
"1740102","Collaborative Research: SI2:SSE: Extending the Physics Reach of LHCb in Run 3 Using Machine Learning in the Real-Time Data Ingestion and Reduction System","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","09/01/2017","08/24/2017","Michael Sokoloff","OH","University of Cincinnati Main Campus","Standard Grant","Seung-Jong Park","08/31/2021","$224,621.00","","mike.sokoloff@uc.edu","2600 CLIFTON AVE","CINCINNATI","OH","452202872","5135564358","CSE","1253, 7244, 8004","7433, 8004, 8005","$0.00","In the past 200 years, physicists have discovered the basic constituents of ordinary matter and the developed a very successful theory to describe the interactions (forces) between them.  All atoms, and the molecules from which they are built, can be described in terms of these constituents.  The nuclei of atoms are bound together by strong nuclear interactions.  Their decays result from strong and weak nuclear interactions. Electromagnetic forces bind atoms together, and bind atoms into molecules. The electromagnetic, weak nuclear, and strong nuclear forces are described in terms of quantum field theories.  The predictions of these theories can be very, very precise, and they have been validated with equally precise experimental measurements.  Most recently, a new fundamental particle required to unify the weak and electromagnetic interactions, the Higgs boson, was discovered at the Large Hadron Collider (LHC), located at the CERN laboratory in Switzerland. Despite the vast amount of knowledge acquired over the past century about the fundamental particles and forces of nature, many important questions still remain unanswered. For example, most of the matter in the universe that interacts gravitationally does not have ordinary electromagnetic or nuclear interactions.  As it has only been observed via its gravitation interactions, it is called dark matter.  What is it?  Equally interesting, why is there so little anti-matter in the universe when the fundamental interactions we know describe matter and anti-matter as almost perfect mirror images of each other? The LHC was built to discover and study the Higgs boson and to search for answers to these questions. The first data-taking run (Run 1, 2010-2012) of the LHC was a huge success, producing over 1000 journal articles, highlighted by the discovery of the Higgs boson. The current LHC run (Run 2, 2015-present) has already produced many world-leading results; however, the most interesting questions remained unanswered. The LHCb experiment, located on the LHC at CERN, has unique potential to answer some of these questions. LHCb is searching for signals of dark matter produced in high-energy particle collisions at the LHC, and performing high-precision studies of rare processes that could reveal the existence of the as-yet-unknown forces that caused the matter/anti-matter imbalance observed in our universe. The primary goal of this project - supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences - is developing and deploying software utilizing Machine Learning (ML) that will enable the LHCb experiment to significantly improve its discovery potential in Run 3 (2021-2023). Specifically, the ML developed will greatly increase the sensitivity to many proposed types of dark matter and new forces by making it possible to much more efficiently identify and study potential signals -- using the finite computing resources available.  <br/><br/>The data sets collected by the LHC experiments are some of the largest in the world. For example, the sensor arrays of the LHCb experiment, on which both PIs work, produce about 100 terabytes of data per second, close to a zettabyte of data per year. Even after drastic data-reduction performed by custom-built read-out electronics, the data volume is still about 10 exabytes per year, comparable to the largest-scale industrial data sets. Such large data sets cannot be stored indefinitely; therefore, all high energy physics (HEP) experiments employ a data-reduction scheme executed in real time by a data-ingestion system - referred to as a trigger system in HEP - to decide whether each event is to be persisted for future analysis or permanently discarded. Trigger-system designs are dictated by the rate at which the sensors can be read out, the computational power of the data-ingestion system, and the available storage space for the data. The LHCb detector is being upgraded for Run 3 (2021-2023), when the trigger system will need to process 25 exabytes per year. Currently, only 0.3 of the 10 exabytes per year processed by the trigger are analyzed using high-level computing algorithms; the rest is discarded prior to this stage using simple algorithms executed on FPGAs. To process all the data on CPU farms, ML will be used to develop and deploy new trigger algorithms. The specific objectives of this proposal are to more fully characterize LHCb data using ML and build algorithms using these characterizations: to replace the most computationally expensive parts of the event pattern recognition; to increase the performance of the event-classification algorithms; and to reduce the number of bytes persisted per event without degrading physics performance. Many potential explanations for dark matter and the matter/anti-matter asymmetry of our universe are currently inaccessible due to trigger-system limitations. As HEP computing budgets are projected to be approximately flat moving forward, the LHCb trigger system must be redesigned for the experiment to realize its full potential. This redesign must go beyond scalable technical upgrades; radical new strategies are needed."
"1450327","SI2-SSI: Collaborative Research:  Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS, CDS&E","08/01/2015","06/09/2017","Boyce Griffith","NC","University of North Carolina at Chapel Hill","Standard Grant","Amy Walton","07/31/2021","$940,056.00","Robert O'Bara","boyceg@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","CSE","1253, 8004, 8069, 8084","7433, 8004, 8009, 8084, 9251","$0.00","Many biological and biomedical systems involve the interaction of a flexible structure and a fluid. These systems range from the writhing and coiling of DNA, to the beating and pumping of cilia and flagella, to the flow of blood in the body, to the locomotion of fish, insects, and birds. This project aims to develop advanced software infrastructure for performing dynamic computer simulations of such biological and biomedical systems. To facilitate the deployment of this software in a range of scientific and engineering applications, this project will develop new software capabilities in concert with new computer models that use the software. Specific application domains to be advanced in this project include models of aquatic locomotion that can be used to understand the neural control of movement and ultimately to develop new treatments for neurological pathologies such as spinal cord injuries, and models that simulate the interaction between the electrophysiology of the heart and the contractions of the heart that pump blood throughout the body, which could lead to improved approaches to treating heart disease. The software to be developed within the project is freely available online and is used by a number of independent research groups in a variety of scientific and engineering domains. It is being actively used in projects that model different aspects of cardiovascular dynamics, such as platelet aggregation and the dynamics of natural and prosthetic heart valves, and in projects that study other biological problems, including cancer dynamics, insect flight, aquatic locomotion, and the dynamics of phytoplankton. The software is also being applied to non-biological problems, including nanoscale models of colloidal suspensions and models of active particles. The improved methods and software to be developed in this project will thereby have a broad and sustained impact on a large number of ongoing research efforts in the biological and biomedical sciences and other scientific and engineering disciplines.<br/><br/>The immersed boundary (IB) method is a broadly applicable framework for modeling and simulating fluid-structure interaction (FSI). The IB method was introduced to model the fluid dynamics of heart valves, and subsequent development initially focused on simulating cardiac fluid dynamics. This methodology is broadly useful, however, and has been applied to a variety of problems in which a fluid flow interacts with immersed structures, including elastic bodies, bodies with known or prescribed deformational kinematics, and rigid bodies. Extensions of the IB method have also been developed to model electrophysiological systems and systems with chemically active structures. To improve the efficiency of the IB method, the PI has developed adaptive versions of the IB method that employ structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where needed. These methods have been implemented within the IBAMR software framework, which provides parallel implementations of the IB method and its extensions that leverage high-quality computational libraries including SAMRAI, PETSc, and libMesh. This project will further extend the IBAMR software by implementing modeling and discretization technologies required by the research applications of current and prospective users of the software, by developing improved solver infrastructure facilitated by the implementation of native support for structured AMR discretizations in the PETSc library, and by integrating with existing high-quality software tools for model development, deployment, and analysis. IBAMR is freely distributed online and is used within a number of independent research groups both to the further development of the IB method and also to its application to simulate diverse problems in fluid dynamics and FSI. By enhancing IBAMR, this project will also enhance the ability of these and other researchers to construct detailed models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations. This project will also develop general-purpose support for AMR discretizations in PETSc, a software library with thousands of active users, ~400 downloads per month, and numerous applications. The work of this project will help to grow the IBAMR user community of students and researchers by developing UI tools for building models, running simulations, and analyzing results. Students will be actively engaged in all aspects of the project, including code, method, and model development."
"1663747","SI2-SSI Collaborative Research: The SimCardio Open Ssource Multi-Physics Cardiac Modeling Package","OAC","FD-Fluid Dynamics, Engineering of Biomed Systems, BMMB-Biomech & Mechanobiology, Software Institutes, Smart and Connected Health, CDS&E","09/01/2017","05/04/2017","Shawn Shadden","CA","University of California-Berkeley","Standard Grant","Rob Beverly","08/31/2023","$943,832.00","","shadden@berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","CSE","1443, 5345, 7479, 8004, 8018, 8084","026Z, 028E, 7433, 7479, 8004, 8009, 9263","$0.00","Cardiovascular (CV) simulations have become a crucial component of fundamental research in surgical planning, device design, diagnosis, and disease mechanisms. The project team has previously developed SimVascular (www.simvascular.org), which is currently the only open source software package providing a complete pipeline from medical image data segmentation to patient specific blood flow simulation and analysis in arteries and veins. The SimCardio open source project will extend and enhance the functionality of SimVascular to the realm of heart modeling, providing the first fully integrated computer model of cardiac physiology and function. This will help basic science and medical researchers perform computer modeling in numerous diseases affecting heart function in children and adults. This computer modeling software will enable researchers to build models of the heart and vascular anatomy directly from medical imaging data, which can be used for personalized treatment planning and medical device design, ultimately leading to new treatments for patients with cardiovascular disease. <br/><br/>The SimCardio project will create a unique open source software package for multi-physics cardiac modeling and simulations. SimCardio will include a new multi-physics finite element solver with capabilities for large-deformation fluid-structure interaction (FSI) to capture ventricular contraction and heart valve dynamics, non-linear and visco-elastic material models, cardiac mechanics models of active heart contraction, and electrophysiology. This will be facilitated by sustainable software infrastructure bridging the cardiovascular fluid and solid mechanics communities. The project will provide a new user-interface for high-throughput construction of patient-specific cardiac and vascular models. SimCardio will broaden the applicability of SimVascular to problems including heart valves, heart failure, cardiomyopathy, aortic dissection, structural congenital heart defects and medical devices. To facilitate adoption, the project will publicly provide accompanying educational and training materials, and maintain a sustainable software ecosystem to increase the user community and ensure continued availability and evolution."
"1663684","Collaborative Research: SI2-SSI: Modules for Experiments in Stellar Astrophysics","OAC","STELLAR ASTRONOMY & ASTROPHYSC, OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","05/01/2017","04/27/2017","Francis Timmes","AZ","Arizona State University","Standard Grant","Rob Beverly","04/30/2023","$785,006.00","","fxt44@mac.com","660 S MILL AVE STE 312","TEMPE","AZ","852813670","4809655479","CSE","1215, 1253, 7244, 8004","1206, 7433, 7569, 8004, 8009, 8084","$0.00","Stars are the most commonly observed celestial objects, and remain at the forefront of astronomical research.  The goal of this project is to sustain the MESA software framework as a key piece of software infrastructure for the astronomy community while building new innovative scientific capabilities and educational networks. MESA (Modules for Experiments in Stellar Astrophysics) is a software instrument that solves the equations governing the evolution of stars.  The MESA project 1) has attracted over 900 registered users world-wide; 2) has over 10,000 downloads of the source code; 3) has received over 12,000 archived and searchable posts about community discussions of MESA; 4) has been cited about 1000 times and has a current citation rate of about 300/year; 5) papers that cite MESA have themselves generated over 10,000 citations; 6) provides a Software Development Kit to build MESA across a variety of platforms; 7) delivers an annual Summer School program that now has over 150 graduates; 8) hosts a web-portal for the astronomy community to share knowledge and tools; 9) offers a prototype of a cloud resource for education, MESA-Web.  These metrics provide evidence that MESA is becoming standard software for understanding evolving stars.  <br/><br/>This project supports the MESA software framework and its user community. MESA provides solvers for one-dimensional fully coupled structures and composition equations governing stellar evolution. It is based on an implicit finite volume scheme with adaptive mesh refinement and sophisticated time-step controls; state-of-the-art modules provide equation of state, opacity, nuclear reaction rates, element diffu- sion, boundary conditions, and changes to the mass of the star. MESA is an open source library that employs contemporary numerical approaches, supports shared memory parallelism based on OpenMP, and is designed with present and future multi-core and multi-thread architectures in mind. MESA combines composable, interoperable, robust, efficient, thread-safe numerical and physics modules for provenance-capable simulations of a wide range of stellar evolution scenarios ranging from giant planets to low mass single stars to massive star binaries. MESA?s domain of applicability continues to grow, with recent extensions enabling users to model oscillations, rota- tion, binary stars, and explosions. Recent technical innovations allow for user plugins and provide bit-for-bit consistent results across all supported platforms.  The products of the MESA project has driven and will continue to drive significant innovation in the stellar, gravitational wave, nuclear, exoplanet, galactic, and cosmological communities."
"1550514","SI2-SSI: Collaborative Research: Einstein Toolkit Community Integration and Data Exploration","OAC","COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","11/16/2020","Roland Haas","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Varun Chandola","06/30/2022","$450,000.00","Matthew Turk, Roland Haas","rhaas@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","7244, 8004","7433, 7569, 8009, 8084","$0.00","A new astronomy has arrived with the recent detection of gravitational waves. Modeling of sources of gravitational radiation is more than ever a critical necessity in order to interpret the observations. The project Einstein Toolkit has as overarching mission to provide the scientific community with a sustainable software platform of core computational tools for research focused on astrophysical systems endowed with complex multi-scale/multi-physics properties which are governed by Einstein's equations of General Relativity. The central premise of the project Einstein Toolkit is to create a broad and vibrant community of users, a community where interdisciplinary collaborations are the norm and not the exception, a community driving advances in the next generation of high-performance computing cyberinfrastructure. The main objectives of the project Einstein Toolkit are: developing software tools for a radical increase in scientific productivity, achieving sustainability of the software ecosystem, addressing software engineering challenges, and the curation of data from general relativistic numerical simulations. <br/><br/>This project will achieve its goals through two major activity areas. Regarding the software ecosystem and its sustainability, the scheduler that handles the flow of tasks in a problem will be redesigned to be more versatile and to improve its performance. In addition new software modules will be developed to broaden the choices of  initial data and matter sources, as well as modules for problems requiring a high degree of experimentation with equations and numerical methodologies.  A new general relativistic magneto-hydrodynamics code will also be integrated in the Einstein Toolkit. The second activity area involves building  a simulation data repository. The repository will allows user to compare results, contribute data,  test  innovative ideas and algorithms for gravitational wave data analysis, and to explore or discover new phenomena in sources of gravitational radiation. The broader impact effort in the project Einstein Toolkit will be organized in two major activity areas. The first involves community integration. The project will support a program of ease-of-use on-line tutorials and a workshops/tutorial series. The program will  help small groups or individual investigators familiarizing with the codes and modules in the toolkit as well as pathways to become a developer. Regarding outreach and education, the project Einstein Toolkit will enable interdisciplinary training of students and postdocs in numerical relativity, computational astrophysics and computer science. The effort will includes developing a teaching resources bank for educational activities involving computational topics applied to gravitational physics and astrophysics. The educational resources will be suitable for computational courses in general relativity and astrophysics at both the graduate and undergraduate level.<br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division in the Directorate of Mathematical and Physical Sciences."
"1339765","SI2-SSI: Collaborative Research: Building Sustainable Tools and Collaboration for Volcanic and Related Hazards","OAC","Petrology and Geochemistry, DEEP EARTH PROCESSES SECTION, Software Institutes, Front in Earth Sys Dynamics, EarthCube","10/01/2013","07/02/2018","Matthew Jones","NY","SUNY at Buffalo","Continuing Grant","Stefan Robila","09/30/2019","$1,416,491.00","Marcus Bursik, Abani Patra, Matthew Jones, Matthew Jones, Greg Valentine, Tevfik Kosar","jonesm@ccr.buffalo.edu","520 LEE ENTRANCE STE 211","AMHERST","NY","142282577","7166452634","CSE","1573, 7571, 8004, 8016, 8074","019Z, 7433, 8009, 9102, 9179","$0.00","This project is focused on creating and upgrading software infrastructure for a large community of scientists engaged in volcanology research and associated hazard analysis.  Specifically, the project will reengineer three widely used tools (TITAN2D - block and ash flows, TEPHRA and Puff - ash transport and dispersal) and develop support for workflows that use these tools to analyze risk from volcanic hazards. Reengineering will encompass modularization so researchers may easily experiment with different modeling approaches, incorporation of techniques to make the tools efficient on new computing architectures like GPUs and many-core chips. The workflows are intended to tackle the challenges of managing complex and often large data flows associated with these tools in validation processes and in probabilistic inference based on the outcomes of the modeling. The tools and workflows will be made available using the popular vhub.org platform. This project will help provide a standard well managed hardware/software platform and approaches to standardize the documentation associated with input data, source code, and output data. This will ensure that model calculations are reproducible.<br/><br/>The wider use of these high fidelity tools and their use in mitigating hazards is likely to have a significant effect on hazard analysis and management. The project will also engage in several major workshops and in training activities. Project personnel will also engage in the Earthcube initiative - popularizing computational methodologies, online access and dissemination mechanisms through the VHub platform."
"1740130","SI2-SSE: Entangled Quantum Dynamics in Closed and Open Systems, an Open Source Software Package for Quantum Simulator Development and Exploration of Synthetic Quantum Matter","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, COMPUTATIONAL PHYSICS, Software Institutes","09/01/2017","08/29/2017","Lincoln Carr","CO","Colorado School of Mines","Standard Grant","Rob Beverly","08/31/2022","$499,978.00","","lcarr@mines.edu","1500 ILLINOIS ST","GOLDEN","CO","804011887","3032733000","CSE","1253, 1712, 7244, 8004","026Z, 057Z, 7203, 7433, 7569, 8004, 8005, 8084, 9216","$0.00","On the way to universal quantum computing, quantum simulators, literally ""analog"" quantum computers, have already been a huge success and are fulfilling Feynman's original 1982 vision of quantum computing. Each such simulator requires a dedicated experimental platform, and costs on the order of several million dollars to build. Such experiments have many interacting parts often requiring a complex rearrangement and months of work in order to perform a specified quantum computation. A widely accessible and easy to use software tool to shortcut design considerations for quantum simulator experimentalists is much needed. This project will create such a tool. Initially, the project researchers will design and release on SourceForge an open source software package centered around 1D matrix product state (MPS) and matrix product density operator (MPDO) methods, for both closed and open quantum systems, which any experimentalist can download and easily use locally to design and benchmark their quantum simulator architecture of choice. The software elements will include (i) prebuilt generalized Ising, Hubbard, and other models, (ii) multi-legged ladders to extend into quasi-1D, (iii) different time propagation methods for short and long-range interactions, and (iv) supplemental exact diagonalization and quantum trajectory methods. Secondly, the project members will create an even simpler graphical version via a web interface in collaboration with the Science Gateways Community Institute which allows any experimentalist to run quick simulations and tests on local dedicated high-performance computing resources at the Colorado School of Mines in a secure and user-friendly format. Finally, the project team will develop a key new software element in terms of a series of increasingly accurate discretization schemes to model continuum quantum simulators and mesoscopic limits, as well as control systematic error in experiments.<br/><br/><br/>MPS/MPDO methods enable the treatment of outstanding quantum problems for design of new materials dubbed synthetic quantum matter, push the boundaries of quantum mechanics into strongly correlated physics, where particles and even quasiparticles lose meaning, and allow exploration of totally new realms of entangled dynamics. The software package proposed here will make these extraordinary capacities accessible to the over 150 quantum simulator experimental groups, as well as the many theoretical/computational groups investigating entangled quantum dynamics, thus greatly speeding up exploration of quantum simulator physics. The software package will provide a platform for exploring far-from-equilibrium open quantum system dynamical models, an important topic in the theoretical and computational physics community at present due to the new and expanded capacity offered by quantum simulators; and it will provide an educational venue for new graduate students entering research groups and faculty developing courses in computational physics and related areas. Broader impact activities include integration of computation across the Colorado School of Mines undergraduate physics curriculum as a model for departments across the country to achieve this AAPT/APS goal, e.g. redesigning the mathematical physics course to cover equal parts analytical and computational methods, including specific research skills for summer REUs and internships. Finally, graduate students will be trained in critical analysis for scientific problem solving and rigorous numerical techniques for high-performance computing, including open source science via a science gateway, an approach key to success in a number of arenas in society, from the materials genome initiative to the space program.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering and the Physics Division, Materials Research Division, and Office of Multidisciplinary Activities in the Directorate of Mathematical and Physical Sciences."
"1339774","Collaborative Research: SI2-SSI: Open Gateway Computing Environments Science Gateways Platform as a Service (OGCE SciGaP)","OAC","Information Technology Researc, Cross-BIO Activities, Software Institutes","10/01/2013","09/14/2016","Marlon Pierce","IN","Indiana University","Standard Grant","Bogdan Mihaila","09/30/2020","$2,519,986.00","Suresh Marru","marpierc@indiana.edu","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","CSE","1640, 7275, 8004","019Z, 7433, 8009, 9179","$0.00","Science Gateways are virtual environments that dramatically accelerate scientific discovery by enabling scientific communities to utilize distributed computational and data resources (that is, cyberinfrastructure). Successful Science Gateways provide access to sophisticated and powerful resources, while shielding their users from the resources' complexities. Given Science Gateways' demonstrated impact on progress in many scientific fields, it is important to remove barriers to the creation of new gateways and make it easier to sustain them. The Science Gateway Platform (SciGaP) project will create a set of hosted infrastructure services that can be easily adopted by gateway providers to build new gateways based on robust and reliable open source tools. The proposed work will transform the way Science Gateways are constructed by significantly lowering the development overhead for communities requiring access to cyberinfrastructure, and support the efficient utilization of shared resources.<br/><br/>SciGaP will transform access to large scale computing and data resources by reducing development time of new gateways and by accelerating scientific research for communities in need of access to large-scale resources. SciGaP's adherence to open community and open governance principles of the Apache Software Foundation will assure open source software access and open operation of its services. This will give all project stakeholders a voice in the software and will clear the proprietary fog that surrounds cyberinfrastructure services. The benefits of SciGaP services are not restricted to scientific fields, but can be used to accelerate progress in any field of endeavor that is limited by access to computational resources. SciGaP services will be usable by a community of any size, whether it is an individual, a lab group, a department, an institution, or an international community. SciGaP will help train a new generation of cyberinfrastructure developers in open source development, providing these early career developers with the ability to make publicly documented contributions to gateway software and to bridge the gap between academic and non-academic development."
"1450344","Collaborative Research: SI2-SSI: Task-Based Environment for Scientific Simulation at Extreme Scale (TESSE)","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, EDUCATION AND WORKFORCE, Software Institutes","05/15/2015","04/17/2017","Robert Harrison","NY","SUNY at Stony Brook","Standard Grant","Bogdan Mihaila","04/30/2019","$622,808.00","","robert.harrison@stonybrook.edu","W5510 FRANKS MELVILLE MEMORIAL L","STONY BROOK","NY","117940001","6316329949","CSE","1253, 1712, 7361, 8004","7433, 7556, 8004, 8009, 8084, 9216, 9251","$0.00","This project, TESSE, is developing a new-generation programming environment that uniquely address the needs of emerging computational models in chemistry and other fields, and allowing these models to reap the benefits of the computer hardware of tomorrow. The project thus is bringing closer the ability to predict properties of matter and design matter transformations of importance to the technological leadership and energy security of the US.  TESSE's impact has scientific, cyberinfrastructure, and interdisciplinary educational aspects.  TESSE helps carry widely used scientific applications (NWChem, MADNESS, MPQC, etc.) into a whole new generation of many-core and accelerated architectures, while at the same time dramatically improving programmer productivity. In terms of cyberinfrastructure, TESSE's PaRSEC runtime system delivers capabilities that many libraries and advanced applications will require for high performance on large scale and hybridized systems.<br/><br/>The goals of this project, Task-based Environment for Scientific Simulation at Extreme Scale (TESSE), are to design and demonstrate via substantial scientific simulations within chemistry and other disciplines a prototype software framework that provides a groundbreaking response to the twin problems of portable performance and programmer productivity for advanced scientific applications on emerging massively-parallel, hybrid, many-core systems. TESSE will create a viable foundation for a new generation of science codes, one which enables even more rapid exploration of new physical models, provides greatly enhanced performance portability through directed acyclic graph (DAG) scheduling and auto-tuned kernels, and works towards full interoperability between major chemistry packages through compatible runtimes and data structures. TESSE will mature to become a standard, widely available, community-based and broadly-applicable parallel programming environment complementing and rivaling MPI/OpenMP. This is needed due to the widely appreciated shortfalls of existing mainstream programming models and the already huge successes of the existing DAG-based runtimes that are the foundation of the next generation of NSF- and DOE-supported (Sca)LAPACK high-performance linear algebra libraries."
"1047871","SI2-SSI: REAL-TIME LARGE-SCALE PARALLEL INTELLIGENT CO2 DATA ASSIMILATION SYSTEM","OAC","ADVANCES IN BIO INFORMATICS, INTERFAC PROCESSES & THERMODYN, Integrat & Collab Ed & Rsearch, Software Institutes","09/15/2010","06/11/2012","Anna Michalak","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Daniel Katz","06/30/2013","$1,564,244.00","Clayton Scott, Xuanlong Nguyen, Vineet Yadav, Michael Cafarella","michalak@stanford.edu","503 THOMPSON STREET","ANN ARBOR","MI","481091340","7347636438","CSE","1165, 1414, 7699, 8004","1165, 1414, 7699","$0.00","Intellectual Merit: This proposal seeks to address this need by creating a state-of-the-art autonomous software platform for real-time integration of in-situ and satellite-based atmospheric CO2 measurements within a Data Assimilation (DA) system for producing estimates of global land and oceanic CO2 exchange at weekly to bi-weekly intervals. The proposed software infrastructure will be capable of autonomous processing of large volumes of data through a multi-stage pipeline, without the delays conventionally associated with such processing. Within the DA component, we will provide options for multiple DA algorithms for estimating global CO2 exchange. Users will, for the first time, have the capability to use these multiple methods as part of a single system for comparing estimates of CO2 exchange, and to obtain an improved understanding of the relative advantages of the various DA methods. As part of the analysis component of the software, we will build a carbon-climate surveillance system by drawing from a range of techniques in pattern recognition and high-dimensional statistical inference. This system will be able to detect and analyze localized variations in CO2 exchange within any user-specified spatio-temporal window. In addition, summaries of the CO2 exchange will be provided at annual and monthly temporal scales for continents and countries.<br/><br/>Broader Impacts: This software can be used by researchers and governmental institutions for evaluating both the natural components of the carbon cycle and anthropogenic carbon emissions, as well as in the design of new satellites for improved monitoring of CO2. All data and software will be publicly available and open-source development platforms will be used whenever possible.  The algorithm prototypes developed as part of this project will be used in undergraduate and graduate courses at the University of Michigan, and will be made available online for educators at other institutions. Finally, the project will train three graduate students, with a focus on developing their cross-disciplinary skills in the field of Earth science, statistics, computer science, and atmospheric science."
"1550475","Collaborative Research: SI2-SSI: Swift/E: Integrating Parallel Scripted Workflow into the Scientific Software Ecosystem","OAC","Software Institutes","10/01/2016","09/13/2016","Martin McCullagh","CO","Colorado State University","Standard Grant","Rob Beverly","09/30/2019","$50,000.00","","martin.mccullagh@okstate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","CSE","8004","7433, 8004, 8009, 9102","$0.00","Science and engineering research increasingly relies on repeated execution of a complex series of steps (i.e., workflows) to form hypotheses; conduct experiments; analyze results; and refine theory.   Computation is often essential throughout the workflow and in this case, software can improve productivity by managing the computational and data workflow.  Swift is one such open-source workflow system that has been developed and widely used in diverse areas ranging from materials simulations and climate modeling to neuroscience and genomics. This project extends the capabilities of Swift by integrating it with other software systems that enable collaboration, usability, maintainability, and productivity. The new ecosystem, Swift/E, will enable scientists and engineers to more productively create and run computational workflow campaigns of larger scale, and debug, execute, adapt, and disseminate them faster and easier than has been possible to date. These workflows embody and communicate the computational methods specific to each domain of scientific inquiry. Swift/E achieves community engagement and extensive productivity benefits for a large user community through an integrated program of research, education, and software dissemination. The project engages and serves science and engineering communities by creating patterns of practice for building and sharing reusable workflow libraries, and by training students, educators, and researchers in their use.  To advance the education of the next generation of computationally trained scientists, Swift/E powers a network of NSF-supported ""e-Labs"" that teach the concepts of collaborative parallel computational science at high school and undergraduate levels, reaching over a thousand students annually.<br/><br/>The open-source Swift/E ""ecosystem"" integrates Swift with several scientific software elements that play a major role in the national and global cyberinfrastructure of today. These elements are: Swift for the parallel scripting of scientific workflow; Globus for data cataloging, management, and high-speed wide-area transport; the Web-based Galaxy workflow portal for workflow composition, execution, and collaborative sharing; Jupyter for the interactive development, testing, debugging, and assembly of high level programming and workflow languages; Python and R for productively expressing high-level computational logic; and ""git"" and related tools and Web portals for revision control, code dissemination and sharing, and for the collaborative engagement of developers.  Swift's implicitly parallel programming language is minimal and compact.  Swift provides a facility for embedding other scripting languages (currently Python, R, Julia and Tcl) into its runtime environment.  This project merges newer extreme-scale ""Swift/T"" capabilities with the flexible and portable original ""Swift/K"" version to make the core Swift/E software element more powerful and flexible while lowering it?s ongoing support cost. Swift/E enhances usability by extending Swift's troubleshooting and inter-language integration facilities.  And with enhanced and innovative workflow sharing archives, new training materials, and a sustained program for user support and self-sustaining and expanding community engagement, the Swift/E project engages, supports, and sustains a large global science and engineering user base."
"1440581","Collaborative: SI2-SSE - A Plug-and-Play Software Platform of Robotics-Inspired Algorithms for Modeling Biomolecular Structures and Motions","OAC","Special Projects - CCF, Software Institutes, CDS&E","02/01/2015","08/08/2014","Amarda Shehu","VA","George Mason University","Standard Grant","Bogdan Mihaila","01/31/2019","$217,288.00","","ashehu@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","2878, 8004, 8084","2878, 7433, 8004, 8005, 8084, 9216","$0.00","This project aims to develop a novel plug-and-play platform of open-source software elements to advance algorithmic research in molecular biology. The focus is on addressing the algorithmic impasse faced by computational chemists and biophysicists in structure-function related problems involving dynamic biomolecules central to our biology. The software platform resulting from this project provides the critical software infrastructure to support transformative research in molecular biology and computer science that benefits society at large by advancing our modeling capabilities and in turn our understanding of the role of biomolecules in critical mechanisms in a living and diseased cell.<br/><br/>The project addresses the current impasse on the length and time scales that can be afforded in biomolecular modeling and simulation. It does so by integrating cutting-edge knowledge from two different research communities, computational chemists and biophysicists focused on detailed physics-based simulations, and AI researchers focused on efficient search and optimization algorithms. The software elements integrate sophisticated energetic models and molecular representations with powerful search and optimization algorithms for complex modular systems inspired from robot motion planning. The plug-and-play feature of the software platform supports putting together novel algorithms, such as wrapping Molecular Dynamics or Monte Carlo as local search operators within larger robotics-inspired exploration frameworks, and adding emerging biomolecular representations, models, and search techniques even beyond the timeline of this project."
"1642404","SI2- SSE:  Symbolic Toolboxes for Differential Geometry and Mathematical Physics","OAC","GEOMETRIC ANALYSIS, Software Institutes","10/01/2016","09/06/2016","Ian Anderson","UT","Utah State University","Standard Grant","Rob Beverly","09/30/2021","$314,161.00","Charles Torre","ian.anderson@usu.edu","1000 OLD MAIN HILL","LOGAN","UT","843221000","4357971226","CSE","1265, 8004","7433, 8004, 8005","$0.00","This project develops the DifferentialGeometry (DG) software for research and educational use across a broad spectrum of disciplines, from mathematics to physics and engineering. With this software many pencil and paper calculations in differential geometry and its applications, calculations which were previously intractable, can now be performed quickly, reliably, and with relative ease.  DG provides extensive mathematical infrastructure which supports the formulation of new conjectures, the creation of examples and application of theoretical results, the ability to easily verify many results in the existing scientific literature,  and the ability to effortlessly share complex calculations with collaborators, colleagues, and students. DG libraries also provide access - for both experts and non-experts - to large tracts of scientific and mathematical knowledge. A number of undergraduate and graduate students will participate in this project, performing software development and exploring applications of DG to research problems in mathematics and physics.  In particular, DG provides an excellent means to get undergraduates involved in advanced research projects which normally would be accessible only to graduate students.<br/><br/>This project creates symbolic computational toolboxes and libraries to support research needs in differential geometry, relativity and field theory, differential equations and integrable systems, and Lie theory. These toolboxes and libraries will provide new infrastructure for symbolic computing in differential geometry and its applications; meet specific user community demands; and explore new areas where symbolic methods have heretofore been unused. Project highlights include new objects and environments for working with submanifolds, general connections, differential operators, and constrained jet spaces. Tools for analyzing asymptotic structure of spacetimes represent an innovative use of computer algebra. A new toolbox will be created which incorporates much of the extensive mathematical literature on the classification of Lie subalgebras. This project will provide, for the first time, a comprehensive symbolic toolkit for investigations of integrable Partial Differential Equations (PDE). New libraries of symbolic data include symmetric and isotropy irreducible homogeneous spaces, solutions of relativistic field equations and their properties, integrable PDE and their properties. As libraries of symbolic data are created, DG is used to validate and correct results in the literature. Software development and community engagement projects which will ensure sustainability are included."
"1148453","Collaborative Research: SI2-SSI: An Interactive Software Infrastructure for Sustaining Collaborative Community Innovation in the Hydrologic Sciences","OAC","ADVANCES IN BIO INFORMATICS, ECOSYSTEM STUDIES, Methodology, Measuremt & Stats, Hydrologic Sciences, EnvS-Environmtl Sustainability, PetaApps, CDI TYPE I","07/01/2012","08/11/2016","David Tarboton","UT","Utah State University","Standard Grant","Rajiv Ramnath","06/30/2018","$2,401,939.00","Jennifer Arrigo, Richard Hooper, David Maidment, Daniel Ames, Jonathan Goodall","davidtarboton@usu.edu","1000 OLD MAIN HILL","LOGAN","UT","843221000","4357971226","CSE","1165, 1181, 1333, 1579, 7643, 7691, 7750","1165, 1181, 1333, 145E, 1579, 7433, 7643, 7691, 7750, 8004, 8009","$0.00","Water, its quality, quantity, accessibility, and management, is crucial to society. However, our ability to model and quantitatively understand the complex interwoven environmental processes that control water and its availability is severely hampered by inadequate tools related to hydrologic data discovery, systems integration, modeling/ simulation, and education. This project develops sustainable cyberinfrastructure for better access to water-related data and models in the hydrologic sciences, enabling hydrologists and other associated communities to collaborate and combine data and models from multiple sources. It will provide new ways in which hydrologic knowledge is created and applied to better understand water availability, quality, and dynamics.  It will also help to provide a more comprehensive understanding of the interactions between natural and engineered aspects of the water cycle. These goals will be achieved through the development of interoperable cyberinfrastructure tools and the creation of an online collaborative environment, called HydroShare, which enables scientists to easily discover and access hydrologic and related data and models, retrieve them to their desktop, and perform analyses in a high performance computing environment.  The software to be developed will take advantage of existing NSF cyberinfrastructure (iRODS, HUBzero, CSDMS, CUAHSI HIS) and be created as open source code. Its development will be end user-driven.  In terms of broader impacts, the project builds essential infrastructure for science by developing software tools and computing environments to allow better understanding of the impacts of climate change (i.e., floods, droughts, biofuels, etc.) and to allow improved water resource development and the management of freshwater resources both above and below ground.   Resulting software will be made publicly available and provides a strong student and workforce training/education component.  In addition, the project supports an institution in an EPSCoR state and engages, as a PI, a person who is from a group under-represented in the sciences and engineering."
"1739549","SI2-SSE: GenApp - A Transformative Generalized Application Cyberinfrastructure","OAC","Software Institutes","10/01/2017","08/29/2017","David Fushman","MD","University of Maryland, College Park","Standard Grant","Seung-Jong Park","09/30/2020","$130,394.00","","fushman@umd.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Scientific computing and computational analysis are becoming integral aspects of virtually any field of science, be it exact sciences like Physics, Chemistry and Biology, or social sciences. Efforts of many research laboratories are focused on creation of scientific codes for data generation, analysis and interpretation. However, publicly funded and often hard won scientific codes developed in a typical research laboratory too frequently become unsustainable  beyond the lifetime of funding or shortly after staff rotation. Projects that are funded to afford expensive computer science expertise simply to maintain and update existing software divert scarce resources from the lab's primary goals and often translates the problem without resolving it.  Only a select number of researchers receive sufficient funding to maintain and update software, limiting the dissemination of new ideas and techniques. The diversity and continually changing nature of software environments compounds the issues.  Enabling user utilization presents hurdles in deployment, access and training.  These issues also create barriers to the implementation of new ideas embodied in new codes.  The GenApp project's goals are to address these issues. To begin with, GenApp enables the rapid dissemination of scientific codes to researchers with minimal software expertise. As more researchers use these codes, more of them become vested in the codes, which helps their sustainability. <br/><br/>The fundamental goal of this project is to advance the GenApp framework into a transformative tool to broadly benefit the scientific software developer community. GenApp is a generalized application generation framework intended for rapid deployment of scientific codes, which can generate both science gateways and stand-alone applications. Among the main unique features of GenApp are the minimal technical expertise requirement for the end user and an open-end design ensuring sustainability of generated applications. To produce fully functional applications, GenApp weaves libraries of fragments and user defined modules as directed by simple definition files, created from a uniform, logical, and simple-to-encode general interface definition file provided by GenApp.  This general definition file and the underlying software can be reused indefinitely to produce applications in a variety of existing and yet-to-be defined software environments. Preserving such simplicity with GenApp's maturation is one of the main developmental strategies. To achieve the goal of GenApp four focus Aims have been proposed. The first is infrastructure development, which includes general enhancements to the capabilities of GenApp. The second is documentation, training, dissemination, outreach and sustainability - all important aspects to produce a software product that is useful to the community. The third is simply feedback, since user and developer feedback will help drive the first two Aims.  The final Aim includes two structural biology domain science applications that will adopt and drive GenApp development.  GenApp will see its primary practical utilization in making highly demanding novel computational and analysis tools accessible to experimentalists and theoreticians working in the nuclear magnetic resonance (NMR) and small-angle scattering (SAS) domains of structural biology. The GenApp framework will serve as a platform for applications utilizing advanced tools requiring efficient use of HPC resources, tools for modeling SAS data with molecular simulations, and a large software suite for a combined analysis of NMR and SAS measurements coupled to computational modeling. Easy access to these powerful tools will enable hitherto impossible studies of a number of fundamental biological problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1535150","SI2-SSE: Development of a Software Framework for Formalizing Forcefield Atom-Typing for Molecular Simulation","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes, DMREF","09/01/2015","08/22/2017","Christopher Iacovella","TN","Vanderbilt University","Standard Grant","Bogdan Mihaila","08/31/2019","$501,836.00","Peter Volgyesi, Janos Sallai","christopher.r.iacovella@vanderbilt.edu","110 21ST AVE S","NASHVILLE","TN","372032416","6153222631","CSE","1253, 1712, 8004, 8292","7433, 8005, 8400, 9150","$0.00","Molecular simulation plays a key role in understanding the atomistic and molecular level interactions that underlie many natural and man-made materials and processes. Classical molecular simulations rely upon forcefields to describe the various interactions that exist between atoms and/or groups of atoms. The availability of forcefields for molecular simulation has reduced the effort researchers must devote to the difficult and costly task of determining the interactions between species, allowing them to instead focus on the motivating scientific questions. However, determining which parameters in a forcefield to use is still often a tedious and error prone task. This difficulty is related to the strong dependence of the parameters on the chemical context of the atoms; the chemical context may depend on the local bonded environment of an atom in a molecule, the local environment of neighboring atoms, the type of molecule(s) being considered, the phase of the molecule(s), etc. Forcefields can contain tens or hundreds of different types of the same element, where each type represents the element in a different chemical context. Atom typing can be challenging, often requiring the user to consult textual comments scattered in parameter files or the scientific literature where the parameters were published. Unfortunately, as of today, the documentation of a typical forcefield tends to be scarce and unstructured, commonly expressed in plain English or in an ad-hoc shorthand notation, leading to ambiguities and increasing the likelihood of incorrect usage. While there are freely available tools to aid in atom-typing, these are typically specific to a particular forcefield or simulator and capture the atom-typing and parameterization rules in ways that are hard to maintain, debug, and evolve. The central tenet of this project is that there is an imminent need in the research community for a forcefield agnostic formalism to express atom-typing and parameterization rules in a way that is expressive enough for human consumption, while being machine readable to enable automation in complex scientific workflows.<br/> <br/>This work proposes to establish a formalism to express the chemical context for which a particular forcefield parameter is applicable (i.e., forcefield usage semantics) and an atom-typing tool that interprets this formalism to generate forcefield parameterizations that are provably correct. Annotating forcefields with this formalism will serve as clear, unambiguous documentation of the atom-types and parameter usage, and also allows ambiguities or inconsistencies in forcefield specifications to be programmatically pinpointed during development. Successfully developing this framework will simplify the rules needed for atom-typing, which is crucial as forcefields continue to grow, specialize, and become more complex. The machine-readable annotations of forcefield usage semantics will enable automating tedious and error prone tasks and have the potential to enable new application areas, ranging from automated forcefield comparison and cross-validation, to complex simulation workflows integrating multiple forcefields and simulator tools. An open online forcefield repository containing the annotated forcefields, associated open source software, and documentation on how to use, annotate, and develop forcefields within the proposed framework will be developed to disseminate results and foster community involvement."
"1450280","Collaborative Research: SI2-SSI: ELSI-Infrastructure for Scalable Electronic Structure Theory","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Data Cyberinfrastructure, Software Institutes","06/15/2015","04/14/2022","Volker Blum","NC","Duke University","Standard Grant","Varun Chandola","05/31/2023","$1,561,465.00","Jianfeng Lu","volker.blum@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","CSE","1253, 1712, 7726, 8004","026Z, 054Z, 062Z, 077Z, 7433, 8009, 8084, 9216","$0.00","Predictive, so-called ab initio electronic structure calculations, particularly those based on the Kohn-Sham density functional theory (DFT) are now a widely used scientific workhorse with applications in virtually all sciences, and increasingly in engineering and industry. In materials science, they enable the computational (""in silico"") design of new materials with improved properties. In biological or pharmacological research, they provide molecular-level insights into the function of macromolecules or drugs. In the search for new energy solutions, they give molecular-level insights into new solar cell designs, catalytic processes, and many others. A key bottleneck in many applications and calculations is the ""cubic scaling wall"" of the so-called Kohn-Sham eigenvalue problem with system size (i.e., the effort increases by a factor of 1,000 if the model size increases by a factor of 10). This project will establish an open source software infrastructure ""ELSI"" that offers a common, practical interface to initially three complementary solution strategies to alleviate or overcome the difficulty associated with solving the Kohn-Sham eigenvalue problem. ELSI will enable a broad range of end user communities, centered around different codes with, often, unique features that tie a specialized group of scientists to that particular solution, to easily incorporate state-of-the-art solution strategies for a key problem they all share. By providing these effective, accessible solution strategies, we will open up major areas for electronic structure theory where DFT based predictive methodologies are not applicable today. This will in turn open doors for new development in materials science, chemistry, and all related areas. Commitments to support ELSI exist from some of the most important electronic structure developer communities, as well as from industry and government leaders in high-performance computing. Thus, we will create a strong U.S. based infrastructure that leverages the large user and developer base from a globally active community developing DFT methods for materials research.<br/><br/>ELSI will support and enhance three state-of-the-art approaches, each best suited for a specific problem range: (i) The ELPA (EigensoLvers for Petascale Applications) library, a leading library for efficient, massively parallel solution of eigenvalue problems (for small- and mid-sized problems up to several 1,000s of atoms), (ii) the OMM (Orbital Minimization Method) in a recent re-implementation, which circumvents the eigenvalue problem by focusing on a reduced, auxiliary problem (for systems in the several 1,000s of atoms range), and (iii) the PEXSI (Pole EXpansion and Selective Inversion) library, a proven reduced scaling (at most quadratic scaling) solution for general systems (for problems with 1,000s of atoms and beyond). By establishing standardized interfaces in a style already familiar to many electronic structure developers, ELSI will enable production electronic structure codes that use it to significantly reduce the ""scaling wall"" of the eigenvalue problem. First, ELSI will help them make efficient use of the most powerful computational platforms available. The target platforms are current massively parallel computers and multicore architectures, GPU based systems and future manycore processors. Second, the project will make targeted methodological improvements to ELPA, OMM, and PEXSI, e.g., a more effective use of matrix sparsity towards very large systems. The focus on similar computational architectures and similar methodological enhancements will lead to significant cross-fertilization and synergy between these approaches."
"1450177","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","OAC","Physical & Dynamic Meteorology, Software Institutes, EarthCube","08/01/2015","08/04/2015","Brian Ancell","TX","Texas Tech University","Standard Grant","Bogdan Mihaila","07/31/2019","$166,428.00","","brian.ancell@ttu.edu","2500 BROADWAY","LUBBOCK","TX","79409","8067423884","CSE","1525, 8004, 8074","4444, 7433, 8009","$0.00","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities."
"1148493","SI2-SSI:  A Sustainable Community Software Framework for Petascale Earthquake Modeling","OAC","Geotechnical Engineering and M, PetaApps, Software Institutes","08/01/2012","08/07/2012","Thomas Jordan","CA","University of Southern California","Standard Grant","Rajiv Ramnath","07/31/2016","$2,522,784.00","Jacobo Bielak, Kim Olsen, Yifeng Cui","tjordan@usc.edu","3720 S FLOWER ST","LOS ANGELES","CA","900894304","2137407762","CSE","1636, 7691, 8004","037E, 043E, 1576, 7433, 8009","$0.00","Earthquakes have major economic and societal consequences as can be seen from the aftermath of the recent large earthquakes in Japan, Chile, and New Zealand.  This multidisciplinary project, which includes both geoscientists, computer scientists, and structural engineers, integrates high-level and middle-level scientific software elements developed by the Southern California Earthquake Center (SCEC) into a software environment for integrated seismic modeling that can be used for seismic hazard analysis.  The framework includes integration of community velocity models, codes for dynamic and pseudo-dynamic rupture generation, deterministic and stochastic earthquake engines, and the applications necessary to employ forward simulations in two types of inverse problems: seismic source imaging and full 3D tomography.  Modifications to already existing software packages slated to be significantly enhanced in the course of the workflow will allow simulations to be run on petascale machines and allow the better managing of scientific workflows.  The work also focuses on software lifecycle issues such as  model formation, verification, prediction, and validation and support the use of petascale computers by earthquake scientists.  The goal of the project is to facilitate the incorporation of better theory and data into computationally intensive modeling of earthquake processes.  Software will be designed to interface smoothly with OpenSHA, as well as OpenSEES, PEER, and NEES.  Project partners will also develop and test two computational platforms, one that will have a user-friendly interface  for calculating seismographs and the other will generate large suites of simulations for a layered earthquake hazard model. Models will be validated against datasets for 13 well-recorded historic California earthquakes of magnitude 6.0 or higher.  The initial API will take advantage of the asynchronous IO features of Fortran 2003 with plans for adding C/C++ and Python interfaces.  All codes developed will be open-source and publicly available and software distribution will be accompanied by sample input datasets and example forecast results.  Broader impacts include the development of a new generation of time-dependent earthquake forecasts to produce ground-shake hazard maps, partnership with a federal agency and the private sector.  It also includes a component of student and postdoctoral training and outreach to user communities.  Undergraduate interns, many of whom have historically been from groups under-represented in STEM fields, will be trained in use of the software during an 8-week summer training course."
"1663914","Collaborative Research: SI2-SSI: Inquiry-Focused Volumetric Data Analysis Across Scientific Domains: Sustaining and Expanding the yt Community","OAC","Software Institutes","10/01/2017","01/07/2020","Matthew Turk","IL","University of Illinois at Urbana-Champaign","Standard Grant","Seung-Jong Park","09/30/2023","$1,061,721.00","Nathan Goldbaum","mjturk@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","8004","7433, 8004, 8009","$0.00","Scientific discovery across the physical sciences is increasingly dependent on the analysis of volumetric - or three-dimensional - data, that may come from a supercomputer simulation, direct measurement, or mathematical models. Researchers typically seek to extract meaningful insights from this data by visualizing and analyzing it in various ways. The ways in which scientists process volumetric data are actually quite similar across domains, but cross-disciplinary knowledge transfer and tool development is blocked by barriers of terminology. This project seeks to enhance an analysis and visualization toolkit named yt that is currently primarily used for astrophysical simulations. yt allows scientists to access and analyze data at several different levels by providing an interface that is designed to answer questions motivated by the underlying scientific problem, while worrying less about details such data formats, specific analysis techniques etc. yt's utilization in computational astrophysics has dramatically increased access to advanced algorithms for both visualization and analysis, and fostered the growth of a community of researchers sharing techniques and results. This project seeks to make yt available and adopted by scientists in other domains of science thus reproducing its success in astrophysics in these other science domains. This project will expand the yt community beyond theoretical astrophysics and enable and promote collaboration and advanced data analysis in the fields of meteorology, seismology and global tomography, observational astronomy, hydrology and oceanography, and plasma physics. <br/><br/>Improvements to the yt project will proceed along four principal technical avenues. The first is to develop a system that adapts the way yt presents data via a set of domain contexts that encode the ontology, domain-specific vocabulary, and common analysis tasks for a given field of study. This will include creating a domain context system as well as a set of five pilot domain contexts developed in collaboration with domain practitioners. The second is to overhaul the yt field system, adding more versatility and enabling significant optimizations. Thirdly, the project team will implement non-spatial indexing schemes, providing methods for accessing and analyzing data that is not organized according to the standard spatial axes. The final improvement will be the development of a non-local analysis system, allowing generalized path traversal as well as domain convolutions. To ensure wide dissemination and use of these improved capabilities, the team will design domain-specific documentation and training materials, and organize outreach and training events for early-career researchers. This will consist of both hands-on technical workshops and curricula developed in collaboration with Data Carpentry for utilization at other institutions. This combination of technical developments and social investments has been designed to ensure both readiness of the software and engagement of the targeted research communities."
"1440811","SI2-SSE: Development and Implementation of Software Elements using State-of-the-Art Computational Methodology to Advance Modeling Heterogeneities and Mixing in Earth's Mantle","OAC","Geophysics, Software Institutes, EarthCube","08/01/2014","08/18/2016","Elbridge Puckett","CA","University of California-Davis","Standard Grant","Alan Sussman","07/31/2018","$502,715.00","Magali Billen","egpuckett@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","CSE","1574, 8004, 8074","7433, 8004, 8005, 9251","$0.00","This project involves the development and implementation of scientific software elements (SSEs), based on modern, high-resolution numerical methods for modeling steep gradients and sharp interfaces of material properties in viscous fluids in the presence of thermal convection. The goal of this project is to address a compelling need in geodynamics, in which continuum mechanics is applied to the study of geophysical processes, such as convection in the Earth?s mantle. A primary tool of geodynamics research is computational models of the flow of the extremely viscous interior of the Earth over hundreds of millions to billions of years. A long-standing challenge for these models is the need to accurately model sharp interfaces in temperature, viscosity, and other properties. These arise when, for example, modeling subduction (in which a cold tectonic plate plunges into the hot interior) or rising plumes (in which a hot boundary layer instability rises through the mantle and encounters the cold boundary layer of the tectonic plates). The project will foster interdisciplinary communication and the application of state-of-the-art applied and computational mathematics to fundamental problems in geophysics. It involves early-career mathematical scientists in the application of state-of-the-art numerical algorithms to geodynamics and, in particular, will provide an opportunity to increase the participation of women in mathematics and geodynamics research.<br/><br/>This project involves the design and implementation of state-of-the-art SSEs for computing the evolution of significant processes in the Earth's mantle in which an essential feature of the problem is the presence of one or more moving boundaries, interfaces, or steep gradients in temperature, composition, or viscosity. The SSEs will address two critical issues that currently limit modern mantle convection simulations. All computational models of mantle convection currently in use produce significant overshoot and undershoot in the neighborhood of sharp gradients in temperature and viscosity. The cause of these overshoots and undershoots is a numerical artifact, which is well-known and well-understood in other fields, such as the computational shock physics community. Over the past thirty years researchers in computational shock physics have developed a variety of high-order accurate, monotone numerical methods, which preserve the physically correct maximum and minimum values of the computed quantities, while producing a high-order accurate numerical approximation of these quantities. Another compelling need in computational geodynamics is the ability to track discontinuous jumps in quantities such as material composition. Here high-order accurate interface tracking algorithms are required, since these fields undergo large-scale deformation, yet quantities such as the viscosity must be accurately approximated at the interface between two materials."
"1047586","SI2-SSE: Statistical software for astronomical surveys","AST","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/15/2010","09/21/2010","Gutti Babu","PA","Pennsylvania State Univ University Park","Standard Grant","Nigel Sharp","08/31/2014","$450,000.00","Eric Feigelson","babu@psu.edu","201 Old Main","University Park","PA","168021503","8148651372","MPS","1253, 1798, 8004","1206, 7480","$0.00","Astronomical research is undergoing a transformation due to the proliferation of publicly available online datasets from all types of telescopes, and a large international effort is already underway to federate these diverse datasets for ready use by astronomers.  A particularly important class of data arises from multi-epoch wide-field surveys, which are essentially 'movies' of the sky.  These advances in time domain astronomy are crucial for such diverse and important research topics as exoplanet discovery, supernovae and other transients, variable stars, and accretion phenomena.  However, most astronomers use only a narrow range of classical statistical methods for interpreting these large datasets.  This problem can now be alleviated with the R statistical computing environment and its rapidly growing CRAN add-on packages.  This project will bring the R software capabilities into the astronomical research community and introduce specialized astrostatistical methodology into R.<br/><br/>In particular, the research includes two complementary projects.  First, CRAN packages will be developed for the analysis of time domain data with irregularly spaced observation times.  This is a difficulty rarely encountered in other fields but common in multi-epoch astronomical studies, due to diurnal cycles, satellite orbits, survey cadence patterns, and telescope allocation limitations.  Astronomers have developed a wide range of treatments for such problems, but most have not been evaluated statistically or incorporated into widely-used software packages, so a part of this study will be a statistical evaluation of competing methods.  Second, the prototype VOStat Web service will be developed into a major tool and integrated into the growing Virtual Astronomical Observatory (VAO) software environment.  VOStat will provide dozens of functionalities in many areas of applied statistics: data manipulation and visualization, nonparametric statistics and density estimation, probability density functions, regression and inference, multivariate analysis, clustering and classification, censoring and truncation, time series analysis, spatial point processes and image processing.  These achievements will improve the statistical sophistication within the VAO and for thousands of other astronomical studies.<br/><br/>These software developments will improve the statistical analysis of a large number of astronomical research studies every year.  Coding within R has the simultaneous advantage of inheriting the large infrastructure of methodology and graphics, itself of enormous value to the entire astronomical community.  While the production of CRAN packages directly allows wide dissemination of the code, integrating the code into the VAO software environment through VOStat will make it conveniently accessible to all astronomers.  A strong pedagogical component will further encourage less experienced astronomers to learn and use more advanced statistical methods.  In addition, the CRAN packages on astrostatistical methods for irregular time series may have value to statisticians, physicists and economists who also might encounter datasets of this type."
"1339715","SI2-SSI: Collaborative Research: Scalable, Extensible, and Open Framework for Ground and Excited State Properties of Complex Systems","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Special Projects - CCF, CYBERINFRASTRUCTURE, Software Institutes, CDS&E","10/01/2013","09/10/2014","Laxmikant Kale","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Bogdan Mihaila","09/30/2019","$2,383,226.00","","kale@uiuc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","1253, 1712, 2878, 7231, 8004, 8084","7433, 7569, 8009, 8084, 9216, 9263","$0.00","Computer simulation plays a central role in helping us understand, predict, and engineer the physical and chemical properties of technological materials systems such as semiconductor devices, photovoltaic systems, chemical reactions and catalytic behavior. Despite significant progress in performing realistic simulations in full microscopic detail, some problems are currently out of reach: two examples are the modeling of electronic devices with multiple functional parts based on new materials such as novel low power computer switches that would revolutionize the Information Technology industry, and the photovoltaic activity of complex interfaces between polymers and inorganic nanostructures that would enhance US energy self-reliance.  The research program of this collaborative software institute aims to create an open and effective scientific software package that can make efficient use of cutting-edge high performance computers (HPC) to solve challenging problems involving the physics and chemistry of materials.  By having such software available, this software initiative will have multiple broad impacts.  First, the community of materials scientists will be able to study next-generation problems in materials physics and chemistry, and computer science advances that enable the software will be demonstrated and made accessible for both communities which will help cross-fertilize further such collaborative efforts.  Second, the capability of simulating and engineering more complex materials systems and technological devices could play a role in helping the US continue is competitive edge in science, technology, and education. Third, through training of young scientists, direct outreach to the broader scientific community through workshops and conferences, and educational programs ranging from secondary to graduate levels, the power, importance, and capabilities of computational modeling, materials science, and computer science methodologies that enable the science will be communicated to a broad audience.  Finally, by enabling the refinement of existing materials systems as well as discovery of new materials systems, the resulting scientific advances can help broadly impact society via technological improvements: in terms of the two examples provided above, (a) the successful design of new electronic device paradigms helps significantly advance the digital revolution by permitting the introduction of smaller, more efficient, and more capable electronic circuits and information processing systems, and (b) successful creation of inexpensive, easy-to-fabricate, and durable photovoltaic materials and devices can lead to cleaner forms of energy production while reducing reliance on fossil fuels.<br/><br/>The technical goal is to greatly enhance the open software tool OPENATOM to advance discovery in nanoscience and technology. OPENATOM will be delivered as a open, robust and validated software package capable of utilizing HPC architectures efficiently to describe the electronic structure of complex materials systems from first principles.  In terms of describing electronic ground-states, OPENATOM will be enhanced by features such as improved configurational sampling methods, hybrid density functionals, and incorporation of fast super-soft pseudopotential techniques. In addition, the team will incorporate the many-body GW-BSE approach for electronic excitations that permits accurate computation of electronic energy levels, optical absorption and emission, and luminescence.  Ultimately, such an extensible software framework will permit accurate electronic structure computations to employ effectively future HPC platforms with 10,000,000 cores."
"1550554","SI2-SSI:  Collaborative Research: ParaTreet: Parallel Software for Spatial Trees in Simulation and Analysis","OAC","Software Institutes","09/01/2016","08/19/2016","Laxmikant Kale","IL","University of Illinois at Urbana-Champaign","Standard Grant","Rob Beverly","08/31/2018","$170,000.00","John Hart","kale@uiuc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","8004","7433, 8004, 8009","$0.00","Many scientific and visualization methods involve organizing the data they are processing into a hierarchy (also known as a ""tree"").   These applications and methods include: astronomical simulations of particles moving under the influence of gravity, analysis of spatial data (that is, data that describes objects with respect to their relative position in space), photorealistic rendering of virtual environments,reconstruction of surfaces from laser scans, collision detection when simulating the movement of physical objects, and many others.   Tree data structures, and the algorithms used to work on these structures, are heavily used in these applications because they help to make these applications run much faster on supercomputers. However, implementing tree-based algorithms can require a significant effort, particularly on modern highly parallel computers.  This project will create ParaTreet, a software toolkit for parallel trees, that will enable rapid development of such applications.  Details of the parallel aspects will be hidden from the programmer, who will be able to quickly evaluate the relative merits of different trees and algorithms even when applied to large datasets and very computation-intensive applications. The combination of such an abstract and extensible framework with a portable adaptive runtime system will allow scientists to effectively use parallel hardware ranging from small clusters to petascale-class machines, for a wide variety of tree-based applications. This project will demonstrate the feasibility of such an approach as well as generate evidence of community adoption of this technology. If successful, this project will enable NSF-supported researchers to solve science problems faster as well as to tackle more complex problems, thus serving NSF's science mission.<br/><br/><br/>This project builds upon an existing collaboration on Computational Astronomy and the resultant software base in the ChaNGa (Charm N-body GrAvity solver) code. ChaNGa is a software package that performs collisionless N-body simulations, and can perform cosmological simulations with periodic boundary conditions in co-moving coordinates or simulations of isolated stellar systems. This project will extend ChaNGa with a parallel tree toolkit called ParaTreet and associated applications, that will allow scientists to effectively utilize small clusters as well as very large supercomputers for parallel tree-based calculations.  The key data structure in ParaTreet is an asynchronous software-based tree data cache, which maintains a writeback local copy of remote tree data. We plan to support a variety of spatial decomposition methods and the associated trees, including Oct-trees, KD-trees, inside-outside trees, ball trees, R-trees, and their combinations. Different trees are useful in different application circumstances, and the software will allow their relative merits to be evaluated with relative ease. The framework will support a variety of parallel work decomposition methods, including those based on space filling curves, and support dynamic rearrangement of parallel work at runtime. The algorithms supported will range from Barnes-Hut with various multipole expansions, data clustering, collision detection, surface reconstruction, ray intersection, etc. The software includes a collection of dynamic load balancing strategies in the Charm++ framework that can be tuned for specific problem structures. It also includes support for clusters of accelerators, such as GPGPUs. This project will demonstrate the feasibility of such an approach as well as generate evidence of community adoption of this technology."
"1535070","SI2-SSE: AttackTagger: Early Threat Detection for Scientific Cyberinfrastructure","OAC","Software Institutes","09/01/2015","09/08/2015","Alexander Withers","IL","University of Illinois at Urbana-Champaign","Standard Grant","Stefan Robila","08/31/2020","$499,136.00","Ravishankar Iyer, Randal Butler, Zbigniew Kalbarczyk, Adam Slagell","alexw1@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","8004","7433, 8004, 8005","$0.00","The cyber infrastructure that supports science research (such as the cyberinfrastructure that provides access to unique scientific instrumentation such as a telescope, or an array of highly distributed sensors placed in the field, or a computational supercomputing center) faces the daunting challenge of defending against cyber attacks. Modest to medium research project teams have little cyber security expertise to defend against the increasingly diverse, advanced and constantly evolving attacks. Even larger facilities that have with security expertise are often overwhelmed with the amount of security log data they need to analyze in order to identify attackers and attacks, which is the first step to defending against them.  The challenges of the traditional approach of identifying an attacker are amplified by the lack of tools and time to detect attacks skillfully hidden in the noise of ongoing network traffic. The challenge is not necessarily in deploying additional monitoring but to identify this malicious traffic by utilizing all available information found in the plethora of security, network, and system logs that are already being actively collected.  This project proposes to build and deploy,  is needed in research environments, an advanced log analysis tool, named AttackTagger, that can scale to be able to address the dramatic increase in security log data, and detect emerging threat patterns in today's constantly evolving security landscape. AttackTagger will make science research in support of national priorities more secure.<br/><br/>AttackTagger will be a sophisticated log analysis tool designed to find potentially malicious activity, such as credential theft, by building factor graph models for advanced pattern matching.  AttackTagger will integrate with existing security software so as to be easily deployable within existing security ecosystems and to offload processing and computational work onto better suited components.  It can consume a wide variety of system and network security logs.  AttackTagger accomplishes advanced pattern matching by utilizing a Factor Graph model, which is a type of probabilistic graphical model that can describe complex dependencies among random variables using an undirected graph representation, specifically a bipartite graph. The bipartite graph representation consists of variable nodes representing random variables, factor nodes representing local functions (or factor functions , and edges connecting the two types of nodes. Variable dependencies in a factor graph are expressed using a global function, which is factored into a product of local functions. In the practice of the security domain, using factor graphs is more flexible to define relations among the events and the user state compared to Bayesian Network and Markov Random Field approaches. Specifically, using factor graphs allows capturing sequential relation among events and enables integration of the external knowledge, e.g., expert knowledge or a user profile."
"1148461","SI2-SSE: Supporting Generic Programming in C++ for Modular and Reliable Large-Scale Software","OAC","Information Technology Researc, Software Institutes","09/01/2012","10/27/2016","Gabriel Dos Reis","TX","Texas A&M Engineering Experiment Station","Standard Grant","Seung-Jong Park","08/31/2017","$499,999.00","Bjarne Stroustrup","gdr@cs.tamu.edu","3124 TAMU","COLLEGE STATION","TX","778433124","9798626777","CSE","1640, 8004","7942, 8004, 8005","$0.00","Generic programming has the potential of an effective methodology for building large-scale, reliable, maintainable, and efficient software artifacts.  It is supported by the C++ programming language through the ""template"" mechanism.  In the hands of experts, C++ templates are formidable abstraction tools, key to the success of libraries such as the Standard Template Library, and many freely available and commercial software libraries and products.  Unfortunately, the practice of template-based structured generic programming remains restricted to relatively few highly trained individuals.  <br/><br/>A primary objective of this project is to investigate and develop software tools and programming models that support scalable and modular generic libraries.  Bringing structured generic programming methodology to mainstream at the scale done for object-oriented programming entails the invention of new programming language constructs and compiler construction techniques that go beyond conventional technologies. In particular, the apparent complexities of templates and arcane technical details must be concealed; code generation has to surpass C++'s current successful applications of templates both in quality and compile time for industrial scale programs.  At the core of this project is the investigation of a direct linguistic support for requirements on template arguments (""concepts""), and their implementations in an open source compiler and libraries made freely available to the public, the research and education community."
"1743178","Collaborative Research: NISC SI2-S2I2 Conceptualization of CFDSI: Model, Data, and Analysis Integration for End-to-End Support of Fluid Dynamics Discovery and Innovation","OAC","FD-Fluid Dynamics, Special Initiatives, Software Institutes, CDS&E","03/01/2018","08/25/2022","Kenneth Jansen","CO","University of Colorado at Boulder","Continuing Grant","Seung-Jong Park","08/31/2023","$321,838.00","Jed Brown, Alireza Doostan, John Evans, John Farnsworth","jansenke@colorado.edu","3100 MARINE ST STE 481 572 UCB","BOULDER","CO","803090001","3034926221","CSE","1443, 1642, 8004, 8084","026Z, 8004, 8005, 9263","$0.00","Fluid dynamics is a broad field spanning a large number of science and engineering problem domains that are critical to a wide variety of important applications including manufacturing, climate analysis, environment, health, transportation, propulsion, and power generation. To support the fluid dynamics research and applications community, this project seeks to engage the community in order to conceptualize a future institute, the Computational Fluid Dynamics Software Infrastructure (CFDSI), that will broadly develop, share, and apply computational tools for the generation and analysis of fluid dynamics data from both experimental and computational sources.  After its conceptualization, CFDSI will create and extend tools for problem definition, solution, and analysis of both computational and experimental investigations. The primary objective is to facilitate the sharing of computational tools and data resources through a rich and extensible set of software components that can be integrated into a wide range of existing fluid dynamics analysis tools. By improving the efficiency of tools and their ease of use, the ability for scientists and engineers to accurately predict and understand how complex fluid flows behave will be enhanced, having a significant impact on design, innovation, and discovery across the vast range of applications where fluid dynamics plays a role. CFDSI even has the potential to impact on K-12, undergraduate and graduate education by making a wide variety of resources available to students for fluid dynamics investigations.<br/><br/>The conceptualized institute will make a wide variety of powerful simulation, data, and analysis resources available to the fluid dynamics research community by lowering or eliminating barriers associated with the adoption and use of these resources. The software infrastructure will have a number of positive impacts on the fluid dynamics research community. To do so, CFDSI will connect the best research in fluid dynamics to the best research in data science/analytics within a highly sustainable software development environment. Specifically, CFDSI will: 1) enhance the dissemination of fluid dynamics data resources and advances in CFD modeling, 2) facilitate collaboration in fluid dynamics research, especially between computational and experimental researchers, 3) enable detailed comparisons between different data sources and detailed validation of computational models, 4) ease the use of advanced CFD models, methods, and codes in new and complex applications, 5) facilitate advanced analytics, such as uncertainty quantification, data compression, and optimization, 6) provide students access to advanced CFD methods and data resources, both computational and experimental, to enhance both graduate and undergraduate education in fluid dynamics, 7) improve the sustainability of current and future CFD software, and 8) facilitate the management of the growing body of fluid dynamics data sets. These outcomes will greatly enhance the effectiveness and productivity of research in fluid dynamics. In particular, they will transform the conduct of fluid dynamics research by: 1) making it more collaborative, 2) enhancing the credibility of research results, 3) enabling discovery, 4) reducing the cost of pursuing new research questions, and 5) diversifying and widening the fluid dynamics community through lowering the barriers associated with accessing and adopting CFD codes and large data sets. Software components will be designed for both analysis of experimental and computational databases as well as direct integration into CFD codes. The latter will enable in situ data analytics to address the growing chasm between data creation rate (solver performance) and data storage rate/volume (I/O resources). After conceptualization and implementation, CFDSI will enable more effective fluid dynamics research and thus impact the wide variety of application domains in which fluid dynamics is critical including manufacturing climate, environment, health, transportation, propulsion, and power generation (including conventional, alternative, and nuclear sources) which will, in turn, strongly impact our economy. Additionally, CFDSI will provide the capability for immersive simulations and experiments that will close the loop on idea, insight, discovery, and design through establishing links to in situ data analytics and problem redefinition during ongoing simulations or experiments. Finally, CFDSI will impact other problem domains governed by partial differential equations (e.g. solid mechanics) by serving as a model and starting point for similar domain-specific software infrastructures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1339581","Collaborative Research: SI2-SSE: Modules for Experiments in Stellar Astrophysics","OAC","STELLAR ASTRONOMY & ASTROPHYSC, OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","01/01/2014","08/29/2013","Lars Bildsten","CA","University of California-Santa Barbara","Standard Grant","Rajiv Ramnath","09/30/2017","$163,586.00","","bildsten@itp.ucsb.edu","3227 CHEADLE HALL","SANTA BARBARA","CA","931060001","8058934188","CSE","1215, 1253, 8004","1206, 7433, 8005","$0.00","As the most commonly observed objects, stars remain at the forefront of astrophysical research. Technical advances in detectors, computer processing power, networks and data storage have enabled new sky surveys. Many of these search for transient events at optical wavelengths, such as the Palomar Transient Factory and Pan-STARRS1 that probe ever-larger areas of the sky and ever-fainter sources, opening up the vast discovery space of ""time domain astronomy"". The recent Kepler and COROT space missions achieved nearly continuous monitoring of more than 100,000 stars. The stellar discoveries from these surveys include revelations about stellar evolution, rare stars, unusual explosion outcomes, and remarkably complex binary star systems. The immediate future holds tremendous promise, as both the space-based survey Gaia and the ground based Large Synoptic Survey Telescope come to fruition. This tsunami of data has created a new demand for a reliable and publicly available research and education tool in computational stellar astrophysics that will reap the full scientific benefits of these discoveries while also creating a collaborative environment where theory, computation and interpretation can come together to address critical scientific issues. This demand by the stellar community led to our release of the Modules for Experiments in Stellar Astrophysics (MESA) software project in 2011.  MESA has driven, and will continue to drive with support from this award, innovation in the stellar community as well as the exoplanet, galactic, and cosmological communities. Educators have widely deployed MESA in their undergraduate and graduate stellar evolution courses because MESA is a community platform with an active support network for leading-edge scientific investigations. Stellar astrophysics research, and all the communities that rely on stellar astrophysics, will be significantly enhanced by sustaining innovative development of MESA.<br/><br/>This award supports the Modules for Experiments in Stellar Astrophysics (MESA) software project and user community.  MESA solves the 1D fully coupled structure and composition equations governing stellar evolution. It is based on an implicit finite difference scheme with adaptive mesh refinement and sophisticated timestep controls; state-of-the-art modules provide equation of state, opacity, nuclear reaction rates, element diffusion, boundary conditions, and changes to the mass of the star. MESA is an open source library that employs contemporary numerical approaches, supports shared memory parallelism based on OpenMP, and is written with present and future multi-core and multi-thread architectures in mind. MESA combines the robust, efficient, thread-safe numerical and physics modules for simulations of a wide range of stellar evolution scenarios ranging from very-low mass to massive stars. Innovations in MESA and its domain of applicability continues to grow, just recently extended to include giant planets, oscillations, and rotation.  This project will sustain MESA as a key piece of software infrastructure for stellar astrophysics while building new scientific and educational networks."
"1047879","SI2-SSI: Accelerating the Pace of Research through Implicitly Parallel Programming","OAC","Information Technology Researc, Special Projects - CCF, Software Institutes","10/01/2010","09/13/2010","David August","NJ","Princeton University","Standard Grant","Sol Greenspan","03/31/2016","$1,740,214.00","David Walker","august@cs.princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","CSE","1640, 2878, 8004","1640, 2878","$0.00","Today, two trends conspire to slow down the pace of science, engineering, and academic research progress in general.  First, researchers increasingly rely on computation to process ever larger data sets and to perform ever more computationally-intensive simulations.  Second, individual processor speeds are no longer increasing with every computer chip generation as they once were.  To compensate, processor manufacturers have moved to including more processors, or cores, on a chip with each generation.  To obtain peak performance on these multicore chips, software must be implemented so that it can execute in parallel and thereby use the additional processor cores. Unfortunately, writing efficient, explicitly parallel software programs using today's software-development tools takes advanced training in computer science, and even with such training, the task remains extremely difficult, error-prone, and time consuming.  This project will create a new high-level programming platform, called Implicit Parallel Programming (IPP), designed to bring the performance promises of modern multicore machines to scientists and engineers without the costs associated with having to teach these users how to write explicitly parallel programs.  In the short term, this research will provide direct and immediate benefit to researchers in several areas of science as the PIs will pair computer science graduate students with non-computer science graduate students to study, analyze, and develop high-value scientific applications.  In the long term, this research has the potential to fundamentally change the way scientists obtain performance from parallel machines, improve their productivity, and accelerate the overall pace of science.  This work will also have major educational impact by developing courseware and tutorial materials, useable by all scientists and engineers, on the topics of explicit and implicit parallel computing.<br/><br/>IPP will operate by allowing users to write ordinary sequential programs and then to augment them with logical specifications that expand (or abstract) the set of sequential program behaviors.  This capacity for abstraction will provide parallelizing compilers with the flexibility to more aggressively optimize programs than would otherwise be possible.  In fact, it will enable effective parallelization techniques where they were impossible before.  The language design and compiler implementation will be accompanied by formal semantic analysis that will be used to judge the correctness of compiler transformations, provide a foundation for about reasoning programs, and guide the creation of static analysis and program defect detection algorithms.  Moreover since existing programs and languages can be viewed as (degenerately) implicitly parallel, decades of investment in human expertise, languages, compilers, methods, tools, and applications is preserved.  In particular, it will be possible to upgrade old legacy programs or libraries from slow sequential versions without overhauling the entire system architecture, but merely by adding a few auxiliary specifications.  Compiler technology will help guide scientists and engineers through this process, further simplifying the task.  Conceptually, IPP restores an important layer of abstraction, freeing programmers to write high-level code, designed to be easy to understand, rather than low-level code, architected according to the specific demands of a particular parallel machine."
"1440396","SI2-SSE: Dynamic Adaptive Runtime Systems for Advanced Multipole Method-based Science Achievement","OAC","Special Projects - CCF, Software Institutes","10/01/2014","06/16/2016","Matthew Anderson","IN","Indiana University","Standard Grant","Almadena Chtchelkanova","09/30/2017","$522,676.00","Bo Zhang, Jackson DeBuhr, Thomas Sterling","andersmw@indiana.edu","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","CSE","2878, 8004","7433, 8004, 8005, 9251","$0.00","Multipole methods, including the fast multipole method and the Barnes-Hut algorithm, contribute to a broad range of end-user science applications extending from molecular dynamics to galaxy formation. Multipole methods are widely applied to N-body like problems where the individual interactions of a large number of distant objects can be treated as a single interaction under the appropriate conditions.  This simplification eliminates the need for computing individual pairwise interactions and results in a drastic speed-up of computation.  However, conventional parallel multipole methods are facing serious challenges to remain competitive as computational resources approach Exascale. Many applications employing multipole methods describe very dynamic physical processes, both in their time dependence and in their range of relevant spatial scales, while conventional implementations of multipole methods are essentially static in nature leading to computational inefficiencies. <br/>This project provides a fine-grained data-driven approach for multipole methods in order to address the limitations of conventional practices and improve scalability and efficiency. The software library employs dynamic adaptive execution methods with multipole-specific strategies for fault tolerance and exception handling while simplifying the implementation of the fast multipole method and the Barnes-Hut algorithm for end-users.  The project software library immediately impacts science applications based on multipole methods by improving application scalability and efficiency and providing fault tolerance, a global address space, and an Exascale-ready execution model which integrates the entire system stack.   The software library provides a portable and easy-to-use interface that allows scientists to work more efficiently and take advantage of high performance computing resources more effectively.  The software library also serves to inform the evolution and development of other languages and programming models aiming to improve performance by shifting to message-driven techniques."
"1450377","Collaborative Research: SI2-SSI: Data-Intensive Analysis for High Energy Physics (DIANA/HEP)","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","05/01/2015","05/08/2018","G.J. Peter Elmer","NJ","Princeton University","Continuing Grant","Bogdan Mihaila","04/30/2020","$1,145,564.00","","gelmer@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","CSE","1253, 7244, 8004","7433, 8005, 8009, 8084","$0.00","Advanced software plays a fundamental role for large scientific projects. The primary goal of DIANA/HEP (Data Intensive ANAlysis for High Energy Physics) is developing state-of-the-art tools for experiments that acquire, reduce, and analyze petabytes of data. Improving performance, interoperability, and collaborative tools through modifications and additions to packages broadly used by the community will allow users to more fully exploit the data being acquired at CERN's Large Hadron Collider (LHC) and other facilities. These experiments are addressing questions at the heart of physics: What are the underlying constituents of matter? And how do they interact? With the discovery of the Higgs boson in 2012, the Standard Model of particle physics is complete. It provides an excellent description of known particles and forces.  However, the most interesting questions remain open: What is the dark matter which pervades the universe? Does space-time have additional symmetries or extend beyond the 3 spatial dimensions we know? What is the mechanism stabilizing the Higgs boson mass from enormous quantum corrections? The next generation of experiments will collect exabyte-scale data samples to provide answers. Analyzing this data will require new and better tools. <br/><br/>First, the project will provide the CPU and IO performance needed to reduce the iteration time so crucial to explore new ideas. It will develop software to effectively exploit emerging many- and multi-core hardware. It will establish infrastructure for a higher-level of collaborative analysis, building on the successful patterns used for the Higgs boson discovery and enabling a deeper communication between the theoretical community and the experimental community. DIANA?s products will sit in the ROOT framework, already used by the HEP community of more than 10000 particle and nuclear physicists. By improving interoperability with the larger scientific software ecosystem, DIANA will incorporate best practices and algorithms from other disciplines into HEP. Similarly, the project will make its computing insights, tools, and novel ideas related to collaborative analysis, standards for data preservation, and best practices for treating software as a research product available to the larger scientific community.  Finally, to improve the quality of the next generation of software engineers in HEP, DIANA will host an annual workshop on analysis tools and establish a fellowship program."
"1147606","EAGER: : Best Practices and Models for Sustainability for Robust Cyberinfrastructure Software","OAC","Software Institutes","09/01/2011","08/25/2011","Craig Stewart","IN","Indiana University","Standard Grant","Daniel Katz","08/31/2013","$296,637.00","Von Welch, Eric Wernert, Richard Knepper","stewart@iu.edu","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","CSE","8004","7916","$0.00","The NSF Software Infrastructure for Sustained Innovation (SI2) program solicitation states that software is ""central to NSF's vision of a Cyberinfrastructure Framework for 21st Century Science and Engineering (CIF21),"" and goes on to emphasize that in general software is essential to computational and data-enabled science. Indeed, the SI2 program is one vehicle by which the NSF hopes to enable sustained and well supported software providing services and functionality needed by the US science and engineering community. This yearlong study of cyberinfrastructure projects will identify best practices in the development, deployment, and support of robust cyberinfrastructure software. <br/><br/>Through a combination of detailed case studies and surveys of software producers and users, the investigators will identify best practices for the process of moving software from a ""discovery"" process to well maintained and sustainable infrastructure for 21st century science and engineering, focusing in particular on the following: Given a piece of software that provides interesting capabilities and a community that wants to use (and possibly contribute to the further development of) that software, what steps are necessary to transform that software from ""interesting tool"" to ""robust and widely used element of national infrastructure, contributing to the NSF vision for CIF21"" - ands then support and maintain that tool sustainably? This research will lead to greater availability of widely usable software tools and curriculum materials, increasing the quality of education in computer science, computational science, and STEM disciplines."
"1642446","SI2-SSE: Visualizing Astronomy Repository Data using WorldWide Telescope Software Systems","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","10/01/2016","09/21/2021","Patrick Heidorn","AZ","University of Arizona","Standard Grant","Varun Chandola","07/31/2022","$499,997.00","Douglas Roberts, Julie Steffen","heidorn@u.arizona.edu","888 N EUCLID AVE RM 510","TUCSON","AZ","857194824","5206266000","CSE","1253, 1798, 8004","1206, 7433, 8004, 8005","$0.00","WorldWide Telescope (WWT) provides a powerful data-visualization interface for data exploration and presentation. Through the open source WWT visualization software systems, this project enables the broader use of institutional and community-based, researcher-oriented astronomy data repositories and computational tools. WWT will be integrated with Astrolabe, a University of Arizona (UA) data repository targeted at researchers with legacy data, mostly supporting scholarly articles, and being built to provide dataset access using the (NSF-funded) CyVerse cyberinfrastructure. By creating a full-featured WWT web-based front-end, modularized for ease of connection to diverse astronomy data archives and computational tools, this project incorporates previously un-curated data. The UA and American Astronomical Society (AAS) project team will engage the astronomy research community to participate in these developments to make WWT a community-defined interface for data archives with linkages provided between data, software tools, and journal publications. By expanding the functionality and enabling the integration of WWT into repositories through a web-based version, WWT will become a common tool in the astronomers' workflow including supporting citizen science activities carried out by volunteers. This enables strong connections between research, education, and outreach so that an astronomer could use WWT as part of their research activities, and then share the work with educational communities and the public to benefit society.<br/><br/>The astronomy community faces many challenges related to the large scale of big data, specifically: (1) the visualization of specific datasets in the context of other observations; and (2) organizing and sharing underlying data to enable reuse, citation, and verification of results. Data archives often end up reinventing visualization systems, in turn duplicating previous efforts, and resulting in visualization interfaces built around the data rather than the needs of researchers. The astronomy researcher workflow incorporates depositing data to make it discoverable through search and browsing, accessible through open access, actionable through connections to existing tools as well as community-developed tools running on CyVerse, and finally visualizing or citing data. Through this project, WWT will provide a powerful interface for browsing, interacting with high performance and high throughput processing, and displaying data retrieved from searches of the archives. Effective searching will be enabled by integration with community-managed taxonomy, in the form of the Unified Astronomy Thesaurus (UAT) in both the Astrolabe functions and the WWT interface. In creating a full-featured, web-based client modular version of WWT as a front-end to archives, starting with Astrolabe, and integrating the UAT into search and browsing functions, this project will both serve the broad community of astronomy researchers as well as mitigating costs for archives to develop this visualization capacity."
"1642443","SI2-SSE: PERTURBO: A Software for Accelerated Discovery of Microscopic Electronic Processes in Materials","OAC","DMR SHORT TERM SUPPORT, CDS&E, DMREF","10/01/2016","09/06/2016","Marco Bernardi","CA","California Institute of Technology","Standard Grant","Seung-Jong Park","09/30/2020","$500,000.00","","bmarco@caltech.edu","1200 E CALIFORNIA BLVD","PASADENA","CA","91125","6263956219","CSE","1712, 8084, 8292","7433, 8004, 8005, 8084, 8396, 8400, 8607, 8990, 9215, 9216","$0.00","The electronic, optical, and thermal properties of materials are determined by microscopic electron collisions that take place inside materials on a time scale of a trillionth of a second (a picosecond), and are thus very hard to study experimentally. This project supports the development of a computer program, called PERTURBO, which will be used to study the dynamics of electrons in materials, with a view to understanding electron collisions. The only input needed by PERTURBO to solve the equations of quantum mechanics that control electron motion is the atomic structure of the material under study. In addition, PETURBO use innovative computational techniques to solve these equations efficiently. Thus PERTURBO may be used not just to study existing materials but also to study the next generation of more complex materials and devices. The application of PERTURBO to materials will enable disruptive advances in electronics, lighting, energy, and other technologies. Further, experimental facilities of national interest, such as the free-electron laser and other ambitious experiments to understand materials on a fundamental level, are in critical need of theoretical and computational tools to understand electron dynamics at microscopic levels. PERTURBO fills an important gap in the scientific software needed for the future development of science and technology in the United States, thus serving NSF's science mission, and places the country at the leading edge of materials and device research. Finally, the code will be freely available, user-friendly, and widely usable; users will include academic research groups, national laboratories, and industry.<br/><br/>This award supports the development of PERTURBO, a robust software platform to compute from first principles the scattering processes among electrons, phonons, defects, photons, and excitons in materials. The modular architecture of PERTURBO revolves around fast parallel routines to compute electron-phonon, electron-electron, electron-defect, and electron-photon scattering processes. The code employs novel techniques to compute and interpolate the matrix elements and converge the timescale for these scattering processes. PERTURBO imports inputs from density functional theory, density functional perturbation theory, and GW-BSE calculations, and is interfaced with routines to compute and output transport properties and ultrafast electron dynamics. For enhanced usability, a Python driver will be developed to automatically run the calculations and analyze the results. PERTURBO allows the users to quantitatively study charge, heat, and energy transport in novel materials, including semiconductors, insulators, metals, nanostructures, and interfaces, as well as ultrafast electron and excited state dynamics. The code fills&#8203; a major void in the current software ecosystem. In particular, it addresses the need to more deeply understand electron dynamics in the post-Moore's law era of electronics, as well as study ultrafast electronic processes of key importance for advanced ultrafast time-resolved spectroscopies."
"1450429","SI2-SSI: Collaborative Proposal: Performance Application Programming Interface for Extreme-Scale Environments (PAPI-EX)","OAC","Software Institutes, CDS&E","09/01/2015","08/27/2015","Jack Dongarra","TN","University of Tennessee Knoxville","Standard Grant","Seung-Jong Park","08/31/2021","$2,126,446.00","Heike Jagode, Anthony Danalis","dongarra@icl.utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","8004, 8084","7433, 8009, 8084, 9150","$0.00","Modern High Performance Computing (HPC) systems continue to increase in size and complexity. Tools to measure application performance in these increasingly complex environments must also increase the richness of their measurements to provide insights into the increasingly intricate ways in which software and hardware interact. The PAPI performance-monitoring library has provided a clear, portable interface to the hardware performance counters available on all modern CPUs and some other components of interest (scattered across the chip and system). Widely deployed and widely used, PAPI has established itself as fundamental software infrastructure, not only for the scientific computing community, but for many industry users of HPC as well.  But the radical changes in processor and system design that have occurred over the past several years pose new challenges to PAPI and the HPC software infrastructure as a whole.  The PAPI-EX project integrates critical PAPI enhancements that flow from both governmental and industry research investments, focusing on processor and system design changes that are expected to be present in every extreme scale platform on the path to exascale computing.<br/><br/>The primary impact of PAPI-EX is a direct function of the importance of the PAPI library. PAPI has been in predominant use by tool developers, major national HPC centers, system vendors, and application developers for over 15 years. PAPI-EX builds on that foundation. As important research infrastructure, the PAPI-EX project allows PAPI to continue to play its essential role in the face of the revolutionary changes in the design and scale of new systems. In terms of enhancing discovery and education, the list of partners working with PAPI-EX includes NSF computing centers, major tool developers, major system vendors, and individual community leaders, and this diverse group will help facilitate training sessions, targeted workshops, and mini-symposia at national and international meetings. Finally, the active promotion of PAPI by many major system vendors means that PAPI, and therefore PAPI-EX, will continue to deliver major benefits for government and industry in many domains.<br/><br/>PAPI-EX addresses a hardware environment in which the cores of current and future multicore CPUs share various performance-critical resources (a.k.a., 'inter-core' resources), including power management, on-chip networks, the memory hierarchy, and memory controllers between cores. Failure to manage contention for these 'inter-core' resources has already become a major drag on overall application performance. Consequently, the lack of ability to reveal the actual behavior of these resources at a low level, has become very problematic for the users of the many performance tools (e.g., TAU, HPCToolkit, Open|SpeedShop, Vampir, Scalasca, CrayPat, Active Harmony, etc.). PAPI-EX enhances and extends PAPI to solve this critical problem and prepare it to play its well-established role in HPC performance optimization. Accordingly, PAPI-EX targets the following objectives: (1) Develop shared hardware counter support that includes system-wide and inter-core measurements; (2) Provide support for data-flow based runtime systems; (3) Create a sampling interface to record streams of performance data with relevant context; (4) Combine an easy-to-use tool for text-based application performance analysis with updates to PAPI?s high-level API to create a basic, ?out of the box? instrumentation API."
"1450310","Collaborative Research: SI2-SSI: Data-Intensive Analysis for High Energy Physics (DIANA/HEP)","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","05/01/2015","05/07/2018","Kyle Cranmer","NY","New York University","Continuing Grant","Bogdan Mihaila","04/30/2021","$939,189.00","","kyle.cranmer@wisc.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","CSE","1253, 7244, 8004","7433, 8009, 8084","$0.00","Advanced software plays a fundamental role for large scientific projects. The primary goal of DIANA/HEP (Data Intensive ANAlysis for High Energy Physics) is developing state-of-the-art tools for experiments that acquire, reduce, and analyze petabytes of data. Improving performance, interoperability, and collaborative tools through modifications and additions to packages broadly used by the community will allow users to more fully exploit the data being acquired at CERN's Large Hadron Collider (LHC) and other facilities. These experiments are addressing questions at the heart of physics: What are the underlying constituents of matter? And how do they interact? With the discovery of the Higgs boson in 2012, the Standard Model of particle physics is complete. It provides an excellent description of known particles and forces.  However, the most interesting questions remain open: What is the dark matter which pervades the universe? Does space-time have additional symmetries or extend beyond the 3 spatial dimensions we know? What is the mechanism stabilizing the Higgs boson mass from enormous quantum corrections? The next generation of experiments will collect exabyte-scale data samples to provide answers. Analyzing this data will require new and better tools. <br/><br/>First, the project will provide the CPU and IO performance needed to reduce the iteration time so crucial to explore new ideas. It will develop software to effectively exploit emerging many- and multi-core hardware. It will establish infrastructure for a higher-level of collaborative analysis, building on the successful patterns used for the Higgs boson discovery and enabling a deeper communication between the theoretical community and the experimental community. DIANA?s products will sit in the ROOT framework, already used by the HEP community of more than 10000 particle and nuclear physicists. By improving interoperability with the larger scientific software ecosystem, DIANA will incorporate best practices and algorithms from other disciplines into HEP. Similarly, the project will make its computing insights, tools, and novel ideas related to collaborative analysis, standards for data preservation, and best practices for treating software as a research product available to the larger scientific community.  Finally, to improve the quality of the next generation of software engineers in HEP, DIANA will host an annual workshop on analysis tools and establish a fellowship program."
"1664137","SI2-SSI: FAMII: High Performance and Scalable Fabric Analysis, Monitoring and Introspection Infrastructure for HPC and Big Data","OAC","Software Institutes","07/01/2017","11/21/2017","Dhabaleswar Panda","OH","Ohio State University","Standard Grant","Rob Beverly","06/30/2020","$800,000.00","Karen Tomko, Xiaoyi Lu, Hari Subramoni, Kevin Manalo","panda@cse.ohio-state.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","CSE","8004","026Z, 7433, 7942, 8004, 8009","$0.00","As the computing, networking, heterogeneous hardware, and storage<br/>technologies continue to evolve in High-End Computing (HEC) platforms,<br/>it becomes increasingly essential and challenging to understand the<br/>interactions between time-critical High-Performance Computing (HPC)<br/>and Big Data applications, the software infrastructures upon which<br/>they rely for achieving high-performing portable solutions, the<br/>underlying communication fabric these high-performance middlewares<br/>depend on and the schedulers that manage HPC clusters.  Such<br/>understanding will enable all involved parties (application<br/>developers/users, system administrators, and middleware developers) to<br/>maximize the efficiency and performance of the individual components<br/>that comprise a modern HPC system and solve different grand challenge<br/>problems. There is a clear need and unfortunate lack of a high-performance and<br/>scalable tool that is capable of analyzing and correlating the<br/>communication on the fabric with the behavior of HPC/Big Data<br/>applications, underlying middleware and the job scheduler on existing<br/>large HPC systems.  The proposed synergistic and collaborative effort,<br/>undertaken by a team of computer and computational scientists from OSU<br/>and OSC, aims to create an integrated software infrastructure <br/>for high-performance and scalable Fabric Analysis, Monitoring and<br/>Introspection for HPC and Big Data. This tool will achieve the<br/>following objectives: 1) be portable, easy to use and easy to<br/>understand, 2) have high performance and scalable rendering and<br/>storage techniques and, 3) be applicable to the different<br/>communication fabrics and programming models that are likely to be<br/>used on existing large HPC systems and emerging exascale systems.  The<br/>transformative impact of the proposed research and development effort<br/>is to design a comprehensive analysis and performance monitoring tool<br/>for applications of current and next generation multi<br/>petascale/exascale systems to harness the maximum performance and<br/>scalability.<br/><br/>The proposed research and the associated infrastructure will have a<br/>significant impact on enabling optimizations of HPC and Big Data<br/>applications that have previously been difficult to provide. These<br/>potential outcomes will be demonstrated by using the proposed<br/>framework to validate a variety of HPC and Big Data benchmarks and<br/>applications under multiple scenarios.  The integrated middleware and<br/>tools will be made publicly available to the community through public<br/>repositories and publications in the top forums, enabling other MPI<br/>and Big Data stacks to adopt the designs.  Research results will also<br/>be disseminated to the collaborating organizations of the<br/>investigators to impact their HPC software products and<br/>applications. The proposed research directions and their solutions<br/>will be used in the curriculum of the PIs to train undergraduate and<br/>graduate students, including under-represented minorities and female<br/>students. The technical challenges addressed by the proposal include: 1)<br/>Scalable visualization of large and complex HEC networks so as to<br/>provide a near instant rendering to end users, 2) A generalized data<br/>gathering scheme which is easily portable to multiple communication<br/>fabrics, novel compute architectures and high-performance middleware,<br/>3) Enhanced data storage performance through optimized database<br/>schemas and the use of memory-backed key value stores/databases, 4)<br/>Support in MPI, PGAS, and Big Data libraries to enable the proposed<br/>monitoring, analysis, and introspection framework, and 5) Enabling<br/>deeper introspection of particular regions of application.  The<br/>research will also be driven by a set of HPC and Big Data<br/>applications. The transformative impact of the proposed research and<br/>development effort is to design a comprehensive analysis and<br/>performance monitoring tool for applications of current and next<br/>generation multi petascale/exascale systems to harness the maximum<br/>performance and scalability."
"1148515","SI2-SSI: Distributed Workflow Management Research and Software in Support of Science","OAC","Software Institutes","04/01/2012","08/30/2012","Ewa Deelman","CA","University of Southern California","Standard Grant","Rajiv Ramnath","03/31/2018","$2,153,597.00","Miron Livny","deelman@isi.edu","3720 S FLOWER ST FL 3","LOS ANGELES","CA","900074304","2137407762","CSE","8004","8004, 8009","$0.00","This award funds the enhancement of state-of-the-art workflow technologies and their promotion within a broad range of scientific domains. The overarching goal is to advance scientific discovery by providing scientists with tools that can manage computations on national cyberinfrastructure in a way that is reliable and scalable.  <br/><br/>The key technology supported by this award is the Pegasus Workflow Management System (Pegasus).  This program of work includes the development, support, and maintenance of Pegasus. Pegasus allows users to declaratively describe their workflow, then makes a plan that maps this description onto the available execution resources and executes the plan. This approach is scalable, reliable, and supports applications running on campus resources, clouds, and national cyberinfrastructure. <br/><br/>The work conducted under this award will 1) enhance the sustainability of the Pegasus software through the expanded adoption of sound software engineering practices and improved usability, 2) enhance core capabilities, especially in the area of data management, to meet user requirements and make Pegasus easier to integrate into end-to-end scientific environments, 3) promote the adoption of workflow management technologies within domain and computer sciences.  <br/><br/>Intellectual Merit: Pegasus WMS brings innovative and powerful frameworks to the desk of the scientist. Through close collaboration with a broad community of engaged users, experimentation in large-scale distributed computing is made possible. This experimentation supports the development of new scientific workflow management concepts, frameworks and technologies. The proposed work also supports scientific reproducibility by providing a workflow management system that integrates and automates data, metadata, and provenance management functions. <br/><br/>Broader Impact:  Pegasus WMS has been adopted by scientists from different domains and has been integrated into end-user environments such as workflow composition tools and portals. The program of outreach and education facilitated by this award will expand the impact of Pegasus through tutorials, workshops, meetings with potential users, and online materials.  The proposed interface enhancements will allow more end-user environments to leverage Pegasus? capabilities and will extend the impact of Pegasus to a broader spectrum of users."
"1216853","SI2: Conceptualization: Dynamic Languages for Scalable Data Analytics","OAC","ADVANCES IN BIO INFORMATICS, Special Projects - CCF, CI-TEAM, Software Institutes","10/01/2012","11/18/2013","Jan Vitek","IN","Purdue University","Standard Grant","Daniel Katz","03/31/2014","$200,000.00","William Cleveland, Ananth Grama, Suresh Jagannathan, Olga Vitek","j.vitek@neu.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","CSE","1165, 2878, 7477, 8004","7433, 8211","$0.00","This planning grant gathers scientific community requirements for a set of capabilities termed Scalable Data Analytics.  The project investigates community needs to support scientific discovery by  providing an effective interface between extant hardware resources, data sources and repositories, and system software infrastructure. The proposed effort focuses on software environments and tools for data acquisition, management, visualization, sharing, and analysis for the working scientist, which can scale up to massively parallel and cloud fabrics, but, crucially, which can as easily scale down to a single laptop.<br/><br/>Software systems for data analytics are integral to the fabric of scientific innovation. The ability to acquire, process, and analyze large amounts of complex structured and unstructured data is at the core of diverse disciplines. While scientists can exploit large repositories of software tools optimized and refined over the years, significant new challenges are posed by the rapidly evolving characteristics of scientific datasets. These challenges are addressed by software systems that enable development of new software incrementally, modification of existing methods, or techniques for integrating pipelines of off-the-shelf components. For such application needs, scientists increasingly rely on dynamic computer programming languages. These languages facilitate interactive prototyping, support rapid development, and can be embedded or used to manage complex scientific software pipelines."
"1450168","Collaborative Research: SI2-SSI: Big Weather Web:  A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","OAC","Physical & Dynamic Meteorology, Software Institutes, EarthCube","08/01/2015","08/04/2015","Gretchen Mullendore","ND","University of North Dakota Main Campus","Standard Grant","Alan Sussman","07/31/2018","$168,182.00","","gretchen@atmos.und.edu","264 CENTENNIAL DR","GRAND FORKS","ND","582026059","7017774151","CSE","1525, 8004, 8074","4444, 7433, 8009, 9150","$0.00","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities."
"1740212","SI2-SSE: Collaborative Research: Integrated Tools for DNA Nanostructure Design and Simulation","OAC","Software Institutes, CDS&E, DMREF","09/01/2017","08/29/2017","Aleksei Aksimentiev","IL","University of Illinois at Urbana-Champaign","Standard Grant","Rob Beverly","08/31/2020","$249,994.00","","aksiment@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","8004, 8084, 8292","026Z, 067E, 084E, 7433, 8004, 8005, 9263","$0.00","Nanotechnology could one day revolutionize several activities of great importance to our national interest, including how we manufacture consumer products, how we diagnose and treat disease, and how we detect and neutralize threats to our defense. One promising approach to atomically precise construction is adapting molecular building blocks from living organisms such as DNA, RNA, and proteins, and repurposing them to self-assemble into prescribed shapes, devices, and materials. A key bottleneck to progress is the complexity of designing, building, and testing nanostructures comprised of thousands or millions of atoms. The goal of this project is to accelerate development of bio-inspired nanostructures by integrating two widely adopted software tools used in bio-nanostructure design and physics-based molecular simulation. The products of this effort will enhance our fundamental capability to understand and precisely engineer self-assembled biomolecular nanostructures, which, when coupled with experimental validation in the laboratory, will enable future demand-meeting applications of bionanotechnology.<br/><br/>Toward realizing the goal of programming matter with nanoscale precision, this project will develop software interfaces between two classes of molecular design programs that, until now, have been evolving independently from one another. A widely adopted DNA structure design program, Cadnano, will be extended to utilize the results of physics-based microscopic simulations, enabling an iterative structure design process. A leading molecular graphics program, VMD (Visual Molecular Dynamics), will be developed to seamlessly visualize Cadnano designs, provide their structural interpretation, and enable further modification of the structures using an arsenal of computational structural biology and nanotechnology tools. Both developments will utilize recent advances in cloud computing technologies, making the DNA structure design software available anywhere and to anyone in a platform-independent manner.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Civil, Mechanical and Manufacturing Innovation in the Directorate of Engineering."
"1440420","SI2-SSE: Scalable Big Data Clustering by Random Projection Hashing","OAC","Special Projects - CCF, Software Institutes","09/01/2014","08/08/2014","Philip Wilsey","OH","University of Cincinnati Main Campus","Standard Grant","Bogdan Mihaila","08/31/2019","$498,127.00","","philip.wilsey@uc.edu","2600 CLIFTON AVE","CINCINNATI","OH","452202872","5135564358","CSE","2878, 8004","2878, 7433, 8004, 8005","$0.00","This project plans to develop a distributed algorithm for secure clustering of high dimensional data sets.  Fields in health and biology are significantly benefited by data clustering scalability.  Bioinformatic problems such as Micro Array clustering, Protein-Protein interaction clustering, medical resource decision making, medical image processing, and clustering of epidemiological events all serve to benefit from larger dataset sizes.  The algorithm under development, called Random Projection Hash or RPHash, utilizes aspects of locality sensitive hashing (LSH) and multi-probe random projection for computational scalability and linear achievable gains from parallel speed.  Furthermore, RPHash provides data anonymization through destructive manipulation of the data preventing de-anonymization attacks beyond standard best practices database security methods.  RPHash will be deployable on commercially available cloud resources running the Hadoop (MRv2) implementation of MapReduce.  The exploitation of general purpose cloud processing solutions allows researchers to scale their processing needs using virtually limitless commercial processing resources.<br/><br/>The RPHash algorithm uses various recent techniques in data mining along with a new approach toward achieving algorithmic scalability on distributed systems.  The basic intuition of RPHash is to combine multi-probe random projection with discrete space quantization.  Regions of high density are then regarded as centroid candidates.  To follow common parameterized, k-means methods, the top k regions will be selected.  The focus on a randomized, and thus non-deterministic, clustering algorithm is somewhat uncommon in computing, but common for ill-posed, combinatorially restrictive problems such as clustering and partitioning.  Despite theoretical results showing that k-means has an exponential worst case complexity, many real world problems tend to fair much better under k-means and other similar algorithms."
"1743180","Collaborative Research: NISC SI2-S2I2 Conceptualization of CFDSI: Model, Data, and Analysis Integration for End-to-End Support of Fluid Dynamics Discovery and Innovation","OAC","FD-Fluid Dynamics, Special Initiatives, Software Institutes, CDS&E","03/01/2018","06/27/2018","Beverley McKeon","CA","California Institute of Technology","Continuing Grant","Stefan Robila","08/31/2020","$44,284.00","","mckeon@caltech.edu","1200 E CALIFORNIA BLVD","PASADENA","CA","91125","6263956219","CSE","1443, 1642, 8004, 8084","026Z, 8004, 8005, 9102, 9263","$0.00","Fluid dynamics is a broad field spanning a large number of science and engineering problem domains that are critical to a wide variety of important applications including manufacturing, climate analysis, environment, health, transportation, propulsion, and power generation. To support the fluid dynamics research and applications community, this project seeks to engage the community in order to conceptualize a future institute, the Computational Fluid Dynamics Software Infrastructure (CFDSI), that will broadly develop, share, and apply computational tools for the generation and analysis of fluid dynamics data from both experimental and computational sources.  After its conceptualization, CFDSI will create and extend tools for problem definition, solution, and analysis of both computational and experimental investigations. The primary objective is to facilitate the sharing of computational tools and data resources through a rich and extensible set of software components that can be integrated into a wide range of existing fluid dynamics analysis tools. By improving the efficiency of tools and their ease of use, the ability for scientists and engineers to accurately predict and understand how complex fluid flows behave will be enhanced, having a significant impact on design, innovation, and discovery across the vast range of applications where fluid dynamics plays a role. CFDSI even has the potential to impact on K-12, undergraduate and graduate education by making a wide variety of resources available to students for fluid dynamics investigations.<br/><br/>The conceptualized institute will make a wide variety of powerful simulation, data, and analysis resources available to the fluid dynamics research community by lowering or eliminating barriers associated with the adoption and use of these resources. The software infrastructure will have a number of positive impacts on the fluid dynamics research community. To do so, CFDSI will connect the best research in fluid dynamics to the best research in data science/analytics within a highly sustainable software development environment. Specifically, CFDSI will: 1) enhance the dissemination of fluid dynamics data resources and advances in CFD modeling, 2) facilitate collaboration in fluid dynamics research, especially between computational and experimental researchers, 3) enable detailed comparisons between different data sources and detailed validation of computational models, 4) ease the use of advanced CFD models, methods, and codes in new and complex applications, 5) facilitate advanced analytics, such as uncertainty quantification, data compression, and optimization, 6) provide students access to advanced CFD methods and data resources, both computational and experimental, to enhance both graduate and undergraduate education in fluid dynamics, 7) improve the sustainability of current and future CFD software, and 8) facilitate the management of the growing body of fluid dynamics data sets. These outcomes will greatly enhance the effectiveness and productivity of research in fluid dynamics. In particular, they will transform the conduct of fluid dynamics research by: 1) making it more collaborative, 2) enhancing the credibility of research results, 3) enabling discovery, 4) reducing the cost of pursuing new research questions, and 5) diversifying and widening the fluid dynamics community through lowering the barriers associated with accessing and adopting CFD codes and large data sets. Software components will be designed for both analysis of experimental and computational databases as well as direct integration into CFD codes. The latter will enable in situ data analytics to address the growing chasm between data creation rate (solver performance) and data storage rate/volume (I/O resources). After conceptualization and implementation, CFDSI will enable more effective fluid dynamics research and thus impact the wide variety of application domains in which fluid dynamics is critical including manufacturing climate, environment, health, transportation, propulsion, and power generation (including conventional, alternative, and nuclear sources) which will, in turn, strongly impact our economy. Additionally, CFDSI will provide the capability for immersive simulations and experiments that will close the loop on idea, insight, discovery, and design through establishing links to in situ data analytics and problem redefinition during ongoing simulations or experiments. Finally, CFDSI will impact other problem domains governed by partial differential equations (e.g. solid mechanics) by serving as a model and starting point for similar domain-specific software infrastructures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1657286","Collaborative Research: SI2-SSI: Software Framework for Electronic Structure of Molecules and Solids","OAC","Software Institutes","07/21/2016","09/20/2016","Garnet Chan","CA","California Institute of Technology","Standard Grant","Amy Walton","07/31/2020","$600,000.00","","garnetc@caltech.edu","1200 E CALIFORNIA BLVD","PASADENA","CA","91125","6263956219","CSE","8004","7433, 8009, 9216","$0.00","Many traditionally experimental disciplines such as chemistry and materials science are rapidly changing due to our increasing ability to predict properties of molecules and materials purely by simulation. This is particularly true when molecules meet solid surfaces - due to the particular challenges of experiments in such a setting. Yet the molecule-on-surface frontier encompasses a vast class of problems of tremendous practical importance:  industrial applications facilitated by surface processes are estimated to produce globally more than than 15 trillion USD worth of goods and products.  This research will improve our ability to simulate the physics and chemistry of molecules on surfaces by extending the advanced simulation methodologies that were originally developed by for modeling electrons in molecules. This project will not only advance our fundamental understanding of the surface science but also open a road to technological applications relevant to producing and storing clean energy and in designing improved catalysts. The research may result in a new computer software framework for simulating electrons in molecules and materials. This software will be a unique contribution to the U.S. cyberinfrastructure and spur further innovation by other researchers in the US and worldwide, who will be able to access its source code for free. The software framework will also serve as an education platform for training computational chemists and materials scientists.<br/><br/>A frontier simulation challenge lies at the intersection of the two domains of chemistry and materials science - namely to determine, with predictive accuracy, the properties and chemistry of molecules on solid surfaces. The molecule-on-surface frontier encompasses a vast class of problems of tremendous practical importance: heterogeneous catalysis, photovoltaics, and emerging electronic materials. Yet, from a simulation perspective, it is not currently possible to efficiently combine the recent advances in highly accurate many-body molecular and periodic condensed phase methodologies in these problems, due to a significant gap between how the electronic structure theories of molecules and materials are formulated, as reflected in distinct algorithms and disjoint codebases. The goal of this project is to reduce and/or completely eliminate the gap between molecular and solid-state electronic structure methodologies, in theory, algorithms, and in usable community software implementations. This will be achieved by building an ambitious Electronic structure for Molecules and Solids (EMOS) software framework that will permit accurate computation of the first-principles electronic structure of both molecules and solids on an equivalent footing - and with the high efficiency necessary for high-throughput screening or ab initio molecular dynamics. These efforts build on the leading track-record of the principal investigators in developing open-source quantum chemistry software as well as automated computer implementation and high-performance parallel libraries. The project will allow the advances from molecular electronic structure - embedding, reduced-scaling many-body methodology, accurate excited-state electronic structure, and others - to be applied routinely to molecules, materials, and combinations of the two as relevant to surface chemistry. This has great potential to advance the state-of-the-art in treatment of electronic structure and open new lines of theoretical inquiry. The resulting open-source production-quality toolkit will be validated against experimental data for a host of surface phenomena, from exciton dynamics to surface spectroscopy and catalysis. An open-source US-based advanced materials code is a long-standing omission in U.S. cyberinfrastructure. As a high-performance framework for simulation of electronic structure of molecules, solids, and their interfaces with unprecedented accuracy, EMOS will be a significant contribution to this effort. Further, the modular component based structure will be able to be integrated with other major electronic structure packages through the reuse of the modules. This project will provide invaluable training opportunities to the students and postdocs who will develop the software framework under the direct supervision of principal investigators. In addition, each project site will contribute to the development of a stakeholder network for EMOS by hosting, each summer, visiting students and faculty representing the broader theoretical community, to train them on the use of EMOS in research and education. The project team will also use EMOS in teaching classes and summer schools, building on already established efforts in this area; these efforts will also be extended to an online setting."
"1642453","SI2-SSE: The Next Generation of The Montage Image Mosaic Engine: Beyond Mosaics","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","10/01/2016","09/08/2016","Graham Berriman","CA","California Institute of Technology","Standard Grant","Amy Walton","12/31/2019","$499,795.00","","gbb@ipac.caltech.edu","1200 E CALIFORNIA BLVD","PASADENA","CA","91125","6263956219","CSE","1253, 1798, 8004","1206, 7433, 8004, 8005","$0.00","Data sets produced by modern space missions and ground-based telescopes are so large and complex that they threaten to overwhelm astronomers' ability to analyze them.  In response, the astronomy and IT communities are actively engaged in developing powerful new tools that can process these data.  The Montage Image Mosaic Engine is a participant in this community of next generation tool development.  Up to now, Montage has been used for high-performance processing of images, so that astronomers can take small images and create a mosaic that reveals the structure of a large region of the sky.  With this new funding, Montage will be extended from a processing engine into a ""one-stop shopping"" toolkit that will enable astronomers to manage their image data as well as process them.  They will be able to analyze their data and repair defects found in them, discover and access datasets released worldwide that will make analysis of their own data more valuable, compare and integrate their data sets with other data in web browsers, create their own data archives that can be accessed, as required, by team members or the public, and process large scale image mosaics through an on-demand service that will be hosted by a new astronomy data sharing platform called SciServer.  All these features will dramatically reduce the overhead needed in managing data and will lead to a corresponding increase in productivity.  Montage will at the same time retain all its existing capabilities.  Montage is written as a toolkit rather than as an application, so astronomers can use each of its functions independently of each other and embed them in their processing environments.  It is written in a language, C, that optimizes processing, runs on all Unix based operating systems used by all astronomers, and functions just as well on single machines as on massive computational facilities.  These features enable Montage to be used by astronomers analyzing data on their desktops as well as by teams of scientists creating complex, massive new data sets for distribution to the astronomy community.  One example is its use in the analysis of images to detect new near-Earth asteroids.  It has also been taken up by the computer science community, who use it as an example of a real-world application in driving the development of powerful new cyberinfrastructure platforms for processing massive scientific data sets.  One of the important current topics in scientific software is how to make powerful new tools sustainable so that they can be updated and used over many years.  Montage is a potential model of sustainable software, and is active in national debates on approaches to sustainability.  A version of Montage will be developed that will operate on Windows platforms, used by educators and students.  This brings Montage to high-school students, youth and amateur astronomers.  Partnerships have been built with two groups of educators who will work to integrate Montage into tools used by high-school students.  For example, Montage will be integrated into tools used by students and youth to process images through a network of educational telescopes called SkyNet.<br/><br/>This project will deploy new functionality currently existing as proof-of-concept prototypes, developed as a result of community input, that will transform Montage into a ""data and metadata access, data management and curation toolkit in a box,"" while retaining all its current image processing functionality.  The new functions are: discovery and acquisition of data at scale in space and in time, using R-tree based indexing schemes; tools for analyzing and repairing metadata; tools for visualizing sky coverage of multiple data sets; migration of an on-demand image mosaic service from IPAC to operate at scale on the SciServer platform; investigation of processing images in browsers with Javascript-enabled tools, and the development of custom image processing environments. Finally, a Windows-enabled version of Montage will be delivered.  Montage will continue to have all the benefits of its architecture.  It is written in C for portability and sustainability, is highly scalable and delivered as a toolkit that is easy to incorporate into processing environments.  Montage is the only mosaic engine with all these characteristics.  It is delivered with an Open Source BSD 3-clause license and accessible on GitHub.  Development uses rigorous software engineering methods, and exploits a hybrid of the evolutionary prototyping and staged delivery development models.  Astronomy data sets are becoming much larger and at the same time more complex.  Montage will provide the tools to access, discover and manage data sets and their metadata.  Many more datasets will be made accessible in this way, and astronomers will be able to develop their own data access services without the overhead of maintaining a database.  Because it is written in ANSI-C and has structured inputs and outputs, astronomers will be able to integrate it into any *nix-based platform or environment to support data management and integration.  The capability to search in space and time will be of immense value in querying upcoming time-domain data sets such as ZTF and LSST.  The on-demand service will be an exemplar of deploying image processing services at scale.  Thus, Montage will be of value to large missions and projects, as well as the long-tail of scientists working in small groups.  The impact of Montage will be measured by tracking growth in citations to Montage, and to the creation of data sets that use the software, and by tracking usage of services and environments that use Montage as part of their underpinnings.  Montage is having considerable impact on broad intellectual and educational areas outside astronomical research: (1) Advancing Learning and Discovery- It is used in developing Education and Public Outreach products such as a 5-color Galactic Plane image of Herschel data.  It has found applicability in undergraduate research projects.  The Windows version will bring Montage to high-school students, youth and amateur astronomers.  Browser processing of images may lead to more sophisticated Citizen Science projects; (2) Dissemination to Enhance Scientific and Technological Understanding- It is recognized as an example of sustainable software, and the development team is active in national debates on the issues of software transparency and sustainability; (3) Enhancing Infrastructure for Research and Education- Montage is an exemplar application used to develop national cyber-infrastructure."
"1743188","SI2-S2I2 Conceptualization: Conceptualizing a US Research Software Sustainability Institute (URSSI)","OAC","Software Institutes","12/15/2017","06/18/2020","Karthik Ram","CA","University of California-Berkeley","Standard Grant","Rob Beverly","06/30/2021","$499,999.00","Jeffrey Carver, Daniel Katz, Sandra Gesing, Nicholas Weber","karthik.ram@berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","CSE","8004","026Z, 8004, 9102","$0.00","Many science advances have been possible thanks to use of software. This software, also known as ""research software"", has become essential to progress in science and engineering. The scientists who develop the software are experts in their discipline, but do not have sufficient understanding of the practices that make software development easier, and the software more robust, reliable, maintainable and sustainable.  This project will work with these scientists to understand how the research community can best work together to design and maintain better software with lower effort, so that they and others can continue to use it over long periods of time.  This project will conduct several workshops and a survey in order to gather and understand the community's needs and the software expertise of its members. These needs will be widely disseminated via newsletters and via a website. The primary deliverable of this project will be a design and strategic plan for a US Research Software Sustainability Institute (URSSI) which will serve as a community hub and provide services to scientists that will help them create improved, more sustainable software. This software in turn will accelerate the progress of science, thus serving NSF's mission.<br/><br/>Modern research is inescapably digital, with data and publications most often created, analyzed, and stored electronically, using tools and methods expressed in software. This ""research software"" is essential to progress in science, engineering, and all other fields, but it is not developed in an efficient or sustainable way. The researchers who develop this software, while well-versed in their discipline, generally do not have sufficient training and understanding of best practices that ease development and maintainability and that encourage sustainability and reproducibility. In response, this project is conceptualizing a US Research Software Sustainability Institute that will validate and address at least three classes of concerns (functioning of the individual and team, the research software, and the research field itself), impacting all software development and maintenance projects across all of NSF. URSSI conceptualization includes workshops and a widely-distributed survey that engages important stakeholder communities to learn about the software they produce and use, and the ways they contemplate sustaining it, following the paths blazed by other successful software institutes. Communication is a key component of this project, with newsletters, a web site, survey outputs, and social media used to provide broad dissemination and engagement. The workshops, survey, and community management approach allow the conceptualization project to iteratively build on existing, extensive understanding of the challenges for sustainable software and its developers.  The project also addresses how URSSI could formalize, diversify, and improve the pipeline under which students enter universities, learn about and contribute to software, then graduate to full-time positions where they make use of their software skills, to increase the diversity of those entering research software development and to retain diversity over their university careers. The conceptualization team has accumulated hundreds of person-years of combined experience by thinking, researching, and living scientific software; this will be combined with feedback from the broader community. It leverages existing collaborations to expand both the community and the project's knowledge of its needs, to plan the best possible URSSI. The results will create an eager supportive community, a concrete institute plan configured to offer valued services, and a published survey and data that demonstrates community need."
"1642369","Collaborative Research: SI2-SSE: WRENCH: A Simulation Workbench for Scientific Worflow Users, Developers, and Researchers","OAC","Software Institutes","01/01/2017","09/12/2016","Henri Casanova","HI","University of Hawaii","Standard Grant","Stefan Robila","12/31/2019","$257,956.00","","henric@hawaii.edu","2425 CAMPUS RD SINCLAIR RM 1","HONOLULU","HI","96822","8089567800","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Many scientific breakthroughs can only be achieved by performing complex processing of vast amounts of data efficiently.  In domains as crucial to our society as climate modeling, oceanography, particle physics, seismology, or computational biology (and in fact in most fields of physics, chemistry, and biology today), scientists nowadays routinely define ""scientific workflows"". These workflows are complex descriptions of scientific processes as data and inter-dependent computations on these data. When executed, typically with great expenses of computing, storage, and networking hardware, these workflows can produce groundbreaking results. A famous and recent example is the workflow that was used as part of the LIGO project to confirm the first detection of gravitational waves from colliding black holes. Scientific workflows are mainstays in today's science. Their efficient  execution (in terms of speed, reliability, and cost) is thus crucial. This project seeks to provide a software framework, called WRENCH (Workflow Simulation Workbench), that will make it possible to simulate large-scale hypothetical scenarios quickly and accurately on a single computer, obviating the need for expensive and time-consuming trial and error experiments. WRENCH potentially enables scientists to make quick and informed choices when executing their workflows, software developers to implement more efficient software infrastructures to support workflows, and researchers to develop novel efficient algorithms to be embedded within these software infrastructures.  In addition, WRENCH makes it possible to bring scientific workflow content into undergraduate and graduate computer science curricula. This is because meaningful knowledge can be gained by students using a single computer and the WRENCH software stack, making such learning possible even at institutions without access to high-end computing infrastructures, such as many non-Ph.D.-granting and minority-serving institutions. As a result, this work will contribute to producing computer science graduates better equipped to take an active role in the advancing of science.  Due to its potentially transformative impact on scientific workflow usage, development, research, and education, this project promises to promote the progress of science across virtually all its fields, ultimately resulting in broad and numerous benefits to our society.<br/><br/>Scientific workflows have become mainstream for conducting large-scale scientific research.  As a result, many workflow applications and Workflow Management Systems (WMSs) have been developed as part of the cyberinfrastructure to allow scientists to execute their applications seamlessly on a range of distributed platforms.  In spite of many success stories, building large-scale workflows and orchestrating their executions efficiently (in terms of performance, reliability, and cost) remains a challenge given the complexity of the workflows themselves and the complexity of the underlying execution platforms.  A fundamental necessary next step is the establishment of a solid ""experimental science"" approach for future workflow technology development. Such an approach is useful for scientists who need to design workflows and pick execution platforms, for WMS developers who need to compare alternate design and implementation options, and for researchers who need to develop novel decision-making algorithms to be implemented as part of WMSs.  The broad objective of this work is to provide foundational software, the Workflow Simulation Workbench (WRENCH), upon which to develop the above experimental science approach.  Capitalizing on recent advances in distributed application and platform simulation technology, WRENCH makes it possible to (i) quickly prototype workflow, WMS implementations, and decision-making algorithms; and (ii) evaluate/compare alternative options scalably and accurately for arbitrary, and often hypothetical, experimental scenarios.  This project will define a generic and foundational software architecture, that is informed by current state-of-the-art WMS designs and planned future designs.  The implementation of the components in this architecture when taken together form a generic ""scientific instrument"" that can be used by workflow users, developers, and researchers.  This scientific instrument will be instantiated for several real-world WMSs and used for a range of real-world workflow applications. In a particular case-study, it will be used with a popular WMS (Pegasus) to revisit published results and scheduling algorithms in the area of workflow planning optimizations. The objective is to demonstrate the benefit of using an experimental science approach for WMS research.  Another impact of this project is that it  makes it possible to include scientific workflow content pervasively in undergraduate and graduate computer science curricula, even for students without any access to computing infrastructure, by defining meaningful pedagogic activities that only require a computer and the WRENCH software stack. This educational impact will be demonstrated in the classroom in both undergraduate and graduate courses at our institutions."
"1661663","SI2-SSI: Lidar Radar Open Software Environment (LROSE)","OAC","Physical & Dynamic Meteorology, Software Institutes, EarthCube","08/15/2016","11/28/2016","Michael Bell","CO","Colorado State University","Standard Grant","Seung-Jong Park","01/31/2022","$2,499,996.00","","mmbell@hawaii.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","CSE","1525, 8004, 8074","4444, 7433, 8004, 8009, 9150","$0.00","Modern radars and lidars are a diverse class of instruments, capable of detecting molecules, aerosols, birds, bats and insects, winds, moisture, clouds, and precipitation. Scientists and engineers use them to perform research into air quality and pollution, dangerous biological plumes, cloud physics, cloud extent, climate models, numerical weather prediction, road weather, aviation safety, severe convective storms, tornadoes, hurricanes, floods, and movement patterns of birds, bats and insects. Radars and lidars are critical for protecting society from high impact weather and understanding the atmosphere and biosphere, but they are complex instruments that produce copious quantities of data that pose many challenges for researchers, students, and instrument developers. This project will develop a new set of tools called the Lidar Radar Open Software Environment (LROSE) to meet these challenges and help address the 'big data' problem faced by users in the research and education communities. This project will open new avenues of scientific investigation, including data assimilation to improve weather forecasts, and help to maximize returns on NSF investments in weather and climate research by providing better software tools to researchers, students, and educators. Improving the effectiveness of NSF research will provide significant scientific and societal benefits through an improved understanding of many diverse scientific topics that are relevant to public safety, national defense, and the global economy.<br/><br/>The LROSE project will develop a 'Virtual Toolbox' with a set of software tools needed for a diverse set of scientific applications. LROSE will be packaged so that it can be run on a virtual machine (VM), either locally or in the cloud, and stocked with core algorithm modules for those typical processing steps that are well understood and documented in the peer-reviewed literature. LROSE will enable the user community to use the core toolset to develop new research modules that address the specific needs of the latest scientific research. Through the VM Toolbox and a core software framework, other developers of open-source radar software can then provide their own compatible software tools to the set. By combining the open source approach with recent developments in virtual machines and cloud computing, we will develop a system that is both highly capable and easy to run on virtually any hardware, without the complexity of a compilation environment. The LROSE project will build on existing prototypes and available software elements, while facilitating community development of new techniques and algorithms to distribute a suite of documented software modules for performing radar and lidar analysis. These modules will each implement accredited scientific methods referencing published papers. The infrastructure and modules will allow researchers to run standard procedures, thereby improving the efficiency and reproducibility of the analyses, and encourage researchers to jointly develop new scientific approaches for data analysis. The use of collaborative open source methods will lead to a suite of available algorithmic modules that will allow scientists to explore radar and lidar data in new, innovative ways. Researchers will benefit from the improved toolset for advancing understanding of weather and climate, leading to a positive outcome in the advancement of scientific knowledge and societal benefits."
"1743185","Collaborative Research: NISC SI2-S2I2 Conceptualization of CFDSI: Model, Data, and Analysis Integration for End-to-End Support of Fluid Dynamics Discovery and Innovation","OAC","FD-Fluid Dynamics, Special Initiatives, Software Institutes, CDS&E","03/01/2018","06/27/2018","Mark Shephard","NY","Rensselaer Polytechnic Institute","Continuing Grant","Stefan Robila","08/31/2020","$65,000.00","Onkar Sahni, Cameron Smith","shephard@rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","1443, 1642, 8004, 8084","026Z, 8004, 8005, 9102, 9263","$0.00","Fluid dynamics is a broad field spanning a large number of science and engineering problem domains that are critical to a wide variety of important applications including manufacturing, climate analysis, environment, health, transportation, propulsion, and power generation. To support the fluid dynamics research and applications community, this project seeks to engage the community in order to conceptualize a future institute, the Computational Fluid Dynamics Software Infrastructure (CFDSI), that will broadly develop, share, and apply computational tools for the generation and analysis of fluid dynamics data from both experimental and computational sources.  After its conceptualization, CFDSI will create and extend tools for problem definition, solution, and analysis of both computational and experimental investigations. The primary objective is to facilitate the sharing of computational tools and data resources through a rich and extensible set of software components that can be integrated into a wide range of existing fluid dynamics analysis tools. By improving the efficiency of tools and their ease of use, the ability for scientists and engineers to accurately predict and understand how complex fluid flows behave will be enhanced, having a significant impact on design, innovation, and discovery across the vast range of applications where fluid dynamics plays a role. CFDSI even has the potential to impact on K-12, undergraduate and graduate education by making a wide variety of resources available to students for fluid dynamics investigations.<br/><br/>The conceptualized institute will make a wide variety of powerful simulation, data, and analysis resources available to the fluid dynamics research community by lowering or eliminating barriers associated with the adoption and use of these resources. The software infrastructure will have a number of positive impacts on the fluid dynamics research community. To do so, CFDSI will connect the best research in fluid dynamics to the best research in data science/analytics within a highly sustainable software development environment. Specifically, CFDSI will: 1) enhance the dissemination of fluid dynamics data resources and advances in CFD modeling, 2) facilitate collaboration in fluid dynamics research, especially between computational and experimental researchers, 3) enable detailed comparisons between different data sources and detailed validation of computational models, 4) ease the use of advanced CFD models, methods, and codes in new and complex applications, 5) facilitate advanced analytics, such as uncertainty quantification, data compression, and optimization, 6) provide students access to advanced CFD methods and data resources, both computational and experimental, to enhance both graduate and undergraduate education in fluid dynamics, 7) improve the sustainability of current and future CFD software, and 8) facilitate the management of the growing body of fluid dynamics data sets. These outcomes will greatly enhance the effectiveness and productivity of research in fluid dynamics. In particular, they will transform the conduct of fluid dynamics research by: 1) making it more collaborative, 2) enhancing the credibility of research results, 3) enabling discovery, 4) reducing the cost of pursuing new research questions, and 5) diversifying and widening the fluid dynamics community through lowering the barriers associated with accessing and adopting CFD codes and large data sets. Software components will be designed for both analysis of experimental and computational databases as well as direct integration into CFD codes. The latter will enable in situ data analytics to address the growing chasm between data creation rate (solver performance) and data storage rate/volume (I/O resources). After conceptualization and implementation, CFDSI will enable more effective fluid dynamics research and thus impact the wide variety of application domains in which fluid dynamics is critical including manufacturing climate, environment, health, transportation, propulsion, and power generation (including conventional, alternative, and nuclear sources) which will, in turn, strongly impact our economy. Additionally, CFDSI will provide the capability for immersive simulations and experiments that will close the loop on idea, insight, discovery, and design through establishing links to in situ data analytics and problem redefinition during ongoing simulations or experiments. Finally, CFDSI will impact other problem domains governed by partial differential equations (e.g. solid mechanics) by serving as a model and starting point for similar domain-specific software infrastructures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1148258","Collaborative Research:  SI2-SSI:  Empowering the Scientific Community with Streaming Data Middleware:  Software Integration into Complex Science Environments","OAC","ADVANCES IN BIO INFORMATICS, ECOSYSTEM STUDIES, Software Institutes, Cybersecurity Innovation","08/01/2012","08/07/2012","Corinna Gries","WI","University of Wisconsin-Madison","Standard Grant","Daniel Katz","07/31/2015","$203,327.00","","cgries@wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","CSE","1165, 1181, 8004, 8027","1165, 1181, 7434, 8004, 8009, 8027","$0.00","This project will catalyze a new generation of sensor-based streaming data applications by building on the foundation of the Open Source DataTurbine (OSDT) middleware (www.dataturbine.org). This will involve strategic software development, systems integration and testing, science experiments, and education and outreach. The software developments will include (A) tools for configuring, testing, and controlling distributed sensor systems, (B) software interfaces that are compliant with the Open Geospatial Consortium Sensor Web Enablement standards, (C) a new OSDT-Matlab interface, and (D) OSDT cybersecurity enhancements. These software products will be integrated into the production research infrastructures at Purdue, the University of Connecticut, the North Temperate Lake LTER Network site, and the Monterey Bay Aquarium Research Institute. The software will be professionally developed and managed as community resources. The intellectual merit of this project will be advances in software engineering, interoperability standards, and systems management for complex real-time sensor systems as well as advances on important open questions in civil engineering, marine science, and ecology. This project will yield broader impacts by enabling new science applications, enhancing productivity, and accelerating innovation across NSF directorates. To ensure that these broader impacts are realized, the team will provide student research opportunities, develop curriculum materials and training courses, and cultivate a community of software users."
"1550461","SI2-SSI: Collaborative Research: Einstein Toolkit Community Integration and Data Exploration","OAC","COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","07/10/2019","Pablo Laguna","GA","Georgia Tech Research Corporation","Continuing Grant","Amy Walton","01/31/2021","$462,164.00","David Bader","pablo.laguna@austin.utexas.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","CSE","7244, 8004","7433, 7569, 8009, 8084","$0.00","A new astronomy has arrived with the recent detection of gravitational waves. Modeling of sources of gravitational radiation is more than ever a critical necessity in order to interpret the observations. The project Einstein Toolkit has as overarching mission to provide the scientific community with a sustainable software platform of core computational tools for research focused on astrophysical systems endowed with complex multi-scale/multi-physics properties which are governed by Einstein's equations of General Relativity. The central premise of the project Einstein Toolkit is to create a broad and vibrant community of users, a community where interdisciplinary collaborations are the norm and not the exception, a community driving advances in the next generation of high-performance computing cyberinfrastructure. The main objectives of the project Einstein Toolkit are: developing software tools for a radical increase in scientific productivity, achieving sustainability of the software ecosystem, addressing software engineering challenges, and the curation of data from general relativistic numerical simulations. <br/><br/>This project will achieve its goals through two major activity areas. Regarding the software ecosystem and its sustainability, the scheduler that handles the flow of tasks in a problem will be redesigned to be more versatile and to improve its performance. In addition new software modules will be developed to broaden the choices of  initial data and matter sources, as well as modules for problems requiring a high degree of experimentation with equations and numerical methodologies.  A new general relativistic magneto-hydrodynamics code will also be integrated in the Einstein Toolkit. The second activity area involves building  a simulation data repository. The repository will allows user to compare results, contribute data,  test  innovative ideas and algorithms for gravitational wave data analysis, and to explore or discover new phenomena in sources of gravitational radiation. The broader impact effort in the project Einstein Toolkit will be organized in two major activity areas. The first involves community integration. The project will support a program of ease-of-use on-line tutorials and a workshops/tutorial series. The program will  help small groups or individual investigators familiarizing with the codes and modules in the toolkit as well as pathways to become a developer. Regarding outreach and education, the project Einstein Toolkit will enable interdisciplinary training of students and postdocs in numerical relativity, computational astrophysics and computer science. The effort will includes developing a teaching resources bank for educational activities involving computational topics applied to gravitational physics and astrophysics. The educational resources will be suitable for computational courses in general relativity and astrophysics at both the graduate and undergraduate level.<br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division in the Directorate of Mathematical and Physical Sciences."
"1148168","SI2-SSE: Development of a GPU Accelerated Gibbs Ensemble Monte Carlo Simulation Engine","OAC","OFFICE OF MULTIDISCIPLINARY AC, MATERIALS AND SURFACE ENG, DMR SHORT TERM SUPPORT, CHEMISTRY PROJECTS, Software Institutes","09/01/2012","06/07/2013","Jeffrey Potoff","MI","Wayne State University","Standard Grant","Rajiv Ramnath","08/31/2016","$336,000.00","Loren Schwiebert","jpotoff@wayne.edu","5057 WOODWARD STE 13001","DETROIT","MI","482024050","3135772424","CSE","1253, 1633, 1712, 1991, 8004","1253, 1633, 1712, 1982, 1991, 7237, 7433, 7569, 7573, 8004, 8005, 9215, 9216, 9251, 9263","$0.00","This award supports the development of a general purpose Gibbs Ensemble Monte Carlo (GEMC) simulation engine that uses low-cost graphics processing units (GPU) for acceleration.   The primary objectives of this work are to develop and implement: 1) GPU accelerated configurational-bias methods, 2) efficient algorithms for the computation of Ewald sums on the GPU and 3) automated tuning of the code for different GPUs.  This work builds on the PIs? existing particle-based GPU-GEMC engine and will introduce functionality that enables the simulation of biological processes and adsorption in porous materials.  The code will be written to maintain compatibility with the file formats used by the software packages NAMD and VMD, simplifying simulation setup and data analysis.  The resulting simulation engine will be released under the GNU General Public License v3 (GPLv3) and made available to users via the Internet.  <br/><br/>The software tools developed with support from this award will enable high throughput computational screening of materials for CO2 sequestration, improved materials for the stabilization of drug dispersions, and provide molecular level insight to fundamental biological processes such as membrane fusion.  The use of graphics processors for the bulk of the computational effort is expected to provide one to two orders of magnitude reduction in computational time compared to traditional serial, CPU bound code, which will allow for the simulation of systems of greater size and complexity than with existing tools.  The development of the proposed GPU-accelerated Monte Carlo simulation engine will enhance the cyber-infrastructure of the biology chemistry, chemical engineering, materials science and physics communities.  The GPU-GEMC simulation engine will be promoted through conference presentations at national and international meetings, via a dedicated website, and through publication in peer-reviewed literature.<br/><br/>This award will enhance education at the graduate and undergraduate levels.   Research topics from this work will be integrated into existing courses on GPU computing and molecular simulation.  Graduate and undergraduate students will have the opportunity to work as part of a multidisciplinary team composed of engineers and computer scientists.  Students will be recruited from groups traditionally underrepresented in STEM fields."
"1148111","SI2:SSE-Collaborative Research: Advanced Software Infrastructure for Biomechanical Inverse Problems","OAC","Mechanics of Materials and Str, BMMB-Biomech & Mechanobiology, Software Institutes","06/01/2012","06/20/2012","Assad Oberai","NY","Rensselaer Polytechnic Institute","Standard Grant","Rajiv Ramnath","05/31/2016","$290,301.00","Christopher Carothers","aoberai@usc.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","1630, 7479, 8004","1630, 7479, 8004, 8005","$0.00","Biomechanical imaging refers to the remote measurement of the mechanical properties of tissues, in-situ and in-vivo. Images of the tissue can be thus created by visualizing the mechanical property distributions. This technique relies on imaging tissue while it is deformed by a set of externally applied forces. Through image processing, the displacement field everywhere in the region of interest is inferred. An inverse problem for the relevant mechanical properties is then solved, given the measured displacement fields, an assumed form of the tissue's constitutive equation, and the law of conservation of momentum. Images of reconstructed parameters find applications in the detection, diagnosis and treatment monitoring of disease, and in designing patient specific models for surgical training and planning. Over the last decade we have developed a software package (NLACE) to solve this inverse problem eciently in different application domains. Through this award we will make enhancements to NLACE that will make it easier to utilize and modify, and extend its user base to a wider community.<br/><br/>The specific tasks for enhancing NLACE can be divided into two categories: (a) Steps to transition from a working prototype of NLACE to a software resource for the community. These include establishing an Input/Output standard for NLACE, creation of a GUI for input data and for monitoring the progress of the solution, hosting NLACE distributions, and creating sets of test data and documentation for its release. (b) Tasks that would enhance the functional capability of NLACE. These include the creation of a user-defined hyperelastic material model module to address a large class of tissue and material types, parallelization of NLACE on distributed memory, shared memory and GPU platforms, and quantifying uncertainty in the spatial distribution of the reconstructed parameters. We will measure our progress through user-feedback obtained during annual validation tests performed by a committed focus user-group that will test all aspects of the proposed research.<br/><br/>The proposed improvements to NLACE will further its application in the detection, diagnosis and treatment monitoring of diseases, generation of patient-specic models for surgical planning and image-guidance applications, and studies in biomechanics and mechanobiology. The parallel and uncertainty quantification strategies developed for NLACE can be applied to a broad class of inverse problem with PDE constraints, including acoustic and electromagnetic scattering, seismic inversion, diffuse optical tomography, aquifer permeability and thermometry. Our outreach plans ensure the dissemination of NLACE to our focus group and to a broader community through hosting on the Simtk NIH center website. Finally, two graduate students will be trained in the elds of computational science and mathematics and biomechanics, and results from the proposed research will be presented at conferences on computational and biomedical science and engineering."
"1450180","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","OAC","Physical & Dynamic Meteorology, Software Institutes, EarthCube","08/01/2015","08/04/2015","Mohan Ramamurthy","CO","University Corporation For Atmospheric Res","Standard Grant","Bogdan Mihaila","07/31/2019","$98,702.00","","mohan@ucar.edu","3090 CENTER GREEN DR","BOULDER","CO","803012252","3034971000","CSE","1525, 8004, 8074","4444, 7433, 8009","$0.00","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities."
"1339723","SI2-SSI: Collaborative Research: STORM: A Scalable Toolkit for an Open Community Supporting Near Realtime High Resolution Coastal Modeling","OAC","PHYSICAL OCEANOGRAPHY, Special Projects - CNS, Special Projects - CCF, Software Institutes, EarthCube","10/01/2014","08/26/2014","Richard Luettich","NC","University of North Carolina at Chapel Hill","Standard Grant","Bogdan Mihaila","09/30/2020","$759,047.00","","rick_luettich@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","CSE","1610, 1714, 2878, 8004, 8074","7433, 8009","$0.00","The ADCIRC coastal circulation and storm surge model has a long standing track record of extremely high societal impact. It has been used to define risk (e.g., 100-yr, 500-yr coastal flood levels) for the FEMA National Flood Insurance Program in coastal states from New York to Texas, it has been used to design the multi-billion dollar Hurricane and Storm Damage Risk Reduction System around greater New Orleans and southern Louisiana by the Army Corps of Engineers, it is currently run operationally by NOAA/NWS National Center for Environmental Prediction to forecast storm surge, to name just a few of its current and recent applications. Thus there is a well-established user network in place to convert improvements in ADCIRC into significant broader impacts. The proposed research provides transformative intellectual contributions that focus on applying new parallelization schemes to enable major advances in the algorithms, implementation and utilization of the ADCIRC model. The broadening of ADCIRC to a multi-algorithmic framework and the resulting performance gains that are anticipated will help ensure ADCIRC's sustainability as a core community model for at least the next 20 years. In addition, the proposed collaboration will impact computer science by serving as a high impact use case to inform the design of new approaches to efficient scalable computing. Together, the advancements in coastal modeling and parallelization technology will make a significant contribution to the science of modeling and HPC. The results of the proposed research will be disseminated to a wider community through ongoing educational outreach activities at the participating organizations as well as through refereed conference and journal papers, and invited presentations. The involvement of graduate students and post-doctoral fellows will be crucial towards the success of this project. The PIs have a long history of training and mentoring students and post-docs in computational science and engineering, coastal engineering and marine science. The recruitment and involvement of underrepresented groups in these efforts has always been a high priority. In addition, aspects of the proposed research will be incorporated into the curricula of several courses taught by the PIs in the areas of finite element methods, scientific computation, hydrology and oceanography.<br/>The aim of this project is to broaden the ADCIRC coastal circulation and storm surge model from a successful, but somewhat static coastal modeling tool that is tied to a single solution algorithm and the MPI parallelization paradigm, to a dynamic computational platform that is comprised of multiple solution algorithms, that readily admits new solution algorithms and that is built on a transformational new parallelization scheme that will allow us to scale to at least 256k compute cores on modern high performance computing (HPC) systems. We will do this by creating a living, evolving coastal modeling framework that will continue to lead the community in merging physical science / engineering and high performance computing and we will make the framework available to the broader community as a sustainable long term solution for its coastal modeling needs. In addition we will utilize these advancements in the highly demanding coastal storm surge forecasting system that we presently operate to demonstrate both improved robustness and speed of the model solution. We expect this effort will shorten the time required to provide reliable forecasting results and improve our ability to provide highly resolved, accurate, and physically complete predictions on an unprecedented scale. Concurrently, it should enable the use of smaller resources for simulations of increased scale which improves the usability and widens the applicability of ADCIRC in a broader community. The development of tightly integrated web-oriented products like CERA (www.coastalemergency.org) will enable the wide and timely dissemination of forecast modeling results to reach a broad audience."
"1148090","Collaborative Research: SI2-SSI: An Interactive Software Infrastructure for Sustaining Collaborative Community Innovation in the Hydrologic Sciences","OAC","ADVANCES IN BIO INFORMATICS, Software Institutes","07/01/2012","07/16/2012","Ray Idaszak","NC","University of North Carolina at Chapel Hill","Standard Grant","Rajiv Ramnath","06/30/2018","$2,128,078.00","David Valentine, Xiaohui Carol Song, Venkatesh Merwade, Lawrence Band","rayi@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","CSE","1165, 8004","1165, 7433, 8004, 8009","$0.00","Water, its quality, quantity, accessibility, and management, is crucial to society. However, our ability to model and quantitatively understand the complex interwoven environmental processes that control water and its availability is severely hampered by inadequate tools related to hydrologic data discovery, systems integration, modeling/ simulation, and education. This project develops sustainable cyberinfrastructure for better access to water-related data and models in the hydrologic sciences, enabling hydrologists and other associated communities to collaborate and combine data and models from multiple sources. It will provide new ways in which hydrologic knowledge is created and applied to better understand water availability, quality, and dynamics.  It will also help to provide a more comprehensive understanding of the interactions between natural and engineered aspects of the water cycle. These goals will be achieved through the development of interoperable cyberinfrastructure tools and the creation of an online collaborative environment, called HydroShare, which enables scientists to easily discover and access hydrologic and related data and models, retrieve them to their desktop, and perform analyses in a high performance computing environment.  The software to be developed will take advantage of existing NSF cyberinfrastructure (iRODS, HUBzero, CSDMS, CUAHSI HIS) and be created as open source code. Its development will be end user-driven.  In terms of broader impacts, the project builds essential infrastructure for science by developing software tools and computing environments to allow better understanding of the impacts of climate change (i.e., floods, droughts, biofuels, etc.) and to allow improved water resource development and the management of freshwater resources both above and below ground.   Resulting software will be made publicly available and provides a strong student and workforce training/education component.  In addition, the project supports an institution in an EPSCoR state and engages, as a PI, a person who is from a group under-represented in the sciences and engineering."
"1450455","SI2-SSI: CRESCAT, A Computational Research Ecosystem for Scientific Collaboration on Ancient Topics, Spanning the Full Data Life Cycle","OAC","Methodology, Measuremt & Stats, Software Institutes","09/01/2015","08/14/2017","David Schloen","IL","University of Chicago","Standard Grant","Bogdan Mihaila","08/31/2019","$1,750,000.00","Thomas Levy, Kathleen Morrison, Hakizumwami B. Runesha","d-schloen@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","CSE","1333, 8004","7433, 8004, 8009","$0.00","This project integrates, tests, and documents a suite of interoperable software tools to support collaborative research. The tools are collectively called CRESCAT (Computational Research Ecosystem for Scientific Collaboration on Ancient Topics). The initial focus is on disciplines that deal with dynamic interactions and structural changes within spatially situated populations over long time spans in the past, e.g., paleobiology, archaeology, and economic history. Despite their differences, these disciplines have similar computational needs for modeling and analyzing data. Moreover, the same software can be used in many other disciplines, enabling economies of scale by building and maintaining a common set of interoperable tools to serve a wide range of researchers, while spanning the full research data life cycle, consisting of (1) acquisition, (2) integration, (3) analysis, (4) publication, and (5) archiving of data. An intuitive graphical user interface is provided for end-user researchers to work with their data in all stages of the life cycle without cumbersome manual data transfers and transformations. The project will address a major computational problem that affects many scientific disciplines due to the challenge of integrating and analyzing data of diverse origins based on heterogeneous spatial, temporal, and taxonomic ontologies. Thus it will have a broad impact in the sciences and beyond by showing how to represent explicitly the full variability of individual judgments and the divergent conceptualizations and terminologies through which those judgments are expressed, with explicit attribution of each observation, interpretation, and conceptual ontology to a particular named person or group. Unlike many computational tools for scientific research, which assume a degree of ontological consensus that does not exist, CRESCAT conforms to actual research practices. It does not impose a standardized ontology, thereby ignoring or suppressing the inevitable disagreements and conflicting interpretations that arise among researchers. Instead, it represents ontological diversity, observational uncertainty, and interpretive disagreement explicitly within a larger common framework in which end users can query, analyze, and compare the full range of observations, interpretations, and terminologies to inform their own judgments about the evidence. CRESCAT is designed to allow scientific disagreements and observational and interpretive uncertainties to be represented digitally in a way that exposes these differences themselves as data for analysis and debate. Thus, in addition to the practical goal of building a more efficient shared framework for advanced research, the proposed work will provoke theoretical reflection about how computational tools should relate to scientific practice.<br/><br/>The CRESCAT project is an interdisciplinary collaboration between computer scientists, paleobiologists, geoscientists, archaeologists, economic historians, and other social scientists. The goal is to demonstrate the value of an integrative software ecosystem that spans the social and natural sciences and can facilitate any research characterized by overlapping models of temporal and spatial relations or by conflicting terminologies and taxonomies. CRESCAT's representation of scientific knowledge eschews forced standardization, which is impractical in many cases due to lack of an enforcement mechanism and is also questionable in principle since divergent ontologies often legitimately reflect different theoretical assumptions and research agendas. Central to the CRESCAT suite of tools is an innovative data-integration system that represents explicitly both research data and the ontologies inherent in the data. An ontology is defined here as a conceptual model of entities and the relationships among them in a given domain of knowledge, in contrast to a schema,&#148;which is the implementation of an ontology in logical data structures within a working system. CRESCAT's data-integration system operates at a level of abstraction sufficient to provide a predictable and efficiently queryable database structure based on an abstract global schema, which in turn is based on an upper ontology specified in terms of fundamental concepts and relationships applicable to all scientific and scholarly disciplines. The data-integration system is implemented in an enterprise-class XML/XQuery DBMS that serves as a data warehouse (using the non-relational graph data model), in which is stored diverse data from a wide range of research projects representing many disciplines. The terminology and conceptual distinctions of each research project are fully preserved. The approach to research data taken in the CRESCAT project is (1) coherent, tightly integrating software tools and data formats within a single analytical framework; (2) open-ended, interconnecting existing tools while allowing the addition of new tools in the future; (3) non-exclusive, in no way preventing its component tools from participating in other software ecosystems; (4) scalable, designed to handle large-scale data management, analysis, and visualization; and (5) sustainable, maintaining shared resources to meet common needs for software and technical support and thus enabling substantial economies of scale."
"1148484","Collaborative Research:  SI2-SSI: SciDaaS - Data Management as a Service","OAC","Software Institutes","04/01/2012","02/23/2017","Ian Foster","IL","University of Chicago","Standard Grant","Rajiv Ramnath","03/31/2018","$2,798,891.00","","foster@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","CSE","8004","7433, 8004, 8009","$0.00","The SciDaaS project will develop and operate a suite of innovative research data management services for the NSF community. These services, to be accessible at www.globusonline.org, will allow research laboratories to outsource a range of time-consuming research data management functions, including storage and movement, publication, and metadata management. SciDaaS research will investigate what services are most needed by NSF researchers; how best to present these services to integrate with diverse research laboratory environments; and how these services are used in practice across different research communities.<br/><br/>SciDaaS will greatly reduce the cost to the individual researcher of acquiring and operating sophisticated scientific data management capabilities. In so doing, it has the potential to dramatically expand use of advanced information technology in NSF research and thus accelerate discovery across many fields of science and engineering. By providing a platform for researchers to publicly share data at an incremental cost, SciDaaS will also reduce barriers to free exchange among researchers and contribute to the democratization of science."
"1148485","Collaborative Research: SI2-SSI: A Linear Algebra Software Infrastructure for Sustained Innovation in Computational Chemistry and other Sciences","OAC","Software Institutes","06/01/2012","06/28/2012","Jeff Hammond","IL","University of Chicago","Standard Grant","Rajiv Ramnath","05/31/2015","$104,710.00","","jeff_hammond@acm.org","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","CSE","8004","1253, 1640, 1712, 1991, 7433, 8004, 8009, 9216, 9263","$0.00","Linear algebra is a branch of mathematics that provides the foundation for a significant fraction of computations in science and engineering. Historically, the importance of linear algebra is such that highly specialized codes written by computer scientists have been used by the community of scientific programmers as a vital part of their application programs.   With the rapid changes in computer architecture during the last several years, it would seem that corresponding modifications in linear algebra routines would be warranted. However, such progress is not in evidence; the development of such routines has been just incremental, involving successive rewrites of routines that had their genesis in the last quarter of the last century.   Correspondingly, there is something of a disconnect between the current ""state-of-the-art"" linear algebra libraries, modern computer architectures, and applications that utilize the libraries.<br/> <br/>The new project will create a new, vertically integrated framework and implementation that revisits every layer of software, from low-level kernels to higher level functionality.  The vertical integration is completed with a new generation of software for computational chemistry applications, guaranteeing that the developed software, to be freely available to the public, supports sustained innovation in that domain and other sciences.  The development builds on the FLAME project, which has been funded by NSF and industry for more than a decade."
"1740112","SI2-SSE: Collaborative Research: Software Framework for Strongly Correlated Materials: from DFT to DMFT","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2017","08/30/2017","Hyowon Park","IL","University of Illinois at Chicago","Standard Grant","Seung-Jong Park","05/31/2021","$250,001.00","","hyowon@uic.edu","809 S MARSHFIELD RM 520","CHICAGO","IL","606124305","3129962862","CSE","1712, 8004","026Z, 054Z, 7433, 8004, 8005, 9102, 9150, 9216","$0.00","The main objective of this project is to develop advanced computational (ab-initio) tools that bridge the gap between the existing complex theories that describe the behavior of strongly-correlated electron materials, and the scientists working in other diverse fields who want to investigate the physical properties of the strongly correlated materials using modernstate-of-the-art computational methodologies. These strongly-correlated materials show a large set of interesting properties that can impact different fields as in opto-catalysis, magneto-optics, magneto-transport, high temperature superconductivity and magneto-electricity. The intriguing properties of such strongly-correlated materials includes unconventional superconductivity, complex charge spin and orbital ordering, metal-to-insulator transitions, and excellent thermoelectricity that have promising applications in modern technology. The existence of strong electron-electron interactions limits the use of existing Density Functional Theory (DFT) to understand the electronic structure of the strongly-correlated materials. However, recent developments of a new theory, named Dynamical Mean Field Theory (DMFT), has enabled researchers to correctly describe the electronic structure of the strongly correlated materials. In this project, the PIs will develop advanced Python-based computational research tools that will enable the researchers from diverse fields to investigate the properties of the strongly-correlated materials using DMFT. The specific applications include -- correct prediction of the electronic structure, vibrational properties and elastic properties of the strongly-correlated materials. The developed software tools will be freely available and open source and a user-manual will be made available for training purposes.<br/><br/><br/>The main goal of this project is to provide end users of various electronic structure codes with a flexible Python-based interface that does not rely on the extensive user experience or specific parameters to perform calculations for strongly-correlated materials and to develop new software to calculate electronic, vibrational, and elastic properties of strongly-correlated materials by using Dynamical Mean Field Theory (DMFT) methods starting from a Density Functional Theory (DFT) calculation. The developed software tools will be powerful enough to allow scientists in different fields to calculate the diverse electronic properties of a wide range of strongly-correlated materials with the state-of-the-art computational methodologies. Furthermore, these software packages will allow the correct electronic structure calculations in a minimal set of parameters, by offering to the end user the possibility of using three different methodologies to describe basic physics of strongly-correlated materials.All the developed computer software will be designed to enable the non-expert materials scientists and engineers to investigate the novel properties of the strongly-correlated materials. The scientific aim of this project also concerns the evolution of electronic correlations for several complex oxinitrides and Heusler alloys, in particular the dependence of several physical observables with respect to external fields such as pressure and strain. Targeted physical properties include electronic, vibrational, and elastic. The technical goal consists of the development of an open-source software that will address the scientific issues raised by the research on calculating properties of the strongly-correlated materials.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences."
"1339798","SI2-SSE: Collaborative Research: Software Elements for Transfer and Analysis of Large-Scale Scientific Data","OAC","Software Institutes","09/01/2013","08/29/2013","Rajkumar Kettimuthu","IL","University of Chicago","Standard Grant","Rob Beverly","04/30/2018","$99,995.00","","kettimut@mcs.anl.gov","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","CSE","8004","7433, 8005","$0.00","As science has become increasingly data-driven, and as data volumes and velocity are increasing,  scientific advance in many areas will  only be feasible if critical `big-data' problems are addressed  - and even more importantly,  software tools embedding these solutions are readily available to the scientists. Particularly, the major challenge being faced by current data-intensive scientific research efforts    is that while the dataset sizes continue to grow rapidly, neither among network bandwidths,   memory capacity of parallel machines,   memory access speeds,    and disk bandwidths are increasing at the same rate.<br/>Building on top of recent research at Ohio State University, which includes work on automatic data virtualization, indexing methods for scientific data,   and a novel bit-vectors based sampling method, the goal of this project is to fully develop, disseminate, deploy, and support  robust  software elements addressing challenges in data transfers and analysis.   The prototypes that have been already developed at Ohio State are being extended into two  robust   software elements: an extention of GridFTP (Grid Partial-File Transport Protocol)that allows users to specify a subset of the file to be transferred, avoiding unnecessary transfer of the entire file; and Parallel Readers  for NetCDF and HDF5 for Paraview and VTK, data subsetting and sampling tools for  NetCDF and  HDF5 that perform data selection and  sampling at  the I/O level, and in parallel.<br/>This project impacts a number of scientific areas,   i.e., any area that involves   big (and growing) dataset sizes and  need for data transfers and/or visualization.  This project also contributes to computer science research in `big data',  including scientific (array-based)  databases,  and visualization.  Another contribution will be towards preparation  of the broad science and engineering research community  for big data handling and analytics."
"1339873","SI2-SSI: Sustaining Globus Toolkit for the NSF Community (Sustain-GT)","OAC","Software Institutes","10/01/2013","07/25/2016","Steven Tuecke","IL","University of Chicago","Standard Grant","Alan Sussman","09/30/2018","$1,439,888.00","","tuecke@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","CSE","8004","7433, 8004, 8009","$0.00","Science and engineering depend increasingly on the ability to collaborate and federate resources across distances. This observation holds whether a single investigator is accessing a remote computer, a small team is analyzing data from an engineering experiment, or an international collaboration is involved in a multi-decade project such as the Large Hadron Collider (LHC). Any distributed collaboration and resource federation system requires methods for authentication and authorization, data movement, and remote computation. Of the many solutions that have been proposed to these problems, the Globus Toolkit (GT) has proven the most persistently applicable across multiple fields, geographies, and project scales. GT resource gateway services and client libraries are used by tens of thousands of people every day to perform literally tens of millions of tasks at thousands of sites, enabling discovery across essentially every science and engineering discipline supported by the NSF. As new, innovative techniques and technologies for collaboration and scientific workflows are developed, and as new computing and instrument resources are added to the national cyberinfrastructure, these technologies and other improvements must be added and integrated into GT so that it can continue to provide an advanced and robust technology for solving scientific research problems.<br/><br/>The Sustain-GT project builds on past success to ensure that GT resource gateway services will continue to meet the challenges faced by NSF science and engineering communities. These challenges include: multiple-orders-of-magnitude increases in the volume of data generated, stored, and transmitted; much bigger computer systems and correspondingly larger and more complex computations; much faster networks; many more researchers, educators, and students engaged in data-intensive and computational research; and rapidly evolving commodity Web and Cloud computing environments. With the help of a new User Requirements Board, Sustain-GT will respond to community demands to evolve the GT resource gateway services with superior functionality, scalability, availability, reliability, and manageability. Sustain-GT will also provide the NSF community with high quality support and rapid-response bug fix services, as is required to sustain a heavily used, production system like GT."
"1450262","Collaborative Research: SI2-SSI:Task-based Environment for Scientific Simulation at Extreme Scale (TESSE)","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","05/15/2015","05/20/2015","Edward Valeev","VA","Virginia Polytechnic Institute and State University","Standard Grant","Bogdan Mihaila","04/30/2019","$600,000.00","","evaleev@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","CSE","1253, 1712, 8004","7433, 8009, 8084, 9216","$0.00","This project, TESSE, is developing a new-generation programming environment that uniquely address the needs of emerging computational models in chemistry and other fields, and allowing these models to reap the benefits of the computer hardware of tomorrow. The project thus is bringing closer the ability to predict properties of matter and design matter transformations of importance to the technological leadership and energy security of the US.  TESSE's impact has scientific, cyberinfrastructure, and interdisciplinary educational aspects.  TESSE helps carry widely used scientific applications (NWChem, MADNESS, MPQC, etc.) into a whole new generation of many-core and accelerated architectures, while at the same time dramatically improving programmer productivity. In terms of cyberinfrastructure, TESSE's PaRSEC runtime system delivers capabilities that many libraries and advanced applications will require for high performance on large scale and hybridized systems.<br/><br/>The goals of this project, Task-based Environment for Scientific Simulation at Extreme Scale (TESSE), are to design and demonstrate via substantial scientific simulations within chemistry and other disciplines a prototype software framework that provides a groundbreaking response to the twin problems of portable performance and programmer productivity for advanced scientific applications on emerging massively-parallel, hybrid, many-core systems. TESSE will create a viable foundation for a new generation of science codes, one which enables even more rapid exploration of new physical models, provides greatly enhanced performance portability through directed acyclic graph (DAG) scheduling and auto-tuned kernels, and works towards full interoperability between major chemistry packages through compatible runtimes and data structures. TESSE will mature to become a standard, widely available, community-based and broadly-applicable parallel programming environment complementing and rivaling MPI/OpenMP. This is needed due to the widely appreciated shortfalls of existing mainstream programming models and the already huge successes of the existing DAG-based runtimes that are the foundation of the next generation of NSF- and DOE-supported (Sca)LAPACK high-performance linear algebra libraries."
"1147680","SI2-SSE Collaborative Research: SPIKE-An Implementation of a Recursive Divide-and-Conquer Parallel Strategy for Solving  Large  Systems of Linear Equations","OAC","OFFICE OF MULTIDISCIPLINARY AC, DYNAMICAL SYSTEMS, Software Institutes, CDS&E-MSS","06/01/2012","06/20/2012","Matthew Knepley","IL","University of Chicago","Standard Grant","Rajiv Ramnath","05/31/2015","$117,710.00","","knepley@buffalo.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","CSE","1253, 7478, 8004, 8069","7433, 8005","$0.00","Drs. Negrut, Sameh, and Knepley will investigate, produce, and maintain a methodology and its software implementation that leverage emerging heterogeneous hardware architectures to solve billion-unknowns linear systems in a robust, scalable, and efficient fashion. The two classes of problems targeted under this project are banded dense and sparse general linear systems.<br/><br/>This project is motivated by the observation that the task of solving a linear system is one of the most ubiquitous ingredients in the numerical solution of Applied Mathematics problems. It is relied upon for the implicit integration of Ordinary Differential Equation (ODE) and Differential Algebraic Equation (DAE) problems, in the numerical solution of Partial Differential Equation (PDE) problems, in interior point optimization methods, in least squares approximations, in solving eigenvalue problems, and in data analysis. In fact, the vast majority of nonlinear problems in Scientific Computing are solved iteratively by drawing on local linearizations of nonlinear operators and the solution of linear systems. Recent advances in (a) hardware architecture; i.e., the emergence of General Purpose Graphics Processing Unit (GP-GPU) cards, and (b) scalable solution algorithms, provide an opportunity to develop a new class of parallel algorithms, called SPIKE, which can robustly and efficiently solve very large linear systems of equations.<br/><br/>Drawing on its divide-and-conquer paradigm, SPIKE builds on several algorithmic primitives: matrix reordering strategies, dense linear algebra operations, sparse direct solvers, and Krylov subspace methods. It provides a scalable solution that can be deployed in a heterogeneous hardware ecosystem and has the potential to solve billion-unknown linear systems in the cloud or on tomorrow?s exascale supercomputers. Its high degree of scalability and improved efficiency stem from (i) optimized memory access pattern owing to an aggressive pre-processing stage that reduces a generic sparse matrix to a banded one through a novel reordering strategy; (ii) good exposure of coarse and fine grain parallelism owing to a recursive, divide-and-conquer solution strategy; (iii) efficient vectorization in evaluating the coupling terms in the divide-and-conquer stage owing to a CPU+GPU heterogeneous computing approach; and (iv) algorithmic polymorphism, given that SPIKE can serve both as a direct solver or an effective preconditioner in an iterative Krylov-type method.<br/><br/>In Engineering, SPIKE will provide the Computer Aided Engineering (CAE) community with a key component; i.e., fast solution of linear systems, required by the analysis of complex problems through computer simulation. Examples of applications that would benefit from this technology are Structural Mechanics problems (Finite Element Analysis in car crash simulation), Computational Fluid Dynamics problems (solving Navier-Stokes equations in the simulation of turbulent flow around a wing profile), and Computational Multibody Dynamics problems (solving Newton-Euler equations in large granular dynamics problems).<br/><br/>SPIKE will also be interfaced to the Portable, Extensible Toolkit for Scientific Computation (PETSc), a two decades old flexible and scalable framework for solving Science and Engineering problems on supercomputers. Through PETSc, SPIKE will be made available to a High Performance Computing user community with more than 20,000 members worldwide. PETSc users will be able to run SPIKE without any modifications on vastly different supercomputer architectures such as the IBM BlueGene/P and BlueGene/Q, or the Cray XT5. SPIKE will thus run scalably on the largest machines in the world and will be tuned for very different network and hardware topologies while maintaining a simple code base.<br/><br/>The experience collected and lessons learned in this project will augment a graduate level class, ?High Performance Computing for Engineering Applications? taught at the University of Wisconsin-Madison. A SPIKE tutorial and research outcomes will be presented each year at the International Conference for High Performance Computing, Networking, Storage and Analysis. A one day High Performance Computing Boot Camp will be organized each year in conjunction with the American Society of Mechanical Engineers (ASME) conference and used to disseminate the software outcomes of this effort. Finally, this project will shape the research agendas of two graduate students working on advanced degrees in Computational Science."
"1047956","SI2-SSI:  A Productive and Accessible Development Workbench for HPC Applications Using the Eclipse Parallel Tools Platform","OAC","INTERFAC PROCESSES & THERMODYN, Special Projects - CCF, Software Institutes","10/01/2010","08/28/2013","Jay Alameda","IL","University of Illinois at Urbana-Champaign","Standard Grant","Rudolf Eigenmann","09/30/2014","$1,434,000.00","Allen Malony, Marc Snir, Steven Brandt, Gregory Watson","alameda@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","1414, 2878, 8004","1414, 2878","$0.00","As supercomputers become more powerful, they become more complex. In order to take advantage of the increased power, scientific applications that run on these supercomputers will have to become more complex and will have to take advantage of more processing cores. Even those who are expert at optimizing these applications are quickly being overwhelmed. The Workbench for HPC Applications (W-HPC) project is transforming the way these experts develop, debug, optimize, and run their applications. Using the Eclipse platform, W-HPC provides a robust and portable way to manage computational science and engineering code development for a range of research disciplines. W-HPC also includes a targeted education and outreach program including outreach to minority-serving institutions that will train new users, explain the advantages of using Eclipse-based tools, and encourage users participate in the development of new tools.<br/><br/>The next generation of petascale systems will give unprecedented power to the scientific community as they tackle grand challenge problems. However, in order to take advantage of the huge potential performance improvements, application size and complexity will increase substantially as projects become multi-institutional and multi-disciplinary. The Workbench for HPC Applications project is transforming the way the community develops, debugs, optimizes, and runs its applications. As part of the project, the Eclipse Parallel Tools Platform (Eclipse PTP) is being enhanced. Eclipse PTP provides an open source, robust, portable, and sustainable development environment suitable for use with a broad range of scientific codes. Targeted education and outreach activities are also part of the project. They will train new users, explain the advantages of using Eclipse-based tools, and encourage users participate in the development of new tools."
"1148371","Collaborative Research: SI2-SSI: A Comprehensive Performance Tuning Framework for the MPI Stack","OAC","Information Technology Researc, Software Institutes","06/01/2012","06/04/2012","Dhabaleswar Panda","OH","Ohio State University","Standard Grant","Rob Beverly","05/31/2016","$1,251,644.00","Karen Tomko","panda@cse.ohio-state.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","CSE","1640, 8004","1640, 7433, 8004, 8009","$0.00","The Message Passing Interface (MPI) is a very widely used parallel programming model on modern High-End Computing (HEC) systems. Many performance aspects of MPI libraries, such as latency, bandwidth, scalability, memory footprint, cache pollution, overlap of computation and communication etc. are highly dependent on system configuration and application requirements. Additionally, modern clusters are changing rapidly with the growth of multi-core processors and commodity networking technologies such as InfiniBand and 10GigE/iWARP. They are becoming diverse and heterogeneous with varying number of processor cores, processor speed, memory speed, multi-generation network adapters/switches, I/O interface technologies, and accelerators (GPGPUs), etc.  Typically, any MPI library deals with the above kind of diversity in platforms and sensitivity of applications by employing various runtime parameters. These parameters are tuned during its release, or by<br/>system administrators, or by end-users.  These default parameters may or may not be optimal for all system configurations and applications.<br/><br/>The MPI library of a typical proprietary system goes through heavy performance tuning for a range of applications.  Since commodity clusters provide greater flexibility in their configurations (processor, memory and network), it is very hard to achieve optimal tuning using released version of any MPI library, with its default settings. This leads to the following broad challenge: ""Can a comprehensive performance tuning framework be designed for MPI library so that the next generation InfiniBand, 10GigE/iWARP and RoCE clusters and applications will be able to extract `bare-metal' performance and maximum scalability?""  The investigators, involving computer<br/>scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) as well as computational scientists from the Texas Advanced Computing Center (TACC) and San Diego Supercomputer Center (SDSC), University of California San Diego (UCSD), will be addressing the above challenge with innovative solutions.<br/><br/>The investigators will specifically address the following challenges: 1) Can a set of static tools be designed to optimize performance of an MPI library during installation time?  2) Can a set of dynamic tools with low overhead be designed to optimize performance on a per-user and per-application basis during production runs?  3) How to incorporate the proposed performance tuning framework with the upcoming MPIT interface?  4) How to configure MPI libraries on a given system to deliver different optimizations to a set of driving applications?  and 5) What kind of benefits (in terms of performance, scalability, memory efficiency and reduction in cache pollution) can be achieved by the proposed tuning framework?  The research will be driven by a set of applications from established NSF computational science researchers running large scale simulations on the TACC Ranger and other systems at OSC, SDSC and OSU.  The proposed designs will be integrated into the open-source MVAPICH2 library."
"1534941","SI2-SSE: Enhanced Software Tools for Biomolecular Free Energy Calculations","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes, DMREF","09/01/2015","08/24/2015","Celeste Sagui","NC","North Carolina State University","Standard Grant","Bogdan Mihaila","08/31/2019","$500,000.00","Christopher Roland","sagui@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","CSE","1253, 1712, 8004, 8292","1982, 7237, 7433, 7569, 7573, 8005, 8400, 9215, 9216, 9263","$0.00","The central role of atomistic simulation-based free energy calculations for basic chemical and biological research is now firmly established. The free energy is the central quantity that guides the behavior of a system at, or near, equilibrium, determining such characteristics as molecular conformations, molecular binding, chemical reactions, etc. Unfortunately, accurate and reliable free energies are very difficult to calculate, particularly for many biomolecular systems characterized by rugged free energy landscapes.  Hence, special techniques are required for calculating such free energy landscapes.  Having previously developed the so-called Adaptively Biased Molecular Dynamics method, enhanced with other methods (either developed or adapted by our group) for phase space sampling, this project will further develop the capabilities of this software thereby enlarging the kinds of simulation problems that can be tackled. The software will be released to the public as open source software and as parts of the AMBER software package. In terms of scientific applications, this project also will investigate the conformation and properties of proteins with Intrinsically Disordered Regions, and the binding of DNA with a special class of transcription factors.<br/><br/>The single most important quantity for describing biomolecular and chemical systems in equilibrium is the free energy. However, calculating free energies is notoriously difficult and computationally expensive. This problem is particularly pressing for many biomolecular systems, which are characterized by complicated free energy landscapes that are hard to explore with regular molecular dynamics simulations. The PIs previously developed the Adaptively Biased Molecular Dynamics method (ABMD) with Multiple Walkers, and Replica Exchange Molecular Dynamics (REMD) extensions. ABMD is an umbrella sampling method with a time-dependent biasing potential for calculating free energy landscapes and conformational sampling. The software suite, along with Steered Molecular Dynamics (SMD) extensions, has been released to the public as part of the AMBER software package. This project will take this set of software tools to the next level by developing the capability to handle quaternion-based collective variables, the so-called Milestoning technique, and self-directed, interacting multiple walkers. Envisioned applications relate to proteins with Intrinsically Disordered Regions (IDRs) and mechanisms of DNA binding by basic Helix-Loop-Helix (bHLH) domains in transcription factors."
"1339773","Collaborative Research: SI2-SSE: A Petascale Numerical Library for Multiscale Phenomena Simulations","OAC","Software Institutes, CDS&E","10/01/2013","09/11/2013","Diego Donzis","TX","Texas A&M Engineering Experiment Station","Standard Grant","Rajiv Ramnath","09/30/2016","$139,879.00","","donzis@tamu.edu","3124 TAMU","COLLEGE STATION","TX","778433124","9798626777","CSE","8004, 8084","7433, 8005, 8084","$0.00","Multiscale phenomena are a grand challenge for theory, simulations and experiments. It is difficult to understand phenomena at both highest and lowest scales, as well as all those in between. This challenge shows up in diverse fields. One of the long-standing problems in cosmology is understanding cosmological structure formation. In computer simulations of this process the challenge is increasing grid resolution while retaining the essential physics. In all-atom molecular dynamics simulations of enzymes the challenge is simulating systems with a large number of atoms while resolving long-range interactions and having sufficiently high throughput. Such simulations are critical in understanding important biological processes and eventually designing new drugs. Another prime example of multiscale phenomena is turbulent flows, a rich and complex subject of great relevance to many of the main technological issues of the day, including climate, energy, and the management of oil and biohazards. Understanding turbulent flows is critical for design of new transportation vehicles, improving efficiency of combustion processes and managing their environment pollution. Here simulations have been historically limited, and remain so, due to extremely high computational cost, even using the high-end computational systems available to researchers today. Reducing this cost, and efficiently using the computational resources, often requires specialized expertise, as well as significant development time and cost many research groups cannot afford. This project will develop a powerful suite of critical software components to provide tools for performing simulations of multiscale phenomena. <br/><br/>The suite will implement state-of-the-art techniques for reducing communication cost, which has become the most important contributing factor to the total simulation cost, especially at larger scales. It will provide a flexible set of features that will make it usable in a great number of codes across the disciplines. In particular, the library will include user-friendly interfaces for Fourier transforms, spectral and compact differentiation in three dimensions, in addition to widely used communication routines (transposes, halo exchanges). This combination of emphasis on scalable performance and richness of features makes this suite unique among other libraries in existence today. Given the extraordinary challenge of simulations of multiscale phenomena, this library will provide a realistic path towards the Exascale. The suite will be available under an open source license. User outreach will be undertaken through the project website, mailing list, user surveys and presentations."
"1148287","SI2-SSE: General Tensor Software Elements for Quantum Chemistry, Tensor Network Theories, and Beyond","OAC","OFFICE OF MULTIDISCIPLINARY AC, CHEMISTRY PROJECTS, Software Institutes","06/01/2012","06/25/2012","Garnet Chan","NY","Cornell University","Standard Grant","Daniel Katz","10/31/2012","$386,818.00","","garnetc@caltech.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","CSE","1253, 1991, 8004","1253, 1991, 7433, 7683, 8004, 8005, 9216, 9263","$0.00","The PI will develop reusable software components to accelerate innovation in science that relies on high-dimensional tensor computations. While the components are informed by use-cases taken from quantum chemistry, the challenges of tensor computation are universal, and span diverse areas of science and engineering. These problems range from simulations of nuclear spin spectra, to quantum chemical calculations on molecules, to psychometric analysis, to numerical general relativity. The overarching aim of the funded work is to develop reusable tensor software elements, based on modern sustainable software practices, to benefit tensor algorithmic development in the scientific community at large. In addition, beyond the broad scientific impacts of the software, our education and outreach agenda comprises a multi-tiered effort to uplift the ability of the science and engineering community to reason about high-dimensional tensor and matrix computations.<br/><br/>Important aspects of the software elements will include (i) expressive programming interfaces for rapid prototyping of tensor based theories (ii) layered tensor libraries for dense, block-sparse, and out-of-core tensors that provide peak-performance implementations of the above interfaces, (iii) a multi-linear algebra package for general high dimensional computations, based on the matrix product state approach, and (iv) tensor virtual machine technology that abstracts algorithm development from hardware, and which provides a framework for optimizing compiler transformations to adapt algorithms to the memory access, communication networks, and processor characteristics of modern computer architectures."
"1339844","SI2-SSE: Multiscale Software for Quantum Simulations in Materials Design, Nano Science and Technology","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","09/01/2013","08/29/2013","Jerzy Bernholc","NC","North Carolina State University","Standard Grant","Rajiv Ramnath","08/31/2017","$500,000.00","Carl Kelley, Wenchang Lu, Miroslav Hodak","bernholc@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","CSE","1253, 1712, 8004","7433, 7569, 8005, 9216, 9263","$0.00","The emergence of petascale computing platforms brings unprecedented opportunities for transformational research through simulation. However, future breakthroughs will depend on the availability of high-end simulation software, which will fully utilize these unparalleled resources and provide the long-sought third avenue for scientific progress in key areas of national interest. This award will deliver a set of open source petascale quantum simulation tools in the broad areas of materials design, nano science and nanotechnology. Materials prediction and design are key aspects to the recently created Materials Genome initiative, which seeks to ""deploy advanced materials at least twice as fast, at a fraction of the cost."" Computational materials design is the critical aspect of that initiative, which relies on computation guiding experiments. The outcomes of the latter will in turn lead to follow-up computation in an iterative feedback loop. Nanoscience, which studies properties of materials and processes on fundamental scale of nanometers, promises development of materials and systems with radically new properties. However, the nanoscale properties are hard to measure and even harder to predict theoretically. Only simulations that can fully account for the complexity and variability at that fundamental scale stand a chance of predicting and utilizing the macroscopic properties that emerge. This truly requires petascale resources and efficient petascale software tools.<br/> <br/>This award will develop software tools build on the real-space multigrid (RMG) software suite and distribute them to the national user community. The RMG code already scales to 128,000 CPU cores and 18,000 GPU nodes. The award will further enhance RMG through development of new iterative methods with improved convergence, optimization of additional modules for existing and new petascale computing platforms, and creation of ease-to-use interfaces to the main codes. Workshops in RMG usage will be conducted at XSEDE workshops and other meetings of NSF supercomputing centers. RMG will be distributed through a web portal, which will also contain user forums and video tutorials, recorded at live user sessions. A library of representative examples for the main petascale platforms will be maintained. RMG will enable quantum simulations of unprecedented size, enabling studies of the building blocks of functional nano or bio-nano structures, which often involve thousands of atoms and must be described with the requisite fidelity. The development of petascale quantum simulation software and its user community will lead to cross-fertilization of ideas both within and across fields. Students and postdocs trained in this area will have significant opportunities for advancement and making substantial impact on their own."
"1550526","Collaborative Research: SI2-SSI: Adding Volunteer Computing to the Research Cyberinfrastructure","OAC","Software Institutes","08/01/2016","08/02/2016","Michael Zentner","IN","Purdue University","Standard Grant","Rajiv Ramnath","07/31/2017","$107,269.00","","mzentner@ucsd.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","CSE","8004","7433, 8004, 8009","$0.00","The aggregate computing power of consumer devices - desktop and laptop computers, tablets, smartphones - far exceeds that of institutional computing resources.  ""Volunteer computing"" uses these consumer devices, volunteered by their owners, to do scientific computing. In addition to providing additional, much-needed computational resources to scientists, volunteer computing publicizes scientific research and engages citizens in science. BOINC is the primary software system for volunteer computing.  It was developed at UC Berkeley with NSF support starting in 2002. Until now, BOINC has been based on a model of independent competing projects.  Scientists set up their own BOINC servers, port their applications to run on BOINC, and publicize their projects to attract volunteers.  There are about 40 such projects, in many areas of science: examples include Einstein@home, CERN, and SETI@home (astrophysics), Rosetta@home and GPUGrid.net (biomedicine), Climateprediction.net (climate study), and IBM World Community Grid (multiple applications).  Together these projects have about 400,000 active volunteers and 12 PetaFLOPS of computing throughput. This model, while successful to an extent, has reached a limit.  The number of projects and volunteers has stagnated.  Volunteer computing is supplying lots of computing power, but only to a few research projects.  For other scientists, there are two major barriers.  First, creating a BOINC project has significant overhead: learning a new technology, creating a public web site, generating publicity, and so on.  Second, volunteer computing is risky and uncertain; there is no guarantee that a new project will attract volunteers. This project aims to break this barrier, and to make volunteer computing available to all scientists doing high-throughput computing, by replacing the competing-projects model with a new ""central broker"" model. The new model has two related parts: 1) the integration of BOINC with existing high-throughput computing facilities such as supercomputing centers and science portals. Jobs currently run on cluster nodes will be transparently offloaded to volunteer computers. Scientists using these facilities will see faster turnaround times; they'll benefit from volunteer computing without even knowing it's there. 2) The project will change the volunteer interface so that participants sign up for scientific areas and goals rather then for particular projects. For example, a participant might sign up to contribute to cancer research. A central broker, to be developed as part of this project, would dynamically assign their computing resources to projects doing that type of research. This project mobilizes public support for and interest in scientific research by encouraging ""volunteer computing"" and engaging citizens in the conduct of the research itself. It simultaneously advances NSF's mission to advance science while broadening citizen engagement.<br/><br/>The first year of this project will prototype each of these parts, and will integrate BOINC with TACC and nanoHub. Integrating BOINC with existing HTC systems involves several subtasks: 1) Job routing: modifying existing job processing systems used by TACC and nanoHub (Launcher and Rappture respectively) to decide when a group of jobs should be offloaded to BOINC. This decision might involve the estimated runtime of the jobs, input and output file sizes, data sensitivity, the deadline or priority of the jobs, and the identity of the job submitter. 2) Job format conversion: mapping job descriptions (input/output file specifications, resource and timing requirements) to their BOINC equivalents. 3) Application packaging: adapting existing applications (such as nanoHub's simulation tools and TACC's Autodock) to run under BOINC. We will use BOINC's virtual machine facility, which packages an application as a virtual machine image (VirtualBox or Docker) and a program to be run within the VM.  This allows existing Linux applications to run on consumer desktop platforms such as Windows and Mac, as well as providing a strong security sandbox and an efficient application-independent checkpoint/restart mechanism. 4) File handling: moving input and output files between existing storage systems (typically inaccessible from outside firewalls) to Internet-visible servers.  This will use existing BOINC components that manage files based on hashes to eliminate duplicate transfer and storage of files. 5) Job monitoring and control: adapting existing web- or command-line based tools for monitoring the progress of batches of jobs, and for aborting jobs, to work with BOINC. This will use existing Web RPCs provided by BOINC for these purposes. This project will carry out these tasks by designing and implementing new software as needed, testing for correctness, performance, and scalability, and deploying it in a production environment. The second part of the project - a brokering system for allocating computing power based on volunteer scientific preferences - will be designed and prototyped.  This involves several subtasks: 1) Designing a schema for volunteer preferences, including scientific areas and sub-areas, project nationality and institutions, specific projects and applications, inclusions/exclusions, and so on. 2) Designing a schema for assigning attributes to job streams (e.g. their area, sub-area, institution, etc.), and for assigning quotas or priorities to job streams. 3) Designing a relational database for storing the above information. 4) Designing and implementing policies for assigning volunteer resources to job streams in a way that respects volunteer preferences and optimizes quota, fairness, and throughput criteria.  This will be implemented as a BOINC ""account manager"" so that volunteers see a single interface rather than lots of separate projects and web sites."
"1740218","NSCI: SI2-SSE: An Extensible Model to Support Scalable Checkpoint-Restart for DMTCP Across Multiple Disciplines","OAC","CYBERINFRASTRUCTURE, Software Institutes","01/01/2018","10/15/2020","Gene Cooperman","MA","Northeastern University","Standard Grant","Seung-Jong Park","12/31/2021","$408,000.00","","gene@ccs.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7231, 8004","026Z, 077Z, 7433, 8004, 8005, 9251","$0.00","Checkpointing is a technique that periodically saves the state of a long-running computer program to disk.  If a computer crash occurs during the running of the program, one can then restart the program state from a previously saved ""checkpoint"" file on disk. The goal of this project is to discover, implement and deploy novel techniques for adapting checkpointing so as to provide a more robust capability easily usable across applications supporting the research of a variety of scientific and engineering disciplines. In particular, a problem with the classic (transparent) checkpoint model is that these packages do not model, and hence cannot recreate upon restart, communications between the original program and other external processes or programs.  In this project, a virtualization model for commonly used mechanisms for communication will be developed so that on restart, external communications are emulated. Checkpointing is used across academia, industry, and government,  particularly  by those with long-running high performance computing programs.  Thus, the project  outcomes have broad applicability and value.  The project has the added benefit of educating the next generation of students in valuable and highly transferable system skills. <br/><br/>Today, transparent checkpoint-restart today is used primarily for fault tolerance, and primarily in closed systems with no external communication.  DMTCP is a twelve-year old open source checkpointing project.  Its currently evolving process virtualization model of checkpointing enables an application to support complex applications that interact with external subsystems.  The project explores and extends a model of process virtualization in order to adapt checkpoint-restart to multiple, novel applications, and to extend its use across multiple scientific and engineering disciplines.  Example disciplines that will benefit include:  supercomputing (and in particular, forging a path toward practical exascale checkpointing); novel strategies for flexible resource managers (batch queues) for computer clusters that adapt to the current workload; and better support for hardware circuit emulators for Electronic Design Automation (EDA).  Example challenges include the need to support transparent checkpointing over the newer low-latency networks such as Omni-Path, integration of application-specific checkpointing with transparent DMTCP-style checkpointing, the need to avoid ""flooding"" back-end storage during checkpointing in high-end clusters, and new types of resource managers that benefit from the flexibility of arbitrarily suspending running jobs through checkpointing.  Rather than build ad hoc solutions for each of the above, this work will provide a simple model allowing end users to easily build their own extensions to support checkpointing of the external subsystems.  The simple model will be derived by generalizing over solutions to many of the example challenges described above.  In addition to fault tolerance, the technology holds advantages for:  fast startup (checkpoint after process initialization,in order to restart and skip this phase in future sessions); debugging (e.g. checkpoint every 30 seconds); reproducible bug reports; extended interactive sessions (e.g. checkpoint before dinner and restart the next day); and so on.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740305","SI2-SSE: Improving Scikit-Learn Usability and Automation","OAC","Software Institutes","09/01/2017","10/26/2020","Nicolas Hug","NY","Columbia University","Standard Grant","Seung-Jong Park","01/31/2022","$399,356.00","","nh2611@columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","CSE","8004","7433, 8004, 8005","$0.00","Machine learning is a central component in many data-driven research areas, but its adoption is limited by the often complex choice of data processing, model, and parameter settings. The goal of this project is to create software tools that enable automatic machine learning, that is, solving predictive analytics tasks without requiring the user to explicitly specify the algorithm or model parameters used for prediction.  The software developed in this project will enable a wider use of machine learning, by providing tools to apply machine learning without requiring knowledge of the details of the algorithms involved.<br/><br/>The project, supported by the Office of Advanced Cyberinfrastructure, and the Division of Computing and Communication Foundations, extends the existing scikit-learn project, a machine learning library for Python, which is widely used in academic research across disciplines.  The project will add features to this library to lower the amount of expert knowledge required to apply models to a new problem, and to facilitate the interaction with automated machine learning systems. The project will also create a separate software package that includes models for automatic supervised learning, with a very simple interface, requiring minimal user interaction. In contrast to existing research projects, this project focuses on creating easy-to-use tools that can be used by researchers without extensive training in machine learning or computer science."
"1339756","SI2-SSI: Collaborative: The XScala Project: A Community Repository for Model-Driven Design and Tuning of Data-Intensive Applications for Extreme-Scale Accelerator-Based Systems","OAC","CYBERINFRASTRUCTURE","10/01/2013","09/16/2013","Viktor Prasanna","CA","University of Southern California","Standard Grant","Alan Sussman","09/30/2018","$748,914.00","","prasanna@usc.edu","3720 S FLOWER ST FL 3","LOS ANGELES","CA","900074304","2137407762","CSE","7231","7433, 8009, 9145","$0.00","The increasing gap between processor and memory performance -- referred to as the memory wall -- has led high-performance computing vendors to design and incorporate new accelerators into their next-generation systems. Representative accelerators include recon&#64257;gurable hardware such as FPGAs, heterogeneous processors such as CPU+GPU processors, highly multicore and multithreaded processors, and manycore co-processors and general-purpose graphics processing units, among others. These accelerators contain myriad innovative architectural features, including explicit control of data motion, large-scale SIMD/vector processing, and multithreaded stream processing. Such features provide abundant opportunities for developers to achieve high-performance for applications that were previously deemed hard to optimize. This project aims to develop tools that will assist developers in using hardware accelerators (co-processors) productively and effectively.<br/> <br/>This project's specific technical focus is on data-intensive kernels including large dictionary string matching, dynamic programming, graph theory, and sparse matrix computations that arise in the domains of biology, network security, and the social sciences. The project is developing XScala, a software framework for designing efficient accelerator kernels. The framework contains a variety of design time and run-time performance optimization tools. The project concentrates on data-intensive kernels, bound by data movement. It proposes optimization techniques including (a) enhancing and exploiting maximal concurrency to hide data movement; (b) algorithmic reorganization to improve spatial and/or temporal locality; (c) data structure transformations to improve locality or reduce the size of the data (compressed structures); and (d) prefetching, among others. The project is also developing a public software repository and forum, called the XBazaar, for community-developed accelerator kernels. This project includes workshops, tutorials, and the PIs class and summer projects as various means by which to increase community involvement. The broader impacts include productive use of emerging classes of accelerator-augmented computer systems; creation of an open and accessible community repository, the XBazaar, for distributing accelerator-tuned computational kernels, software, and models; training of graduate and undergraduate students; and dissemination through publications, presentations at scientific meetings, lectures, workshops, and tutorials. The framework itself will be released as open-source code and as precompiled binaries for several common platforms, through the XBazaar, as an initial step toward building a community around accelerator kernels."
"1339707","SI2-SSE: Developing Sustainable Software Elements to Support the Growing Field of Public Participation in Scientific Research","OAC","Cross-Directorate  Activities, AISL, Software Institutes","11/01/2013","08/08/2013","Gregory Newman","CO","Colorado State University","Standard Grant","Rajiv Ramnath","10/31/2016","$493,076.00","Melinda Laituri, Stacy Lynn","Gregory.Newman@ColoState.Edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","CSE","1397, 7259, 8004","7433, 7477, 8009","$0.00","Across the globe, citizen science projects are becoming increasingly poised to address social and environmental challenges and answer broad scientific questions. Although rapidly increasing in number, these projects need easy-to-use software tools for data management, analysis, and visualization to be successful. This project transforms how citizen science projects unfold locally, regionally, and globally by creating software that supports the full spectrum of project activities. It empowers projects to ask and answer their own local questions while contributing data critical to larger-scale issues. These tools will allow projects to announce training events; track volunteers; create datasheets; enter, review, analyze, and visualize data; publish reports; discover resources; integrate data; and ensure that data are contributed to repositories (e.g., DataONE, NEON, GBIF, HydroShare, and EOL). Tools will be made available to citizen science projects and will be delivered as reusable software elements for use in existing websites; as website features on CitSci.org; and as Application Programming Interface (API) services and mobile applications.<br/> <br/>The tools will expand the national reach, local appeal, computational abilities, visualization techniques, statistical analysis capabilities, and interoperability of the nations? cyber-infrastructure. Using participatory design and agile methods, the project will: (1) develop reusable software elements that citizen science organizations can embed into their own websites, (2) harden and expand the functionality and capabilities of CitSci.org through new website features, and (3) extend the APIs of CitSci.org and develop associated mobile applications to increase system and tool interoperability. The target user communities will include citizen science project coordinators. It will deliver customizable tools and services related to all project activities and engage projects across a wide array of disciplines. Project coordinators will be able to customize all tools developed to suit their specific project needs. Adoption and use of the tools developed will create a cyber-ready workforce capable of collecting, contributing, and applying high quality ecological, geophysical, social, and human health related observations to solve real-world problems. These broader impacts will help the citizen science community better understand effective models of public engagement to ensure more impactful application of citizen science to societal challenges."
"1550463","SI2-SSI: Advancing and Mobilizing Citizen Science Data through an Integrated Sustainable Cyber-Infrastructure","OAC","AISL, Software Institutes","11/01/2016","07/31/2019","Gregory Newman","CO","Colorado State University","Standard Grant","Seung-Jong Park","10/31/2021","$1,098,743.00","Melinda Laituri, Stacy Lynn, Louis Liebenberg","Gregory.Newman@ColoState.Edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","CSE","7259, 8004","026Z, 043Z, 7433, 8004, 8009","$0.00","Citizen science engages members of the public in science. It advances the progress of science by involving more people and embracing new ideas. Recent projects use software and apps to do science more efficiently. However, existing citizen science software and databases are ad hoc, non-interoperable, non-standardized, and isolated, resulting in data and software siloes that hamper scientific advancement. This project will develop new software and integrate existing software, apps, and data for citizen science - allowing expanded discovery, appraisal, exploration, visualization, analysis, and reuse of software and data. Over the three phases, the software of two platforms, CitSci.org and CyberTracker, will be integrated and new software will be built to integrate and share additional software and data. The project will: (1) broaden the inclusivity, accessibility, and reach of citizen science; (2) elevate the value and rigor of citizen science data; (3) improve interoperability, usability, scalability and sustainability of citizen science software and data; and (4) mobilize data to allow cross-disciplinary research and meta-analyses. These outcomes benefit society by making citizen science projects such as those that monitor disease outbreaks, collect biodiversity data, monitor street potholes, track climate change, and any number of other possible topics more possible, efficient, and impactful through shared software. <br/><br/>The project will develop a cyber-enabled Framework for Advancing Buildable and Reusable Infrastructures for Citizen Science (Cyber-FABRICS) to elevate the reach and complexity of citizen science while adding value by mobilizing well-documented data to advance scientific research, meta-analyses, and decision support. Over the three phases of the project, the software of two platforms, CitSci.org and CyberTracker, will be integrated by developing APIs and reusable software libraries for these and other platforms to use to integrate and share data and software. Using participatory design and agile methods over four years, the project will: (1) broaden the inclusivity, accessibility, and reach of citizen science; (2) elevate the value and rigor of citizen science software and data; (3) improve interoperability, usability, scalability and sustainability of citizen science software and data; and (4) mobilize data to allow cross-disciplinary research and meta-analyses. These outcomes benefit society by making citizen science projects and any number of other possible topics more possible, efficient, and impactful through shared software and data. Adoption of Cyber-FABRICS infrastructure, software, and services will allow anyone with an Internet or cellular connection, including those in remote, underserved, and international communities, to contribute to research and monitoring, either independently or as a team.  This project is also being supported by the Advancing Informal STEM Learning (AISL) program, which seeks to advance new approaches to, and evidence-based understanding of, the design and development of STEM learning in informal environments."
"1450488","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","OAC","Physical & Dynamic Meteorology, Software Institutes, EarthCube","08/01/2015","10/04/2017","Carl Maltzahn","CA","University of California-Santa Cruz","Standard Grant","Bogdan Mihaila","07/31/2019","$695,525.00","","carlosm@ucsc.edu","1156 HIGH ST","SANTA CRUZ","CA","950641077","8314595278","CSE","1525, 8004, 8074","7433, 8009","$0.00","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities."
"1550487","Collaborative Research: SI2-SSI: Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion","OAC","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, Software Institutes, CDS&E-MSS","09/01/2016","08/10/2016","Youssef Marzouk","MA","Massachusetts Institute of Technology","Standard Grant","Seung-Jong Park","08/31/2020","$524,968.00","Matthew Parno","ymarz@mit.edu","77 MASSACHUSETTS AVE","CAMBRIDGE","MA","021394301","6172531000","CSE","1266, 1271, 8004, 8069","4444, 7433, 8004, 8009, 8251, 9263","$0.00","Scientists often use mathematical models to predict the behavior of natural and engineered systems. These models are therefore fundamental to scientific and engineering progress and hence relevant to NSF's science mission. Most models of realistic physical systems  use complex formulae (such as, partial differential equations) involving many variables. When using such a model for predicting the future behavior of a system, a scientist has to provide initial values for all the variables.  This can be difficult because input values may not be directly measureable. Thus, scientists often must use ""inverse"" computations to calculate the initial input values of the variables of a system model based on external observations of the real world. In other words, scientists seek to infer inputs to a computer model of a physical process from real observational data of the outputs. There are many examples of inverse computations, ranging from computing the important dimensions of an organ from its CAT scan, reconstructing the source of a sound by measuring its volume and frequency at various places, calculating the density of the Earth from measurements of its gravity field, or calculating the initial condition of the atmosphere (temperature, pressure, etc.) from satellite and weather station observations over a time interval. Inverse problems are ubiquitous across all of science and engineering (and beyond). Many solutions exist for inverse problems, i.e. solutions that fit the data to the observations. However, there are variations in the solutions identified. That is, the solutions of an inverse problem are subject to uncertainty. Bayesian inferencing provides a systematic mathematical framework for characterizing this uncertainty. However, the Bayesian solution of inverse problems for large-scale complex models require enormous computational power. Only recently have algorithms begun to emerge that are computationally tractable. However, these algorithms have remained out of the reach of the mainstream of scientists who solve inverse problems, due to their complexity and the need for deeper information from the forward model. This project aims to develop, distribute, and support open-source software that encodes state-of-the-art algorithms for the solution of large-scale complex Bayesian inverse problems and is robust, scalable, flexible, modular, widely accessible, and easy to use.<br/><br/>The project builds heavily on two complementary open-source software libraries the team has been developing: MUQ at MIT, and hIPPYlib at UT-Austin/UC-Merced. MUQ provides a spectrum of powerful Bayesian inversion models and algorithms, but expects forward models to come equipped with gradients/Hessians to permit large-scale solution. hIPPYlib implements powerful large-scale gradient/Hessian-based inverse solvers in an environment that can automatically generate needed derivatives, but it lacks full Bayesian capabilities. By integrating these two complementary libraries, the project will result in a robust, scalable, and efficient software framework that realizes the benefits of each to tackle complex large-scale Bayesian inverse problems across a broad spectrum of scientific and engineering disciplines. The resulting software, that will be distributed under an open-source license, will provide an environment for rapid development of inverse models equipped with gradient/Hessian information; benchmark problems for evaluation and comparison of algorithms; and tutorial problems for training and testing purposes."
"1440585","Collaborative Research:  SI2-SSE:  Pythia Network Diagnosis Infrastructure (PuNDIT)","OAC","Software Institutes, Campus Cyberinfrastructure","09/01/2014","08/25/2014","Warren Matthews","GA","Georgia Tech Research Corporation","Standard Grant","Rajiv Ramnath","08/31/2017","$237,407.00","","warren.matthews@oit.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","CSE","8004, 8080","7433, 8005","$0.00","In today's world of distributed collaborations of scientists, there are many challenges to providing effective infrastructures to couple these groups of scientists with their shared computing and storage resources. The Pythia Network Diagnostic InfrasTructure (PuNDIT) project will integrate and scale research tools and create robust code suitable for operational needs to address the difficult challenge of automating the detection and location of network problems. <br/><br/>PuNDIT will build upon the de-facto standard perfSONAR network measurement infrastructure to gather and analyze complex real-world network topologies coupled with their corresponding network metrics to identify possible signatures of network problems from a set of symptoms. For example if the symptoms suggest a router along the path has buffers configured too small for high performance, Pythia will return a diagnosis of ""Small Buffer"". If symptoms indicate non-congestive packet-loss for a particular network segment, the user can be notified of a possible ""Bad Network Segment"". A primary goal for PuNDIT is to convert complex network metrics into easily understood diagnoses in an automated way."
"1450089","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","OAC","Physical & Dynamic Meteorology, Software Institutes, EarthCube","08/01/2015","08/04/2015","Russ Schumacher","CO","Colorado State University","Standard Grant","Bogdan Mihaila","07/31/2019","$177,173.00","","russ.schumacher@colostate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","CSE","1525, 8004, 8074","4444, 7433, 8009","$0.00","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities."
"1449723","Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","08/01/2015","06/24/2015","Charles Sherrill","GA","Georgia Tech Research Corporation","Standard Grant","Seung-Jong Park","07/31/2021","$600,000.00","","sherrill@gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","CSE","1253, 8004","7433, 8009","$0.00","Computational chemistry has become ubiquitous throughout the fields of science and engineering. Among the many uses of computational chemistry codes are to aid in the design of new materials with specific properties, to understand protein folding dynamics related to the human body, to understand the mechanisms of enzyme catalysis to produce biofuels, and to identify and characterize gaseous species in the atmosphere. Because computational chemistry simulations can require large amounts of computer, memory and disk space, one focus of this research is to reduce the demands on computer resources by exploiting methods that can efficiently fragment large molecules into smaller pieces and at the same time take advantage of computers that have many thousands of computer cores. The principal investigators are developers of several computational chemistry programs, each of which has unique functionalities that have taken multiple person-years to develop and implement. In order to minimize further development efforts and to maximize the utility of the unique features, a second key focus of this research will be to develop the ability of each program to access the unique features and data of the other programs. A major bottleneck in the effort to construct high performance computers with more and more compute cores is the power that is required to drive such systems. One way that the research team will address the power issue is to explore the utility of low power architectures, such as graphical processing units for computational chemistry calculations. All of the newly developed codes will be made available to the user community by web download at no cost.<br/><br/>This project presents an integrated computational science approach to very high quality electronic structure and dynamics calculations that will (a) be broadly accessible to both the development and applications communities, (b) have the capability to address problems of great interest, such as the properties of liquids and solvent effects, and photochemical/photobiological dynamics with high accuracy, (c) provide interoperability and sustainability into the foreseeable future, and (d) solve bottlenecks including power consumption using accelerators. A primary goal is to provide to the broad community new, accurate approaches that may be easily used, with a clear path forward to further code improvement and development. As part of the proposed effort, in addition to the usual outlets of journal articles and public presentations, the investigators will organize workshops at prominent national meetings, so that the expertise and software developed will be available to as broad a group of users as possible.  All of the developed codes will be available on the web for easy downloads. The proposed research will develop new paradigms for interoperability, with a focus on the highly popular program suites GAMESS, NWChem, PSI4  and the AIMS dynamics code. Also included will be a cloud-based client-server model, a common quantum chemistry driver and novel data management approaches. The integration of the codes will be accomplished by making use of the combined expertise of the PIs in developing interoperable methods and data interfaces in computational science. In addition, several new methods will be developed, including novel explicit (R12) correlation methods that will be integrated with the most accurate levels of theory: multi-reference and the most accurate and novel coupled cluster methods, as well as the derivation and implementation of analytic derivatives. Applicability to large molecular systems will be made feasible by drawing upon novel fragmentation methods that scale nearly perfectly to the petascale."
"1642411","SI2-SSE: STAMLA: Scalable Tree Algorithms for Machine Learning Applications","OAC","Software Institutes","01/01/2017","09/08/2016","Vincent Reverdy","IL","University of Illinois at Urbana-Champaign","Standard Grant","Stefan Robila","12/31/2019","$499,992.00","Robert Brunner","vreverdy@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","8004","7433, 8004, 8005","$0.00","The voluminous growth in data together with the burgeoning field of data science have greatly increased the demand for machine learning, a field of computer science that focuses on the development of programs that can learn and change in response to new data. With increasing access to large volumes of data, practitioners often resort to machine learning to construct more precise models of nature, or to learn fundamentally new concepts. For example, machine learning can help improve the accuracy of weather and climate predictions, model the efficacy of drugs and their interactions, and identify specific features, such as a face, from a large set of images or videos. But with the growth of data volumes, the speed with which machine can learn from data is decreasing. As a result, new techniques are required to accelerate learning algorithms so that they can be applied to larger and more complex data sets. This work will develop new approaches to improve the performance of a wide class of machine learning algorithms. Specifically, this work will leverage the C++ programming language and recent research into fundamental bit operations to make fast tree-like data structures that underlie many of the most commonly used implementations of machine learning algorithms. In particular, algorithms in the scikit-learn library, the most widely used machine learning library written in the Python programming language, will be accelerated by using our these new tree data structures. Given the widespread adoption of the scikit learn library, this work will impact diverse fields from Astronomy to Biology to Geoscience to Physics. The scikit learn library is also one of the more popular libraries for teaching (and understanding) machine learning. With an explosion of books, blogs, and tutorials that use scikit learn algorithms and pipelines to demonstrate specific types of machine learning such as classification, regression, clustering, and feature extraction, this project will immediately impact a wide range of people from seasoned practitioners, engineers gaining additional training, and students at universities and colleges across the nation. In addition, these tree data structures will be submitted for inclusion in the C++ standard, which would impact millions of developers world-wide. Finally, the algorithms will be implemented under an open source license in a public forum.<br/><br/><br/>The STAMLA project aims at developing efficient and scalable tree algorithms inspired from high performance simulation codes for machine learning. Over the last few years, machine learning has become a popular technique in data mining to extract information from data sets, build models and make predictions across a wide range of application areas. However, current tools have been built in high-level languages with more focus on functionalities than on pure performance. But as scientific experiments are accumulating more and more data, and as complex models are requiring larger and larger training sets, scalability issues are emerging. At the same time, in high performance computing, petascale simulations have shown that fundamental data structure optimizations can have a significant impact on overall code performance. In particular, by replacing straightforward tree implementations with implicit trees based on hash tables, simulation codes are able to make the most of modern architecture, leveraging cache and vectorization. This research project will apply this knowledge to machine learning algorithms in order to overcome the limitations of existing libraries and make analyses of extremely large data sets possible. The proposed research includes the development of three library layers, built on top of each other. The first layer is a library of fast bit manipulation tools for tree indexing. It extends existing research that has already demonstrated two to three orders of magnitude improvements compared to standard solutions provided by compilers. The second layer is a tree building blocks library developed using generative programming techniques. This layer will provide developers with generic tools to build efficient implicit trees for specific domains and optimized at compile time to make the most of the targeted architecture. Finally, the third layer consists in a contribution package to the scikit-learn library to leverage the data structures introduced in the second layer. Together, these three layers form a consistent set that propagates low level optimizations based on high performance computing practices to one of the most widely used high level machine learning library. As machine learning is domain independent, the results of this project have the potential to impact all data intensive applications relying on machine learning algorithms based on tree data structures. Moreover, in addition to being developed in an open source framework via a public repository, the three library layers will be released through different channels: 1) the bit manipulation tools will aim at standardization in the C++ language through a collaboration with the ISO C++ Standards Committee 2) the tree building blocks will be proposed for inclusion in the Boost C++ libraries and 3) the machine learning algorithms will be published as a contribution package of the scikit-learn library. These channels will ensure a large adoption of the tools developed throughout this project, and their long-term support by well established communities."
"1534872","SI2-SSE: ShareSafe: A Framework for Researchers and Data Owners to Help Facilitate Secure Graph Data Sharing","OAC","Software Institutes","09/01/2015","06/23/2016","Raheem Beyah","GA","Georgia Tech Research Corporation","Standard Grant","Bogdan Mihaila","08/31/2019","$506,000.00","","rbeyah@ece.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","CSE","8004","7433, 8004, 8005, 9251","$0.00","There is a critical need for information/data sharing to solve some of our most significant academic and societal problems. These data are increasing in size and are becoming much more complex; in many cases, they can be considered structured. An example of structured data is data describing disease propagation in a specific population. Widespread sharing of data can, among many other things, help corporations increase their revenues, help reduce the spread of communicable diseases, accelerate the cure of some of the most significant diseases, and enable reproducible experiments amongst researchers. Although there is little disagreement that sharing data has tremendous benefits, it is still not as widespread as it should be. This is, in part, due to privacy concerns with sharing datasets. This project will develop an open source system (ShareSafe) that allows data owners to evaluate the security (such as resistance to de-anonymization attacks) and utility of their anonymized datasets before release, which will help facilitate the data sharing process.<br/><br/>The overarching goals of this project are to develop a software framework, ShareSafe, that (1) helps structured data owners (e.g., social network researchers, epidemiologists) evaluate the security (against modern de-anonymization attacks) and utility of their datasets when using simple and state-of-the-art anonymization techniques; and (2) to provide structured data security/privacy researchers a uniform platform to comprehensively study, evaluate, and compare existing/newly developed techniques for structured data utility and privacy. ShareSafe is a comprehensive, user-friendly framework with the following capabilities: ShareSafe will enable data owners to: (1) anonymize their datasets with all of the state-of-the art anonymization techniques; (2) measure the utility of anonymized datasets using state-of-the-art utility measurement techniques; (3) evaluate the practical security of their datasets by subjecting them to state-of-the-art de-anonymization attacks; and (4) evaluate the theoretical security of their datasets by subjecting them to state-of-the-art de-anonymization quantification (de-anonymizability analysis) techniques. Understanding the results from (2)-(4) allows data owners to determine which anonymization algorithm suits their needs when sharing datasets.  Finally, the aforementioned techniques will be implemented in a uniform manner as open source software, allowing graph data security/privacy researchers the ability to comprehensively study, evaluate, and compare existing/newly developed techniques for graph data utility and privacy."
"1047980","Collaborative Research: SI2-SSE: Software for integral equation solvers on manycore and heterogeneous architectures","OAC","OFFICE OF MULTIDISCIPLINARY AC, DYNAMICAL SYSTEMS, COFFES, Software Institutes","09/15/2010","09/07/2010","George Biros","GA","Georgia Tech Research Corporation","Standard Grant","Thomas F. Russell","11/30/2012","$250,000.00","","gbiros@gmail.com","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","CSE","1253, 7478, 7552, 8004","1253, 7478, 7552","$0.00","We propose to develop and deploy mathematical software for boundary-value problems in three-dimensional complex geometries.  The algorithms in the library will be based on integral equation formulations.  The library will be designed to scale on novel computing platforms that comprise special accelerators and manycore architectures. <br/><br/>Integral equations can be used to conduct simulations on many problems in science and engineering with significant societal impact.  Three example applications on which the proposed simulation technologies will have an impact in this project are microfluidic chips, biomolecular electrostatics, and plasma physics.  First, microfluidic chips are submillimeter-sized devices used for medical diagnosis and drug design.  Optimizing the function of such devices at low cost requires efficient computer simulation tools, such as the ones we propose to develop.  Second, understanding the structure and function of biomolecules such as DNA and proteins is crucial in biotechnology.  The proposed technologies can be used to resolve bimolecular electrostatic interactions.  Third, plasma physics, which is related to fusion nuclear reactors, includes electrostatic interactions in complex geometries, and the proposed work will enable large-scale three-dimensional simulations. <br/><br/>The key features of the proposed software are: (1) parallel fast multipole methods, (2) efficient geometric modeling techniques for complex geometries, (3) simple library interfaces that allow use of the proposed software by non-experts, and (4) scalability on heterogeneous architectures.<br/><br/>Along with our research activities, an educational and dissemination program will be designed to communicate the results of this work to students and researchers.  Several postdoctoral, graduate, and undergraduate students will be involved with the project.  Additional educational activities will include research experiences for undergraduates, leveraging ongoing programs such as NSF REUs.  We will encourage participation by women, minorities, and underrepresented groups."
"1339781","SI2-SSE: A Sustainable Wireless Sensor Software Development Framework for Science and Engineering Researchers","OAC","Special Projects - CCF, Software Institutes","09/01/2013","04/10/2015","Raheem Beyah","GA","Georgia Tech Research Corporation","Standard Grant","Rajiv Ramnath","08/31/2017","$506,000.00","Selcuk Uluagac","rbeyah@ece.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","CSE","2878, 8004","7433, 8005, 9251","$0.00","In the last decade, wireless sensor networking has been one of the most popular research areas for computer engineers and scientists. The use of wireless sensors has also started to gain popularity among researchers other than computer engineers and scientists. Today, sensors are used to enable civil engineers to monitor the structural health of deteriorating infrastructure such as highways and bridges, farmers to develop precision-agriculture techniques, ecologists to observe wildlife in their natural habitat, geophysicists to capture seismic activity of volcanoes, and in many other application areas. However, the task of developing software applications for wireless sensors is challenging for researchers because sensors have limited technical capabilities and software implementations for sensors require meticulous procedures to provide a desired level of services (e.g., reliability, security) for applications. This situation is even further exacerbated for researchers in other fields of engineering and science (e.g., civil engineers and geophysicists) as they may not have a rigorous programming background. Therefore, to facilitate the design, development and implementation of wireless sensor applications, this project provides a new framework called PROVIZ, which integrates visualization and programming functionalities into a common platform. PROVIZ is an open-source, platform independent, modular, and extensible framework for heterogeneous wireless sensor monitoring and application development. It consists of a set of easy-to-use simplified languages (one domain specific scripting language, one icon-based drag-and-drop style visual language) and a simple programming editor for developing wireless sensor applications and a mechanism for (re)programming wireless sensor nodes remotely over-the-air by distributing the generated application image.  PROVIZ has the capability to visualize wireless sensor data captured from (1) a packet sniffer, (2) a binary packet trace file (e.g., PSD format), and (3) an external simulator running a wireless sensor application. PROVIZ also has the capability to process data from multiple sniffers simultaneously to visualize a large wireless sensor deployment.<br/> <br/>PROVIZ will be instrumental to scientists and engineers working with wireless sensors in many disciplines (e.g., civil engineering, ecology, agriculture). PROVIZ will allow these researchers to easily program sensors and to focus more on the tasks in their domains by significantly reducing the overhead of learning how to program sensors. The PROVIZ project will be conducted as an open source project, enabling interested software developers to benefit from and add to it. Further, a variation of PROVIZ will be used to present sensor networking concepts to middle school underrepresented minority students in Georgia. Given the proliferation of wireless sensor utilization in various engineering and science fields, it is envisioned that the success of the PROVIZ project will help contribute to the growth of the future cyber workforce in the U.S."
"1739772","Collaborative Research: SI2: SSE: Extending the Physics Reach of LHCb in Run 3 Using Machine Learning in the Real-Time Data Ingestion and Reduction System","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","09/01/2017","08/24/2017","Mike Williams","MA","Massachusetts Institute of Technology","Standard Grant","Seung-Jong Park","08/31/2021","$275,000.00","","mwill@mit.edu","77 MASSACHUSETTS AVE","CAMBRIDGE","MA","021394301","6172531000","CSE","1253, 7244, 8004","7433, 8004, 8005","$0.00","In the past 200 years, physicists have discovered the basic constituents of ordinary matter and the developed a very successful theory to describe the interactions (forces) between them.  All atoms, and the molecules from which they are built, can be described in terms of these constituents.  The nuclei of atoms are bound together by strong nuclear interactions.  Their decays result from strong and weak nuclear interactions. Electromagnetic forces bind atoms together, and bind atoms into molecules. The electromagnetic, weak nuclear, and strong nuclear forces are described in terms of quantum field theories.  The predictions of these theories can be very, very precise, and they have been validated with equally precise experimental measurements.  Most recently, a new fundamental particle required to unify the weak and electromagnetic interactions, the Higgs boson, was discovered at the Large Hadron Collider (LHC), located at the CERN laboratory in Switzerland. Despite the vast amount of knowledge acquired over the past century about the fundamental particles and forces of nature, many important questions still remain unanswered. For example, most of the matter in the universe that interacts gravitationally does not have ordinary electromagnetic or nuclear interactions.  As it has only been observed via its gravitation interactions, it is called dark matter.  What is it?  Equally interesting, why is there so little anti-matter in the universe when the fundamental interactions we know describe matter and anti-matter as almost perfect mirror images of each other? The LHC was built to discover and study the Higgs boson and to search for answers to these questions. The first data-taking run (Run 1, 2010-2012) of the LHC was a huge success, producing over 1000 journal articles, highlighted by the discovery of the Higgs boson. The current LHC run (Run 2, 2015-present) has already produced many world-leading results; however, the most interesting questions remained unanswered. The LHCb experiment, located on the LHC at CERN, has unique potential to answer some of these questions. LHCb is searching for signals of dark matter produced in high-energy particle collisions at the LHC, and performing high-precision studies of rare processes that could reveal the existence of the as-yet-unknown forces that caused the matter/anti-matter imbalance observed in our universe. The primary goal of this project - supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences - is developing and deploying software utilizing Machine Learning (ML) that will enable the LHCb experiment to significantly improve its discovery potential in Run 3 (2021-2023). Specifically, the ML developed will greatly increase the sensitivity to many proposed types of dark matter and new forces by making it possible to much more efficiently identify and study potential signals -- using the finite computing resources available.  <br/><br/>The data sets collected by the LHC experiments are some of the largest in the world. For example, the sensor arrays of the LHCb experiment, on which both PIs work, produce about 100 terabytes of data per second, close to a zettabyte of data per year. Even after drastic data-reduction performed by custom-built read-out electronics, the data volume is still about 10 exabytes per year, comparable to the largest-scale industrial data sets. Such large data sets cannot be stored indefinitely; therefore, all high energy physics (HEP) experiments employ a data-reduction scheme executed in real time by a data-ingestion system - referred to as a trigger system in HEP - to decide whether each event is to be persisted for future analysis or permanently discarded. Trigger-system designs are dictated by the rate at which the sensors can be read out, the computational power of the data-ingestion system, and the available storage space for the data. The LHCb detector is being upgraded for Run 3 (2021-2023), when the trigger system will need to process 25 exabytes per year. Currently, only 0.3 of the 10 exabytes per year processed by the trigger are analyzed using high-level computing algorithms; the rest is discarded prior to this stage using simple algorithms executed on FPGAs. To process all the data on CPU farms, ML will be used to develop and deploy new trigger algorithms. The specific objectives of this proposal are to more fully characterize LHCb data using ML and build algorithms using these characterizations: to replace the most computationally expensive parts of the event pattern recognition; to increase the performance of the event-classification algorithms; and to reduce the number of bytes persisted per event without degrading physics performance. Many potential explanations for dark matter and the matter/anti-matter asymmetry of our universe are currently inaccessible due to trigger-system limitations. As HEP computing budgets are projected to be approximately flat moving forward, the LHCb trigger system must be redesigned for the experiment to realize its full potential. This redesign must go beyond scalable technical upgrades; radical new strategies are needed."
"1642385","SI2-SSE: Collaborative Research: High Performance Low Rank Approximation for Scalable Data Analytics","OAC","Software & Hardware Foundation, Software Institutes","11/01/2016","09/08/2016","Grey Ballard","NC","Wake Forest University","Standard Grant","Amy Walton","10/31/2021","$167,713.00","","ballard@wfu.edu","1834 WAKE FOREST RD","WINSTON SALEM","NC","271096000","3367585888","CSE","7798, 8004","7433, 7942, 8004, 8005","$0.00","Big Data analytics is at the core of discovery covering vast areas such as medical informatics, business analytics, national security, and materials sciences. This project aims to model some of the key data analytics problems and design, verify, and deploy scalable methods for knowledge extraction.  The algorithms developed will be able to handle data sets of extreme sizes and will be deployable on advanced computer hardware. The goal is to realize orders-of-magnitude improvements over existing data analytics technologies, developing algorithms that are robust to incompleteness, noise, ambiguity, and high dimension in the data.  Particular focus will be parallel and distributed algorithms that can efficiently solve large problems and produce accurate solutions.  The proposed research and software development will allow domain experts to tackle Big Data sets requiring large parallel systems.  The improved performance will enable fast and scalable data analysis across applications, from social network analysis to study citizens' attitudes toward sustainability-related issues to computational marketing techniques that refine customers' shopping experiences.  The proposed work will help bridge the gap between computational science and data analytics ecosystems, two fields that stand to make great advancements from cross-fertilization.  The education and outreach plan includes graduate course creation, engagement of under-represented groups via both undergraduate and graduate research experiences, and community-building efforts by workshop and mini-symposium organization.<br/><br/>With the advent of internet-scale data, the data mining and machine learning community has adopted Nonnegative Matrix Factorization (NMF) for performing numerous tasks such as topic modeling, background separation from video data, hyper-spectral imaging, web-scale clustering, and community detection.  The goals of this proposal are to develop efficient parallel algorithms for computing nonnegative matrix and tensor factorizations (NMF and NTF) and their variants using a unified framework, and to produce a software package called Parallel Low-rank Approximation with Nonnegative Constraints (PLANCK) that delivers the high performance, flexibility, and scalability necessary to tackle the ever-growing size of today's data sets. The algorithms will be generalized to NTF problems and extend the class of algorithms we can efficiently parallelize; our software framework will allow end-users to use and extend our techniques.  Rather than developing separate software for each problem domain and mathematical technique, flexibility will be achieved by characterizing nearly all of the current NMF and NTF algorithms in the context of a block coordinate descent framework. Using this framework the shared computational kernels can be separated, which usually extend run times, from the algorithm-specific computations. Finally, the usability and practicality of the proposed software will be maintained by being application driven, establishing collaborations with early end-users, and by incrementally generalizing the framework in terms of both algorithms and problems."
"1642380","Collaborative Research: SI2-SSE: High-Performance Workflow Primitives for Image Registration and Segmentation","OAC","Software Institutes","10/01/2016","09/08/2016","James Shackleford","PA","Drexel University","Standard Grant","Seung-Jong Park","09/30/2021","$390,000.00","Nagarajan Kandasamy","shack@drexel.edu","3141 CHESTNUT ST","PHILADELPHIA","PA","191042816","2158956342","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Image registration and segmentation are vital enabling technologies for addressing many complex, data driven problems. Examples include individualized medical treatment where disease progression is monitored by analyzing MRI, CT, or ultrasound images over time; identifying anatomical structures in medical images; recognizing objects and people in video footage; and extracting imageable biometrics such as fingerprints, faces, and the iris. Images and videos can now be easily acquired at a rate that far surpasses our capacity to perform advanced image analysis.  For this reason, advanced registration and segmentation algorithms are not routinely used for many large-scale and time sensitive applications because they require more processing time than is available. This project will remedy this situation by developing a high-performance software package for image registration and segmentation, suitable to be run on massively parallel processors, and building a strong user and developer base around it. All software developed through the project will be open source and licensed under the MIT License. Improvements in processing speed achieved by the proposed platform will have significant impact in disciplines such as computer vision, digital forensics, and biomedical image analysis. Finally, the project team is committed to the diversity mission of Drexel University and will reach out to under-represented groups when recruiting graduate students for this project. Selected research tasks will be integrated within existing courses and curriculum will be developed for new experiential programs stemming from this effort.<br/><br/>The overall goal of this project is to develop a high-performance, many-core CPU and GPU accelerated algorithmic software package for attacking classes of problems that depend on solutions to data-dense inverse problems such as registration, segmentation, tomography, and parameter estimation. The specific technical approach involves developing algorithmic primitives required by a broad class of inference and analysis based workflows. Probabilistic primitives for building generative, discriminative, and conditional random field classification models will be implemented with emphasis on object segmentation.  Specialized registration operators will be developed for spline and voxel-driven algorithms. These primitives will be developed within the single instruction multiple data paradigm which utilizes many-core processing architectures via OpenMP, CUDA, and OpenCL. The workflow will be supplemented by a graphical user interface (GUI), providing a feature rich studio of tools that expose high-performance primitives to scientists visually and intuitively. The platform architecture will be designed as a distributed system service targeting locally administered scientific computing clusters where the number of compute nodes will be able to scale with load requirements. The GUI and the computational core may either run in a distributed client-server configuration or together locally on a single high performance workstation. Emphasis will be placed on documentation and video/written tutorials necessary for adoption. The project team will use an open software development model to build a strong user base comprising both novice users as well as researchers with the need to implement new algorithms on top of a stable software infrastructure. It is expected that the availability of this tool and its source code will catalyze an increase in quantitative image analysis spanning across research disciplines."
"1550476","Collaborative Research: SI2-SSI: Swift/E: Integrating Parallel Scripted Workflow into the Scientific Software Ecosystem","OAC","Software Institutes","10/01/2016","09/13/2016","Mitchell Wayne","IN","University of Notre Dame","Standard Grant","Amy Walton","09/30/2019","$81,000.00","","mwayne@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","8004","7433, 8004, 8009, 9102","$0.00","Science and engineering research increasingly relies on repeated execution of a complex series of steps (i.e., workflows) to form hypotheses; conduct experiments; analyze results; and refine theory.   Computation is often essential throughout the workflow and in this case, software can improve productivity by managing the computational and data workflow.  Swift is one such open-source workflow system that has been developed and widely used in diverse areas ranging from materials simulations and climate modeling to neuroscience and genomics. This project extends the capabilities of Swift by integrating it with other software systems that enable collaboration, usability, maintainability, and productivity. The new ecosystem, Swift/E, will enable scientists and engineers to more productively create and run computational workflow campaigns of larger scale, and debug, execute, adapt, and disseminate them faster and easier than has been possible to date. These workflows embody and communicate the computational methods specific to each domain of scientific inquiry. Swift/E achieves community engagement and extensive productivity benefits for a large user community through an integrated program of research, education, and software dissemination. The project engages and serves science and engineering communities by creating patterns of practice for building and sharing reusable workflow libraries, and by training students, educators, and researchers in their use.  To advance the education of the next generation of computationally trained scientists, Swift/E powers a network of NSF-supported ""e-Labs"" that teach the concepts of collaborative parallel computational science at high school and undergraduate levels, reaching over a thousand students annually.<br/><br/>The open-source Swift/E ""ecosystem"" integrates Swift with several scientific software elements that play a major role in the national and global cyberinfrastructure of today. These elements are: Swift for the parallel scripting of scientific workflow; Globus for data cataloging, management, and high-speed wide-area transport; the Web-based Galaxy workflow portal for workflow composition, execution, and collaborative sharing; Jupyter for the interactive development, testing, debugging, and assembly of high level programming and workflow languages; Python and R for productively expressing high-level computational logic; and ""git"" and related tools and Web portals for revision control, code dissemination and sharing, and for the collaborative engagement of developers.  Swift's implicitly parallel programming language is minimal and compact.  Swift provides a facility for embedding other scripting languages (currently Python, R, Julia and Tcl) into its runtime environment.  This project merges newer extreme-scale ""Swift/T"" capabilities with the flexible and portable original ""Swift/K"" version to make the core Swift/E software element more powerful and flexible while lowering it?s ongoing support cost. Swift/E enhances usability by extending Swift's troubleshooting and inter-language integration facilities.  And with enhanced and innovative workflow sharing archives, new training materials, and a sustained program for user support and self-sustaining and expanding community engagement, the Swift/E project engages, supports, and sustains a large global science and engineering user base."
"1550482","SI2-SSI: Collaborative Research: ENKI: Software Infrastructure that ENables Knowledge Integration for Modeling Coupled Geochemical and Geodynamical Processes","OAC","Petrology and Geochemistry, Software Institutes, EarthCube","09/01/2016","08/19/2016","Mark Ghiorso","WA","OFM Research","Standard Grant","Seung-Jong Park","08/31/2020","$734,907.00","","ghiorso@ofm-research.org","28430 NE 47TH PL","REDMOND","WA","980538841","4258804418","CSE","1573, 8004, 8074","7433, 8004","$0.00","Earth scientists seek to understand the mechanisms of planetary evolution from a process perspective in order to promote the progress of science.  They model the chemistry of melting of the interiors of planets as a result of heat flow within the body.  They calculate the flows of energy and mass from the interior to the surface. They model the interaction of fluids and rocks, which drives chemical weathering and the formation of ore deposits.  They seek to understand the synthesis and stabilities of organic compounds and their economic and biological roles.  They study the interactions of atmosphere, oceans, biosphere and land as a dynamically coupled evolving chemical system. To achieve this level of understanding of planetary evolution, Earth scientists use software tools that encode two fundamentally different types of models: (1) thermodynamic models of naturally occurring materials, and (2) models of transport that track physical flows of both fluids and solids.  Much of the fundamental science of planetary evolution lies in understanding coupled thermodynamic and transport models. This grant funds development of a software infrastructure that supports this coupled modeling of the chemical evolution of planetary bodies.   It is their aim to establish an essential and active community resource that will engage a large number of researchers, especially early career scientists, in the exercise of model building and customization.  <br/><br/>This is a project to create ENKI, a collaborative model configuration and testing portal that will transform research and education in the fields of geochemistry, petrology and geophysics.  ENKI will provide software tools in computational thermodynamics and fluid dynamics.  It will support development and access to thermochemical models of Earth materials, and establish a standard infrastructure of web services and libraries that permit these models to be integrated into fluid dynamical transport codes. This infrastructure will allow scientific questions to be answered by quantitative simulations that are presently difficult to impossible because of the lack of interoperable software frameworks.  ENKI, via the adoption of state-of-the-art model interfacing (OpenMI) and deployment environments (HubZero), will modernize how thermodynamic and fluid dynamic models are used by the Earth science community in five fundamental ways: (1) provenance tracking will enable automatic documentation of model development and execution workflows, (2) new tools will assist users in updating thermochemical models as new data become available, with the ability to merge these data and models into existing repositories and frameworks, (3) automated code generation will eliminate the need for users to manually code web services and library modules, (4) visualization tools and standard test suites will facilitate validation of model outcomes against observational data, (5) collaborative groups will be able to share and archive models and modeling workflows with associated provenance for publication. With these tools we seek to transform the large community of model users, who currently depend on a small group of dedicated and experienced researchers for model development and maintenance, into an empowered ensemble of model developers who take ownership of the process and bring their own expertise, intuition and perspective to shaping the software tools they use in daily research.  ENKI development will be community driven.  Participation of a dedicated and diverse group of early career professionals will guide us in user interface development - insuring portal capabilities are responsive to user needs, and in development of a rich set of documentation, tutorials and examples.  All software associated with this project will be released as open source."
"1550346","SI2-SSI:Collaborative Research: ENKI: Software infrastructure that ENables Knowledge Integration for Modeling Coupled Geochemical and Geodynamical Processes","OAC","Software Institutes, EarthCube","09/01/2016","08/19/2016","Dimitri Sverjensky","MD","Johns Hopkins University","Standard Grant","Seung-Jong Park","08/31/2020","$217,827.00","","sver@jhu.edu","3400 N CHARLES ST","BALTIMORE","MD","212182608","4439971898","CSE","8004, 8074","7433, 8004, 8009","$0.00","Earth scientists seek to understand the mechanisms of planetary evolution from a process perspective in order to promote the progress of science.  They model the chemistry of melting of the interiors of planets as a result of heat flow within the body.  They calculate the flows of energy and mass from the interior to the surface. They model the interaction of fluids and rocks, which drives chemical weathering and the formation of ore deposits.  They seek to understand the synthesis and stabilities of organic compounds and their economic and biological roles.  They study the interactions of atmosphere, oceans, biosphere and land as a dynamically coupled evolving chemical system. To achieve this level of understanding of planetary evolution, Earth scientists use software tools that encode two fundamentally different types of models: (1) thermodynamic models of naturally occurring materials, and (2) models of transport that track physical flows of both fluids and solids.  Much of the fundamental science of planetary evolution lies in understanding coupled thermodynamic and transport models. This grant funds development of a software infrastructure that supports this coupled modeling of the chemical evolution of planetary bodies.   It is their aim to establish an essential and active community resource that will engage a large number of researchers, especially early career scientists, in the exercise of model building and customization.  <br/><br/>This is a project to create ENKI, a collaborative model configuration and testing portal that will transform research and education in the fields of geochemistry, petrology and geophysics.  ENKI will provide software tools in computational thermodynamics and fluid dynamics.  It will support development and access to thermochemical models of Earth materials, and establish a standard infrastructure of web services and libraries that permit these models to be integrated into fluid dynamical transport codes. This infrastructure will allow scientific questions to be answered by quantitative simulations that are presently difficult to impossible because of the lack of interoperable software frameworks.  ENKI, via the adoption of state-of-the-art model interfacing (OpenMI) and deployment environments (HubZero), will modernize how thermodynamic and fluid dynamic models are used by the Earth science community in five fundamental ways: (1) provenance tracking will enable automatic documentation of model development and execution workflows, (2) new tools will assist users in updating thermochemical models as new data become available, with the ability to merge these data and models into existing repositories and frameworks, (3) automated code generation will eliminate the need for users to manually code web services and library modules, (4) visualization tools and standard test suites will facilitate validation of model outcomes against observational data, (5) collaborative groups will be able to share and archive models and modeling workflows with associated provenance for publication. With these tools we seek to transform the large community of model users, who currently depend on a small group of dedicated and experienced researchers for model development and maintenance, into an empowered ensemble of model developers who take ownership of the process and bring their own expertise, intuition and perspective to shaping the software tools they use in daily research.  ENKI development will be community driven.  Participation of a dedicated and diverse group of early career professionals will guide us in user interface development - insuring portal capabilities are responsive to user needs, and in development of a rich set of documentation, tutorials and examples.  All software associated with this project will be released as open source."
"1550234","SI2-SSI: Collaborative Research: Paratreet: Parallel Software for Spatial Trees in Simulation and Analysis","OAC","Software Institutes","09/01/2016","08/19/2016","Thomas Quinn","WA","University of Washington","Standard Grant","Rob Beverly","08/31/2018","$139,977.00","Magdalena Balazinska","TRQ@astro.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","8004","7433, 8004, 8009","$0.00","Many scientific and visualization methods involve organizing the data they are processing into a hierarchy (also known as a ""tree"").   These applications and methods include: astronomical simulations of particles moving under the influence of gravity, analysis of spatial data (that is, data that describes objects with respect to their relative position in space), photorealistic rendering of virtual environments,reconstruction of surfaces from laser scans, collision detection when simulating the movement of physical objects, and many others.   Tree data structures, and the algorithms used to work on these structures, are heavily used in these applications because they help to make these applications run much faster on supercomputers. However, implementing tree-based algorithms can require a significant effort, particularly on modern highly parallel computers.  This project will create ParaTreet, a software toolkit for parallel trees, that will enable rapid development of such applications.  Details of the parallel aspects will be hidden from the programmer, who will be able to quickly evaluate the relative merits of different trees and algorithms even when applied to large datasets and very computation-intensive applications. The combination of such an abstract and extensible framework with a portable adaptive runtime system will allow scientists to effectively use parallel hardware ranging from small clusters to petascale-class machines, for a wide variety of tree-based applications. This project will demonstrate the feasibility of such an approach as well as generate evidence of community adoption of this technology. If successful, this project will enable NSF-supported researchers to solve science problems faster as well as to tackle more complex problems, thus serving NSF's science mission.<br/><br/><br/>This project builds upon an existing collaboration on Computational Astronomy and the resultant software base in the ChaNGa (Charm N-body GrAvity solver) code. ChaNGa is a software package that performs collisionless N-body simulations, and can perform cosmological simulations with periodic boundary conditions in co-moving coordinates or simulations of isolated stellar systems. This project will extend ChaNGa with a parallel tree toolkit called ParaTreet and associated applications, that will allow scientists to effectively utilize small clusters as well as very large supercomputers for parallel tree-based calculations.  The key data structure in ParaTreet is an asynchronous software-based tree data cache, which maintains a writeback local copy of remote tree data. We plan to support a variety of spatial decomposition methods and the associated trees, including Oct-trees, KD-trees, inside-outside trees, ball trees, R-trees, and their combinations. Different trees are useful in different application circumstances, and the software will allow their relative merits to be evaluated with relative ease. The framework will support a variety of parallel work decomposition methods, including those based on space filling curves, and support dynamic rearrangement of parallel work at runtime. The algorithms supported will range from Barnes-Hut with various multipole expansions, data clustering, collision detection, surface reconstruction, ray intersection, etc. The software includes a collection of dynamic load balancing strategies in the Charm++ framework that can be tuned for specific problem structures. It also includes support for clusters of accelerators, such as GPGPUs. This project will demonstrate the feasibility of such an approach as well as generate evidence of community adoption of this technology."
"1565676","Inspiration, Frustration, and Fascination: An Excursion into Low-Oxidation State Main Group Chemistry","CHE","Chemical Synthesis","06/01/2016","04/11/2016","Gregory Robinson","GA","University of Georgia Research Foundation Inc","Standard Grant","George Richter-Addo","05/31/2020","$510,000.00","Yuzhong Wang","robinson@chem.uga.edu","310 EAST CAMPUS RD TUCKER HALL R","ATHENS","GA","306021589","7065425939","MPS","6878","","$0.00","The Chemical Synthesis Program of the Chemistry Division supports the research project by Professor Gregory H. Robinson, a faculty member in the Department of Chemistry at The University of Georgia.  Professor Robinson and his research team are studying the unique chemistry of low-oxidation state main group chemical compounds. The goal of this research is to exploit the unique stabilizing effects of organic bases (a class of compounds known as carbenes) on highly reactive main group molecules. For example, important molecules like disilicon (Si2) are only detectable at extremely low temperatures. In contrast, diphosphorus (P2) is typically only detectable at very high temperatures. The Robinson team has developed a means to stabilize molecules like Si2 and P2 (and many others) at room temperature, thus allowing the convenient study of the structure and reactivity of these important molecules. In particular, these researchers recently reported the first stable molecular examples of silicon oxides. This project investigates the synthesis of more ambitious silicon oxides. This chemistry has the potential for us to learn more about the silicon-oxygen interface with possible implications to computer chips and semiconductors. These researchers will also attempt to synthesize molecules containing large silicon and arsenic clusters. This project lies at the heart of main group chemistry, a field of inorganic chemistry that has traditionally received more emphasis in Europe. Outreach activities involving women and traditionally under-represented groups is central to this research. The students engaged in this work are acquiring valuable synthetic and experimental skills that make them highly valuable in the employment market.<br/><br/>An ambitious program to explore challenging areas of low-oxidation main group chemistry is underway. The Robinson laboratory has developed N-heterocyclic carbenes (NHC or L:) and N-heterocyclic dicarbene (NHDC) derivatives that are being used as a unique platform from which many unusual low-oxidation state main group species can be synthetically stabilized. Major synthetic goals in this work include: (a) carbene-based multisilylenes; (b) carbene-stabilized silicon atom and clusters; (c) carbene-stabilized heteronuclear diatomic molecules [i.e., silicon carbides, diatomic III-V (13-15) species, arsenic phosphide (AsP)]. The recent report by this laboratory of carbene-stabilization of elusive silicon oxides (Nature Chem. 2015, 7, 509) has encouraged these workers to develop the long-sought molecular chemistry of SOx. Consequently, the syntheses of a series of carbene-stabilized silicon oxides (such as SiO, SiO2, Si2O, Si2O2, and Si3O6, etc.) and silicon hydrides [Si3H2 and Si2H2 (parent disilyne)] are being pursued. These carbene-stabilized silicon oxides may be further utilized to develop the corresponding transition-metal-modified derivatives and transfer silicon oxide clusters into organic or organometallic substrates. In addition, carbene-stabilized bis-silylenes are explored as potential transfer agents for the disilyne unit. The transition metal chemistry of carbene-stabilized zero-oxidation-state main group species are being examined in the work. Research findings from the Robinson laboratory have repeatedly challenged traditional theories of structure and bonding in inorganic chemistry and some of this has begun to appear in chemistry textbooks. Students engaged in this work are acquiring valuable synthetic, crystallographic, and computational skills. The Robinson laboratory has a positive record of extending the chemistry enterprise to larger segments of the human resource as a number of women and African Americans have been trained in his laboratory. In addition, Professor Robinson has developed a popular seminar course entitled ""Molecules That Changed History""."
"1740300","Collaborative Research:   SI2-SSE:   An open source multi-physics platform to advance fundamental understanding of plasma physics and enable impactful application of plasma systems","OAC","PLASMA PHYSICS, OFFICE OF MULTIDISCIPLINARY AC, Special Initiatives, Software Institutes","09/01/2017","12/04/2020","Steven Shannon","NC","North Carolina State University","Standard Grant","Rob Beverly","05/31/2021","$160,000.00","","scshanno@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","CSE","1242, 1253, 1642, 8004","004Z, 026Z, 1062, 7433, 7569, 8004, 8005, 8396","$0.00","As the world moves toward a more sustainable life cycle for vital resources, new techniques for the synthesis, modification, or remediation of materials will be needed. Techniques that utilize plasma discharges will make significant contributions to a more sustainable nexus spanning food, water, and energy. To advance the fundamental understanding of these plasma-based systems and how they interact with the materials that will drive this higher level of sustainability, the ability to simulate both the complex interactions within the plasma itself and the complex interaction of the plasma with surrounding materials is needed. This project will provide a powerful simulation platform to the scientific community that will enable the study of plasma chemistry formation and plasma material interaction with a level of fidelity that is not currently available to researchers around the world. The open-source framework for this platform will enable researchers from institutions around the world to contribute to the capabilities of this framework and advance the underlying science of these systems to move toward a more sustainable food, energy, and water nexus.<br/><br/><br/>To advance plasma-based technology that will enable greater sustainability in the future food, energy, and water nexus, there exists an overarching need for advances in simulation capability that address four unifying research challenges, 1.) Plasma Produced Selectivity in Reaction Mechanisms in the Volume and on Surfaces, 2.) Interfacial Plasma Phenomena, 3.) Multiscale, Non-Equilibrium Chemical Physics, and 4.) Synergy and Complexity in Plasmas. This research effort will expand, deploy, and support a powerful open-source multi-physics platform that will enable advanced simulation in these unifying research areas. A plasma science simulation application will be expanded to include complex multi-phase chemistries, multiple-domain simulation of the interface between plasmas and other material phases, and fully coupled electro-magnetic treatment of plasma systems that will link plasma formation mechanisms with underlying chemical and electrical multi-phase interactions. Zapdos will be supported on the existing multi-physics Object Oriented Simulation Environment (MOOSE) and will leverage the existing support, verification, revision tracking, and training infrastructure and best known methods employed by both MOOSE and the 22 developed applications (including Zapdos) that currently reside on the MOOSE framework. This proposal will leverage collaboration not only between the two partnering universities, but with framework developers (Idaho National Laboratory), existing users (Oak Ridge National Laboratory), and the broader plasma community (APS Topical Meeting on Gaseous Electronics) to develop efficient development, deployment, support, and training of this impactful simulation tool.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering, the Physics Division and the Office of Multidisciplinary Activities in the Directorate of Mathematical and Physical Sciences, and the Division of Chemical, Bioengineering, Environmental, and Transport Systems in the Directorate of Engineering."
"1739145","SI2-SSE: Infrastructure Enabling Broad Adoption of New Methods That Yield Orders-of-Magnitude Speedup of Molecular Simulation Averaging","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2017","06/27/2018","David Kofke","NY","SUNY at Buffalo","Standard Grant","Seung-Jong Park","09/30/2021","$508,234.00","Andrew Schultz","kofke@buffalo.edu","520 LEE ENTRANCE STE 211","AMHERST","NY","142282577","7166452634","CSE","1712, 8004","054Z, 7433, 8004, 8005, 9216, 9251","$0.00","There is, today, a strong expectation that future materials will be studied in huge numbers first on the computer, and the best candidates for synthesis in the laboratory will be identified computationally. In this way engineers can efficiently formulate new materials that are lighter, stronger, or otherwise more functionally effective. Such advances are needed across all fields of technology, from energy to medicine to transportation to manufacturing. Recent advances from the molecular modeling community toward quantifying atomic interactions are rapidly eliminating a key obstacle to realization of this vision. Yet, an important obstacle remains: the thermal properties of materials --  those that are important at all but the lowest temperatures -- are needed to predict crystal structures and properties at conditions of practical interest. These properties are too expensive to compute for many materials at once, as needed for a computation-based screening effort. The project team has developed an algorithm that significantly accelerates these calculations without any loss of accuracy, and therefore goes a long way toward removing this obstacle. The aim of this project is to make this breakthrough available to researchers who are using molecular simulation to understand and develop new materials. To this end, this project will refine and extend these methods, and then add computer code to widely-used molecular simulation packages so that they can perform calculations using these new techniques. The team is additionally making efforts to promote awareness and ensure ease-of-use of the methods and their implementation.<br/><br/>""Mapped averaging"" is a recently published scheme for the reformulation of ensemble averages. The framework uses approximate results from statistical mechanical theory to derive new ensemble averages (mapped averages) that represent exactly the error in the theory. Well-conceived mapped averages can be computed by molecular simulation with remarkable precision and efficiency; in favorable cases the computational savings are many orders of magnitude. For crystalline systems, a harmonic approximation provides a suitable starting point, allowing simulation to compute precisely the anharmonic contribution to the properties. The result is a technique for computing crystalline properties with unprecedented, transformative efficiency. The aim of this project is to implement these methods on well-established and widely used software packages for simulation of crystalline systems, and to develop mapped averages for new applications of interest to the users of these systems. The theoretical basis for this project appeared in the literature very recently (2015), so the proposed work is completely novel. The techniques are not trivial to understand and are tedious implement, hence adoption by the larger community will require this targeted infrastructure development to make them more accessible to casual users. The full development team includes the computational scientists and software engineers who coded, maintain and distribute the packages where these elements will be introduced. This group assists the project investigators to interface with the simulation packages while ensuring that the new codes are written to the highest standards. The full development team works together also to ensure that the software elements are thoroughly validated for correctness and usability. In addition to the implementation, the project also aims to expand the scope of the mapped-averaging method to encompass properties and substances to which it was not previously applied. This project enables mapped averaging methods to be employed on several widely-used molecular simulation packages: viz, LAMMPS, HOOMD, Cassandra, and VASP, which altogether have a base encompassing thousands of users. Software elements implemented in this project are in many cases completely transparent to the users of these packages, and can be employed by them with no added complication, to speed up their calculations by orders of magnitude. Thus the efforts made in this project will produce an enabling technology, giving scientists and engineers new capabilities to formulate materials for practical applications. Development tools and scripts are constructed in this project, which will facilitate the extension of mapped-averaging methods by other developers to even more molecular simulation packages, material properties, and molecular model systems. Software developed for this project is distributed open-source. Knowledge developed in this project is consolidated to form course materials made available on the web, and used as part of a large component of a graduate molecular simulation course taught by the PI. Training of 1 PhD student and numerous MS and undergraduates occurs across the project period. A strong dissemination effort involving papers, documentation, presentations, and workshops ensure that these methods and tools are understood and adopted by the community. Finally, instructional, graphically-oriented molecular simulation modules are developed and made available on the web to convey concepts related to harmonic and anharmonic components of crystalline behavior, with unique capabilities made possible by the mapped averaging framework.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740282","SI2-SSE: Collaborative Research: Integrated Tools for DNA Nanostructure Design and Simulation","OAC","Software Institutes, CDS&E, DMREF","09/01/2017","08/29/2017","Shawn Douglas","CA","University of California-San Francisco","Standard Grant","Rob Beverly","08/31/2020","$250,000.00","","Shawn.Douglas@ucsf.edu","1855 FOLSOM ST STE 425","SAN FRANCISCO","CA","941434249","4154762977","CSE","8004, 8084, 8292","026Z, 067E, 084E, 7433, 8004, 8005, 9263","$0.00","Nanotechnology could one day revolutionize several activities of great importance to our national interest, including how we manufacture consumer products, how we diagnose and treat disease, and how we detect and neutralize threats to our defense. One promising approach to atomically precise construction is adapting molecular building blocks from living organisms such as DNA, RNA, and proteins, and repurposing them to self-assemble into prescribed shapes, devices, and materials. A key bottleneck to progress is the complexity of designing, building, and testing nanostructures comprised of thousands or millions of atoms. The goal of this project is to accelerate development of bio-inspired nanostructures by integrating two widely adopted software tools used in bio-nanostructure design and physics-based molecular simulation. The products of this effort will enhance our fundamental capability to understand and precisely engineer self-assembled biomolecular nanostructures, which, when coupled with experimental validation in the laboratory, will enable future demand-meeting applications of bionanotechnology.<br/><br/>Toward realizing the goal of programming matter with nanoscale precision, this project will develop software interfaces between two classes of molecular design programs that, until now, have been evolving independently from one another. A widely adopted DNA structure design program, Cadnano, will be extended to utilize the results of physics-based microscopic simulations, enabling an iterative structure design process. A leading molecular graphics program, VMD (Visual Molecular Dynamics), will be developed to seamlessly visualize Cadnano designs, provide their structural interpretation, and enable further modification of the structures using an arsenal of computational structural biology and nanotechnology tools. Both developments will utilize recent advances in cloud computing technologies, making the DNA structure design software available anywhere and to anyone in a platform-independent manner.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Civil, Mechanical and Manufacturing Innovation in the Directorate of Engineering."
"1739491","SI2-SSE:GeoVisuals Software: Capturing, Managing, and Utilizing GeoSpatial Multimedia Data for Collaborative Field Research","OAC","Methodology, Measuremt & Stats, Software Institutes","09/01/2017","08/31/2020","Ye Zhao","OH","Kent State University","Standard Grant","Seung-Jong Park","08/31/2021","$500,000.00","Andrew Curtis, Xiang Lian, Xinyue Ye","zhao@cs.kent.edu","1500 HORNING RD","KENT","OH","442420001","3306722070","CSE","1333, 8004","7433, 8004, 8005","$0.00","Videos, photos, and narratives (audio, text, graphics) enriched with geospatial coordinates can be used to capture spatial data and associated contextual information for a variety of challenging environments. These data can fill gaps where no data exists, capture ephemeral or constantly changing information, and provide contextual insights that elevate local knowledge beyond more traditional methods. Example applications might include how a slum in Haiti changes after a vaccination intervention following a cholera outbreak, or identifying patterns of homeless camps (and their temporal stability) in Los Angeles. The GeoVisuals software system and a Web-based GeoVisuals Data Repository bridge the gap between the diversity of researcher needs and data infrastructure challenges often encountered ""in the field"". The computing infrastructures help domain researchers and decision-makers capture, manage, query and visualize such big, dynamic data to conduct exploratory and analytical tasks. The utilization of the tools spans across disciplines as diverse as anthropology, clinical health, criminology, disaster management, epidemiology, geography, planning, and sociology. GeoVisuals is being developed with collaboration from many domain researchers in academia, various branches of government, and non-profit sectors, and funded by the NSF Office of Advanced Cyberinfrastructure and the Directorate of Social, Behavioral and Economic Sciences (SBE). It is shared to users as public-licensed software with free access.<br/><br/>The GeoVisuals software consists of a mobile data collection module, a data transmission module, a data storage and computing module, and a visual analytics module. The software helps users to: (1) Capture and transfer geospatial multimedia data in a variety of different field settings ranging from developed countries with advanced IT infrastructures, to countries that still lack a reliable access to the Internet; (2) Manage and explore the collected data, when the data scale and dimensions impose technical challenges for non-IT professionals. It allows for the easy merging and query of GPS, video, audio, narratives, and so on - with flexibility for multiple input types; (3) Apply qualitative, quantitative, and spatial data analysis with mining algorithms, visual representations, and interactions. This robust and easy-to-use software enables the users to conduct information foraging and sense making on geospatial multimedia data. The GeoVisuals Data Repository allows field researchers to share their own data and access the analytical and visualization functions through a web-based platform, being inspired by the open access GenBank for biomedical researchers. The software and the data repository advance the research activities and grant applications of worldwide researchers and educators related to field studies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1450338","Collaborative Research: SI2-SSI: Landlab: A Flexible, Open-Source Modeling Framework for Earth-Surface Dynamics","OAC","Geomorphology & Land-use Dynam, Software Institutes, EarthCube","08/01/2015","07/14/2015","Nicole Gasparini","LA","Tulane University","Standard Grant","Seung-Jong Park","07/31/2020","$532,320.00","","ngaspari@tulane.edu","6823 SAINT CHARLES AVE","NEW ORLEANS","LA","701185665","5048654000","CSE","7458, 8004, 8074","7433, 8009, 9150","$0.00","Earth scientists discover and predict the behavior of the natural world in part through the use of computer models. Models are used to study a wide range of phenomena, such as soil erosion, flooding, plant growth, landslide occurrence, and many other processes. Models can be used to develop scientific understanding by comparing model calculations with the real world. They can also be used to make predictions about how nature will behave under certain conditions. In the areas of geosciences that deal with the earth's surface, one bottleneck in the development and use of models is the effort required to build, test, and debug the necessary software. This project contributes to scientific research and discovery by creating open source software tools that scientists can use to more efficiently create, modify, or combine computer models that represent portions of earth's surface and the processes occurring thereon. The project combines software development with user training and web-based resources, so that the products will be openly accessible, well documented, and widely disseminated within the relevant scientific communities. The project also contributes to K-12, undergraduate, and graduate-level education in computational modeling and earth-surface processes.<br/><br/>This project catalyzes research in earth-surface dynamics by developing a software framework that enables rapid creation, refinement, and reuse of two-dimensional (2D) numerical models. The phrase earth-surface dynamics refers to a remarkably diverse group of science and engineering fields that deal with our planet's surface and near-surface environment: its processes, its management, and its responses to natural and human-made perturbations. Scientists who want to use an earth-surface model often build their own unique model from the ground up, re-coding the basic building blocks of their model rather than taking advantage of codes that have already been written. Whereas the end result may be novel software programs, many person-hours are lost rewriting existing code, and the resulting software is often idiosyncratic, poorly documented, and unable to interact with other software programs in the same scientific community and beyond, leading to lost opportunities for exploring an even wider array of scientific questions than those that can be addressed using a single model. The Landlab model framework seeks to eliminate these redundancies and lost opportunities, and simultaneously lower the bar for entry into numerical modeling, by creating a user- and developer-friendly software library that provides scientists with the fundamental building blocks needed for modeling earth-surface dynamics. The framework takes advantage of the fact that nearly all surface-dynamics models share a set of common software elements, despite the wide range of processes and scales that they encompass. Providing these elements in the context of a popular scientific programming environment, with strong user support and community engagement, contributes to accelerating progress in the diverse sciences of the earth's surface.<br/><br/>The Landlab modeling framework is designed so that grid creation, data storage, and sharing of data among process components is done for the user. The framework is generic enough so that the coupling of two process components, whether they are squarely within a geoscience subdiscipline, or they cross subdisciplines, is the same. This architecture makes it easy to explore a wide range of questions in the geosciences without the need for much coding. Further, the model code is primarily written in Python, a language that is relatively easy for casual programmers to learn. Because of these attributes, the Landlab modeling framework has the potential to add computational modeling to the toolbox of a wide array of geoscientists, and to clear a path for trans-disciplinary earth-surface modeling that explores societally relevant topics, such as land-cover changes in response to climate change, as well as modeling of topics that currently require the coupling of sophisticated but harder-to-use models, such as sediment source-to-sink dynamics. The project will generate several proof-of-concept studies that are interesting in their own right, and demonstrate the capabilities of the modeling framework. Community engagement is fostered by presenting clinics and demonstrations at professional venues aimed at hydrologists, sedimentologists, critical zone scientists, and the broader earth and environmental sciences community. The team also supports visits by individual scientists for on-site training beyond these clinics. Input/output tools allow compatibility with data from relevant NSF-supported research. The project supports four graduate students and three postdoctoral researchers, and also includes modeling workshops for fifth to seventh grade girls in a community that has a greater than 50% minority population."
"1664190","Collaborative Research: SI2-SSI: Expanding Volunteer Computing","OAC","Software Institutes","05/15/2017","05/08/2017","David Anderson","CA","University of California-Berkeley","Standard Grant","Seung-Jong Park","09/30/2020","$999,999.00","","davea@ssl.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","CSE","8004","7433, 8004, 8009","$0.00","Volunteer computing (VC) uses donated computing time consumer devices such as home computers and smartphones to do scientific computing. It has been shown that VC can provide greater computing power, at lower cost, than conventional approaches such as organizational computing centers and commercial clouds. BOINC is the most common software framework for VC. Essentially, donors of computing time simply have to load BOINC on their computer or smartphone, and then register to donate at the BOINC web site. VC provides ""high throughput computing"": handling lots of independent jobs, with performance goals based on the rate of job completion rather than completion time for individual jobs. This type of computing (all known as high-throughput computing) is in great demand in most areas of science. Until now, the adoption of VC has been limited by its structure. For example, VC projects (such as Einstein@home and Rosetta@home) are operated by individual research groups, and volunteers must browse and choose from among many such projects. As a result, there are relatively few VC projects, and volunteers are mostly tech-savvy computer enthusiasts.  This project aims to solve these problems using two complementary development efforts: First, it will add BOINC-based VC conduits to two major high-performance computing providers: (a) the Texas Advanced Computing Center, a supercomputer center, and (b) nanoHUB, a web portal for nano science that provides computing capabilities.Also, a unified control interface to VC will be developed, tentatively called Science United, where donors can register.  The project  will benefit thousands of scientists who use these facilities, and it will create technology that makes it easy for other HPC providers to add their own VC back ends. Also, Science United will provide a simpler interface to BOINC volunteers where they will register to support scientific areas, rather than specific projects. Science United will also serve as an allocator of computing power among projects. Thus, new projects will no longer have to do their own marketing and publicity to recruit volunteers. Finally, the creation of a single VC ""brand"" (i.e Science United) will allow coherent marketing of VC to the public. By creating a huge pool of low-cost computing power that will benefit thousands of scientists, and increasing public awareness of and interest in science, the project plans to establish VC as a central and long-term part of the U.S. scientific cyber infrastructure.<br/><br/>Adding VC to an existing HPC facility involves several technical issues, which will be addressed as follows: (1) Packaging science applications (which typically run on Linux cluster nodes) to run on home computers (mostly Windows, some Mac and Linux): the team is developing an approach using VirtualBox and Docker, in which the application and its environment (Linux distribution, libraries, executables) are represented as a set of layers comprising a Docker image, which is then run as a container within a Linux virtual machine on the volunteer device. This has numerous advantages: it reduces the work of packaging applications to near zero; it minimizes network traffic because a given Docker layer is downloaded to a host only once; and it provides a strong security sandbox so that volunteer computers are protected from buggy or malicious applications, (2) File management: Input and output files must be moved between existing private servers and public-facing servers that are accessible to the outside Internet. A file management system will be developed, based on Web RPCs, for this purpose. This system will use content-based naming so that a given file is transferred and stored only once. It also maintains job/file associations so that files can be automatically deleted from the public server when they are no longer needed. (3) Submitting and monitoring jobs:  BOINC provides a web interface for efficiently submitting and monitoring large batches of jobs. These were originally developed as part of a system to migrate HTCondor jobs to BOINC. This project is extending it to support the additional requirements of TACC and nanoHUB. Note that these new capabilities are not specific to TACC or nanoHUB: they provide the glue needed to easily add BOINC-based VC to any existing HTC facility. The team is also developing RPC bindings in several languages (Python, C++, PHP). The other component of the project, Science United, is a database-driven web site and an associated web service for the BOINC clients. Science United will control volunteer hosts (i.e. tell them which projects to work for) using BOINC's ""Account Manager"" mechanism, in which the BOINC client on each host periodically contacts Science United and is told what projects to run. Project servers, not Science United, will distribute jobs and files. Science United will define a set of ""keywords"" for science areas (physics, biomedicine, environment, etc.) and for location (country, institution). Projects will be labelled with appropriate keywords. Volunteers will have a yes/no/maybe interface for specifying the types of jobs they want to run. Science United will thus provide a mechanism in which a fraction of total computing capacity can be allocated to a project for a given period. Because total capacity changes slowly over time, this allows near-certain guaranteed allocations. Science United will embody a scheduling system that attempts to enforce allocations, honor volunteer preferences, and maximize throughput. Finally, Science United will do detailed accounting of computing. Volunteer hosts will tell Science United how much work (measured by CPU time and FLOPs, GPU time and FLOPs, and number of jobs) they have done for each project. Science United will maintain historical records of this data for volunteers and projects, and current totals with finer granularity (e.g. for each host/project combination). Finally, Science United will provide web interfaces letting volunteers see their contribution status and history, and letting administrators add projects, control allocations, and view accounting data."
"1642433","SI2-SSE:  Automated Statistical Mechanics for the First-Principles Prediction of Finite Temperature Properties in Hybrid Organic-Inorganic Crystals","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2016","09/06/2016","Anton Van der Ven","CA","University of California-Santa Barbara","Standard Grant","Amy Walton","09/30/2020","$402,095.00","","avdv@engineering.ucsb.edu","3227 CHEADLE HALL","SANTA BARBARA","CA","931060001","8058934188","CSE","1712, 8004","7433, 8004, 8005, 8396, 8400, 8607, 9215, 9216","$0.00","This project seeks to advance computational capabilities in materials science by developing new theoretical and computational tools to predict temperature dependent properties of complex crystalline materials containing organic molecules. The recent discovery that hybrid organic-inorganic compounds can achieve remarkable photovoltaic conversion efficiencies has led to the recognition that a fundamental understanding of these complex compounds is urgently needed and that first-principles computational tools are necessary to enable a prediction of their intrinsic materials properties. The room temperature properties of hybrid organic-inorganic compounds are strongly affected by thermal excitations. Important electronic, thermodynamic and kinetic properties of these compounds therefore cannot be predicted directly with quantum mechanical approaches alone, but require statistical mechanics tools that account for the effects of temperature. A major objective of this project is the development of highly automated statistical mechanics software tools to predict materials properties where disorder due to alloying, atomic vibrations and molecular rotations are rigorously accounted for. These tools will greatly enhance the ability to predict the properties of complex materials from first principles, thereby enabling the directed design of a broad class of new materials with applications in a wide variety of technologies, including energy conversion and storage, carbon capture and organic electronics. The fundamental scientific insights to be generated by this study on hybrid organic/inorganic compounds will lead to invaluable design principles to enable the further improvement of these compounds for photovoltaic applications. The proposed activity will also educate and train graduate students in computational materials science, a field that is increasingly recognized as invaluable in the design and rapid implementation of new materials.<br/><br/>Modern first-principles electronic structure methods have reached a remarkable level of accuracy and ease of use, making them invaluable tools in the design of new materials. Electronic structure methods by themselves, however, do not explicitly account for the role of temperature on thermodynamic and kinetic properties. The properties of many promising materials for energy storage and conversion applications and for transportation applications depend sensitively on temperature due to large entropic contributions arising from atomic-scale excitations and disorder. Most materials of technological relevance are characterized by configurational disorder due to alloying and many high temperature phases are dynamically stabilized by large anharmonic vibrational excitations. Entropic contributions to equilibrium and non-equilibrium properties are especially important in a new class of hybrid organic-inorganic perovskites that show great promise as photovoltaic materials. These compounds belong to a class of crystalline materials that can host molecular species in large interstitial cages and exhibit a wide range of atomic and molecular excitations already at room temperature. Optimal photovoltaic properties are achieved by alloying on all three sublattices of the ABX3 perovskite crystal, leading to configurational disorder in addition to molecular and vibrational excitations. A statistical mechanics approach is therefore essential to accurately predict the electronic, thermodynamic and kinetic properties of these materials. The aim of this project is to develop a statistical mechanics framework and an accompanying highly automated software infrastructure that rigorously accounts for all relevant configurational, vibrational and molecular degrees of freedom in crystalline solids containing interstitial molecular species. The prediction of finite temperature thermodynamic and kinetic properties will rely on effective Hamiltonians that serve to extrapolate highly accurate first-principles electronic structure calculations within Monte Carlo simulations. A major activity of the project is the creation of a highly automated statistical mechanics software package called a Clusters Approach to Statistical Mechanics (CASM) to predict the finite temperature properties of multicomponent crystalline materials from first principles. The application of these tools in a first-principles study of alloyed hybrid organic-inorganic perovskites will generate a fundamental scientific understanding of the relative importance of the various atomic and molecular excitations on electronic structure, phase stability and ionic transport properties."
"1607042","SI2-SSI: Collaborative Research:  Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS, CDS&E","08/01/2015","01/28/2016","Matthew Knepley","TX","William Marsh Rice University","Standard Grant","Amy Walton","10/31/2018","$262,655.00","","knepley@buffalo.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","1253, 8004, 8069, 8084","7433, 8004, 8009, 8084","$0.00","Many biological and biomedical systems involve the interaction of a flexible structure and a fluid. These systems range from the writhing and coiling of DNA, to the beating and pumping of cilia and flagella, to the flow of blood in the body, to the locomotion of fish, insects, and birds. This project aims to develop advanced software infrastructure for performing dynamic computer simulations of such biological and biomedical systems. To facilitate the deployment of this software in a range of scientific and engineering applications, this project will develop new software capabilities in concert with new computer models that use the software. Specific application domains to be advanced in this project include models of aquatic locomotion that can be used to understand the neural control of movement and ultimately to develop new treatments for neurological pathologies such as spinal cord injuries, and models that simulate the interaction between the electrophysiology of the heart and the contractions of the heart that pump blood throughout the body, which could lead to improved approaches to treating heart disease. The software to be developed within the project is freely available online and is used by a number of independent research groups in a variety of scientific and engineering domains. It is being actively used in projects that model different aspects of cardiovascular dynamics, such as platelet aggregation and the dynamics of natural and prosthetic heart valves, and in projects that study other biological problems, including cancer dynamics, insect flight, aquatic locomotion, and the dynamics of phytoplankton. The software is also being applied to non-biological problems, including nanoscale models of colloidal suspensions and models of active particles. The improved methods and software to be developed in this project will thereby have a broad and sustained impact on a large number of ongoing research efforts in the biological and biomedical sciences and other scientific and engineering disciplines.<br/><br/>The immersed boundary (IB) method is a broadly applicable framework for modeling and simulating fluid-structure interaction (FSI). The IB method was introduced to model the fluid dynamics of heart valves, and subsequent development initially focused on simulating cardiac fluid dynamics. This methodology is broadly useful, however, and has been applied to a variety of problems in which a fluid flow interacts with immersed structures, including elastic bodies, bodies with known or prescribed deformational kinematics, and rigid bodies. Extensions of the IB method have also been developed to model electrophysiological systems and systems with chemically active structures. To improve the efficiency of the IB method, the PI has developed adaptive versions of the IB method that employ structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where needed. These methods have been implemented within the IBAMR software framework, which provides parallel implementations of the IB method and its extensions that leverage high-quality computational libraries including SAMRAI, PETSc, and libMesh. This project will further extend the IBAMR software by implementing modeling and discretization technologies required by the research applications of current and prospective users of the software, by developing improved solver infrastructure facilitated by the implementation of native support for structured AMR discretizations in the PETSc library, and by integrating with existing high-quality software tools for model development, deployment, and analysis. IBAMR is freely distributed online and is used within a number of independent research groups both to the further development of the IB method and also to its application to simulate diverse problems in fluid dynamics and FSI. By enhancing IBAMR, this project will also enhance the ability of these and other researchers to construct detailed models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations. This project will also develop general-purpose support for AMR discretizations in PETSc, a software library with thousands of active users, ~400 downloads per month, and numerous applications. The work of this project will help to grow the IBAMR user community of students and researchers by developing UI tools for building models, running simulations, and analyzing results. Students will be actively engaged in all aspects of the project, including code, method, and model development."
"1550528","Collaborative Research: SI2-SSI: Swift/E: Integrating Parallel Scripted Workflow into the Scientific Software Ecosystem","OAC","Software Institutes","10/01/2016","09/13/2016","Joseph Baker","NJ","The College of New Jersey","Standard Grant","Amy Walton","09/30/2020","$68,009.00","","bakerj@tcnj.edu","2000 PENNINGTON RD","EWING","NJ","086281104","6097713255","CSE","8004","7433, 8004, 8009, 9102","$0.00","Science and engineering research increasingly relies on repeated execution of a complex series of steps (i.e., workflows) to form hypotheses; conduct experiments; analyze results; and refine theory.   Computation is often essential throughout the workflow and in this case, software can improve productivity by managing the computational and data workflow.  Swift is one such open-source workflow system that has been developed and widely used in diverse areas ranging from materials simulations and climate modeling to neuroscience and genomics. This project extends the capabilities of Swift by integrating it with other software systems that enable collaboration, usability, maintainability, and productivity. The new ecosystem, Swift/E, will enable scientists and engineers to more productively create and run computational workflow campaigns of larger scale, and debug, execute, adapt, and disseminate them faster and easier than has been possible to date. These workflows embody and communicate the computational methods specific to each domain of scientific inquiry. Swift/E achieves community engagement and extensive productivity benefits for a large user community through an integrated program of research, education, and software dissemination. The project engages and serves science and engineering communities by creating patterns of practice for building and sharing reusable workflow libraries, and by training students, educators, and researchers in their use.  To advance the education of the next generation of computationally trained scientists, Swift/E powers a network of NSF-supported ""e-Labs"" that teach the concepts of collaborative parallel computational science at high school and undergraduate levels, reaching over a thousand students annually.<br/><br/>The open-source Swift/E ""ecosystem"" integrates Swift with several scientific software elements that play a major role in the national and global cyberinfrastructure of today. These elements are: Swift for the parallel scripting of scientific workflow; Globus for data cataloging, management, and high-speed wide-area transport; the Web-based Galaxy workflow portal for workflow composition, execution, and collaborative sharing; Jupyter for the interactive development, testing, debugging, and assembly of high level programming and workflow languages; Python and R for productively expressing high-level computational logic; and ""git"" and related tools and Web portals for revision control, code dissemination and sharing, and for the collaborative engagement of developers.  Swift's implicitly parallel programming language is minimal and compact.  Swift provides a facility for embedding other scripting languages (currently Python, R, Julia and Tcl) into its runtime environment.  This project merges newer extreme-scale ""Swift/T"" capabilities with the flexible and portable original ""Swift/K"" version to make the core Swift/E software element more powerful and flexible while lowering it?s ongoing support cost. Swift/E enhances usability by extending Swift's troubleshooting and inter-language integration facilities.  And with enhanced and innovative workflow sharing archives, new training materials, and a sustained program for user support and self-sustaining and expanding community engagement, the Swift/E project engages, supports, and sustains a large global science and engineering user base."
"1550525","SI2-SSI: Collaborative Research: ParaTreet: Parallel Software for Spatial Trees in Simulation and Analysis","OAC","Software Institutes","09/01/2016","08/19/2016","Milind Kulkarni","IN","Purdue University","Standard Grant","Rob Beverly","08/31/2017","$54,297.00","","milind@purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","CSE","8004","7433, 8004, 8009","$0.00","Many scientific and visualization methods involve organizing the data they are processing into a hierarchy (also known as a ""tree"").   These applications and methods include: astronomical simulations of particles moving under the influence of gravity, analysis of spatial data (that is, data that describes objects with respect to their relative position in space), photorealistic rendering of virtual environments,reconstruction of surfaces from laser scans, collision detection when simulating the movement of physical objects, and many others.   Tree data structures, and the algorithms used to work on these structures, are heavily used in these applications because they help to make these applications run much faster on supercomputers. However, implementing tree-based algorithms can require a significant effort, particularly on modern highly parallel computers.  This project will create ParaTreet, a software toolkit for parallel trees, that will enable rapid development of such applications.  Details of the parallel aspects will be hidden from the programmer, who will be able to quickly evaluate the relative merits of different trees and algorithms even when applied to large datasets and very computation-intensive applications. The combination of such an abstract and extensible framework with a portable adaptive runtime system will allow scientists to effectively use parallel hardware ranging from small clusters to petascale-class machines, for a wide variety of tree-based applications. This project will demonstrate the feasibility of such an approach as well as generate evidence of community adoption of this technology. If successful, this project will enable NSF-supported researchers to solve science problems faster as well as to tackle more complex problems, thus serving NSF's science mission.<br/><br/><br/>This project builds upon an existing collaboration on Computational Astronomy and the resultant software base in the ChaNGa (Charm N-body GrAvity solver) code. ChaNGa is a software package that performs collisionless N-body simulations, and can perform cosmological simulations with periodic boundary conditions in co-moving coordinates or simulations of isolated stellar systems. This project will extend ChaNGa with a parallel tree toolkit called ParaTreet and associated applications, that will allow scientists to effectively utilize small clusters as well as very large supercomputers for parallel tree-based calculations.  The key data structure in ParaTreet is an asynchronous software-based tree data cache, which maintains a writeback local copy of remote tree data. We plan to support a variety of spatial decomposition methods and the associated trees, including Oct-trees, KD-trees, inside-outside trees, ball trees, R-trees, and their combinations. Different trees are useful in different application circumstances, and the software will allow their relative merits to be evaluated with relative ease. The framework will support a variety of parallel work decomposition methods, including those based on space filling curves, and support dynamic rearrangement of parallel work at runtime. The algorithms supported will range from Barnes-Hut with various multipole expansions, data clustering, collision detection, surface reconstruction, ray intersection, etc. The software includes a collection of dynamic load balancing strategies in the Charm++ framework that can be tuned for specific problem structures. It also includes support for clusters of accelerators, such as GPGPUs. This project will demonstrate the feasibility of such an approach as well as generate evidence of community adoption of this technology."
"1534965","SI2-SSE: Quantum Monte Carlo Software for a Broad Electronic Structure Research Community via Minimal Explicit Dependency (MED) programming","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes, DMREF","10/01/2015","08/25/2015","Cyrus Umrigar","NY","Cornell University","Standard Grant","Rob Beverly","09/30/2018","$420,798.00","","cyrusumrigar@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","CSE","1253, 1712, 8004, 8292","7237, 7433, 7569, 8005, 8400, 9215, 9216","$0.00","This project, jointly supported by the Advanced Cyberinfrastructure (ACI) Division, Division of Materials Research (DMR), and Division of Chemistry (CHE) at NSF, will provide a robust, usable software infrastructure for methods for materials analysis, which, when run on high-performance computers, have the potential to revolutionize and accelerate the discovery of materials for a variety of engineering applications ranging from better catalysts, to solar cells to thermoelectrics used for refrigeration.  The big hurdle to doing this is that these methods for solving the fundamental equation of quantum mechanics, known as the Schroedinger equation, are either not sufficiently accurate or are too computationally expensive.  The purpose of this proposal is to develop software for a class of stochastic techniques, called quantum Monte Carlo methods, that will allow scientists and engineers to solve the equation more accurately and thereby predict materials properties with greater accuracy.  The software uses a new programming paradigm to make the use as well as the extension of the software by developers easier.  If researchers find this programming paradigm useful, it may propagate to a much larger research community and may eventually become an integral part of a new programming language. Thus this project will contribute to the basic sciences and potentially contribute to the field of computer science as well.<br/><br/>The goal of this proposal is to develop a software package consisting of a variety of quantum Monte Carlo (QMC) analysis tools. The software will have the capability to compute static and dynamic response properties, thereby greatly enhancing the range of applicability of QMC methods.  Past research on optimizing many-body wave functions will aid in this goal since some of the formalism is the same.  Both real space QMC methods, namely variational Monte Carlo (VMC) and diffusion Monte Carlo (DMC), and, determinant space QMC methods, namely semistochastic QMC (SQMC), which is an extension of the full configuration interaction QMC (FCIQMC) method, will be included, since each of these methods is the method of choice for some set of applications.  The software uses the Minimal Explicit Dependency (MED) programming paradigm that greatly facilitates the development of complex programs. In particular, it lowers the barrier for researchers, who are not familiar with the inner workings of the program to contribute new functionality, thereby contributing to the long term survival of the software."
"1339835","SI2-SSE: E-SDMS: Energy Simulation Data Management System Software","OAC","Software Institutes, CDS&E","10/01/2013","09/11/2013","K. Selcuk Candan","AZ","Arizona State University","Standard Grant","Alan Sussman","09/30/2018","$499,699.00","Maria Luisa Sapino","candan@asu.edu","660 S MILL AVE STE 312","TEMPE","AZ","852813670","4809655479","CSE","8004, 8084","7433, 8005","$0.00","The building sector was responsible for nearly half of CO2 emissions in US in 2009. According to the US Energy Information Administration, buildings consume more energy than any other sector, with 48.7% of the overall energy consumption, and building energy consumption is projected to grow faster than the consumptions of industry and transportation sectors.  As a response to this, by 2030 only 18% of the US building stock is expected to be relying on the current energy management technologies, with the rest either having been retrofitted or designed from the ground up using smart and cleaner energy technologies. These building energy management systems (BEMSs) need to integrate large volumes of data, including (a) continuously collected heating, ventilation, and air conditioning (HVAC) sensor and actuation data, (b) other sensory data, such as occupancy, humidity, lighting levels, air speed and quality, (c) architectural, mechanical, and building automation system configuration data for these buildings, (d) local whether and GIS data that provide contextual information, as well as (e) energy price, consumption, and cost data from electricity (such as smart grid) and gas utilities.  In theory, these data can be leveraged from the initial design and/or retrofitting of buildings with data driven building optimization (including the evaluation of the building location, orientation, and alternative energy-saving strategies) to total cost of ownership (TCOs) simulation tools and day-to-day operation decisions. In practice, however, because of the size and complexity of the data, the varying spatial and temporal scales at which the key processes operate, (a) creating models to support such simulations, (b) executing simulations that involve 100s of inter-dependent parameters spanning multiple spatio-temporal frames, affected by complex dynamic processes operating at different resolutions, and (c) analyzing simulation results are extremely costly.  The energy simulation data management system (e-SDMS) software will address challenges that arise from the need to model, index, search, visualize, and analyze, in a scalable manner, large volumes of multi-variate series resulting from observations and simulations. e-SDMS will, therefore, fill an important hole in data-driven building design and clean-energy (an area of national priority) and will enable applications and services with significant economic and environmental impact.<br/><br/>The key observations driving the research is that many data sets of urgent interest to energy simulations include the following: (a) voluminous, (b) heterogeneous, (c) multi-variate, (d) temporal, (e) inter-related (meaning that the parameters of interest are dependent on each other and constrained with the structure of the building), and (f) multi-resolution (meaning that simulations and observations cover days to months of data and may be considered at different granularities of space, time, and parameters). Moreover, generating an appropriate ensemble of simulations for decision making often requires multiple simulations, each with different parameters settings corresponding to slightly different, but plausible, scenarios. Therefore, significant savings in modeling and analysis can be obtained through data management software supporting modular re-use of existing simulation results in new settings, such as re-contextualization and modular recomposition (or ""sketching"") of building models and if-then analysis of simulation traces under new parameters, new building floorplans, and new contexts.  In developing the energy simulation data management system (e-SDMS), the research addresses the key data challenges that render data-driven energy simulations, today, difficult. This requires (a) a novel building models, simulation traces, and sensor/actuation traces (BSS) data model to accommodate energy simulation data and models, (b) feature analysis and indexing of sensory data and simulation traces along with the corresponding building models, and (c) algorithms for analysis and exploration of simulation traces and re-contextualization of models for new building plans and contextual metadata. This research will therefore, impact computational challenges that arise from the need to model, analyze, index, visualize, search, and recompose, in a scalable manner, large volumes of multi-variate series resulting from energy observations and simulations.  E-SDMS consists of an (a) eViz server, which works as a frontend to e-SDMS, an (b) eDMS middleware for feature extraction, indexing, simulation analysis, and sketching, and an (c) eStore backend for data storage.  To avoid waste and achieve scalabilities needed for managing large data sets, e-SDMS employs novel multi-resolution data partitioning and resource allocation strategies. The multi-resolution data encoding, partitioning, and analysis algorithms are efficiently computable, leverage massive parallelism, and result in high quality, compact data descriptions."
"1550229","SI2-SSI: Collaborative Research: ENKI: Software infrastructure that ENables Knowledge Integration for Modeling Coupled Geochemical and Geodynamical Processes","OAC","Software Institutes, EarthCube","09/01/2016","08/19/2016","Everett Shock","AZ","Arizona State University","Standard Grant","Stefan Robila","08/31/2019","$213,345.00","","eshock@asu.edu","660 S MILL AVE STE 312","TEMPE","AZ","852813670","4809655479","CSE","8004, 8074","7433, 8004, 8009","$0.00","Earth scientists seek to understand the mechanisms of planetary evolution from a process perspective in order to promote the progress of science.  They model the chemistry of melting of the interiors of planets as a result of heat flow within the body.  They calculate the flows of energy and mass from the interior to the surface. They model the interaction of fluids and rocks, which drives chemical weathering and the formation of ore deposits.  They seek to understand the synthesis and stabilities of organic compounds and their economic and biological roles.  They study the interactions of atmosphere, oceans, biosphere and land as a dynamically coupled evolving chemical system. To achieve this level of understanding of planetary evolution, Earth scientists use software tools that encode two fundamentally different types of models: (1) thermodynamic models of naturally occurring materials, and (2) models of transport that track physical flows of both fluids and solids.  Much of the fundamental science of planetary evolution lies in understanding coupled thermodynamic and transport models. This grant funds development of a software infrastructure that supports this coupled modeling of the chemical evolution of planetary bodies.   It is their aim to establish an essential and active community resource that will engage a large number of researchers, especially early career scientists, in the exercise of model building and customization.  <br/><br/>This is a project to create ENKI, a collaborative model configuration and testing portal that will transform research and education in the fields of geochemistry, petrology and geophysics.  ENKI will provide software tools in computational thermodynamics and fluid dynamics.  It will support development and access to thermochemical models of Earth materials, and establish a standard infrastructure of web services and libraries that permit these models to be integrated into fluid dynamical transport codes. This infrastructure will allow scientific questions to be answered by quantitative simulations that are presently difficult to impossible because of the lack of interoperable software frameworks.  ENKI, via the adoption of state-of-the-art model interfacing (OpenMI) and deployment environments (HubZero), will modernize how thermodynamic and fluid dynamic models are used by the Earth science community in five fundamental ways: (1) provenance tracking will enable automatic documentation of model development and execution workflows, (2) new tools will assist users in updating thermochemical models as new data become available, with the ability to merge these data and models into existing repositories and frameworks, (3) automated code generation will eliminate the need for users to manually code web services and library modules, (4) visualization tools and standard test suites will facilitate validation of model outcomes against observational data, (5) collaborative groups will be able to share and archive models and modeling workflows with associated provenance for publication. With these tools we seek to transform the large community of model users, who currently depend on a small group of dedicated and experienced researchers for model development and maintenance, into an empowered ensemble of model developers who take ownership of the process and bring their own expertise, intuition and perspective to shaping the software tools they use in daily research.  ENKI development will be community driven.  Participation of a dedicated and diverse group of early career professionals will guide us in user interface development - insuring portal capabilities are responsive to user needs, and in development of a rich set of documentation, tutorials and examples.  All software associated with this project will be released as open source."
"1535031","SI2-SSE: Collaborative Research: TrajAnalytics: A Cloud-Based Visual Analytics Software System to Advance Transportation Studies Using Emerging Urban Trajectory Data","OAC","Software Institutes","09/01/2015","08/05/2015","Ye Zhao","OH","Kent State University","Standard Grant","Bogdan Mihaila","08/31/2019","$300,000.00","Xinyue Ye","zhao@cs.kent.edu","1500 HORNING RD","KENT","OH","442420001","3306722070","CSE","8004","7433, 8005","$0.00","Advanced technologies in sensing and computing have created urban trajectory datasets of humans and vehicles travelling over urban networks. Understanding and analyzing the large-scale, complex data reflecting city dynamics is of great importance to enhance both human lives and urban environments. Domain practitioners, researchers, and decision-makers need to manage, query and visualize such big and dynamics data to conduct both exploratory and analytical tasks. To support these tasks, this project develops a open source software tool, named TrajAnalytics, which integrates computational techniques of scalable trajectory database, intuitive and interactive visualization, and high-end computers, to explicitly facilitate interactive data analytics of the emerging trajectory data.<br/><br/>The software provides a new platform for researchers in transportation assessment and planning to directly study moving populations in urban spaces. Researchers in social, economic and behavior sciences can use the software to understand the complicated mechanisms of urban security, economic activities, behavior trends, and so on. Specifically, this software advances the research activities in the community of transportation studies. The software also acts as an outreach platform allowing government agencies to communicate more effectively with the public with the real-world dynamics of city traffic, vehicles, and networks. The integration of research and education prepares the next generation workforce, where students across multi-disciplines can benefit through their participation in the development and use of the software.<br/><br/>In order to lay the foundations for effective analysis of urban trajectory datasets, this software (1) facilitates easy access and unprecedented capability for researchers and analysts with a cloud-based storage and computing infrastructure; (2) develops a parallel graph based data model, named TrajGraph, where massive trajectories are managed on a large-scale graph created from urban networks which is naturally amenable for studying urban dynamics; and (3) provides a visualization interface, named TrajVis, which supports interactive visual analytics tasks with a set of visualization tools and interaction functions. A variety of transportation-related researchers from geography, civil engineering, computer science participate in the design, evaluation, and use of the software. This robust and easy-to-use software enables the users to conduct iterative, evolving information foraging and sense making."
"1047919","Collaborative Research SI2-SSE: Comprehensive Sustained Innovation in Acceleration of Molecular Dynamics Simulation and Analysis on Graphical Processing Units.","OAC","PROJECTS","10/01/2010","09/15/2010","Adrian Roitberg","FL","University of Florida","Standard Grant","Evelyn Goldfield","03/31/2012","$50,000.00","","roitberg@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","1978","1253, 1978, 9216, 9263","$0.00","This collaborative pilot project between the San Diego Supercomputer Center at the University of California San Diego, the Quantum Theory Project at the University of Florida and industrial partners NVIDIA Inc. is focused on developing innovative, comprehensive open source software element libraries for accelerating condensed phase Molecular Dynamics (MD) simulations of biomolecules using Graphics Processing Units (GPU). By porting MD techniques to GPUs this project is enabling users to both attain substantial increases in their own local calculations without the need for substantial investment in hardware or infrastructure, and to make effective use of GPU acceleration provided by new machines within the NSF supercomputing centers. The software elements being developed and distributed both within the AMBER MD package and as open source libraries are providing critical software infrastructure in support of transformative research in the fields of chemistry, life science, materials science, environmental and renewable energy research.<br/><br/>The software elements being created in this project have very broad impact. For example, the integration of single and multi-GPU acceleration within the AMBER software alone benefits a very large and established national and international user base. Over 8,000 downloads of the AMBER Tools package from unique IP addresses and more than 500 sites which use the AMBER MD engine testify to the scope of the community of researchers this work impacts. Additionally the open source GPU MD acceleration libraries being produced provide broad impact across multiple domains while outreach workshops are helping to train the next generation of scientists not just in the use and potential benefits of GPU MD acceleration libraries but also in modern MD simulation techniques.<br/><br/>This award is co-funded by the Office of Cyberinfrastructure, the Division of Chemistry and the Office of Multidisciplinary Activites of the Directorate of Mathematical Sciences."
"1339768","SI2-SSI: Collaborative Research: Building Sustainable Tools and Collaboration for Volcanic and Related Hazards","OAC","Software Institutes, EarthCube","10/01/2013","09/16/2013","Charles Connor","FL","University of South Florida","Standard Grant","Rajiv Ramnath","09/30/2017","$194,869.00","","cbconnor@usf.edu","4019 E. Fowler Avenue","Tampa","FL","336172008","8139742897","CSE","8004, 8074","7433, 8009","$0.00","This project is focused on creating and upgrading software infrastructure for a large community of scientists engaged in volcanology research and associated hazard analysis.  Specifically, the project will reengineer three widely used tools (TITAN2D - block and ash flows, TEPHRA and Puff - ash transport and dispersal) and develop support for workflows that use these tools to analyze risk from volcanic hazards. Reengineering will encompass modularization so researchers may easily experiment with different modeling approaches, incorporation of techniques to make the tools efficient on new computing architectures like GPUs and many-core chips. The workflows are intended to tackle the challenges of managing complex and often large data flows associated with these tools in validation processes and in probabilistic inference based on the outcomes of the modeling. The tools and workflows will be made available using the popular vhub.org platform. This project will help provide a standard well managed hardware/software platform and approaches to standardize the documentation associated with input data, source code, and output data. This will ensure that model calculations are reproducible.<br/><br/>The wider use of these high fidelity tools and their use in mitigating hazards is likely to have a significant effect on hazard analysis and management. The project will also engage in several major workshops and in training activities. Project personnel will also engage in the Earthcube initiative - popularizing computational methodologies, online access and dissemination mechanisms through the VHub platform."
"1339804","SI2-SSI: Collaborative Research: Scalable, Extensible, and Open Framework for Ground and Excited State Properties of Complex Systems","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, CYBERINFRASTRUCTURE, Software Institutes, CDS&E","10/01/2013","09/04/2014","Sohrab Ismail-Beigi","CT","Yale University","Continuing Grant","Bogdan Mihaila","09/30/2020","$1,380,247.00","","sohrab.ismail-beigi@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","1253, 1712, 7231, 8004, 8084","7433, 7569, 8009, 8084, 9216, 9263","$0.00","Computer simulation plays a central role in helping us understand, predict, and engineer the physical and chemical properties of technological materials systems such as semiconductor devices, photovoltaic systems, chemical reactions and catalytic behavior. Despite significant progress in performing realistic simulations in full microscopic detail, some problems are currently out of reach: two examples are the modeling of electronic devices with multiple functional parts based on new materials such as novel low power computer switches that would revolutionize the Information Technology industry, and the photovoltaic activity of complex interfaces between polymers and inorganic nanostructures that would enhance US energy self-reliance.  The research program of this collaborative software institute aims to create an open and effective scientific software package that can make efficient use of cutting-edge high performance computers (HPC) to solve challenging problems involving the physics and chemistry of materials.  By having such software available, this software initiative will have multiple broad impacts.  First, the community of materials scientists will be able to study next-generation problems in materials physics and chemistry, and computer science advances that enable the software will be demonstrated and made accessible for both communities which will help cross-fertilize further such collaborative efforts.  Second, the capability of simulating and engineering more complex materials systems and technological devices could play a role in helping the US continue is competitive edge in science, technology, and education. Third, through training of young scientists, direct outreach to the broader scientific community through workshops and conferences, and educational programs ranging from secondary to graduate levels, the power, importance, and capabilities of computational modeling, materials science, and computer science methodologies that enable the science will be communicated to a broad audience.  Finally, by enabling the refinement of existing materials systems as well as discovery of new materials systems, the resulting scientific advances can help broadly impact society via technological improvements: in terms of the two examples provided above, (a) the successful design of new electronic device paradigms helps significantly advance the digital revolution by permitting the introduction of smaller, more efficient, and more capable electronic circuits and information processing systems, and (b) successful creation of inexpensive, easy-to-fabricate, and durable photovoltaic materials and devices can lead to cleaner forms of energy production while reducing reliance on fossil fuels.<br/><br/>The technical goal is to greatly enhance the open software tool OPENATOM to advance discovery in nanoscience and technology. OPENATOM will be delivered as a open, robust and validated software package capable of utilizing HPC architectures efficiently to describe the electronic structure of complex materials systems from first principles.  In terms of describing electronic ground-states, OPENATOM will be enhanced by features such as improved configurational sampling methods, hybrid density functionals, and incorporation of fast super-soft pseudopotential techniques. In addition, the team will incorporate the many-body GW-BSE approach for electronic excitations that permits accurate computation of electronic energy levels, optical absorption and emission, and luminescence.  Ultimately, such an extensible software framework will permit accurate electronic structure computations to employ effectively future HPC platforms with 10,000,000 cores."
"1339772","SI2-SSI: SAGEnext: Next Generation Integrated Persistent Visualization and Collaboration Services for Global Cyberinfrastructure","OAC","Software Institutes","10/01/2013","09/16/2013","Jason Leigh","IL","University of Illinois at Chicago","Continuing Grant","Daniel Katz","06/30/2014","$1,828,077.00","Maxine Brown, Luc Renambot","leighj@hawaii.edu","809 S. Marshfield Avenue","Chicago","IL","606124305","3129962862","CSE","8004","7433, 8009","$0.00","Cyberinfrastructure runs the gamut - from computing, networks, data stores, instruments, observatories, and sensors, to software and application codes - and promises to enable research at unprecedented scales, complexity, resolution, and accuracy. However, it is the research community that must make sense of all the data being amassed, so the SAGEnext (Scalable Adaptive Graphics Environment) framework is an innovative user-centered platform that connects people to their data and colleagues, locally and remotely, via tiled display walls, creating a portal, or wide-aperture ""digital lens,"" with which to view their data and one another. It enables Cyber-Mashups, or the ability to juxtapose and integrate information from multiple sources in a variety of resolutions, as easily as the Web makes access to lower-resolution images today. <br/><br/>SAGEnext expands on a vibrant partnership among national and international universities, supercomputer centers, government laboratories and industry; 100 institutions worldwide use the current version of SAGE. For computational scientists, from such diverse fields as biology, earth science, genomics, or physics, SAGEnext will transform the way they manage the scale and complexity of their data. For computer scientists, SAGEnext is a platform for conducting research in human-computer interaction, cloud computing, and advanced networking. SAGEnext capabilities, integrating visualization application codes, cloud documents, stereo 3D, and new user-interaction paradigms, is unprecedented and heretofore not available, and will have a transformative effect on data exploration and collaboration, making cyberinfrastructure more accessible to end users, in both the laboratory and in the classroom. SAGEnext, integrated with advanced cyberinfrastructure tools, will transform the way today's scientists and future scientists manage the scale and complexity of their data, enabling them to more rapidly address problems of national priority - such as global climate change or homeland security - which benefits all mankind. These same tools can better communicate scientific concepts to public policy and government officials, and via museum exhibitions, to the general public."
"1339797","SI2-SSI: Collaborative Research: Sustained Innovation for Linear Algebra Software (SILAS)","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS","10/01/2013","06/29/2016","Julien Langou","CO","University of Colorado at Denver-Downtown Campus","Continuing Grant","Alan Sussman","09/30/2018","$392,492.00","Rodney James","julien.langou@ucdenver.edu","F428, AMC Bldg 500","Aurora","CO","800452571","3037240090","CSE","1253, 8004, 8069","7433, 8009, 9251","$0.00","As the era of computer architectures dominated by serial processors comes to a close, the convergence of several unprecedented changes in processor design has produced a broad consensus that much of the essential software infrastructure of computational science and engineering is utterly obsolete. Math libraries have historically been in the vanguard of software that must be quickly adapted to such design revolutions because they are the common, low-level software workhorses that do all the most basic mathematical calculations for many different types of applications. The Sustained Innovation for Linear Algebra Software (SILAS) project updates two of the most widely used numerical libraries in the history of Computational Science and Engineering---LAPACK and ScaLAPACK, (abbreviated Sca/LAPACK)---enhancing and hardening them for this ongoing revolution in processor architecture and system design. SILAS creates a layered package of software components, capable of running at every level of the platform deployment pyramid, from the desktop to the largest supercomputers in the world. It achieves three complementary objectives: 1) Wherever possible, SILAS delivers seamless access to the most up-to-date algorithms, numerical implementations, and performance, by way of Sca/LAPACK programming interfaces that are familiar to many computational scientists; 2) Wherever necessary, SILAS makes advanced algorithms, numerical implementations and performance capabilities available through new interface extensions; and 3) SILAS provides a well engineered conduit through which new discoveries at the frontiers of research in these areas can be channeled as quickly as possible to all the application communities that depend on high performance linear algebra. The improvements and innovations included in SILAS derive from a variety of sources. They represent the results (including designs and well tested prototypes) of the PIs' own algorithmic and software research agenda, which has targeted multicore, hybrid and extreme scale system architectures. They are an outcome of extensive and on-going interactions with users, vendors, and the management of large NSF and DOE supercomputing facilities. They flow from cross-disciplinary engagement with other areas of computer science and engineering, anticipating the demands and opportunities of new architectures and programming models. And finally, they come from the enthusiastic participation of the research community in developing and offering enhanced versions of existing Sca/LAPACK codes.<br/><br/>The primary impact of SILAS is a direct function of the importance of the Sca/LAPACK libraries to many branches of computational science. The Sca/LAPACK libraries are the community standard for dense linear algebra and have been adopted and/or supported by a large community of users, computing centers, and HPC vendors. Learning to use them is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. Application domains where Sca/LAPACK have historically been heavily used include (among a host of other examples) airplane wing design, radar cross-section studies, flow around ships and other off-shore constructions, diffusion of solid bodies in a liquid, noise reduction, and diffusion of light through small particles. Moreover, the list of application partners working with SILAS to enhance and transform these libraries for next generation platforms expands this traditional list to include quantum chemistry, adaptive mesh refinement schemes, computational materials science, geophysical flows, stochastic simulation and database research for ""big data"". No other numerical library can claim this breadth of integration with the community. Thus, there is every reason to believe that enhancing these libraries with state of the art methods and algorithms and adapting them for new and emerging platforms (reaching up to extreme scale), will have a correspondingly large impact on the research and education community, government laboratories, and private industry."
"1339690","Collaborative Research: SI2-SSE: UT Wrangler: Understanding the Software Needs of High End Computer Users","OAC","Software Institutes","10/01/2013","09/13/2013","Mark Fahey","TN","University of Tennessee Knoxville","Standard Grant","Rajiv Ramnath","04/30/2016","$259,931.00","","markrfahey@uchicago.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","8004","7433, 8005, 9150","$0.00","This research addresses two important questions: what software do researchers actually use on high-end computers, and how successful are they in their efforts to use it? It is a plan to improve our understanding of individual users' software needs, then leverage that understanding to help stakeholders conduct business in a more efficient, effective, and systematic way. The signature product, UTWrangler, builds on work that is already improving the user experience and enhancing support programs for thousands of users on twelve supercomputers across the United States and Europe.   For the first time, complete, accurate, detailed, and continuous ground truth information about software needs, trends, and issues at the level of the individual job are being delivered.<br/> <br/>UTWrangler will instrument, monitor, and analyze individual jobs on high-end computers to generate a picture of the compilers, libraries, and other software that users need to run their jobs successfully. It will highlight the products our researchers need and do not need, and alert users and support staff to the root causes of software configuration issues as soon as the problems occur. UTWrangler's prototypes prove its value and future impact: simplifying end users' workflows; improving support, training and documentation; saving money; and helping administrators prioritize maintenance of their large base of installed software.  UTWrangler will build on the capabilities of its prototypes, providing a robust, sustainable, second generation mechanism that will help the computational research community make the most effective use of limited computing cycles and labor hours.  And UTWrangler will mitigate the difficulties new users encounter, reporting configuration problems as soon as jobs begin, and identifying opportunities to improve documentation, education and outreach programs."
"1460032","SI2-SSE: Collaborative Proposal: Symbolic-Numeric Approaches to Polynomials","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS","08/16/2014","09/04/2014","Jonathan Hauenstein","IN","University of Notre Dame","Standard Grant","Alan Sussman","08/31/2018","$149,995.00","","hauenstein@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","1253, 8004, 8069","7433, 8005, 8251","$0.00","Polynomial systems arise naturally in many areas of human endeavor. These include the modeling of tumor growth; the design of robotic devices; chemical systems arising in areas ranging from combustion to blood clotting; assorted problems in physics; plus many areas with mathematics. The solution of the polynomial systems answers questions critical to these endeavors. This research will be devoted to developing the next generation of Bertini, an open source software package, which has been used successfully by many researchers on many problems, which include all those mentioned above.<br/><br/>Bertini will be rewritten in C++ to be scriptable and modular, which will allow it to be interfaced transparently with symbolic software. The new Bertini will include tools allowing the user to construct and manipulate homotopies based on the output of Bertini. A major focus of the research will be given to systems of polynomials arising from the discretization of systems of differential equations. The great challenge of these very large systems of polynomials is balanced by the great potential impact new efficient and robust methods of solution will have."
"1450412","Collaborative Research: SI2-SSI: Landlab: A Flexible, Open-Source Modeling Framework for Earth-Surface Dynamics","OAC","Geomorphology & Land-use Dynam, Software Institutes, EarthCube","08/01/2015","07/14/2015","Erkan Istanbulluoglu","WA","University of Washington","Standard Grant","Stefan Robila","07/31/2020","$676,836.00","","erkani@u.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7458, 8004, 8074","7433, 8009","$0.00","Earth scientists discover and predict the behavior of the natural world in part through the use of computer models. Models are used to study a wide range of phenomena, such as soil erosion, flooding, plant growth, landslide occurrence, and many other processes. Models can be used to develop scientific understanding by comparing model calculations with the real world. They can also be used to make predictions about how nature will behave under certain conditions. In the areas of geosciences that deal with the earth's surface, one bottleneck in the development and use of models is the effort required to build, test, and debug the necessary software. This project contributes to scientific research and discovery by creating open source software tools that scientists can use to more efficiently create, modify, or combine computer models that represent portions of earth's surface and the processes occurring thereon. The project combines software development with user training and web-based resources, so that the products will be openly accessible, well documented, and widely disseminated within the relevant scientific communities. The project also contributes to K-12, undergraduate, and graduate-level education in computational modeling and earth-surface processes.<br/><br/>This project catalyzes research in earth-surface dynamics by developing a software framework that enables rapid creation, refinement, and reuse of two-dimensional (2D) numerical models. The phrase earth-surface dynamics refers to a remarkably diverse group of science and engineering fields that deal with our planet's surface and near-surface environment: its processes, its management, and its responses to natural and human-made perturbations. Scientists who want to use an earth-surface model often build their own unique model from the ground up, re-coding the basic building blocks of their model rather than taking advantage of codes that have already been written. Whereas the end result may be novel software programs, many person-hours are lost rewriting existing code, and the resulting software is often idiosyncratic, poorly documented, and unable to interact with other software programs in the same scientific community and beyond, leading to lost opportunities for exploring an even wider array of scientific questions than those that can be addressed using a single model. The Landlab model framework seeks to eliminate these redundancies and lost opportunities, and simultaneously lower the bar for entry into numerical modeling, by creating a user- and developer-friendly software library that provides scientists with the fundamental building blocks needed for modeling earth-surface dynamics. The framework takes advantage of the fact that nearly all surface-dynamics models share a set of common software elements, despite the wide range of processes and scales that they encompass. Providing these elements in the context of a popular scientific programming environment, with strong user support and community engagement, contributes to accelerating progress in the diverse sciences of the earth's surface.<br/><br/>The Landlab modeling framework is designed so that grid creation, data storage, and sharing of data among process components is done for the user. The framework is generic enough so that the coupling of two process components, whether they are squarely within a geoscience subdiscipline, or they cross subdisciplines, is the same. This architecture makes it easy to explore a wide range of questions in the geosciences without the need for much coding. Further, the model code is primarily written in Python, a language that is relatively easy for casual programmers to learn. Because of these attributes, the Landlab modeling framework has the potential to add computational modeling to the toolbox of a wide array of geoscientists, and to clear a path for trans-disciplinary earth-surface modeling that explores societally relevant topics, such as land-cover changes in response to climate change, as well as modeling of topics that currently require the coupling of sophisticated but harder-to-use models, such as sediment source-to-sink dynamics. The project will generate several proof-of-concept studies that are interesting in their own right, and demonstrate the capabilities of the modeling framework. Community engagement is fostered by presenting clinics and demonstrations at professional venues aimed at hydrologists, sedimentologists, critical zone scientists, and the broader earth and environmental sciences community. The team also supports visits by individual scientists for on-site training beyond these clinics. Input/output tools allow compatibility with data from relevant NSF-supported research. The project supports four graduate students and three postdoctoral researchers, and also includes modeling workshops for fifth to seventh grade girls in a community that has a greater than 50% minority population."
"1450471","SI2-SSI: Collaborative Research: A Software Infrastructure for MPI Performance Engineering: Integrating MVAPICH and TAU via the MPI Tools Interface","OAC","Software Institutes","09/01/2015","08/31/2015","Sameer Shende","OR","University of Oregon Eugene","Standard Grant","Bogdan Mihaila","08/31/2020","$1,200,000.00","Allen Malony","sameer@cs.uoregon.edu","5219 UNIVERSITY OF OREGON","Eugene","OR","974035219","5413465131","CSE","8004","7433, 8009","$0.00","Message-Passing Interface (MPI) continues to dominate the supercomputing landscape, being the primary parallel programming model of choice. A large variety of scientific applications in use today are based on MPI.  On the current and next-generation High-End Computing (HEC) systems, it is essential to understand the interaction between time-critical applications and the underlying MPI implementations in order to better optimize them for both scalability and performance. Current users of HEC systems develop their applications with high-performance MPI implementations, but analyze and fine tune the behavior using standalone performance tools.  Essentially, each software component views the other as a blackbox, with little sharing of information or access to capabilities that might be useful in optimization strategies.  Lack of a standardized interface that allows interaction between the profiling tool and the MPI library has been a big impediment.  The newly introduced MPI_T interface in the MPI-3 standard provides a simple mechanism that allows MPI implementers to expose variables representing configuration parameters or performance measurements from within the implementation for the benefit of tools, tuning frameworks, and other support libraries.  However, few performance analysis and tuning tools take advantage of the MPI_T interface and none do so to dynamically optimize at execution time. This research and development effort aims to build a software infrastructure for MPI performance engineering using the new MPI_T interface.<br/><br/>With the adoption of MPI_T in the MPI standard, it is now possible to take positive steps to realize close interaction between and integration of MPI libraries and performance tools.  This research, undertaken by a team of computer scientists from OSU and UO representing the open source MVAPICH and TAU projects, aims to create an open source integrated software infrastructure built on the MPI_T interface which defines the API for interaction and information interchange to enable fine grained performance optimizations for HPC applications.  The challenges addressed by the project include: 1) enhancing existing support for MPI_T in MVAPICH to expose a richer set of performance and control variables; 2) redesigning TAU to take advantage of the new MPI_T variables exposed by MVAPICH; 3) extending and enhancing TAU and MVAPICH with the ability to generate recommendations and performance engineering reports; 4) proposing fundamental design changes to make MPI libraries like MVAPICH ``reconfigurable'' at runtime; and 5) adding support to MVAPICH and TAU for interactive performance engineering sessions.  The framework will be validated on a variety of HPC benchmarks and applications.  The integrated middleware and tools will be made publicly available to the community.  The research will have a significant impact on enabling optimizations of HPC applications that have previously been difficult to provide.  As a result, it will contribute to deriving ""best practice"" guidelines for running on next-generation Multi-Petaflop and Exascale systems.  The research directions and their solutions will be used in the curriculum of the PIs to train undergraduate and graduate students."
"1450088","Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","08/01/2015","06/24/2015","Lyudmila Slipchenko","IN","Purdue University","Standard Grant","Stefan Robila","07/31/2020","$600,000.00","","lslipchenko@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","1253, 8004","7433, 8009","$0.00","Computational chemistry has become ubiquitous throughout the fields of science and engineering. Among the many uses of computational chemistry codes are to aid in the design of new materials with specific properties, to understand protein folding dynamics related to the human body, to understand the mechanisms of enzyme catalysis to produce biofuels, and to identify and characterize gaseous species in the atmosphere. Because computational chemistry simulations can require large amounts of computer, memory and disk space, one focus of this research is to reduce the demands on computer resources by exploiting methods that can efficiently fragment large molecules into smaller pieces and at the same time take advantage of computers that have many thousands of computer cores. The principal investigators are developers of several computational chemistry programs, each of which has unique functionalities that have taken multiple person-years to develop and implement. In order to minimize further development efforts and to maximize the utility of the unique features, a second key focus of this research will be to develop the ability of each program to access the unique features and data of the other programs. A major bottleneck in the effort to construct high performance computers with more and more compute cores is the power that is required to drive such systems. One way that the research team will address the power issue is to explore the utility of low power architectures, such as graphical processing units for computational chemistry calculations. All of the newly developed codes will be made available to the user community by web download at no cost.<br/><br/>This project presents an integrated computational science approach to very high quality electronic structure and dynamics calculations that will (a) be broadly accessible to both the development and applications communities, (b) have the capability to address problems of great interest, such as the properties of liquids and solvent effects, and photochemical/photobiological dynamics with high accuracy, (c) provide interoperability and sustainability into the foreseeable future, and (d) solve bottlenecks including power consumption using accelerators. A primary goal is to provide to the broad community new, accurate approaches that may be easily used, with a clear path forward to further code improvement and development. As part of the proposed effort, in addition to the usual outlets of journal articles and public presentations, the investigators will organize workshops at prominent national meetings, so that the expertise and software developed will be available to as broad a group of users as possible.  All of the developed codes will be available on the web for easy downloads. The proposed research will develop new paradigms for interoperability, with a focus on the highly popular program suites GAMESS, NWChem, PSI4  and the AIMS dynamics code. Also included will be a cloud-based client-server model, a common quantum chemistry driver and novel data management approaches. The integration of the codes will be accomplished by making use of the combined expertise of the PIs in developing interoperable methods and data interfaces in computational science. In addition, several new methods will be developed, including novel explicit (R12) correlation methods that will be integrated with the most accurate levels of theory: multi-reference and the most accurate and novel coupled cluster methods, as well as the derivation and implementation of analytic derivatives. Applicability to large molecular systems will be made feasible by drawing upon novel fragmentation methods that scale nearly perfectly to the petascale."
"1440583","SI2-SSE: Collaborative Proposal: Symbolic-Numeric Approaches to Polynomials","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS","09/01/2014","08/01/2014","Jonathan Hauenstein","NC","North Carolina State University","Standard Grant","Daniel Katz","10/31/2014","$149,995.00","","hauenstein@nd.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","CSE","1253, 8004, 8069","7433, 8005, 8251","$0.00","Polynomial systems arise naturally in many areas of human endeavor. These include the modeling of tumor growth; the design of robotic devices; chemical systems arising in areas ranging from combustion to blood clotting; assorted problems in physics; plus many areas with mathematics. The solution of the polynomial systems answers questions critical to these endeavors. This research will be devoted to developing the next generation of Bertini, an open source software package, which has been used successfully by many researchers on many problems, which include all those mentioned above.<br/><br/>Bertini will be rewritten in C++ to be scriptable and modular, which will allow it to be interfaced transparently with symbolic software. The new Bertini will include tools allowing the user to construct and manipulate homotopies based on the output of Bertini. A major focus of the research will be given to systems of polynomials arising from the discretization of systems of differential equations. The great challenge of these very large systems of polynomials is balanced by the great potential impact new efficient and robust methods of solution will have."
"1440685","SI2-SSE Collaborative Research: Molecular Simulations of Polymer Nanostructures in the Cloud","OAC","DMR SHORT TERM SUPPORT, Software Institutes, CDS&E","10/01/2014","07/31/2014","Coray Colina","PA","Pennsylvania State Univ University Park","Standard Grant","Daniel Katz","12/31/2015","$195,000.00","","colina@chem.ufl.edu","201 Old Main","University Park","PA","168021503","8148651372","CSE","1712, 8004, 8084","024E, 085E, 7237, 7433, 8005, 8400, 9216","$0.00","The use of polymers, their composites, and nanostructures is growing at a fast pace, both by displacing traditional materials and by enabling emerging technologies. Examples range from the all-composite airframe of the Boeing 787 and the new Airbus 350 to wearable electronics. This project aims to develop a software infrastructure to simulate these materials with atomic resolution and make these tools universally accessible and useful via on-line computing. These simulations have the potential to accelerate the development of optimized material formulations that can benefit society and reduce the associated costs by combining physical with computational experiments. Making these advanced tools available for free online simulations and complementing them with tutorials and educational material will encourage their use in the classroom and will impact the education of new generations of engineers and scientists familiar with these powerful tools that will be required to address tomorrow's challenges.<br/><br/>The objective of this effort is to enable pervasive, reproducible molecular simulations of polymeric materials using state-of-the-art tools and with quantified uncertainties building on recent breakthroughs in molecular simulations, cyber-infrastructure and uncertainty quantification. The framework will consist of three main components: i) powerful simulation tools for polymer nano structures including: state-of-the-art molecular builders, a parallel MD engine with stencils that enable efficient structure relaxation and property characterization and post-processing codes; ii) a UQ framework to orchestrate the molecular simulations and propagate uncertainties in input parameters to predictions and compare the predictions to experimental values; iii) databases of force fields and molecular structures as well as predicted and experimental properties. The simulation framework will be deployed via NSF's nanoHUB where users will be able to run online simulations without downloading or installing any software while expert users will have the option to download, modify and contribute to the infrastructure. Usage of and contributions to the software framework will be facilitated and encouraged via online and in-person user guides, learning modules and research tutorials."
"1060067","Collaborative Research: SI2-SSI: Scalable Hierarchical Algorithms for Extreme Computing (SHARE)","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS","09/15/2010","08/04/2011","Lincoln Greenhill","MA","Harvard University","Continuing Grant","Bradley D. Keister","08/31/2015","$230,755.00","","greenhill@cfa.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","1253, 7244","1253, 7244","$0.00","This award supports the development of software tools for advanced algorithms on a cluster of high performance graphics processing units(GPUs).  The initial goal of this library will focus on a single driver application, the fundamental numerical study of Nuclear Forces (Lattice Quantum Chromodynamics: LQCD), and a second target application, the numerical simulation of the exciting nano-technology of graphene.  These are both multi-fermion problems well suited to solution via multi-scale algorithms on many-core architectures.  This pilot project draws on experience gained by the small team at Boston University and Harvard in developing Dirac solvers for lattice field theory.  Two building blocks from prior research are (1) the construction of an adaptive multigrid (MG) solver for the Wilson Dirac operator of LQCD, which demonstrates a 20x speedup compared to the best Krylov solvers in production code and (2) a highly optimized Krylov solver for the same operator on GPUs (but without multigrid), realizing a 10x improvement in price/performance over traditional clusters.  The library will unite these feature and generalize their domain of applicability.  <br/><br/>As an example of the broader impact, it is estimated that combining these two technologies (MG algorithms and GPU architectures) will yield a 100-fold improvement in price/performance for the most compute-intensive component of LQCD simulations.  Such an advance would be truly transformative, making an immediate impact in nuclear and particle physics.  At the same time, it will serve as a prototype of the more generic problem of mapping hierarchical algorithms onto heterogeneous architectures, a challenge of paramount importance on the path to the exascale.  The software library will be designed to bring similar benefits to graphene technology and to evolve to accommodate additional target application and additional domain decomposition algorithm to mitigate the communication bottleneck of Exascale designs.  The award will provide partial support for two postdoctoral scholars who play essential roles in this project."
"1060012","Collaborative Research: SI2-SSI: Scalable Hierarchical Algorithms for Extreme Computing (SHARE)","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","09/15/2010","06/15/2012","Richard Brower","MA","Trustees of Boston University","Continuing Grant","Bogdan Mihaila","02/28/2015","$374,088.00","Claudio Rebbi, Lorena Barba","brower@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","1253, 7244, 8004","1253, 7244","$0.00","This award supports the development of software tools for advanced algorithms on a cluster of high performance graphics processing units(GPUs).  The initial goal of this library will focus on a single driver application, the fundamental numerical study of Nuclear Forces (Lattice Quantum Chromodynamics: LQCD), and a second target application, the numerical simulation of the exciting nano-technology of graphene.  These are both multi-fermion problems well suited to solution via multi-scale algorithms on many-core architectures.  This pilot project draws on experience gained by the small team at Boston University and Harvard in developing Dirac solvers for lattice field theory.  Two building blocks from prior research are (1) the construction of an adaptive multigrid (MG) solver for the Wilson Dirac operator of LQCD, which demonstrates a 20x speedup compared to the best Krylov solvers in production code and (2) a highly optimized Krylov solver for the same operator on GPUs (but without multigrid), realizing a 10x improvement in price/performance over traditional clusters.  The library will unite these feature and generalize their domain of applicability.  <br/><br/>As an example of the broader impact, it is estimated that combining these two technologies (MG algorithms and GPU architectures) will yield a 100-fold improvement in price/performance for the most compute-intensive component of LQCD simulations.  Such an advance would be truly transformative, making an immediate impact in nuclear and particle physics.  At the same time, it will serve as a prototype of the more generic problem of mapping hierarchical algorithms onto heterogeneous architectures, a challenge of paramount importance on the path to the exascale.  The software library will be designed to bring similar benefits to graphene technology and to evolve to accommodate additional target application and additional domain decomposition algorithm to mitigate the communication bottleneck of Exascale designs.  The award will provide partial support for two postdoctoral scholars who play essential roles in this project."
"1047896","SI2-SSE: Reducing the Complexity of Comparative Genomics with Online Analytical Processing","OAC","ADVANCES IN BIO INFORMATICS, Software Institutes","09/15/2010","10/23/2012","Robert Kosara","NC","University of North Carolina at Charlotte","Standard Grant","Anne Maglia","08/31/2015","$448,253.00","Cynthia Gibas","rkosara@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","CSE","1165, 8004","1165","$0.00","Genome comparison is a common bioinformatics analysis task, but a survey of the literature suggests that comparative genomic studies are done in an ad hoc, investigator-dependent, and non-reproducible fashion. Comparative genomics analysis questions can generally be formulated as set queries: what differentiates genome A from genome B, or from a broader group of its taxonomic neighbors? This project will develop a data warehouse-type database system optimized for comparative genomics that is particularly suited to answer these kinds of questions. It will store sequence-linked biological data in a way that supports OLAP (On-Line Analytical Processing) and complex set-based queries. A workflow tool will be developed for guiding the user through core comparative genomic operations, and will serve as an interface for populating the data warehouse. An interactive query tool will allow the user to easily construct complex questions about the data. Set-based as well as individual record results will be presented to the user in a way that can be easily browsed, compared, and exported.<br/><br/>The system will enable the user to generate and compare genomic feature sets following a guided workflow defined by and incorporating common elements of analysis used in current microbial genome studies. Parameters and results from each step in the process will be tracked for later reporting. The system will enable both biology-driven comparison of genomic feature sets, and perhaps more importantly systematic inquiry into and comparison of bioinformatics analysis results obtained at each workflow stage. All software and database structures developed in this project will be made available under an open-source license and as runnable virtual machine images. The latter will make it possible for scientists to get started without complicated installation and configuration procedures, and instead focus on their research questions. All user-facing parts of the software will be accessible through a web browser, making use of the latest developments in browser-based interaction and HTML5. A diverse group of graduate students will receive inter-disciplinary training in this project, and K-12 and underrepresented groups will be included in through ongoing partnerships with the STARS Alliance program."
"1440523","Collaborative: SI2-SSE - A Plug-and-Play Software Platform of Robotics-Inspired Algorithms for Modeling Biomolecular Structures and Motions","OAC","Special Projects - CCF, Software Institutes, CDS&E","02/01/2015","08/08/2014","Adrian Roitberg","FL","University of Florida","Standard Grant","Rajiv Ramnath","01/31/2018","$67,235.00","","roitberg@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","2878, 8004, 8084","2878, 7433, 8004, 8005, 8084, 9216","$0.00","This project aims to develop a novel plug-and-play platform of open-source software elements to advance algorithmic research in molecular biology. The focus is on addressing the algorithmic impasse faced by computational chemists and biophysicists in structure-function related problems involving dynamic biomolecules central to our biology. The software platform resulting from this project provides the critical software infrastructure to support transformative research in molecular biology and computer science that benefits society at large by advancing our modeling capabilities and in turn our understanding of the role of biomolecules in critical mechanisms in a living and diseased cell.<br/><br/>The project addresses the current impasse on the length and time scales that can be afforded in biomolecular modeling and simulation. It does so by integrating cutting-edge knowledge from two different research communities, computational chemists and biophysicists focused on detailed physics-based simulations, and AI researchers focused on efficient search and optimization algorithms. The software elements integrate sophisticated energetic models and molecular representations with powerful search and optimization algorithms for complex modular systems inspired from robot motion planning. The plug-and-play feature of the software platform supports putting together novel algorithms, such as wrapping Molecular Dynamics or Monte Carlo as local search operators within larger robotics-inspired exploration frameworks, and adding emerging biomolecular representations, models, and search techniques even beyond the timeline of this project."
"1047857","Collaborative Research: SI2-SSI: Development of an Integrated Molecular Design Environment for Lubrication Systems (iMoDELS)","OAC","NANOSCALE: INTRDISCPL RESRCH T, Software Institutes","10/01/2011","07/16/2014","Adri van Duin","PA","Pennsylvania State Univ University Park","Continuing Grant","Daniel Katz","09/30/2015","$457,319.00","","acv13@psu.edu","201 Old Main","University Park","PA","168021503","8148651372","CSE","1674, 8004","1674, 8004","$0.00","This project is focused on developing, deploying and distributing the Integrated Molecular Design Environment for Lubrication Systems (iMoDELS), an open-source simulation and design environment (SDE) that encapsulates the expertise of specialists in first principles, forcefields and molecular simulation related to nanoscale lubrication in a simple web-based interface.  The iMoDELS SDE is being developed using model-integrated computing (MIC), a state-of-the-art powerful, well-established, extensible, community-supported, and application-hardened software engineering framework that supports scientific and engineering workflows.  Making iMoDELS broadly accessible is motivated by the high cost (over $800B/yr in the US) of friction and wear, which, along with the methodology to overcome them, lubrication, are collectively known as tribology.  Tribology involves molecular mechanisms occurring on a nanometer scale, and hence understanding tribological behavior on this scale is critical to developing new technologies for reducing wear due to friction.  Deployment of iMoDELS will enable non-computational specialists to be able to evaluate, design and optimize nanoscale lubrication systems, such as hard disk drives, NEMS (nanoelectromechanical systems) and MEMS (microelectromechanical systems), and experiments involving rheological measurements via atomic force microscopes (AFMs) and surface force apparatuses (SFAs).<br/><br/>The iMoDELS SDE brings together a unique combination of materials and computer scientists who will combine their skills to abstract the deep human expertise currently required for the development of simulation-based experiments, thus making broadly available easy-to-use tools to an empirically driven area of science and engineering (nanotribology) of rapidly growing technological importance.  iMoDELS includes the creation and open dissemination of forcefield and simulation results databases that will benefit the simulation community worldwide and catalyze broad-based activity in this area.  The proposed research also includes the interdisciplinary training of undergraduate and graduate students, as well as postdoctoral researchers at the interface of tribology, computational materials sciences, and computer science.  The PIs will use the iMoDELS SDE, and results from it, in presentations used in outreach to local area high school and in classes given to undergraduate students.  The iMoDELS SDE will be vigorously promoted through workshops and presentations at national conferences, and via the dedicated website for development and dissemination."
"1440534","SI2-SSE: Solving Polynomial Systems with PHCpack and phcpy","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS","10/01/2014","08/01/2014","Jan Verschelde","IL","University of Illinois at Chicago","Standard Grant","Bogdan Mihaila","09/30/2019","$464,352.00","","janv@uic.edu","809 S. Marshfield Avenue","Chicago","IL","606124305","3129962862","CSE","1253, 8004, 8069","7433, 8005, 8251","$0.00","Solving polynomial systems is a fundamental problem in mathematics with applications to various fields of science and engineering. The free and open source software PHCpack applies symbolic-numeric and polyhedral methods to solve polynomial systems. As a new interface to PHCpack written in the Python scripting language, phcpy improves the functionality of PHCpack. The implementation on parallel computers to compensate for the cost overhead of multi-precision arithmetic will enable scientists and engineers to solve larger systems faster and more accurately. A web server developed with phcpy will give anyone with an internet connection access to the developed software.<br/><br/>The solvers in PHCpack apply homotopy continuation methods, blending symbolic-numeric with polyhedral algorithms. Numerical approximations to solutions are computed with Newton's method. Solutions are approximated symbolically by Puiseux series, which originate at initial forms defined by the Newton polytopes of the polynomials in the system. The design of phcpy gives a flexible interactive scripting interface, without sacrificing efficiency as compiled code in PHCpack is executed. The package phcpy will provide the tools for a scalable compute server to serve requests submitted to the web server. Multithreaded implementations on multicore processors accelerated by graphics processing units compensate for the cost overhead of double double and quad double arithmetic."
"1048052","SI2-SSE: Cloud-Computing-Clusters for Scientific Research","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","09/15/2010","09/07/2010","John Rehr","WA","University of Washington","Standard Grant","Daryl Hess","08/31/2013","$489,188.00","","jjr@phys.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","1253, 1712, 8004","1253, 1712, 1765, 7237, 7569","$0.00","This award is made on a proposal to the Software Infrastructure for Sustained Innovation. The Office of Cyberinfrastructure and the Division of Materials Research contribute funds to this award. <br/><br/>This award supports developing an internet-based scientific computing environment, particularly for computational materials research. The PI will advance into computational materials research a recent innovation known as cloud computing ? the ability to utilize computer resources connected via the internet, which may be geographically separated by vast distances, on demand to do computing. The PI will develop a computing environment that will be robust, flexible, and relatively easy for the materials research community to use. The PI will focus on specific codes that calculate the states of electrons in materials starting from the identities of the atoms in the material and their positions. Forces between atoms may be calculated from the electronic states. These codes will be used in the short term to calculate complicated motions of atoms and molecules, to understand data from experiments that involve scattering light from materials, to study how specific materials absorb and interact with light to advance solar cell technology.<br/><br/>This award also supports education and outreach activities to enable the research community to effectively use the new computational tools developed under this award, and expose high school students to cutting edge advances in computation."
"1358118","SI2-SSE: Collaborative Research: Lagrangian Coherent Structures for Accurate Flow Structure Analysis","OAC","OFFICE OF MULTIDISCIPLINARY AC, Mechanics of Materials and Str, DYNAMICAL SYSTEMS, COFFES, Software Institutes","09/01/2013","09/13/2013","Shawn Shadden","CA","University of California-Berkeley","Standard Grant","Daniel Katz","12/31/2014","$120,478.00","","shadden@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","1253, 1630, 7478, 7552, 8004","","$0.00","The Lagrangian Coherent Structures (LCS) software elements developed by this project will provide a valuable tool set for fluid mechanics research to extract new discoveries from the vast and growing body of computational and experimental fluid mechanics data. The computation of LCS enables a systematic approach to accurately characterize transport phenomena in complex systems that pose insurmountable challenges to traditional Eulerian approaches. Prior, ah hoc implementations of LCS have already helped in important, real-world challenges including, tracking pollutants in the ocean, developing novel diagnoses and therapies for cardiovascular disease, and helping airplanes to avoid turbulence. We will produce an open LCS software system to provide a modular, extensible and flexible infrastructure to broaden the community of scientists and engineers that benefit from LCS, in problems ranging from fluid dynamics to general dynamical systems. Prof. Shadden will lead the LCS algorithm design and numerical analysis, and Prof. Hart will oversee the package's architectural design and the efficient parallel implementation of its elements."
"1339738","SI2-SSI: Collaborative Research: STORM: A Scalable Toolkit for an Open Community Supporting Near Realtime High Resolution Coastal Modeling","OAC","PHYSICAL OCEANOGRAPHY, Special Projects - CNS, Special Projects - CCF, Software Institutes, EarthCube","10/01/2014","10/20/2014","Joannes Westerink","IN","University of Notre Dame","Standard Grant","Bogdan Mihaila","09/30/2018","$730,000.00","Tim Stitt, Damrongsak Wirasaet","jjw@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","1610, 1714, 2878, 8004, 8074","7433, 8009","$0.00","The ADCIRC coastal circulation and storm surge model has a long standing track record of extremely high societal impact. It has been used to define risk (e.g., 100-yr, 500-yr coastal flood levels) for the FEMA National Flood Insurance Program in coastal states from New York to Texas, it has been used to design the multi-billion dollar Hurricane and Storm Damage Risk Reduction System around greater New Orleans and southern Louisiana by the Army Corps of Engineers, it is currently run operationally by NOAA/NWS National Center for Environmental Prediction to forecast storm surge, to name just a few of its current and recent applications. Thus there is a well-established user network in place to convert improvements in ADCIRC into significant broader impacts. The proposed research provides transformative intellectual contributions that focus on applying new parallelization schemes to enable major advances in the algorithms, implementation and utilization of the ADCIRC model. The broadening of ADCIRC to a multi-algorithmic framework and the resulting performance gains that are anticipated will help ensure ADCIRC's sustainability as a core community model for at least the next 20 years. In addition, the proposed collaboration will impact computer science by serving as a high impact use case to inform the design of new approaches to efficient scalable computing. Together, the advancements in coastal modeling and parallelization technology will make a significant contribution to the science of modeling and HPC. The results of the proposed research will be disseminated to a wider community through ongoing educational outreach activities at the participating organizations as well as through refereed conference and journal papers, and invited presentations. The involvement of graduate students and post-doctoral fellows will be crucial towards the success of this project. The PIs have a long history of training and mentoring students and post-docs in computational science and engineering, coastal engineering and marine science. The recruitment and involvement of underrepresented groups in these efforts has always been a high priority. In addition, aspects of the proposed research will be incorporated into the curricula of several courses taught by the PIs in the areas of finite element methods, scientific computation, hydrology and oceanography.<br/>The aim of this project is to broaden the ADCIRC coastal circulation and storm surge model from a successful, but somewhat static coastal modeling tool that is tied to a single solution algorithm and the MPI parallelization paradigm, to a dynamic computational platform that is comprised of multiple solution algorithms, that readily admits new solution algorithms and that is built on a transformational new parallelization scheme that will allow us to scale to at least 256k compute cores on modern high performance computing (HPC) systems. We will do this by creating a living, evolving coastal modeling framework that will continue to lead the community in merging physical science / engineering and high performance computing and we will make the framework available to the broader community as a sustainable long term solution for its coastal modeling needs. In addition we will utilize these advancements in the highly demanding coastal storm surge forecasting system that we presently operate to demonstrate both improved robustness and speed of the model solution. We expect this effort will shorten the time required to provide reliable forecasting results and improve our ability to provide highly resolved, accurate, and physically complete predictions on an unprecedented scale. Concurrently, it should enable the use of smaller resources for simulations of increased scale which improves the usability and widens the applicability of ADCIRC in a broader community. The development of tightly integrated web-oriented products like CERA (www.coastalemergency.org) will enable the wide and timely dissemination of forecast modeling results to reach a broad audience."
"1147519","Collaborative Research: SI2-SSE: Component-Based Software Architecture for Computational Landscape Modeling","OAC","Geomorphology & Land-use Dynam, Software Institutes, EPSCoR Co-Funding","06/01/2012","07/06/2012","Nicole Gasparini","LA","Tulane University","Standard Grant","Rajiv Ramnath","05/31/2016","$245,572.00","","ngaspari@tulane.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","701185698","5048654000","CSE","7458, 8004, 9150","7433, 7458, 8004, 8005","$0.00","Presently there are no widely adopted software conventions for holistic computational landscape models. To fill this important gap this project adapts and enhances existing landscape modeling codes by introducing a component-based approach to software development.  This project adapts and enhances an existing model -- CHILD (Channel-Hill slope Integrated Landscape Development) -- to provide a set of interoperable, independent modeling components that provide flexible and modular approaches to landscape modeling which are fully compatible with the NSF-funded Community Surface Dynamics Modeling System CSDMS) infrastructure.  In accord with the CSDMS architecture, the software to be developed will also employ the standards and tools of the Common Component Architecture (CCA) software architecture.  Included is the design of an interface for communication with and between the developed components.  The end result will be a set of independent, interoperable C++ software modeling modules that are compatible with the CSDMS modeling toolkit as well as the standards and tools of the CCA structure.  The software will be tested against data on post-wildfire erosion.<br/><br/>This approach was selected to provide maximum flexibility to users by allowing them to plug-and-play, seamlessly linking together selected computing modules to enable custom combinations of components to support modeling for a wide variety of research topics.  Work will include the development of a gridding engine to handle both regular and unstructured meshes and an interface for space-time rainfall input, as well as a surface hydrology component, a sediment erosion-deposition component, a vegetation component, and a simulation driver. <br/><br/>If successful this project will impact many branches of the Earth and environmental sciences by building a new modeling platform and creating essential software infrastructure for science with applications that span hydrology, soil erosion, tectonics, geomorphology, vegetation ecology, stratigraphy, and planetary science.  Broader impacts of the work include creation of classroom modeling exercises for both undergraduate and high schools students, and support of a PI whose gender is under-represented in the STEM fields and who is employed at an institution in an EPSCoR state.  It also includes workforce-training in computational geoscience for graduate students and a postdoc as well as minority undergraduates from Xavier University, a Historically Black University in the New Orleans area."
"1148330","SI2-SSE: Connecting Cyberinfrastructure with the Cooperative Computing Tools","OAC","Software Institutes","04/15/2012","03/12/2013","Douglas Thain","IN","University of Notre Dame","Standard Grant","Rajiv Ramnath","12/31/2016","$507,020.00","","dthain@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","8004","7433, 8004, 8005, 9251","$0.00","This project supports the maintenance and development of the Cooperative Computing Tools. This software package is designed to enable non-privileged users to<br/>harness hundreds to thousands of cores from multiple clusters, clouds, and grids simultaneously. The main components of the software package include Parrot, a virtual file system that interfaces with multiple distributed storage systems, and Makeflow, a workflow engine that interfaces with multiple computing systems. This project will develop, maintain, and support the software across a wide variety of operating systems and national scale cyberinfrastructure in support of high impact scientific applications in fields such as bioinformatics, biometrics, data mining, high energy physics, and molecular dynamics.<br/><br/>Large scale computing systems such as cluster, clouds, and grids now make it easy for end users to purchase large amounts of computing power at the touch of a button.  However, these computing systems are difficult to harness because they each present a different user interface, principle of operation, and programming model.  This project addresses this problem by supporting the development of the Cooperative Computing Tools, a software package that makes it possible for ordinary computer applications to move seamlessly between different service providers. The software is primarily of interest to researchers in scientific domains that require large amounts of computation.  It is currently used by researchers in the fields of bioinformatics, biometrics, data mining, high energy physics, and molecular dynamics."
"1339840","Collaborative Research: SI2-SSI: A Comprehensive Ray Tracing Framework for Visualization in Distributed-Memory Parallel  Environments","OAC","Software Institutes","10/01/2013","09/13/2013","Henry Childs","OR","University of Oregon Eugene","Standard Grant","Rajiv Ramnath","09/30/2017","$235,961.00","","hankchilds@gmail.com","5219 UNIVERSITY OF OREGON","Eugene","OR","974035219","5413465131","CSE","8004","7433, 8009","$0.00","Scientific visualization plays a large role in exploring the scientific simulations that run on supercomputers; new discoveries are often made by studying renderings generated through visualization of simulation results. The standard technique for rendering geometry is rasterization and the most commonly used library for performing this is OpenGL. Many visualization programs (VisIt, Ensight, VAPOR, ParaView, VTK) use OpenGL for rendering. However, recent architectural changes on supercomputers create significant opportunities for alternate rendering techniques. The computational power available on emerging many-core architectures, such as the Intel Xeon Phi processors on TACC?s Stampede system, enable ray-tracing, a higher quality technique. Further, as the amount of geometry per node rises, ray-tracing becomes increasingly cost effective, since its computational costs are proportional to the screen size, not the geometry size. Finally, the software implementation for OpenGL can not be easily mapped to non-GPU multi-core and many-core systems, creating a significant gap; if not closed, visualization will not be possible directly on large supercomputers. This confluence of new, more capable architectures, the increase in geometry per node, and concerns about the durability of the established rendering path all motivate this   work. <br/><br/>To address these trends, this research uses a two-pronged approach. First, the research will replace the OpenGL pathways that are commonly used for visualization with a high-performance, open-source ray tracing engine that can interactively render on both a CPU and on accelerator architectures. This new library will support the OpenGL API and will be usable immediately by any OpenGL-based visualization package without additional code modi&#64257;cation. Second, this research will provide a direct interface to a high-performance distributed ray tracing engine so that applications can take advantage of ray tracing capabilities not easily exposed through the standard OpenGL interface, such as participating media and global illumination simulation. These features will enable the open science community to easily create photo-realistic imagery with natural lighting cues to aid in analysis and discovery. It will further expand the capacity of existing cyberinfrastructure to provide interactive visualization on standard HPC resources. <br/><br/>This work has the potential to revolutionize in situ visualization capabilities by unifying the (potentially hybrid) architecture that efficiently run both simulation and visualization. Communicating with underrepresented groups will be a major component of outreach efforts through the PCARP, MITE and Women in Engineering programs. In addition, the project team will disseminate this work to the general public through NSF XD program, the VisIt visualization toolkit and by exhibiting at forums such as IEEE Visualization, IEEE High Performance Graphics and ACM Supercomputing."
"1147422","SI2-SSE Collaborative Research: SPIKE-An Implementation of a Recursive Divide-and-Conquer Parallel Strategy for Solving Large Systems of Liner Equations","OAC","OFFICE OF MULTIDISCIPLINARY AC, DYNAMICAL SYSTEMS, Software Institutes, CDS&E-MSS","06/01/2012","06/20/2012","Ahmed Sameh","IN","Purdue University","Standard Grant","Almadena Chtchelkanova","05/31/2016","$240,000.00","","sameh@cs.purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","CSE","1253, 7478, 8004, 8069","1253, 7433, 7478, 8005","$0.00","Drs. Negrut, Sameh, and Knepley will investigate, produce, and maintain a methodology and its software implementation that leverage emerging heterogeneous hardware architectures to solve billion-unknowns linear systems in a robust, scalable, and efficient fashion. The two classes of problems targeted under this project are banded dense and sparse general linear systems.<br/><br/>This project is motivated by the observation that the task of solving a linear system is one of the most ubiquitous ingredients in the numerical solution of Applied Mathematics problems. It is relied upon for the implicit integration of Ordinary Differential Equation (ODE) and Differential Algebraic Equation (DAE) problems, in the numerical solution of Partial Differential Equation (PDE) problems, in interior point optimization methods, in least squares approximations, in solving eigenvalue problems, and in data analysis. In fact, the vast majority of nonlinear problems in Scientific Computing are solved iteratively by drawing on local linearizations of nonlinear operators and the solution of linear systems. Recent advances in (a) hardware architecture; i.e., the emergence of General Purpose Graphics Processing Unit (GP-GPU) cards, and (b) scalable solution algorithms, provide an opportunity to develop a new class of parallel algorithms, called SPIKE, which can robustly and efficiently solve very large linear systems of equations.<br/><br/>Drawing on its divide-and-conquer paradigm, SPIKE builds on several algorithmic primitives: matrix reordering strategies, dense linear algebra operations, sparse direct solvers, and Krylov subspace methods. It provides a scalable solution that can be deployed in a heterogeneous hardware ecosystem and has the potential to solve billion-unknown linear systems in the cloud or on tomorrow?s exascale supercomputers. Its high degree of scalability and improved efficiency stem from (i) optimized memory access pattern owing to an aggressive pre-processing stage that reduces a generic sparse matrix to a banded one through a novel reordering strategy; (ii) good exposure of coarse and fine grain parallelism owing to a recursive, divide-and-conquer solution strategy; (iii) efficient vectorization in evaluating the coupling terms in the divide-and-conquer stage owing to a CPU+GPU heterogeneous computing approach; and (iv) algorithmic polymorphism, given that SPIKE can serve both as a direct solver or an effective preconditioner in an iterative Krylov-type method.<br/><br/>In Engineering, SPIKE will provide the Computer Aided Engineering (CAE) community with a key component; i.e., fast solution of linear systems, required by the analysis of complex problems through computer simulation. Examples of applications that would benefit from this technology are Structural Mechanics problems (Finite Element Analysis in car crash simulation), Computational Fluid Dynamics problems (solving Navier-Stokes equations in the simulation of turbulent flow around a wing profile), and Computational Multibody Dynamics problems (solving Newton-Euler equations in large granular dynamics problems).<br/><br/>SPIKE will also be interfaced to the Portable, Extensible Toolkit for Scientific Computation (PETSc), a two decades old flexible and scalable framework for solving Science and Engineering problems on supercomputers. Through PETSc, SPIKE will be made available to a High Performance Computing user community with more than 20,000 members worldwide. PETSc users will be able to run SPIKE without any modifications on vastly different supercomputer architectures such as the IBM BlueGene/P and BlueGene/Q, or the Cray XT5. SPIKE will thus run scalably on the largest machines in the world and will be tuned for very different network and hardware topologies while maintaining a simple code base.<br/><br/>The experience collected and lessons learned in this project will augment a graduate level class, ?High Performance Computing for Engineering Applications? taught at the University of Wisconsin-Madison. A SPIKE tutorial and research outcomes will be presented each year at the International Conference for High Performance Computing, Networking, Storage and Analysis. A one day High Performance Computing Boot Camp will be organized each year in conjunction with the American Society of Mechanical Engineers (ASME) conference and used to disseminate the software outcomes of this effort. Finally, this project will shape the research agendas of two graduate students working on advanced degrees in Computational Science."
"1147161","Collaborative Research: SI2-SSE: Sage-Combinat: Developing and Sharing Open Source Software for Algebraic Combinatorics","OAC","OFFICE OF MULTIDISCIPLINARY AC, DYNAMICAL SYSTEMS, Software Institutes, CDS&E-MSS","06/01/2012","06/19/2012","Gregg Musiker","MN","University of Minnesota-Twin Cities","Standard Grant","Rajiv Ramnath","05/31/2016","$195,688.00","","musiker@math.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","1253, 7478, 8004, 8069","1253, 5514, 7433, 7478, 7683, 8004, 8005","$0.00","Sage is an open source general purpose mathematical software system that has developed explosively within the last six years. Sage-Combinat is a subproject whose mission is ""to improve Sage as an extensible toolbox for computer exploration in (algebraic) combinatorics, and foster code sharing between researchers in this area"".  There is a long tradition of software packages for algebraic combinatorics. These have been crucial in the development of combinatorics since the 1960s.  The originality of the Sage-Combinat project lies in successfully addressing the following simultaneous objectives. It offers a wide variety of interoperable and extensible tools, integrated in a general purpose mathematical software package, as needed for daily computer exploration in algebraic combinatorics; it is developed by a community of researchers spread around the world and across institutions; and it is open source and depends only on open source software. Among the proposers, Stein is founder and lead developer of Sage while Bump, Musiker, and Schilling are strong contributors to Sage-Combinat. Hivert and Thiery (Paris-Sud, Orsay), founders and lead developers of Sage-Combinat, are both strongly affiliated with this project. Some of the research areas addressed in this project include symmetric functions, in particular Macdonald polynomials for arbitrary Cartan types and their nonsymmetric analogues, crystals, rigged configurations and combinatorial R-matrices, affine Weyl groups and Hecke algebras, cluster algebras, and posets.<br/> <br/>The project will develop Sage-Combinat in areas relevant to the ongoing research of the participants, together with relevant underlying infrastructure. The project will include three Sage Days workshops, and will be affiliated with a third scheduled workshop at ICERM. These workshops include a strong outreach component and have been a potent tool for connecting researchers and recruiting Sage users and developers. The grant will also fund a dedicated software development and computation server for Sage-Combinat, to be hosted in the Sage computation farm in Seattle. Emphasis will be placed on the development of thematic tutorials that will make the code accessible to new users. The proposal will also fund graduate student RA support, curriculum development, and other mentoring."
"1147503","SI2-SSI: Collaborative Research:  A Computational Materials Data and Design Environment","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, CHEMISTRY PROJECTS, Software Institutes","10/01/2012","09/20/2012","Gerbrand Ceder","MA","Massachusetts Institute of Technology","Standard Grant","Rajiv Ramnath","09/30/2017","$450,000.00","","gceder@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","1253, 1712, 1991, 8004","1253, 1712, 1982, 1991, 7237, 7433, 7569, 7644, 8004, 8009, 9216, 9263","$0.00","TECHNICAL SUMMARY<br/>The Office of Cyberinfrastructure, Division of Materials Research, and Chemistry Division contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports development of new theory and tools to enable rapid and efficient calculation of atomic level material properties.  The incredible advances in computing power and tools of atomic scale simulation have now made it possible to predict critical properties for existing and new materials without experimental input.  However, present simulation approaches typically require researchers to perform many steps by hand, which is both slow and error prone compared to what a computer can do.  Through computer codes that automate the tasks in first principles modeling human bottlenecks can be removed and predictive capabilities of first principles simulation techniques can be accelerated by orders of magnitude.  Such a high-throughput computing approach will enable generation of critical materials data on an unprecedented scale and open new doors for material science.<br/><br/>The team will develop tools for the specific challenges of predicting point defect properties, atomic diffusion, and surface stability, with a focus on automating steps to enable computations on a massive scale.  The PIs will use state-of-the-art first principles quantum mechanical methods.   Best practices for treating the multiple issues of charged defect calculations, for example convergence with cell size and band gap errors, will be refined and automated for rapid execution.  Similarly, tools to identify diffusion pathways and determine their barriers will be streamlined to allow users to quickly identify transport properties of new systems.  New theoretical approaches to modeling charged surfaces will be developed to enable simulation of surfaces in more realistic environments.  This award will support prediction of properties that play a critical role in advancing a wide range of technologies, from improving semiconductors for next generation computers to better fuel cells for more efficient energy conversion.  Software tools and data produced by this effort will enable researchers to predict properties for thousands of materials with almost no human effort, accelerating the pace at which researchers can develop new materials technologies.<br/><br/>Software and data developed from this award will be shared with academic and industrial researchers through modules on the web, scientific journals and presentations at national and international conferences.  This award supports two workshops to educate researchers about the latest opportunities to use high-throughput computing of atomic scale properties for materials development.  Students will be trained to work at the critical interface of the computer and physical sciences, supporting a generation of scientists who use modern computers to their fullest potential to develop new understanding and technology.<br/><br/>NON-TECHNICAL SUMMARY<br/>The Office of Cyberinfrastructure, Division of Materials Research, and Chemistry Division contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports development of new theory and tools to enable rapid and efficient calculation of atomic level material properties.  The incredible advances in computing power and tools of atomic scale simulation have now made it possible to predict critical properties for existing and new materials without experimental input.  However, present simulation approaches typically require researchers to perform many steps by hand, which is both slow and error prone compared to what a computer can do.  Through computer codes that automate the tasks in first-principles modeling human bottlenecks can be removed and predictive capabilities of first principles simulation techniques can be accelerated by orders of magnitude.  Such a high-throughput computing approach will enable generation of critical materials data on an unprecedented scale and open new doors for material science.<br/><br/>The team will develop tools for the specific challenges of predicting point defect properties, atomic diffusion, and surface stability, with a focus on automating steps to enable computations on a massive scale.  These properties play a critical role in advancing a wide range of technologies, from improving semiconductors for next generation computers to better fuel cells for more efficient energy conversion.  Software tools and data produced by this effort will enable researchers to predict properties for thousands of materials with almost no human effort, accelerating the pace at which researchers can develop new materials technologies.<br/><br/>Software and data developed from this award will be shared with academic and industrial researchers through modules on the web, scientific journals and presentations at national and international conferences.  In particular, this award will support two workshops to educate researchers about the latest opportunities to use high-throughput computing of atomic scale properties for materials development.  This award will train students to work at the critical interface of the computer and physical sciences, supporting a generation of scientists who use modern computers to their fullest potential to develop new understanding and technology."
"1407834","Collaborative Research: SI2-SSI: A Sustainable Open Source Software Pipeline for Patient Specific Blood Flow Simulation and Analysis","OAC","Software Institutes, CDS&E","09/20/2013","11/12/2013","Shawn Shadden","CA","University of California-Berkeley","Standard Grant","Stefan Robila","09/30/2018","$306,276.00","","shadden@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","8004, 8084","7433, 8009, 8084","$0.00","The SimVascular package is a crucial research tool for cardiovascular modeling and simulation, and has contributed to numerous advances in personalized medicine, surgical planning and medical device design. SimVascular is currently the only comprehensive software package that provides a complete pipeline from medical image data segmentation to patient specific blood flow simulation and analysis. This software now forms the backbone in cardiovascular simulation research in a small but active group of domestic and international academic labs. However, since its original release there have been several critical barriers preventing wider adoption by new users, application to large-scale research studies, and educational access. These include 1) the cost and complications associated with embedded commercial components, 2) the need for more efficient geometric model construction tools, 3) lack of sustainable architecture and infrastructure, and 4) a lack of organized maintenance. <br/><br/>This project is addressing the above roadblocks through the following aims:  1) create a sustainable and modular open source SimVascular 2.0 project housed at Stanford Simbios? simtk.org, with documentation, benchmarking and test suites, 2) provide alternatives to all commercial components in the first truly open source release of SimVascular, 3) improve the image segmentation methods and efficiency of model construction to enable high-throughput studies, and 4) enhance functionality by merging state of the art research in optimization, flow analysis, and multiscale modeling. The project leverages existing resources and infrastructure at simtk.org, and builds upon the significant previous investment that enabled the initial open source release of SimVascular. Access is further enhanced by cross-linking with the NIH funded Vascular Model Repository. This project will increase the user base and build a sustainable software platform supported by an active open source community.   Releasing the first fully open source version of SimVascular will enable greater advances in cardiovascular medicine, provide open access to state of the art simulation tools for educational purposes, and facilitate training of young investigators. These efforts will also further promote diversity and attract students to science and engineering by leveraging this software to enable high school field trips to the UCSD StarCAVE to view simulation data using virtual reality."
"1450405","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","OAC","Physical & Dynamic Meteorology, Software Institutes, EarthCube","08/01/2015","08/04/2015","Steven Greybush","PA","Pennsylvania State Univ University Park","Standard Grant","Alan Sussman","07/31/2018","$99,953.00","","sjg213@psu.edu","201 Old Main","University Park","PA","168021503","8148651372","CSE","1525, 8004, 8074","4444, 7433, 8009","$0.00","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities."
"1147466","SI2-SSI Collaborative Research:  A Computational Materials Data and Design Environment","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, CHEMISTRY PROJECTS, Software Institutes","10/01/2012","09/11/2013","Alan Dozier","KY","University of Kentucky Research Foundation","Standard Grant","Alan Sussman","08/31/2018","$298,999.00","Raphael Finkel","adozier@uky.edu","109 Kinkead Hall","Lexington","KY","405260001","8592579420","CSE","1253, 1712, 1991, 8004","1253, 1712, 1982, 1991, 7237, 7433, 7569, 7644, 8004, 8009, 9150, 9216, 9263","$0.00","TECHNICAL SUMMARY<br/>The Office of Cyberinfrastructure, Division of Materials Research, and Chemistry Division contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports development of new theory and tools to enable rapid and efficient calculation of atomic level material properties.  The incredible advances in computing power and tools of atomic scale simulation have now made it possible to predict critical properties for existing and new materials without experimental input.  However, present simulation approaches typically require researchers to perform many steps by hand, which is both slow and error prone compared to what a computer can do.  Through computer codes that automate the tasks in first principles modeling human bottlenecks can be removed and predictive capabilities of first principles simulation techniques can be accelerated by orders of magnitude.  Such a high-throughput computing approach will enable generation of critical materials data on an unprecedented scale and open new doors for material science.<br/><br/>The team will develop tools for the specific challenges of predicting point defect properties, atomic diffusion, and surface stability, with a focus on automating steps to enable computations on a massive scale.  The PIs will use state-of-the-art first principles quantum mechanical methods.   Best practices for treating the multiple issues of charged defect calculations, for example convergence with cell size and band gap errors, will be refined and automated for rapid execution.  Similarly, tools to identify diffusion pathways and determine their barriers will be streamlined to allow users to quickly identify transport properties of new systems.  New theoretical approaches to modeling charged surfaces will be developed to enable simulation of surfaces in more realistic environments.  This award will support prediction of properties that play a critical role in advancing a wide range of technologies, from improving semiconductors for next generation computers to better fuel cells for more efficient energy conversion.  Software tools and data produced by this effort will enable researchers to predict properties for thousands of materials with almost no human effort, accelerating the pace at which researchers can develop new materials technologies.<br/><br/>Software and data developed from this award will be shared with academic and industrial researchers through modules on the web, scientific journals and presentations at national and international conferences.  This award supports two workshops to educate researchers about the latest opportunities to use high-throughput computing of atomic scale properties for materials development.  Students will be trained to work at the critical interface of the computer and physical sciences, supporting a generation of scientists who use modern computers to their fullest potential to develop new understanding and technology.<br/><br/>NON-TECHNICAL SUMMARY<br/>The Office of Cyberinfrastructure, Division of Materials Research, and Chemistry Division contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports development of new theory and tools to enable rapid and efficient calculation of atomic level material properties.  The incredible advances in computing power and tools of atomic scale simulation have now made it possible to predict critical properties for existing and new materials without experimental input.  However, present simulation approaches typically require researchers to perform many steps by hand, which is both slow and error prone compared to what a computer can do.  Through computer codes that automate the tasks in first-principles modeling human bottlenecks can be removed and predictive capabilities of first principles simulation techniques can be accelerated by orders of magnitude.  Such a high-throughput computing approach will enable generation of critical materials data on an unprecedented scale and open new doors for material science.<br/><br/>The team will develop tools for the specific challenges of predicting point defect properties, atomic diffusion, and surface stability, with a focus on automating steps to enable computations on a massive scale.  These properties play a critical role in advancing a wide range of technologies, from improving semiconductors for next generation computers to better fuel cells for more efficient energy conversion.  Software tools and data produced by this effort will enable researchers to predict properties for thousands of materials with almost no human effort, accelerating the pace at which researchers can develop new materials technologies.<br/><br/>Software and data developed from this award will be shared with academic and industrial researchers through modules on the web, scientific journals and presentations at national and international conferences.  In particular, this award will support two workshops to educate researchers about the latest opportunities to use high-throughput computing of atomic scale properties for materials development.  This award will train students to work at the critical interface of the computer and physical sciences, supporting a generation of scientists who use modern computers to their fullest potential to develop new understanding and technology."
"1148346","SI2-SSI: Collaborative Research: A Glass Box Approach to Enabling Open, Deep Interactions in the HPC Toolchain","OAC","Information Technology Researc, Software Institutes","06/01/2012","05/29/2012","Allen Malony","OR","University of Oregon Eugene","Standard Grant","Rajiv Ramnath","05/31/2016","$926,667.00","Sameer Shende","malony@cs.uoregon.edu","5219 UNIVERSITY OF OREGON","Eugene","OR","974035219","5413465131","CSE","1640, 8004","7433, 8004, 8009","$0.00","Parallel computing has entered the mainstream with increasingly large multicore processors and powerful accelerator devices. These compute engines, coupled with tighter integration of faster interconnection fabrics, are drivers for the next-generation high end computing (HEC) machines. However, the computing potential of HEC machines is delivered only through productive parallel program development and efficient parallel execution. This project enables application developers to improve performance on future HEC machines for their scientific and engineering processes. This project challenges the current model for parallel application development via ""black box"" tools and services. Instead, the project offers an open, transparent software infrastructure -- a Glass Box system -- for creating and tuning large-scale, parallel applications.  `Opening up' the tools and services used to create and evaluate peta- and exa-scale codes involves developing interfaces and methods that make tool-internal information and available for new performance management services that improve developer productivity and code efficiency.<br/><br/>The project will explore the information that can be shared 'across the software stack'.  Methods will be developed for analyzing program information, performance data and tool knowledge. The resulting Glass Box system will allow developers to better assess the performance of their parallel codes.  Tool creators can use the performance data to create new analysis and optimization techniques. System developers can also better manage multicore and machine resources at runtime, using JIT compilation and binary code editing to exploit the evolving hardware.  Working with the `Keeneland' NSF Track II machine and our industry partners, the project will create new performance monitoring tools, compiler methods and system-level resource management techniques. The effort is driven by the large-scale codes running on today's petascale machines.  Its broader impact is derived from the interactions with technology developers and application scientists as well as from its base in three universities with diverse student populations."
"1550223","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)","OAC","COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","07/06/2016","Ulrich Heinz","OH","Ohio State University","Standard Grant","Bogdan Mihaila","06/30/2021","$263,673.00","","heinz@mps.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7244, 8004","026Z, 7433, 7569, 8009, 8084","$0.00","Microseconds after the Big Bang, the universe was filled with an extremely hot fluid called the Quark-Gluon Plasma. As the universe expanded, this plasma cooled and condensed into the building blocks of ordinary matter around us: protons, neutrons, and atomic nuclei. Droplets of this fluid, which exists only at temperatures above 2 trillion Kelvin, are generated and studied in the laboratory today using collisions of high-energy heavy ions at Brookhaven National Laboratory and CERN. A key method to study the Quark-Gluon Plasma is the generation of high-energy quarks and gluons in the collision, which interact with the hot plasma and emerge as ""jets"" of particles that are measured by experiments. These jets provide powerful tools to study the internal structure of the plasma, analogous to tomography in medical imaging. However, interpretation of jet measurements requires sophisticated numerical modeling and simulation, and comparison of theory calculations with experimental data demands advanced statistical tools. The JETSCAPE Collaboration, an interdisciplinary team of physicists, computer scientists, and statisticians, will develop a comprehensive software framework that will provide a systematic, rigorous approach to meet this challenge. Training programs, workshops, summer schools and MOOCs, will disseminate the expertise needed to modify and maintain this framework.<br/><br/>The JETSCAPE Collaboration will develop a scalable and portable open source software package to replace a variety of existing codes. The modular integrated software framework will consist of interacting generators to simulate (i) wave functions of the incoming nuclei, (ii) viscous fluid dynamical evolution of the hot plasma, and (iii) transport and modification of jets in the plasma. Integrated advanced statistical analysis tools will provide non-expert users with quantitative methods to validate novel theoretical descriptions of jet modification, by comparison with the complete set of current experimental data. To improve the efficiency of this computationally intensive task, the collaboration will develop trainable emulators that can accurately predict experimental observables by interpolation between full model runs, and employ accelerators such as Graphics Processing Units (GPUs) for both the fluid dynamical simulations and the modification of jets. The collaboration will create this framework with a user-friendly envelope that allows for continuous modifications, updates and improvements of each of its components. The effort will serve as a template for other fields that involve complex dynamical modeling and comparison with large data sets. It will open a new era for high-precision extraction of the internal structure of the Quark-Gluon Plasma with quantifiable uncertainties. <br/><br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. <br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences."
"1642335","Collaborative Research: SI2-SSE: WRENCH: A Simulation Workbench for Scientific Workflow Users, Developers, and Researchers","OAC","Software Institutes","01/01/2017","09/12/2016","Rafael Ferreira da Silva","CA","University of Southern California","Standard Grant","Stefan Robila","12/31/2019","$240,000.00","","rafsilva@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Many scientific breakthroughs can only be achieved by performing complex processing of vast amounts of data efficiently.  In domains as crucial to our society as climate modeling, oceanography, particle physics, seismology, or computational biology (and in fact in most fields of physics, chemistry, and biology today), scientists nowadays routinely define ""scientific workflows"". These workflows are complex descriptions of scientific processes as data and inter-dependent computations on these data. When executed, typically with great expenses of computing, storage, and networking hardware, these workflows can produce groundbreaking results. A famous and recent example is the workflow that was used as part of the LIGO project to confirm the first detection of gravitational waves from colliding black holes. Scientific workflows are mainstays in today's science. Their efficient  execution (in terms of speed, reliability, and cost) is thus crucial. This project seeks to provide a software framework, called WRENCH (Workflow Simulation Workbench), that will make it possible to simulate large-scale hypothetical scenarios quickly and accurately on a single computer, obviating the need for expensive and time-consuming trial and error experiments. WRENCH potentially enables scientists to make quick and informed choices when executing their workflows, software developers to implement more efficient software infrastructures to support workflows, and researchers to develop novel efficient algorithms to be embedded within these software infrastructures.  In addition, WRENCH makes it possible to bring scientific workflow content into undergraduate and graduate computer science curricula. This is because meaningful knowledge can be gained by students using a single computer and the WRENCH software stack, making such learning possible even at institutions without access to high-end computing infrastructures, such as many non-Ph.D.-granting and minority-serving institutions. As a result, this work will contribute to producing computer science graduates better equipped to take an active role in the advancing of science.  Due to its potentially transformative impact on scientific workflow usage, development, research, and education, this project promises to promote the progress of science across virtually all its fields, ultimately resulting in broad and numerous benefits to our society.<br/><br/>Scientific workflows have become mainstream for conducting large-scale scientific research.  As a result, many workflow applications and Workflow Management Systems (WMSs) have been developed as part of the cyberinfrastructure to allow scientists to execute their applications seamlessly on a range of distributed platforms.  In spite of many success stories, building large-scale workflows and orchestrating their executions efficiently (in terms of performance, reliability, and cost) remains a challenge given the complexity of the workflows themselves and the complexity of the underlying execution platforms.  A fundamental necessary next step is the establishment of a solid ""experimental science"" approach for future workflow technology development. Such an approach is useful for scientists who need to design workflows and pick execution platforms, for WMS developers who need to compare alternate design and implementation options, and for researchers who need to develop novel decision-making algorithms to be implemented as part of WMSs.  The broad objective of this work is to provide foundational software, the Workflow Simulation Workbench (WRENCH), upon which to develop the above experimental science approach.  Capitalizing on recent advances in distributed application and platform simulation technology, WRENCH makes it possible to (i) quickly prototype workflow, WMS implementations, and decision-making algorithms; and (ii) evaluate/compare alternative options scalably and accurately for arbitrary, and often hypothetical, experimental scenarios.  This project will define a generic and foundational software architecture, that is informed by current state-of-the-art WMS designs and planned future designs.  The implementation of the components in this architecture when taken together form a generic ""scientific instrument"" that can be used by workflow users, developers, and researchers.  This scientific instrument will be instantiated for several real-world WMSs and used for a range of real-world workflow applications. In a particular case-study, it will be used with a popular WMS (Pegasus) to revisit published results and scheduling algorithms in the area of workflow planning optimizations. The objective is to demonstrate the benefit of using an experimental science approach for WMS research.  Another impact of this project is that it  makes it possible to include scientific workflow content pervasively in undergraduate and graduate computer science curricula, even for students without any access to computing infrastructure, by defining meaningful pedagogic activities that only require a computer and the WRENCH software stack. This educational impact will be demonstrated in the classroom in both undergraduate and graduate courses at our institutions."
"1535086","SI2-SSE: Human- and Machine-Intelligent Software Elements for Cost-Effective Scientific Data Digitization","OAC","ADVANCES IN BIO INFORMATICS, Software Institutes","08/01/2015","04/22/2016","Andrea Matsunaga","FL","University of Florida","Standard Grant","Bogdan Mihaila","07/31/2020","$488,048.00","Mauricio Tsugawa","ammatsun@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","1165, 8004","7433, 8005","$0.00","In the era of data-intensive scientific discovery, Big Data scientists in all communities spend the majority of their time and effort collecting, integrating, curating, transforming, and assessing quality before actually performing discovery analysis. Some endeavors may even start from information not being available and accessible in digital form, and when it is available, it is often in non-structured form, not compatible with analytics tools that require structured and uniformly-formatted data. Two main methods to deal with the volume and variety of data as well as to accelerate the rate of digitization have been to apply crowdsourcing or machine-learning solutions. However, very little has been done to simultaneously take advantage of both types of solutions, and to make it easier for different efforts to share and reuse developed software elements. The vision of the Human- and Machine-Intelligent Network (HuMaIN) project is to accelerate scientific data digitization through fundamental advances in the integration and mutual cooperation between human and machine processing in order to handle practical hurdles and bottlenecks present in scientific data digitization. Even though HuMaIN concentrates on digitization tasks faced by the biodiversity community, the software elements being developed are generic in nature, and expected to be applicable to other scientific domains (e.g., exploring the surface of the moon for craters require the same type of crowdsourcing tool as finding words in text, and the same questions of whether machine-learning tools could provide similar results can be tested).<br/><br/>The HuMaIN project proposes to conduct research and develop the following software elements: (a) configurable Machine-Learning  applications for scientific data digitization (e.g., Optical Character Recognition and Natural Language Processing), which will be made automatically available as RESTful services for increasing the ability of HuMaIN software elements to interoperate with other elements while decreasing the software development time via a new application specification language; (b) workflows leading to a cyber-human coordination system that will take advantage of feedback loops (e.g., based on consensus of crowdsourced data and its quality) for self-adaptation to changes  and increased sustainability of the overall system, (c) new crowdsourcing micro-tasks with ability of being reusable for a variety of scenarios and containing user activity sensors for studying time-effective user interfaces, and (d) services to support automated creation and configuration of crowdsourcing workflows on demand to fit the needs of individual groups. A cloud-based system will be deployed to provide the necessary execution environment with traceability of service executions involved in cyber-human workflows, and cost-effectiveness analysis of all the software elements developed in this project will provide assessment and evaluation of long standing what-if scenarios pertaining human- and machine-intelligent tasks. Crowdsourcing activities will attract a wide range of users with tasks that require low expertise, and at the same time it will expose volunteers to applied science and engineering, potentially attracting interest of K-12 teachers and students."
"1534949","SI2-SSE: Open OnDemand: Transforming Computational Science through Omnidisciplinary Software Cyberinfrastructure","OAC","Special Projects - CCF, Software Institutes","07/01/2015","07/29/2018","David Hudak","OH","Ohio State University","Standard Grant","Stefan Robila","01/31/2019","$558,225.00","Douglas Johnson","dhudak@osc.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","2878, 8004","026Z, 7433, 8005, 8009","$0.00","Supercomputing, or High-Performance Computing (HPC), has the power to advance work in all fields of science and engineering. Unfortunately, the impact of HPC is often limited simply because the computers themselves are difficult to use and scientists and engineers would rather spend their time advancing their disciplines than learn HPC.  Learning esoteric commands is a hurdle to many students and researchers when they first begin to work with traditional HPC systems, which has contributed to the relatively small proportions of women, minorities, and persons with disabilities within related STEM fields. The Open OnDemand project addresses this problem through innovative software that makes HPC no more difficult than using a desktop computer or a web site, hence reducing that initial learning curve. Open OnDemand will provide an enhanced infrastructure for research and education, in that students and educators will be exposed to the same tools and techniques on smaller departmental and classroom systems that they'll eventually utilize at larger HPC centers. This streamlines the pipeline for developing a more globally competitive STEM workforce that is prepared to dive right into computational problems and resources once they graduate from academia.  Open OnDemand also allows scientists and engineers to make specialized domain-specific workflows and expertise available to more collaborators and users, both from academia and industry.  This should result in increased partnerships, and the transfer of more technologies from the public to private sectors.<br/><br/>The web has become the dominant access mechanism for remote compute services in every computing area except high-performance computing (HPC). Accessing HPC cyberinfrastructure (CI) resources today, either at the campus or national level, typically requires  advanced knowledge of UNIX, familiarity with command-line interfaces and installation and configuration of custom client software.  Web applications in HPC today do exist in the form of science gateways. However, gateways have not proliferated in part due to the development and administrative overheads required for each individual gateway.  These factors demonstrate an accessibility gap for HPC.  Open OnDemand is an open source platform for HPC and remote computing access that addresses the accessibility gap. Open OnDemand will be a public release based on the successful OSC OnDemand platform.  OSC OnDemand is a web platform providing Ohio Supercomputer Center (OSC) users integrated access to HPC systems, web applications, and VNC services.  OSC OnDemand has been in production since January 2013, has over 800 distinct users from 27 different NSF fields of science, and its apps have been launched over 70,000 times.  In addition to easing access to HPC services, Open OnDemand centralizes web app overheads, easing support for custom visualization and science gateway apps. Open OnDemand has the ability to transform cyberinfrastructure by providing a platform to enable a new delivery method for scientific web tools like HubZero apps, XSEDE Science Gateways, iPython notebooks and workflow tools like Pegasus.   <br/><br/>Under this project, the Open OnDemand platform will be created by (a) transitioning the existing OSC OnDemand software to a community developed project hosted on GitHub, (b) extending the per-user web server to serve Rails apps, (c) replacing the custom-developed Proxy with an existing open source project like NGINX and (d) replacing the existing Java-based VNC client with an HTML5 solution.  File usage will be improved by updating or replacing AjaXplorer and integrating high performance file transfer functions (sftp and Globus Online).  The accessibly apps will be updated by by (a) upgrading or replacing AnyTerm, (b) updating Job Constructor and (c) integrating Open XDMoD and SUPREMM for job and cluster performance metrics. In addition, community infrastructure will be created including (a) system administrator documentation and discussion forums and (b) training materials based on existing OSC OnDemand materials.  A beta program will be conducted including (a) assisting beta sites with installs, (b) updating training materials to include new functionality, (c) beginning metric reporting and (d) documenting a Galaxy case study for app integration.  HPC center staff will be engaged by through the ""boot camps"" and ""train the trainer"" sessions. Finally, Open OnDemand will be proposed as a project to the XSEDE Campus Bridging Technology Insertion Service. These activities will help meet the following objectives: (1) Transition OSC OnDemand to a community-developed open source software package called Open OnDemand, (2) Improve the interface capabilities of Open OnDemand by updating and expanding the accessibility apps, including integration of Globus Online and Open XDMoD projects. (3) Conduct a program to engage departmental, campus and national HPC users and administrators on enhancing HPC inclusivity through Open OnDemand and (4) Leverage Open OnDemand as a platform to support existing web-based applications such XSEDE Science Gateways and HubZero applications based on our experience supporting a data-intensive biomedical package (the Galaxy Project)."
"1613155","SI2-SSE Collaborative Research: Molecular Simulations of Polymer Nanostructures in the Cloud","OAC","DMR SHORT TERM SUPPORT, Software Institutes, CDS&E","10/22/2015","11/10/2015","Coray Colina","FL","University of Florida","Standard Grant","Alan Sussman","09/30/2018","$149,283.00","","colina@chem.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","1712, 8004, 8084","024E, 085E, 7237, 7433, 8005, 8400, 9216","$0.00","The use of polymers, their composites, and nanostructures is growing at a fast pace, both by displacing traditional materials and by enabling emerging technologies. Examples range from the all-composite airframe of the Boeing 787 and the new Airbus 350 to wearable electronics. This project aims to develop a software infrastructure to simulate these materials with atomic resolution and make these tools universally accessible and useful via on-line computing. These simulations have the potential to accelerate the development of optimized material formulations that can benefit society and reduce the associated costs by combining physical with computational experiments. Making these advanced tools available for free online simulations and complementing them with tutorials and educational material will encourage their use in the classroom and will impact the education of new generations of engineers and scientists familiar with these powerful tools that will be required to address tomorrow's challenges.<br/><br/>The objective of this effort is to enable pervasive, reproducible molecular simulations of polymeric materials using state-of-the-art tools and with quantified uncertainties building on recent breakthroughs in molecular simulations, cyber-infrastructure and uncertainty quantification. The framework will consist of three main components: i) powerful simulation tools for polymer nano structures including: state-of-the-art molecular builders, a parallel MD engine with stencils that enable efficient structure relaxation and property characterization and post-processing codes; ii) a UQ framework to orchestrate the molecular simulations and propagate uncertainties in input parameters to predictions and compare the predictions to experimental values; iii) databases of force fields and molecular structures as well as predicted and experimental properties. The simulation framework will be deployed via NSF's nanoHUB where users will be able to run online simulations without downloading or installing any software while expert users will have the option to download, modify and contribute to the infrastructure. Usage of and contributions to the software framework will be facilitated and encouraged via online and in-person user guides, learning modules and research tutorials."
"1147337","SI2-SSE Collaborative Research: SPIKE-An Implementation of a Recursive Divide-and-Conquer Parallel Strategy for Solving Large Systems of Linear Equations","OAC","OFFICE OF MULTIDISCIPLINARY AC, DYNAMICAL SYSTEMS, Software Institutes, CDS&E-MSS","06/01/2012","06/20/2012","Dan Negrut","WI","University of Wisconsin-Madison","Standard Grant","Rajiv Ramnath","05/31/2015","$251,119.00","","negrut@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","1253, 7478, 8004, 8069","7433, 8005","$0.00","Drs. Negrut, Sameh, and Knepley will investigate, produce, and maintain a methodology and its software implementation that leverage emerging heterogeneous hardware architectures to solve billion-unknowns linear systems in a robust, scalable, and efficient fashion. The two classes of problems targeted under this project are banded dense and sparse general linear systems.<br/><br/>This project is motivated by the observation that the task of solving a linear system is one of the most ubiquitous ingredients in the numerical solution of Applied Mathematics problems. It is relied upon for the implicit integration of Ordinary Differential Equation (ODE) and Differential Algebraic Equation (DAE) problems, in the numerical solution of Partial Differential Equation (PDE) problems, in interior point optimization methods, in least squares approximations, in solving eigenvalue problems, and in data analysis. In fact, the vast majority of nonlinear problems in Scientific Computing are solved iteratively by drawing on local linearizations of nonlinear operators and the solution of linear systems. Recent advances in (a) hardware architecture; i.e., the emergence of General Purpose Graphics Processing Unit (GP-GPU) cards, and (b) scalable solution algorithms, provide an opportunity to develop a new class of parallel algorithms, called SPIKE, which can robustly and efficiently solve very large linear systems of equations.<br/><br/>Drawing on its divide-and-conquer paradigm, SPIKE builds on several algorithmic primitives: matrix reordering strategies, dense linear algebra operations, sparse direct solvers, and Krylov subspace methods. It provides a scalable solution that can be deployed in a heterogeneous hardware ecosystem and has the potential to solve billion-unknown linear systems in the cloud or on tomorrow?s exascale supercomputers. Its high degree of scalability and improved efficiency stem from (i) optimized memory access pattern owing to an aggressive pre-processing stage that reduces a generic sparse matrix to a banded one through a novel reordering strategy; (ii) good exposure of coarse and fine grain parallelism owing to a recursive, divide-and-conquer solution strategy; (iii) efficient vectorization in evaluating the coupling terms in the divide-and-conquer stage owing to a CPU+GPU heterogeneous computing approach; and (iv) algorithmic polymorphism, given that SPIKE can serve both as a direct solver or an effective preconditioner in an iterative Krylov-type method.<br/><br/>In Engineering, SPIKE will provide the Computer Aided Engineering (CAE) community with a key component; i.e., fast solution of linear systems, required by the analysis of complex problems through computer simulation. Examples of applications that would benefit from this technology are Structural Mechanics problems (Finite Element Analysis in car crash simulation), Computational Fluid Dynamics problems (solving Navier-Stokes equations in the simulation of turbulent flow around a wing profile), and Computational Multibody Dynamics problems (solving Newton-Euler equations in large granular dynamics problems).<br/><br/>SPIKE will also be interfaced to the Portable, Extensible Toolkit for Scientific Computation (PETSc), a two decades old flexible and scalable framework for solving Science and Engineering problems on supercomputers. Through PETSc, SPIKE will be made available to a High Performance Computing user community with more than 20,000 members worldwide. PETSc users will be able to run SPIKE without any modifications on vastly different supercomputer architectures such as the IBM BlueGene/P and BlueGene/Q, or the Cray XT5. SPIKE will thus run scalably on the largest machines in the world and will be tuned for very different network and hardware topologies while maintaining a simple code base.<br/><br/>The experience collected and lessons learned in this project will augment a graduate level class, ?High Performance Computing for Engineering Applications? taught at the University of Wisconsin-Madison. A SPIKE tutorial and research outcomes will be presented each year at the International Conference for High Performance Computing, Networking, Storage and Analysis. A one day High Performance Computing Boot Camp will be organized each year in conjunction with the American Society of Mechanical Engineers (ASME) conference and used to disseminate the software outcomes of this effort. Finally, this project will shape the research agendas of two graduate students working on advanced degrees in Computational Science."
"1147910","Collaborative Research SI2-SSE: Sustained Innovation in Acceleration of Molecular Dynamics on Future Computational Environments: Power to the People in the Cloud and on Accelerator","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, CHEMISTRY PROJECTS, Chem Thry, Mdls & Cmptnl Mthds, Software Institutes","06/01/2012","12/17/2012","Adrian Roitberg","FL","University of Florida","Standard Grant","Rajiv Ramnath","05/31/2015","$227,634.00","","roitberg@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","1253, 1712, 1991, 6881, 8004","7237, 7433, 7569, 7573, 7683, 8005, 9216, 9232, 9263","$0.00","This collaborative project between the San Diego Supercomputer Center at the University of California San Diego, the Quantum Theory Project at the University of Florida and industrial partners NVIDIA, Intel and Amazon is focused on developing innovative, comprehensive open source software element libraries for accelerating condensed phase Molecular Dynamics (MD) simulations of biomolecules using next generation accelerator hardware including Intel's MIC system and Graphics Processing Units (GPU). It will extend support to include all major MD techniques and develop open source accelerated analysis libraries. A priority is enhanced sampling techniques including Thermodynamic Integration, constant pH algorithms, Multi-Dimensional Hamiltonian Replica Exchange and Metadynamics. These elements will then be combined, in collaboration with Amazon to support MD as-a-service through easily accessible web front ends to cloud services, including Amazon's EC2 GPU hardware. Transitioning large scale MD workflows from requiring access to large supercomputer hardware to being accessible to all on desktop and cloud resources provides the critical software infrastructure to support transformative research in the fields of chemistry, life science, materials science, environmental and renewable energy.<br/><br/>The software elements created through this project have an extremely broad impact. The integration of comprehensive support for next generation hardware acceleration into the AMBER software alone benefits a very large user base. With over 10,000 downloads of the latest AMBER Tools package from unique IPs and >800 sites using the AMBER MD engines testify to the scope of the community of researchers this work impacts. The development of simple web based front ends for use of elastically scalable cloud resources makes simulations routine for all researchers. Meanwhile education and outreach efforts train the next generation of scientists not just in how to use the MD acceleration libraries and advanced MD simulation techniques developed here but also gets them thinking about how their approach can be transformed given that performance that was previously restricted to large scale supercomputers is now available on individual desktops."
"1265849","SI2-CHE: Collaborative Research: Developing First Principles Monte Carlo Methods for Reactive Phase and Sorption Equilibria in the CP2K Software Suite","CHE","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","04/15/2013","04/02/2013","Joern Ilja Siepmann","MN","University of Minnesota-Twin Cities","Standard Grant","Evelyn Goldfield","03/31/2017","$349,250.00","","siepmann@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","MPS","1253, 8004","5918, 5946, 5950, 7433, 8009, 8650","$0.00","An international research team consisting of Ilja Siepmann, Ben Lynch (University of Minnesota), Neeraj Rai (Mississippi State University), Troy Van Voorhis (Massachusetts Institute of Technology), Ben Slater (University College London), Michiel Sprik (University of Cambridge), Adam Carter (Edinburgh Parallel Computing Centre), Jrg Hutter (University of Zurich), I-Feng Kuo (Lawrence Livermore National Laboratory), Christopher Mundy (Pacific Northwest National Laboratory), Joost VandeVondele (ETH Zurich), and Rodolphe Vuilleumier (University Pierre & Marie Curie Paris) is collaborating to develop and implement new theoretical methods in the CP2K computational chemistry software suite.  These new methodologies enable the predictive modeling of reactive multi-phase systems, including free energy landscapes and product yields, where the system interactions are described by Kohn-Sham density functional theory with van der Waals and hybrid functionals.  Markov chain Monte Carlo approaches utilizing smart moves with asymmetric underlying matrices, such as the aggregation-volume-bias and configurational-bias Monte Carlo methods, and the Gibbs ensemble framework are employed for efficient exploration of the phase space for reactive single- and multi-phase equilibria in bulk and in confinement.  The U.S. based research team is supported jointly by the Chemistry Division in MPS and the Office of Cyberinfrastucture. Funds for the UK based research team are provided by the EPSRC.<br/><br/>The software infrastructure is advanced by the development of efficient and accurate methodologies for reactive phase and sorption equilibria that are applicable to chemical processes in diverse science and engineering applications.  The extensively validated methodologies will be incorporated into the open-source CP2K software suite to make them available to a large user base.  The new software can be used to identify optimal reaction conditions and separation processes for sustainable chemistry.  The collaborative research team plans to use the new software for the investigation of reactive processes that address critical needs of society (fertilizers for food supply, fuels from renewable sources, and environmentally benign chemical processes)."
"1216890","Collaborative Research: SI2-S2I2: High-Performance Computational Science with Structured Meshes and Particles (HPCS-SMP)","OAC","OFFICE OF MULTIDISCIPLINARY AC, CFS-Combustion & Fire Systems, PMP-Particul&MultiphaseProcess, FD-Fluid Dynamics, PHYSICS AT THE INFO FRONTIER, Integrat & Collab Ed & Rsearch, Software Institutes","09/01/2012","08/28/2012","Phillip Colella","CA","University of California-Berkeley","Standard Grant","Daniel Katz","08/31/2014","$366,050.00","James Demmel","pcolella@lbl.gov","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","1253, 1407, 1415, 1443, 7553, 7699, 8004","056E, 058E, 1253, 1407, 1415, 1443, 7433, 7483, 7553, 7699, 8004, 8211, 9216, 9263","$0.00","The starting point for this proposal is a view of scientific simulation articulated in the conclusions of the 2008 National Academy of Sciences Study, The Potential Impact of High-End Capability Computing on Four Illustrative Fields of Science and Engineering: ""Advanced computational science and engineering is a complex enterprise that requires models, algorithms, software, hardware, facilities, education and training, and a community of researchers attuned to its special needs."" (p. 122)<br/><br/>Over the last few years, the design of computer and software systems, particularly as they relate to simulation in the physical sciences, has been organized around a collection of algorithmic patterns / motifs. These patterns have been very productive because they are a natural ""common language"" in which application scientists can express their computations, and for which computer scientists can provide optimized libraries, domain specific languages, compilers, and other software tools.<br/><br/>This project will design an institute focused on a subset of these patterns --- structured grid discretizations of partial differential equations and particle methods, along with the linear and nonlinear solvers that enable their effective use --- with the specific goals of providing simulation capabilities for a set of scientific domains that make heavy use of these patterns. Two major components are envisioned to this proposed institute, called the Institute for High-Performance Computational Science with Structured Meshes and Particles (HPCS-SMP). The first component is a software infrastructure development activity that will be performed by a team whose expertise spans the design and development of mathematical algorithms and software frameworks, as well as the design and development of compilers, runtime systems, and tools that enable one to obtain high performance from emerging multicore and heterogeneous architectures. The second component is an outreach activity, in which algorithms, libraries, and software frameworks developed by the institute will be customized and integrated into simulation codes for stakeholder application domains. At the heart of this activity will be collaborations and partnerships, in which the institute will provide one or more software developers to collaborate with application scientists over a period of months to years to develop a new simulation capability or enhance an existing one.<br/><br/>The design of this institute will be carried out through a series of workshops, each focused on one of five stakeholder science domains that have been identified as using these motifs and that play a central role in various NSF Grand Challenge problems, with participation of both representatives of the science domain and the the relevant mathematics and computer science communities. In addition, there will be a final workshop that will bring together the relevant mathematics and computer science experts to identify cross-cutting themes. These information obtained from these workshops will be used by the project to develop the final conceptual design of the institute, in the form of a document that includes the input from all of the workshops and our analysis of how this leads to a  design of a software institute."
"1147247","Collaborative Research: SI2-SSE: Sage-Combinat: Developing and Sharing Open Source Software for Algebraic Combinatorics","OAC","OFFICE OF MULTIDISCIPLINARY AC, OPERATIONS RESEARCH, DYNAMICAL SYSTEMS, Software Institutes, CDS&E-MSS","06/01/2012","06/19/2012","Anne Schilling","CA","University of California-Davis","Standard Grant","Rajiv Ramnath","05/31/2016","$216,626.00","","anne@math.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","1253, 5514, 7478, 8004, 8069","1253, 5514, 7433, 7478, 7683, 8004, 8005","$0.00","Sage is an open source general purpose mathematical software system that has developed explosively within the last six years. Sage-Combinat is a subproject whose mission is ""to improve Sage as an extensible toolbox for computer exploration in (algebraic) combinatorics, and foster code sharing between researchers in this area"".  There is a long tradition of software packages for algebraic combinatorics. These have been crucial in the development of combinatorics since the 1960s.  The originality of the Sage-Combinat project lies in successfully addressing the following simultaneous objectives. It offers a wide variety of interoperable and extensible tools, integrated in a general purpose mathematical software package, as needed for daily computer exploration in algebraic combinatorics; it is developed by a community of researchers spread around the world and across institutions; and it is open source and depends only on open source software. Among the proposers, Stein is founder and lead developer of Sage while Bump, Musiker, and Schilling are strong contributors to Sage-Combinat. Hivert and Thiery (Paris-Sud, Orsay), founders and lead developers of Sage-Combinat, are both strongly affiliated with this project. Some of the research areas addressed in this project include symmetric functions, in particular Macdonald polynomials for arbitrary Cartan types and their nonsymmetric analogues, crystals, rigged configurations and combinatorial R-matrices, affine Weyl groups and Hecke algebras, cluster algebras, and posets.<br/> <br/>The project will develop Sage-Combinat in areas relevant to the ongoing research of the participants, together with relevant underlying infrastructure. The project will include three Sage Days workshops, and will be affiliated with a third scheduled workshop at ICERM. These workshops include a strong outreach component and have been a potent tool for connecting researchers and recruiting Sage users and developers. The grant will also fund a dedicated software development and computation server for Sage-Combinat, to be hosted in the Sage computation farm in Seattle. Emphasis will be placed on the development of thematic tutorials that will make the code accessible to new users. The proposal will also fund graduate student RA support, curriculum development, and other mentoring."
"1147892","SI2-SSI Collaborative Research:  A Computational Materials Data and Design Environment","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, CHEMISTRY PROJECTS, Software Institutes","10/01/2012","09/20/2018","Kristin Persson","DC","Department of Energy","Interagency Agreement","Stefan Robila","09/30/2017","$249,127.00","","kapersson@lbl.gov","1000 Independence Avenue, SW","Washington","DC","205850002","","CSE","1253, 1712, 1991, 8004","1253, 1712, 1982, 1991, 7237, 7433, 7569, 7644, 8004, 8009, 9216, 9263","$0.00",""
"1148124","SI2:SSE-Collaborative Research: Advanced Software Infrastructure for Biomechanical Inverse Problems","OAC","Mechanics of Materials and Str, BMMB-Biomech & Mechanobiology, Software Institutes","06/01/2012","06/20/2012","Paul Barbone","MA","Trustees of Boston University","Standard Grant","Rajiv Ramnath","05/31/2016","$240,450.00","","barbone@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","1630, 7479, 8004","1630, 7479, 8004, 8005","$0.00","Biomechanical imaging refers to the remote measurement of the mechanical properties of tissues, in-situ and in-vivo. Images of the tissue can be thus created by visualizing the mechanical property distributions. This technique relies on imaging tissue while it is deformed by a set of externally applied forces. Through image processing, the displacement field everywhere in the region of interest is inferred. An inverse problem for the relevant mechanical properties is then solved, given the measured displacement fields, an assumed form of the tissue's constitutive equation, and the law of conservation of momentum. Images of reconstructed parameters find applications in the detection, diagnosis and treatment monitoring of disease, and in designing patient specific models for surgical training and planning. Over the last decade we have developed a software package (NLACE) to solve this inverse problem eciently in different application domains. Through this award we will make enhancements to NLACE that will make it easier to utilize and modify, and extend its user base to a wider community.<br/><br/>The specific tasks for enhancing NLACE can be divided into two categories: (a) Steps to transition from a working prototype of NLACE to a software resource for the community. These include establishing an Input/Output standard for NLACE, creation of a GUI for input data and for monitoring the progress of the solution, hosting NLACE distributions, and creating sets of test data and documentation for its release. (b) Tasks that would enhance the functional capability of NLACE. These include the creation of a user-defined hyperelastic material model module to address a large class of tissue and material types, parallelization of NLACE on distributed memory, shared memory and GPU platforms, and quantifying uncertainty in the spatial distribution of the reconstructed parameters. We will measure our progress through user-feedback obtained during annual validation tests performed by a committed focus user-group that will test all aspects of the proposed research.<br/><br/>The proposed improvements to NLACE will further its application in the detection, diagnosis and treatment monitoring of diseases, generation of patient-specic models for surgical planning and image-guidance applications, and studies in biomechanics and mechanobiology. The parallel and uncertainty quantification strategies developed for NLACE can be applied to a broad class of inverse problem with PDE constraints, including acoustic and electromagnetic scattering, seismic inversion, diffuse optical tomography, aquifer permeability and thermometry. Our outreach plans ensure the dissemination of NLACE to our focus group and to a broader community through hosting on the Simtk NIH center website. Finally, two graduate students will be trained in the elds of computational science and mathematics and biomechanics, and results from the proposed research will be presented at conferences on computational and biomedical science and engineering."
"1148116","Collaborative Research: SI2-SSI: Open Source Support for Massively Parallel, Generic Finite Element Methods","OAC","Software Institutes","08/01/2012","08/09/2012","Wolfgang Bangerth","TX","Texas A&M Research Foundation","Standard Grant","Rajiv Ramnath","05/31/2017","$1,311,834.00","Yassin Hassan","bangerth@colostate.edu","400 Harvey Mitchell Parkway, S","College Station","TX","778454375","9798626777","CSE","8004","7433, 8004, 8009","$0.00","Partial differential equations are used in a wide variety of applications as<br/>mathematical models. Their numerical solution is, consequently, of prime<br/>importance for the accurate simulation and optimization of processes in the<br/>sciences, engineering, and beyond.<br/>The last decade saw the emergence of large and successful libraries that<br/>support such applications. While these libraries provide most of what such<br/>codes need for small-scale computations, many realistic applications yield<br/>problems of hundreds of millions or billions of unknowns and require clusters<br/>with thousands of processor cores, but there is currently little generic<br/>support for such problems, limiting access to the many large publicly<br/>supported computing facilities to experts in computational science and<br/>excluding scientists from many fields for whom computational simulation would<br/>be a useful tool.  This project intends to build the software infrastructure that will allow a<br/>wide cross section of scientists to utilize these large resources.<br/><br/><br/>This project intends to support the software infrastructure for the<br/>large-scale solution of partial differential equations on massively parallel<br/>computational resources in a generic way. It will build on two of the most<br/>successful libraries for scientific computing, the finite element library<br/>deal.II, and Trilinos that provides the parallel linear algebra capabilities<br/>for the former. Specifically, we will: (i) Make support for massively parallel<br/>computations ubiquitous in deal.II; (ii) Research and develop seamless support<br/>for problems with billions of unknowns in both libraries and improve the<br/>interaction between the two; (iii) Exploit intra-node parallelism on today's<br/>clusters; (iv) Ensure the applicability of our work on a broad basis by<br/>implementing two real-world applications. <br/>Both deal.II and Trilinos have large, active and diverse developer and user<br/>communities, and this project will actively engage these communities through<br/>user meetings, short courses, regularly taught classes, mailing lists, and<br/>direct contact in focused projects."
"2037661","COLLABORATIVE RESEARCH: EAGER: Towards Building a CyberInfrastructure for Facilitating the Assessment, Dissemination, Discovery, & Reuse of Software and Data Products","OAC","Software Institutes","09/01/2020","07/22/2020","Ritu Ritu","TX","University of Texas at San Antonio","Standard Grant","Ashok Srinivasan","02/28/2023","$175,000.00","","ritu@wayne.edu","1 UTSA CIRCLE","SAN ANTONIO","TX","782491644","2104584340","CSE","8004","077Z, 7916, 8004","$0.00","Over the last several years, the projects funded through the various NSF programs, such as the Cyberinfrastructure for Sustained Scientific Innovation (CSSI), Data Infrastructure Building Blocks (DIBBs), and Software Infrastructure for Sustained Innovation (SI2) programs, have resulted in innovative software and data products with broad societal impacts. Collecting the information on the short-term and long-term impact of these products on their intended user communities in terms of quantifiable metrics can be important for future funding decisions, and hence is in national interest. However, collecting such information can be a challenging task given the diversity of the NSF-funded products, their usage environments, and their target audiences. Additionally, when a product is composed of (or integrated with) other products, it can be difficult to capture the provenance trail of all the embedded products, which impacts the process of gathering the metrics necessary in evaluating their success. Moreover, the knowledge of the entire technology stack used in a product can enable other developers or adopters of that product in analyzing the code reuse and integration cost. When analyzing the feasibility of integrating software products, or interoperating with them, or extending them, it is also important to check the compatibility of their licenses and software stacks so that one can determine if the products can interoperate legally and seamlessly, and if the derived products can be disseminated as intended. It can be time-consuming to carefully review and understand the impact of the licenses of the base products on any derived product, or to check if one product can co-exist or interoperate with another product. Hence, having a central and a publicly accessible infrastructure for (1) tracking the metrics of the NSF-funded products, (2) checking their license and software stack compatibility, and (3) discovering the software stack and its evolution, can be useful for quantifying the societal impacts of the NSF-funded products and in promoting their dissemination.<br/> <br/>The overarching goal of this project is to develop a software infrastructure for facilitating the assessment, discovery, dissemination, and reuse of publicly accessible software and data products. As a preliminary step towards meeting this goal, this project has initiated research and development activities for prototyping: (1) iTracker: the software infrastructure for tracking the user-defined metrics of products released and deployed on different platforms & computing environments, (2) CompChecker: a license and software-stack compatibility checker for advising the users on the feasibility of integrating or interoperating with existing products, and (3) Discovery Catalog: a prototype of a catalog of NSF-funded products which can display the most recent information captured by iTracker for each product of interest and integrate CompChecker as a feature. The project demonstrates the use of block-chain for securely storing an immutable copy of the metadata related to the cataloged products and this metadata can in turn be useful for tracking the evolution of the products during their life cycle. The project demonstrates the infrastructure required for identifying and promoting the relevant metrics for evaluating different categories of products. The project has the potential of encouraging the developer community to adopt best practices for product dissemination and will likely foster cross-disciplinary collaborations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2037656","COLLABORATIVE RESEARCH: EAGER: Towards Building a CyberInfrastructure for Facilitating the Assessment, Dissemination, Discovery, & Reuse of Software and Data Products","OAC","Software Institutes","09/01/2020","07/22/2020","Subhashini Sivagnanam","CA","University of California-San Diego","Standard Grant","Ashok Srinivasan","08/31/2023","$124,585.00","","sivagnan@sdsc.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","CSE","8004","077Z, 7916, 8004","$0.00","Over the last several years, the projects funded through the various NSF programs, such as the Cyberinfrastructure for Sustained Scientific Innovation (CSSI), Data Infrastructure Building Blocks (DIBBs), and Software Infrastructure for Sustained Innovation (SI2) programs, have resulted in innovative software and data products with broad societal impacts. Collecting the information on the short-term and long-term impact of these products on their intended user communities in terms of quantifiable metrics can be important for future funding decisions, and hence is in national interest. However, collecting such information can be a challenging task given the diversity of the NSF-funded products, their usage environments, and their target audiences. Additionally, when a product is composed of (or integrated with) other products, it can be difficult to capture the provenance trail of all the embedded products, which impacts the process of gathering the metrics necessary in evaluating their success. Moreover, the knowledge of the entire technology stack used in a product can enable other developers or adopters of that product in analyzing the code reuse and integration cost. When analyzing the feasibility of integrating software products, or interoperating with them, or extending them, it is also important to check the compatibility of their licenses and software stacks so that one can determine if the products can interoperate legally and seamlessly, and if the derived products can be disseminated as intended. It can be time-consuming to carefully review and understand the impact of the licenses of the base products on any derived product, or to check if one product can co-exist or interoperate with another product. Hence, having a central and a publicly accessible infrastructure for (1) tracking the metrics of the NSF-funded products, (2) checking their license and software stack compatibility, and (3) discovering the software stack and its evolution, can be useful for quantifying the societal impacts of the NSF-funded products and in promoting their dissemination.<br/> <br/>The overarching goal of this project is to develop a software infrastructure for facilitating the assessment, discovery, dissemination, and reuse of publicly accessible software and data products. As a preliminary step towards meeting this goal, this project has initiated research and development activities for prototyping: (1) iTracker: the software infrastructure for tracking the user-defined metrics of products released and deployed on different platforms & computing environments, (2) CompChecker: a license and software-stack compatibility checker for advising the users on the feasibility of integrating or interoperating with existing products, and (3) Discovery Catalog: a prototype of a catalog of NSF-funded products which can display the most recent information captured by iTracker for each product of interest and integrate CompChecker as a feature. The project demonstrates the use of block-chain for securely storing an immutable copy of the metadata related to the cataloged products and this metadata can in turn be useful for tracking the evolution of the products during their life cycle. The project demonstrates the infrastructure required for identifying and promoting the relevant metrics for evaluating different categories of products. The project has the potential of encouraging the developer community to adopt best practices for product dissemination and will likely foster cross-disciplinary collaborations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1621324","STAR Metrics Workshop on Software and Data Citation and Attribution","OAC","Software Institutes, STAR Metrics","02/15/2016","02/11/2016","Piotr Sliz","MA","Harvard University","Standard Grant","Rajiv Ramnath","08/31/2017","$40,694.00","Merce Crosas","piotr_sliz@hms.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","CSE","8004, 8022","7433, 7556, 8004","$0.00","The creation and dissemination of new and useful research tools and data is critical to scientific progress. This workshop will bring together groups from such diverse and varied science research domains as biomedicine, physical sciences, computer science, linguistics, information science, and the social sciences to collectively discuss areas of interest, share findings and methods, and coordinate future goals and planning related to improving the norms and practices for scientific software and data citation and attribution. This work will contribute to properly recognizing and rewarding those who create and add value to the data and software tools relied on in scientific research. <br/><br/> <br/>Participants in this workshop hail from highly diverse and scientifically distinct areas and fields of research, and as such, have relatively few opportunities to interact face-to-face to share ideas, exchange knowledge and coordinate efforts. This workshop will offer an opportunity for cross-domain coordination, interaction and collaboration between a diverse set of researchers committed to the common goal of improving the systems and methods of citation and attribution of scientific software and data. Workshop participants will engage in extensive attendee-wide discussions on topics related to tools and resources, cross-domain convergence of efforts, and previous and ongoing community efforts regarding software and data citation in science research. Participants will also engage directly with external experts invited to give their perspectives and facilitate community discussion. Written reports suitable for publication from all sessions and panels will be prepared by meeting participants at the culmination of the workshop. <br/><br/>"
"1440715","CC*IIE Integration: RADII: Resource Aware DatacentrIc CollaboratIon Infrastructure","OAC","Information Technology Researc","10/01/2014","09/08/2014","Ilya Baldin","NC","University of North Carolina at Chapel Hill","Standard Grant","Kevin Thompson","09/30/2017","$853,658.00","Claris Castillo, Charles Schmitt, Arcot Rajasekar","ibaldin@renci.org","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","CSE","1640","8002","$0.00","Science collaborations today are multi-institutional and multi-disciplinary, requiring processing of large datasets that are distributed among their participants. They also require advanced computing and networking infrastructure to process the data and arrive at results. A fundamental problem with the way these collaborations operate today lies in the fact that they are structured around the ownership of data or necessary pieces of infrastructure, rather than around the talents and capabilities of participating organizations. This project is developing mechanisms for mapping collaborations onto the new type of dynamically configurable, deeply networked institutional cloud infrastructure, where data is a first-class resource. By integrating data and resource management into a single system RADII aims at improving the productivity of research scientists and reducing time to discovery and operating costs.<br/><br/>RADII leverages two previous NSF investments: dynamic infrastructure technologies developed for NSF GENI initiative and the iRODS data grid system for maintaining large distributed datasets.  The project is developing software tools to represent science collaborations using formal modeling mechanisms, map data processes, computations and storage onto physical infrastructure and flexibly manage the underlying infrastructure to optimize its utilization across multiple collaborations. The project uses several examples of collaborations from genomics to demonstrate how such collaborations can map onto infrastructure in a more flexible and cost effective way compared to today."
"1147041","Travel Grant Proposal to establish a US-UK Collaboration on Adaptive Collective Variables","OAC","Software Institutes","10/01/2011","08/29/2011","Cecilia Clementi","TX","William Marsh Rice University","Standard Grant","Daniel Katz","06/30/2012","$24,400.00","","cecilia@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","8004","","$0.00","This award will enable an international collaboration for software development in computational chemistry between researchers in the United States and the United Kingdom. This international collaboration arose from a joint NSF/EPSRC workshop on Software Development for Grand Challenges in the Chemical Sciences held in Oxford, United Kingdom in June 2011. This award will provide travel for project team members to attend plenary and planning meetings, and for exchange visits to catalyze initial subprojects.  The project will lead to the develop of new software capabilities for the study of protein systems across multiple resolutions. The collaboration will include the sharing of large datasets and the development of computational infrastructure for controlling simulations. Outcomes of this award will include software for computational chemistry and improved international collaboration in support of sustainable software for science and engineering. Interdisciplinary training of young researchers is an important consideration for this work."
"2314202","COLLABORATIVE RESEARCH: EAGER: Towards Building a CyberInfrastructure for Facilitating the Assessment, Dissemination, Discovery, & Reuse of Software and Data Products","OAC","Software Institutes","01/01/2023","01/12/2023","Ritu Ritu","MI","Wayne State University","Standard Grant","Ashok Srinivasan","10/31/2023","$48,623.00","","ritu@wayne.edu","5057 WOODWARD STE 13001","DETROIT","MI","482024050","3135772424","CSE","8004","077Z, 7916, 8004","$0.00","Over the last several years, the projects funded through the various NSF programs, such as the Cyberinfrastructure for Sustained Scientific Innovation (CSSI), Data Infrastructure Building Blocks (DIBBs), and Software Infrastructure for Sustained Innovation (SI2) programs, have resulted in innovative software and data products with broad societal impacts. Collecting the information on the short-term and long-term impact of these products on their intended user communities in terms of quantifiable metrics can be important for future funding decisions, and hence is in national interest. However, collecting such information can be a challenging task given the diversity of the NSF-funded products, their usage environments, and their target audiences. Additionally, when a product is composed of (or integrated with) other products, it can be difficult to capture the provenance trail of all the embedded products, which impacts the process of gathering the metrics necessary in evaluating their success. Moreover, the knowledge of the entire technology stack used in a product can enable other developers or adopters of that product in analyzing the code reuse and integration cost. When analyzing the feasibility of integrating software products, or interoperating with them, or extending them, it is also important to check the compatibility of their licenses and software stacks so that one can determine if the products can interoperate legally and seamlessly, and if the derived products can be disseminated as intended. It can be time-consuming to carefully review and understand the impact of the licenses of the base products on any derived product, or to check if one product can co-exist or interoperate with another product. Hence, having a central and a publicly accessible infrastructure for (1) tracking the metrics of the NSF-funded products, (2) checking their license and software stack compatibility, and (3) discovering the software stack and its evolution, can be useful for quantifying the societal impacts of the NSF-funded products and in promoting their dissemination.<br/> <br/>The overarching goal of this project is to develop a software infrastructure for facilitating the assessment, discovery, dissemination, and reuse of publicly accessible software and data products. As a preliminary step towards meeting this goal, this project has initiated research and development activities for prototyping: (1) iTracker: the software infrastructure for tracking the user-defined metrics of products released and deployed on different platforms & computing environments, (2) CompChecker: a license and software-stack compatibility checker for advising the users on the feasibility of integrating or interoperating with existing products, and (3) Discovery Catalog: a prototype of a catalog of NSF-funded products which can display the most recent information captured by iTracker for each product of interest and integrate CompChecker as a feature. The project demonstrates the use of block-chain for securely storing an immutable copy of the metadata related to the cataloged products and this metadata can in turn be useful for tracking the evolution of the products during their life cycle. The project demonstrates the infrastructure required for identifying and promoting the relevant metrics for evaluating different categories of products. The project has the potential of encouraging the developer community to adopt best practices for product dissemination and will likely foster cross-disciplinary collaborations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1930025","Assessment and Evaluations of NSF OAC-Funded Program Impact on the Scientific Community","OAC","CYBERINFRASTRUCTURE, Software Institutes","09/01/2019","01/24/2020","Changwon Suh","MD","Nexight Group LLC","Standard Grant","Alexis Lewis","06/30/2020","$52,458.00","","csuh@nexightgroup.com","1100 WAYNE AVE","SILVER SPRING","MD","209105642","2406677636","CSE","7231, 8004","7231","$0.00","Cyberinfrastructure (CI) is essential to the advancement and transformation of science and engineering. Over the last decade, the CI community has focused on developing secure, advanced, scalable, and global CI resources, tools, and services, creating an interoperable and collaborative CI ecosystem. In order to move forward strategically, it is critical to understand the impact that CI programs have had on the scientific research community. This award supports an effort to develop an understanding of this impact, by working with the Cyberinfrastructure community to understand and enhance the impact of programs such as NSF's Cyberinfrastructure for Sustained Scientific Innovation (CSSI) program and its predecessors. To this end, the PIs will gather relevant information, conduct a targeted survey, and prepare a report describing findings and mapping out the future directions of CI research.<br/><br/>This work will provide new understanding of the impact of recent Data and Software Programs, through a systematic cross-cutting survey-based assessment. The assessment will seek to evaluate the impact of the activities funded under the Data Infrastructure Building Blocks (DIBBs), Software Infrastructure for Sustained Innovation (SI2), and Cyberinfrastructure for Sustained Scientific Innovation (CSSI) programs. The approach is to (1) determine the scope of the activity and establish a framework for design of the survey (2) design and administer the survey and (3) analyze the results and synthesize the findings into a report. The outcomes will benefit the scientific research community broadly, by providing insights into the most effective methods of research support in Cyberinfrastrucutre, and broadening the impact of advanced Cyberinfrastructure research on the science and engineering domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"0953484","Inspiration, Frustration, and Fascination:  An Excursion into Low-Oxidation State Main Group Chemistry","CHE","Chemical Synthesis","02/01/2010","01/30/2012","Gregory Robinson","GA","University of Georgia Research Foundation Inc","Continuing Grant","Tingyu Li","01/31/2014","$500,000.00","Yuzhong Wang","robinson@chem.uga.edu","310 EAST CAMPUS RD TUCKER HALL R","ATHENS","GA","306021589","7065425939","MPS","6878","9146, MANU","$0.00","This research award in the Chemical Synthesis (SYN) program supports work by Professor Gregory H. Robinson at The University of Georgia to probe the synthesis of a series of provocative low-oxidation state main group compounds. Importantly, sterically demanding N-heterocyclic carbene ligands will be utilized as stabilizing agents in this project. Major goals include: (a) the synthesis of various low-oxidation state boron compounds; (b) the preparation of interesting phosphorus anions and main group clusters; and (c) the synthesis of carbene-stabilized dimetallic complexes. It is expected that the fundamental discoveries that will result from this work will augment and expand our knowledge of highly reactive diatomic molecules such as B2, P2, and Si2. In addition, the carbene-stabilization of highly reactive molecules suggests unprecedented opportunities in synthetic chemistry from catalysis to materials chemistry. <br/> <br/>Undergraduate students, graduate students, and postdoctoral fellows (from diverse backgrounds) engaged in this work will gain valuable experience in basic synthetic chemistry, the manipulation of air-sensitive compounds, single-crystal X-ray diffractometry, and basic computational chemistry. <br/>"
"9358367","NSF Young Investigator","ECCS","EPMD-ElectrnPhoton&MagnDevices, METALS, CERAMICS, & ELEC MATRS, ELECTRONIC/PHOTONIC MATERIALS","08/15/1993","05/27/1997","Gentry Crook","WI","University of Wisconsin-Madison","Continuing Grant","Lawrence Goldberg","05/31/1999","$233,665.00","","crook@engr.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","ENG","1517, 1715, 1775","0000, 1775, 9161, 9162, 9165, 9231, 9297, AMPP, OTHR","$0.00","9358367  Crook  The research performed with this NSF Young Investigator Award involves growth by molecular beam epitaxy (MBE) of novel semiconductor structures for electronic and photonic device applications.  One current research program examines the synthesis and properties of interfaces, superlattices, and alloys formed by combing elemental and III-V compound semiconductors.  The Young Investigators Award will enhance this program by providing funding for additional experiments to examine the properties of novel materials such as (GaAs)1-x(Si2)x alloys and GaAs/Si short-period superlatties.  A research program in planar devices formed by patterning of two-dimensional electron gas structures grown by MBE will be initiated during the period of the award.  Interaction with industry will be pursed for research in these areas, as well as on related topics such as reproducible growth conditions for MBE.  A course on compound semiconductor devices will be developed and first taught in the Fall of 1993, and other new courses will be developed in subsequent years.  ***"
"1556253","IDBR: TYPE A- The ThruProt Analyzer: Bringing Proteomics to the Field Using a Sample-to-Answer Electronic Multiplexed Platform","DBI","INSTRUMENTAT & INSTRUMENT DEVP","04/01/2016","04/06/2018","Mehdi Javanmard","NJ","Rutgers University New Brunswick","Continuing Grant","Robert Fleischmann","03/31/2021","$344,942.00","Paul Falkowski, Debashish Bhattacharya","mehdi.javanmard@rutgers.edu","3 RUTGERS PLZA","NEW BRUNSWICK","NJ","089018559","8489320150","BIO","1108","","$0.00","An award is made to Rutgers University New Brunswick to develop a battery powered handheld sensitive instrument whose purpose is to detect panels of proteins for field based environmental monitoring. A portable device for protein analysis virtually impacts all fields of biology and also the biomedical sciences. Protein analysis of environmental samples typically requires collecting and storing samples and returning weeks to months later in labs to begin analyzing the data. The proposed instrument will instead allow for point-of-use on the field analysis of biological samples. The result of this research will be an instrument that is handheld and generalizable to the needs of individual biological laboratories. This disruptive tool will be valuable for basic biology, as well as clinical, biotechnological, and agricultural research. This tool will also have greater societal benefits including improving agricultural practices and deeper insights into environmental biology, which is necessary for protecting the environment. This is multidisciplinary work that combines engineering, nanofabrication, chemistry, physics, and biology and provides a great opportunity to educate and train graduate students, undergraduates, and high school students.  One of the important educational outcomes of the proposed research includes building teaching modules that will be incorporated into a massive open online course (MOOC) in biosensor and bioinstrumentation development. Collaborations will be developed with the Rutgers Office for Diversity and Academic Success in the Sciences to attract under-represented students to the study of STEM disciplines.<br/><br/>The proposed sample-to-answer portable protein analyzer to be developed will utilize 1) on-chip microfluidic sample preparation, 2) decoupled digital protein detection for quantification of low abundance proteins 3) nanoelectronic barcoding of beads for enabling multiplexing capability, and 4) on-chip magnet based fluidic peristaltic pumping for maintaining an ultra-compact footprint. Nanoelectronic barcoding works by fabricating tunable nano-capacitors on the microparticle surface, effectively modulating the frequency dependent dielectric properties of the particles allowing one bead barcode to be distinguished from another, potentially on the scale of 100-fold multiplexing. By using electronic based solutions for all of the key modules of the proposed instrument, the cost and size of the readout instrumentation can be significantly reduced, thus truly enabling on-the-field analysis. The devices will be validated in the field with environmental samples, benchmarking for detection limit, accuracy, and assay time for a panel of photosynthesis proteins from the green alga Picochlorum. The results will be shared with the engineering and experimental biology communities via presentations at professional meetings and submission to peer-reviewed journals. The research value of the instrument will be broadly disseminated through partnerships with the Rutgers BioMAPS Institute for Quantitative Biology and the Rutgers Center for Integrated Proteomic Research. To prepare for commercialization of the technology and broad distribution, there will be close collaboration with the Rutgers Center for Innovative Ventures of Emerging Technologies and the Rutgers Office of Technology Commercialization."
"1405303","Theoretical Spectroscopy and Thermodynamics for Correlated Electron Materials","DMR","CONDENSED MATTER & MAT THEORY","09/01/2014","07/13/2016","Kristjan Haule","NJ","Rutgers University New Brunswick","Continuing Grant","Daryl Hess","08/31/2018","$300,000.00","","haule@physics.rutgers.edu","3 RUTGERS PLZA","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1765","7433, 7569, 7644, 8084","$0.00","NON-TECHNICAL SUMMARY<br/><br/>This award supports theoretical and computational research and education directed towards a transformative acceleration of progress in our understanding of complex materials, which show prominent competitions of quantum mechanical effects such as strong electron interaction, magnetism and superconductivity.  For simpler materials for which the physical properties can be accurately represented in terms of a system of independent particles in the presence of an average potential, methods based on Density Functional Theory have enabled large-scale simulations of realistic systems with high accuracy. For materials in which electrons interact strongly and the ""independent-particle"" picture is not as reliable, the so-called Dynamical Mean Field Theory method has enabled practical and accurate calculations of their basic properties.  This project is aimed at developing a number of new theoretical tools, which can be used in combination with electronic structure tools based on Dynamical Mean Field Theory to enable theoretical prediction of material properties without using empirical parameters.<br/><br/>The project will enable the construction of a predictive framework for describing the physical properties of materials in which electron-electron interactions play a very important role. The tools and codes developed in this project will allow one to theoretically characterize complex materials, which will lead to improved scientific understanding of quantum many-body effects, and will provide a basis for harnessing such effects to develop functional materials such as strong magnets and novel superconductors. The educational component of this project involves the training of the next generation of scientists in an interdisciplinary environment at the intersection of theoretical physics, computational physics, and materials science. The PI will also be involved in public outreach activities by mentoring and providing summer research experiences for high school students through the Liberty Science Center in New Jersey.<br/><br/>TECHNICAL SUMMARY<br/><br/>To search for new materials with enhanced physical properties it is crucial to develop capabilities for computational characterization of a material. This award supports theoretical and computational research and education directed toward developing a number of theoretical spectroscopic tools, which can be used in combination with electronic structure tools based on Dynamical Mean Field Theory to enable theoretical prediction of material properties using first principles methods. <br/><br/>The spectroscopic tools to be developed will be used for (i) computing dynamical structure factors measured in neutron spectroscopy experiments, (ii) predicting the symmetry of the superconducting order parameter in unconventional superconductors, (iii) Auger spectroscopy, which can be used to measure the strength of correlations and to theoretically estimate the strength of the Coulomb interaction, (iv) Raman spectroscopy which can identify the low frequency excitations of the solid, and (v) computing the free energy of a solid for studying phase transitions at finite temperatures. <br/><br/>The project will enable the construction of a predictive framework for describing the physical properties of correlated materials. The tools and codes developed in this project will allow one to theoretically characterize complex materials, which will lead to improved scientific understanding of quantum many-body effects, and will provide a basis for harnessing such effects to develop functional materials such as strong magnets and novel superconductors. The educational component of this project involves the training of the next generation of scientists in an interdisciplinary environment at the intersection of theoretical physics, computational physics, and materials science. The PI will also be involved in public outreach activities by mentoring and providing summer research experiences for high school students through the Liberty Science Center in New Jersey."
"1252238","EarthCube Domain End-User Workshop:  Engaging the Critical Zone community to bridge long tail science with big data","EAR","EarthCube","10/01/2012","09/26/2012","Anthony Aufdenkampe","PA","Stroud Water Research Center","Standard Grant","Enriqueta Barrera","09/30/2013","$99,922.00","Christopher Duffy, Gregory Tucker","aaufdenkampe@limno.com","970 SPENCER RD","AVONDALE","PA","193119514","6102682153","GEO","8074","7433","$0.00","Critical Zone (CZ) scientists take as their charge the effort to integrate theory, models and data from the multitude of disciplines studying processes on the Earth's surface - from the atmosphere at the vegetation's canopy to the lower boundary of actively cycling ground waters.  As such, critical zone scientists and their data managers are at the front line of efforts to effectively compile and use the ""dark data in the Long Tail"" of earth science and integrate that data with the ""Big Data"" produced by hydrologists, atmospheric scientists, geospatial modelers and molecular biologists. <br/> <br/>The NSF EarthCube initiative recently solicited proposals for domain workshops ""designed to listen to the needs of the end-user groups that make up the geosciences and to understand better how data-enabled science can help them achieve their scientific goals.""  The proponents will convene a workshop to bring together critical zone domain scientists with computer scientists active in EarthCube. <br/><br/>This workshop would thus serve two objectives: (1) engage approximately 45 cyber-literate critical zone scientists in the EarthCube process; and (2) inform about 20 of EarthCube's cyberscientists of the diversity needs of CZ science.  The overall goal of the workshop would be to develop a set of unifying requirements for the integration of ""long tail"" data and ""big data"" and to develop an interactive community of domain and cyber scientists to pursue solutions.  <br/><br/>There are many examples of how cyber-infrastructure developed for geoscientists have broader impacts to the public.  The national weather service data and model forecasts are highlighted on television and other media outlets.  Fishermen, rafters and canoeists rely on USGS gauging data for their recreational activities.  The Model My Watershed platform is harnessing GIS and hydrological modeling for educational purposes in classrooms and informal settings and also by citizen scientists."
"1043843","Scientific Software Security Innovation Institute (S3I2)","OAC","Software Institutes","08/01/2010","07/24/2010","Randal Butler","IL","University of Illinois at Urbana-Champaign","Standard Grant","Daniel Katz","07/31/2012","$49,862.00","James Basney, William Barnett, Douglas Pearson","r-butler@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","8004","7556","$0.00","The scientific enterprise in America requires enterprise class security to assure competitive advantage and efficiently advance discovery and invention.  Across NSF funded activities there is pervasive and increasing concern with the availability, integrity, and confidentiality of scientific workflows, resources, data, and personal information.  Threats include data loss and corruption due to archive failures or malicious attacks, release of private data maliciously or accidentally, or loss of computational or network capacity due to system software failure or compromise. The good news is that there are a number of common security solutions that support NSF researchers today however support for these is limited and often left to the individual development or project teams<br/><br/>This NSF-sponsored workshop will explore the potential for a cross-disciplinary Scientific Software Security Innovation Institute (S3I2) to address the protection, integrity, and reliability of research software, systems, and information. The goal of this workshop is to identify the needs for, and models of, an S3I2 to address secure scientific cyberinfrastructure in the United States. To accomplish this goal, the workshop will bring together representatives from NSF funded projects, researchers, developers, and resource providers.  The workshop will cover the topics of: research security needs; existing tools, systems, processes and organizations that secure research activities and data; outstanding issues to be addressed in research assurance; and organization and operational models for a future security institute targeting the identified security needs.  The workshop will result in an analysis of these needs and solutions and discuss the advantages of potential models for a S3I2 to address America?s research assurance needs."
