Project_name,Funding_agency,Award_number,PI_name,PI_contact,Keyword,Description
Towards the Future Direction of the NSF Program on the Cyberinfrastructure for Sustained Scientific Innovation (CSSI),OAC,2034617,Ritu Ritu,ritu@wayne.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","The NSF Cyberinfrastructure for Sustained Scientific Innovation (CSSI) program has been critical in not only supporting the development and deployment of innovative software and data products, but also in several workforce development and community-building initiatives. The projects funded through the CSSI program have promoted the nation's competitiveness and leadership in the fields of data, High-Performance Computing (HPC), networking, cybersecurity, software, and workforce development. A workshop on the ""Future Direction of the NSF CSSI Program"" can be instrumental in gathering the community-feedback on the current scope and objectives of the CSSI program and guiding its future direction such that the nation's leadership in the aforementioned fields is maintained. The goal of the workshop on the ""Future Direction of the NSF CSSI Program"" is to bring together researchers and practitioners from the industry, academia, and government laboratories to assess the impact of the CSSI program, share best practices in supporting sustainable software and data products in present and future, and identify gaps (if any) in the scope of the CSSI program and needs of the community. A report resulting from this workshop will help inform NSF of future directions for the CSSI program. The workshop will strengthen multi-disciplinary collaborations on developing the cyberinfrastructure (software and data) for supporting research and innovation in different fields of science and technology. This project is critical for enabling the organization of the aforementioned workshop and supporting the participation of a diverse range of researchers and practitioners in the development of the future CSSI solicitations. Some participants may need financial support for traveling to the workshop. This project will help in supporting the travel of such participants.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Towards the Future Direction of the NSF Program on the Cyberinfrastructure for Sustained Scientific Innovation (CSSI),OAC,1946194,Ritu Ritu,ritu@wayne.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","The NSF Cyberinfrastructure for Sustained Scientific Innovation (CSSI) program has been critical in not only supporting the development and deployment of innovative software and data products, but also in several workforce development and community-building initiatives. The projects funded through the CSSI program have promoted the nation's competitiveness and leadership in the fields of data, High-Performance Computing (HPC), networking, cybersecurity, software, and workforce development. A workshop on the ""Future Direction of the NSF CSSI Program"" can be instrumental in gathering the community-feedback on the current scope and objectives of the CSSI program and guiding its future direction such that the nation's leadership in the aforementioned fields is maintained. The goal of the workshop on the ""Future Direction of the NSF CSSI Program"" is to bring together researchers and practitioners from the industry, academia, and government laboratories to assess the impact of the CSSI program, share best practices in supporting sustainable software and data products in present and future, and identify gaps (if any) in the scope of the CSSI program and needs of the community. A report resulting from this workshop will help inform NSF of future directions for the CSSI program. The workshop will strengthen multi-disciplinary collaborations on developing the cyberinfrastructure (software and data) for supporting research and innovation in different fields of science and technology. This project is critical for enabling the organization of the aforementioned workshop and supporting the participation of a diverse range of researchers and practitioners in the development of the future CSSI solicitations. Some participants may need financial support for traveling to the workshop. This project will help in supporting the travel of such participants.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Towards the Future Direction of the NSF Program on the Cyberinfrastructure for Sustained Scientific Innovation (CSSI),OAC,2314201,Ritu Ritu,ritu@wayne.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","The NSF Cyberinfrastructure for Sustained Scientific Innovation (CSSI) program has been critical in not only supporting the development and deployment of innovative software and data products, but also in several workforce development and community-building initiatives. The projects funded through the CSSI program have promoted the nation's competitiveness and leadership in the fields of data, High-Performance Computing (HPC), networking, cybersecurity, software, and workforce development. A workshop on the ""Future Direction of the NSF CSSI Program"" can be instrumental in gathering the community-feedback on the current scope and objectives of the CSSI program and guiding its future direction such that the nation's leadership in the aforementioned fields is maintained. The goal of the workshop on the ""Future Direction of the NSF CSSI Program"" is to bring together researchers and practitioners from the industry, academia, and government laboratories to assess the impact of the CSSI program, share best practices in supporting sustainable software and data products in present and future, and identify gaps (if any) in the scope of the CSSI program and needs of the community. A report resulting from this workshop will help inform NSF of future directions for the CSSI program. The workshop will strengthen multi-disciplinary collaborations on developing the cyberinfrastructure (software and data) for supporting research and innovation in different fields of science and technology. This project is critical for enabling the organization of the aforementioned workshop and supporting the participation of a diverse range of researchers and practitioners in the development of the future CSSI solicitations. Some participants may need financial support for traveling to the workshop. This project will help in supporting the travel of such participants.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Organizing CSSI PI Meeting - Towards a National Cyberinfrastructure Ecosystem,OAC,2006409,Haiying (Helen) Shen,hs6ms@virginia.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","This project will host a 2-day workshop in Seattle, WA, which will bring together the community of Cyberinfrastructure for Sustained Scientific Innovation (CSSI) awardees (with the goal of involving at least one principal investigator (PI) from each Elements, Frameworks, Institute Conceptualizations, and Scientific Software Innovation Institutes project, many of which are collaborative awards) from approximately 250 awards. This will be the first CSSI PI meeting. In addition, PIs from the prior connected solicitations (such as Software Infrastructure for Sustained Innovation (SI2) and Data Infrastructure Building Blocks (DIBBS)) will be invited, as well as PIs on NSF awards in which CSSI seed investments were made (Venture funded PIs as well as CSSI Early Concept Grants for Exploratory Research (EAGER) awardees). In addition, the proximity to Society for Industrial and Applied Mathematics (SIAM) Conference on Parallel Processing for Scientific Computing (PP20) will encourage participation by non-PI community to further inform the academic community of CSSI goals and projects. Goals of this workshop include: (1) Serve as a focused forum for PIs to share technical information with each other, community, and NSF Program Officers; (2) Explore innovative topics emerging within software and data infrastructure communities; (3) Discuss emerging best practices across the supported software and data infrastructure projects; (4) Stimulate thinking on new ways of achieving software and data sustainability; (5) Gather the shared experiences in an online web portal. The workshop is expected to host close to 250 CSSI and other awardees, other speakers and panelists. The proposed workshop will support the exchange of ideas among the current cyberinfrastructure development projects. It will provide guidance on issues related to the development of robust software and data artifacts and to the problem of their sustainability. Involvement of program officers across NSF is expected to help the interdisciplinary CSSI awardees understand the relevance and impact of cyberinfrastructure throughout the NSF. The participation of these awardees, program officers, and other researchers in a common forum will help ensure that the cyberinfrastructure developed as part of CSSI projects will be relevant and broadly applicable to most science and engineering domains. The results of this workshop thus have the potential to guide cyberinfrastructure development and cyberinfrastructure driven research for both the participating projects and for the wider cyberinfrastructure development community.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
2022 NSF CSSI PI Meeting,OAC,2231431,Kerstin Lehnert,lehnert@ldeo.columbia.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","This project will host a 2-day workshop in Alexandria, VA which will bring together the community of Cyberinfrastructure for Sustained Scientific Innovation (CSSI) awardees. The meeting will help to (1) showcase the diversity of projects within the CSSI program, specifically with respect to their scientific applications; (2) significantly advance networking among funded CSSI projects; (3) analyze lessons learned pertaining to sustainability of project outcomes and products; (4) identify requirements and leading practices for data integrity and provenance; (5) create vision and identify tangible steps for creating a diverse and inclusive workforce for CI; and (6) develop recommendations for a sustainable organizing committee process and future meeting structures. The meeting will foster networking and cross-pollination between CSSI and CSSI-related projects and their larger communities to broaden awareness about ongoing developments in this community, identify synergies, catalyze new collaborations, and advance knowledge transfer, coordination, and convergence among these projects. The meeting will serve as a focused forum for PIs to share technical information with each other and NSF Program Officers; to explore innovative topics emerging within software and data infrastructure communities; to discuss emerging best practices and solutions across the supported software and data infrastructure projects; to stimulate thinking on new ways of achieving software and data sustainability; to gather the shared experiences in an online web portal; and to identify ways to increase diversity in the CSSI and broader CI community.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Assessment and Evaluations of NSF OAC-Funded Program Impact on the Scientific Community,OAC,1930025,Changwon Suh,csuh@nexightgroup.com,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Cyberinfrastructure (CI) is essential to the advancement and transformation of science and engineering. Over the last decade, the CI community has focused on developing secure, advanced, scalable, and global CI resources, tools, and services, creating an interoperable and collaborative CI ecosystem. In order to move forward strategically, it is critical to understand the impact that CI programs have had on the scientific research community. This award supports an effort to develop an understanding of this impact, by working with the Cyberinfrastructure community to understand and enhance the impact of programs such as NSF's Cyberinfrastructure for Sustained Scientific Innovation (CSSI) program and its predecessors. To this end, the PIs will gather relevant information, conduct a targeted survey, and prepare a report describing findings and mapping out the future directions of CI research.This work will provide new understanding of the impact of recent Data and Software Programs, through a systematic cross-cutting survey-based assessment. The assessment will seek to evaluate the impact of the activities funded under the Data Infrastructure Building Blocks (DIBBs), Software Infrastructure for Sustained Innovation (SI2), and Cyberinfrastructure for Sustained Scientific Innovation (CSSI) programs. The approach is to (1) determine the scope of the activity and establish a framework for design of the survey (2) design and administer the survey and (3) analyze the results and synthesize the findings into a report. The outcomes will benefit the scientific research community broadly, by providing insights into the most effective methods of research support in Cyberinfrastrucutre, and broadening the impact of advanced Cyberinfrastructure research on the science and engineering domains.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
II-NEW: Infrastructure Acquisition for Research and Education in Statistical Design,CNS,958480,Ronald Blanton,blanton@ece.cmu.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Design and analysis of integrated systems (microprocessors, SoCs, ASICs, memories, MEMS, etc.) is increasingly difficult due to the extreme variations that exist both in the fabrication process and the operating environment. As a result, it is no longer viable to simply perform worst-case analysis during design. Instead, the distribution of designs resulting from these variations must be efficiently and accurately modeled. Compounding this need is the increasing scale of integration. With larger on-chip memories, growing use of diverse computing and communication blocks, and increasing heterogeneity, it is imperative that these systems be well characterized and ""statistically"" designed up to and even beyond ""six sigma"". The Center for Silicon System Implementation (CSSI) at Carnegie Mellon is focused on all aspects of statistical design and analysis that spans from system-level architectures to analog/digital circuits, to the physics and modeling of the complexities found in semiconductor manufacturing. Using the shared compute infrastructure acquired with support from the NSF Computing Research Infrastructure (CRI) program, CSSI is developing various statistical design and analysis methodologies that enable continued use of scaled-CMOS technology. CSSI faculty lead over 70 different research projects in various areas related to integrated system design and manufacturing, and instruct over 20 different graduate/senior-level courses that cover these same areas. The ability to develop and validate new methodologies for statistical design, and train the next-generation of integrated systems researchers and practitioners is being significantly enhanced by the computer infrastructure acquired through support of the NSF CRI program."
CSSI Elements: EWMS - Event Workflow Management Service,OAC,2103963,Benedikt Riedel,briedel@icecube.wisc.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","This award will begin developing an Observation Management System Framework which will help alleviating multiple types of astrophysical data and respective workflow management - a burden shouldered now by human researchers who process large numbers of independent pieces of data (?events?). The core element of this framework - Event Workflow Management System (EWMS) - is a workload manager designed for processing events (simulated readouts from a particle physics detector, recorded data points, images, etc.) with complex workflows. EWMS could become a central core for applying novel computer science methods to improve scientific data processing with an initial focus on Multi-Messenger Astrophysics (MMA) - it will transform how ?event?-based computational problems are tackled on the national CI ecosystem through a set of reusable services. This paradigm is applicable across a wide range of science domains, including astrophysics, astronomy, physics, biology, and other disciplines that deal with Big Data flows. The EWMS will benefit three of NSF?s 10 big ideas ? ?Growing Convergence Research,? ?Harnessing the Data Revolution,? and ?Windows on the Universe"", bringing together data from the particle physics-based detectors (IceCube Neutrino Observatory, High Altitude Water Cherenkov Observatory, Cherenkov Telescope Array), traditional astronomical observatories (large telescopes), and gravitational wave observatories (LIGO, VIRGO, KAGRA). While the data types of each of these experiments or observatories are dramatically different, they all record data in the independent spatio-temporal increments, observations, triggers, events, etc. that are processed and stored separately. The EWMS is applicable to nearly all current workflows in MMA experiments, and by combining these most precise observations via EWMS, scientists will be able to observe the Universe in fundamentally new ways and learn more about its history than any one of these messengers can provide in isolation. Thus, this award addresses and advances the science objectives and goals of the NSF's ""Windows on the Universe: The Era of Multi-Messenger Astrophysics"" program.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Windows on the Universe Program, the Physics at the Information Frontier Program, the IceCube Science Program, the Antarctic Astrophysics and Geospace Sciences Program, and the Office of Polar Programs CyberInfrastructure Program.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: CSSI Frameworks: SAGE3: Smart Amplified Group Environment for Harnessing the Data Revolution,OAC,2004014,Jason Leigh,leighj@hawaii.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","The Big Data revolution necessitates the use of sophisticated tools such as Artificial Intelligence (AI) and Data Visualization to harness the sheer volume, velocity and variety of datasets that are becoming the norm. However, it is the research community that must make sense of the data being amassed, so cyberinfrastructure must extend to people. SAGE3 (Smart Amplified Group Environment) puts the human in the loop by providing scientists with an intuitive framework that integrates state-of-the-art AI technologies with applications, workflows, smart visualizations and collaboration services to help them access, share, explore and analyze their data, come to conclusions, and make decisions with greater speed, accuracy, comprehensiveness and confidence. SAGE3 augments every step of the scientific discovery enterprise - from quickly summarizing large data, to finding trends and similarities or anomalies among one or more linked datasets, to communicating findings to scientists, public policy and government officials, and the general public, to educating the next-generation workforce. Ultimately, it is the scientists and future scientists who must Harness the Big Data revolution to solve the nation's grand challenge problems that will benefit society as a whole - from studying the diversity of life on Earth, to understanding the Earth and its systems from satellite imagery of its poles, to developing response scenarios for natural disasters such as landslides and pandemics that impact the citizens and economies of the world. SAGE3 development focuses on two fundamental components: AI-enhanced smart services and advanced computing resource orchestration to support reproducible work models for secure collaborative work. SAGE3 amplifies user productivity, providing them with commercially available and open-source AI solutions, which autonomously and transparently analyze data while continually learning and improving through user interactions. SAGE3 makes AI technologies broadly accessible, not just a privilege for the technically savvy. SAGE3 further democratizes AI by using Data Visualization to help interpret and explain AI models so users better understand how AI came to its decisions, which engenders user trust and can help identify potentially prejudiced or biased models.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: CSSI Frameworks: SAGE3: Smart Amplified Group Environment for Harnessing the Data Revolution,OAC,2003800,Maxine Brown,maxine@uic.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","The Big Data revolution necessitates the use of sophisticated tools such as Artificial Intelligence (AI) and Data Visualization to harness the sheer volume, velocity and variety of datasets that are becoming the norm. However, it is the research community that must make sense of the data being amassed, so cyberinfrastructure must extend to people. SAGE3 (Smart Amplified Group Environment) puts the human in the loop by providing scientists with an intuitive framework that integrates state-of-the-art AI technologies with applications, workflows, smart visualizations and collaboration services to help them access, share, explore and analyze their data, come to conclusions, and make decisions with greater speed, accuracy, comprehensiveness and confidence. SAGE3 augments every step of the scientific discovery enterprise - from quickly summarizing large data, to finding trends and similarities or anomalies among one or more linked datasets, to communicating findings to scientists, public policy and government officials, and the general public, to educating the next-generation workforce. Ultimately, it is the scientists and future scientists who must Harness the Big Data revolution to solve the nation's grand challenge problems that will benefit society as a whole - from studying the diversity of life on Earth, to understanding the Earth and its systems from satellite imagery of its poles, to developing response scenarios for natural disasters such as landslides and pandemics that impact the citizens and economies of the world. SAGE3 development focuses on two fundamental components: AI-enhanced smart services and advanced computing resource orchestration to support reproducible work models for secure collaborative work. SAGE3 amplifies user productivity, providing them with commercially available and open-source AI solutions, which autonomously and transparently analyze data while continually learning and improving through user interactions. SAGE3 makes AI technologies broadly accessible, not just a privilege for the technically savvy. SAGE3 further democratizes AI by using Data Visualization to help interpret and explain AI models so users better understand how AI came to its decisions, which engenders user trust and can help identify potentially prejudiced or biased models.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Collaborative Research: CSSI: Framework: Data: Clowder Open Source Customizable Research Data Management, Plus-Plus",OAC,1835877,Barbara Minsker,minsker@smu.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Preserving, sharing, navigating, and reusing large and diverse collections of data is now essential to scientific discoveries in areas such as phenomics, materials science, geoscience, and urban science. These data navigation needs are also important when addressing the growing number of research areas where data and tools must span multiple domains. To support these needs effectively, new methods are required that simplify and reduce the amount of effort needed by researchers to find and utilize data, support community accepted data practices, and bring together the breadth of standards, tools, and resources utilized by a community. Clowder, an active curation based data management system, addresses these needs and challenges by distributing much of the data curation overhead throughout the lifecycle of the data, augmenting this with social curation and automated analysis tools, and providing extensible community-dependent means of viewing and navigating data. As an open source framework, built to be extensible at every level, Clowder is capable of interacting with and utilizing a variety of community tools while also supporting different data governance and ownership requirements.The project enhances Clowder's core systems for the benefit of a larger group of users. It increases the level of interoperability with community resources, hardens the core software, and distributes core software development, while continuing to expand usage. Governance mechanisms and a business model are established to make Clowder sustainable, creating an appropriate governance structure to ensure that the software continues to be available, supportable, and usable. The effort engages a number of stakeholders, taking data from diverse but converging scientific domains already using the Clowder framework, to address broad interoperability and cross domain data sharing. The overall effort will transition the grassroots Clowder user community and Clowder's other stakeholders (such as current and potential developers) into a larger organized community, with a sustainable software resource supporting convergent research data needs.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Collaborative Research: CSSI: Framework: Data: Clowder Open Source Customizable Research Data Management, Plus-Plus",OAC,1835834,Kenton McHenry,kmchenry@ncsa.uiuc.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Preserving, sharing, navigating, and reusing large and diverse collections of data is now essential to scientific discoveries in areas such as phenomics, materials science, geoscience, and urban science. These data navigation needs are also important when addressing the growing number of research areas where data and tools must span multiple domains. To support these needs effectively, new methods are required that simplify and reduce the amount of effort needed by researchers to find and utilize data, support community accepted data practices, and bring together the breadth of standards, tools, and resources utilized by a community. Clowder, an active curation based data management system, addresses these needs and challenges by distributing much of the data curation overhead throughout the lifecycle of the data, augmenting this with social curation and automated analysis tools, and providing extensible community-dependent means of viewing and navigating data. As an open source framework, built to be extensible at every level, Clowder is capable of interacting with and utilizing a variety of community tools while also supporting different data governance and ownership requirements.The project enhances Clowder's core systems for the benefit of a larger group of users. It increases the level of interoperability with community resources, hardens the core software, and distributes core software development, while continuing to expand usage. Governance mechanisms and a business model are established to make Clowder sustainable, creating an appropriate governance structure to ensure that the software continues to be available, supportable, and usable. The effort engages a number of stakeholders, taking data from diverse but converging scientific domains already using the Clowder framework, to address broad interoperability and cross domain data sharing. The overall effort will transition the grassroots Clowder user community and Clowder's other stakeholders (such as current and potential developers) into a larger organized community, with a sustainable software resource supporting convergent research data needs.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
CSSI: Frameworks: X-Ion Collisions with a Statistically and Computationally Advanced Program Envelope (X-SCAPE),OAC,2004571,Abhijit Majumder,abhijit.majumder@wayne.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","High-energy colliders around the world currently study the production of, and interaction between, a variety of subatomic particles and environments produced in the collision of protons with protons, with nuclei, or even between two nuclei, as at the Large Hadron Collider (LHC) at the European Center for Nuclear Research (CERN), or at the Relativistic Heavy-Ion Collider (RHIC) at Brookhaven National Laboratory (BNL). The future Electron Ion Collider (EIC), slated for construction at BNL, will study collisions of electrons with protons and nuclei. Interactions at all these experiments are dominated by the strong nuclear force, whose behavior is still not well understood. Direct comparison between theory and experiment requires sophisticated computer simulations, where each stage of a collision is modeled via a combination of established principles or candidate theories. A complete simulation, consisting of a number of sub-simulations, depends on several input parameters. The extracted values for these parameters represent fundamental properties of strongly interacting matter. A simultaneous determination of these parameters, in extensive comparisons with volumes of data from diverse experiments, requires an elaborate statistical and computational framework. The X-SCAPE collaboration, a multi-disciplinary team of physicists, computer scientists and statisticians, is engaged in the construction of such an open-source framework. The expertise required to operate, modify and extend this framework is disseminated to practicing scientists via a combination of dedicated short-term schools (in-person and virtual), topical workshops, web tutorials, conference presentations and detailed publications.The X-SCAPE project encompasses the entire high-energy nuclear physics enterprise by providing a general purpose, modifiable, and modular framework, that incorporates all known interaction processes prevalent at these experiments. The developed software is designed to be portable onto hybrid architectures, where GPU acceleration can be applied to the most demanding computational tasks. Provided as a steadily improving package culminating in annual releases, X-SCAPE includes of i) the default distribution of baseline tools allowing for simplified simulations on commodity CPUs, ii) a refactored product for distributed architectures with GPUs, and iii) Bayesian statistical routines to both emulate and systematically explore the model space of parameters against diverse experimental data.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Physics at the Information Frontier (PIF) Program within the Division of Physics and the Office of Multidisciplinary Activities within the Directorate for Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Collaborative Research: CSSI: Framework: Data: Clowder Open Source Customizable Research Data Management, Plus-Plus",OAC,1835543,Noah Fahlgren,nfahlgren@danforthcenter.org,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Preserving, sharing, navigating, and reusing large and diverse collections of data is now essential to scientific discoveries in areas such as phenomics, materials science, geoscience, and urban science. These data navigation needs are also important when addressing the growing number of research areas where data and tools must span multiple domains. To support these needs effectively, new methods are required that simplify and reduce the amount of effort needed by researchers to find and utilize data, support community accepted data practices, and bring together the breadth of standards, tools, and resources utilized by a community. Clowder, an active curation based data management system, addresses these needs and challenges by distributing much of the data curation overhead throughout the lifecycle of the data, augmenting this with social curation and automated analysis tools, and providing extensible community-dependent means of viewing and navigating data. As an open source framework, built to be extensible at every level, Clowder is capable of interacting with and utilizing a variety of community tools while also supporting different data governance and ownership requirements.The project enhances Clowder's core systems for the benefit of a larger group of users. It increases the level of interoperability with community resources, hardens the core software, and distributes core software development, while continuing to expand usage. Governance mechanisms and a business model are established to make Clowder sustainable, creating an appropriate governance structure to ensure that the software continues to be available, supportable, and usable. The effort engages a number of stakeholders, taking data from diverse but converging scientific domains already using the Clowder framework, to address broad interoperability and cross domain data sharing. The overall effort will transition the grassroots Clowder user community and Clowder's other stakeholders (such as current and potential developers) into a larger organized community, with a sustainable software resource supporting convergent research data needs.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework Implementations: CSSI: CANDY: Cyberinfrastructure for Accelerating Innovation in Network Dynamics,OAC,2104076,Sanjukta Bhowmick,Sanjukta.Bhowmick@unt.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Efficient analysis of dynamic networks is highly important in diverse multidisciplinary real-life applications, such as data mining and analytics, social and biological networks, epidemiology, cyber-physical infrastructures, transportation networks, surface mining, and cybersecurity. Although numerous software exists for analyzing static networks, a comprehensive cyberinfrastructure that supports innovative research challenges in large-scale, complex, dynamic networks is lacking. This multi-university proposal addresses this gap by developing a novel platform, called CANDY (Cyberinfrastructure for Accelerating Innovation in Network Dynamics), based on efficient, scalable parallel algorithm design for dynamic networks and high-performance software development with performance optimization. For broader impact and outreach activities, the investigators will (1) collaborate with multidisciplinary research groups to evaluate the effectiveness of the developed platform, algorithms and software tools; (2) host workshops, webinars, and tutorials to educate research community about the cyberinfrastructure; (3) disseminate project outcomes via a dedicated website, keynote and invited talks, demos, and high-quality publications in peer-reviewed journals and conferences; and (4) train next generation data scientists in the development of CANDY platform, by engaging women and underrepresented minority students, including high school students and rural communities in Missouri, Hispanic and African-American communities in Texas, and First Nation (Native American) community in Oregon.This project will develop the first parallel, scalable, extendable, and user-friendly software platform for updating important properties of dynamic networks. It will also provide the requisite functionalities and tools to modify existing algorithms or create new ones, catering to basic, intermediate and advanced users with different levels of expertise. The CANDY cyberinfrastructure platform will be implemented on different architectures, such as distributed memory, shared memory, and graphics processor units providing user-friendly interfaces. Significant research and development innovations include: (1) a novel hierarchical taxonomy of network analysis algorithms that allows for layered specification of parallel algorithms based on multiple parameters; (2) templates for creating new scalable algorithms for dynamic network analysis; (3) algorithms to partition the streaming set of nodes and edges into network snapshots at changing points; and (4) invariant-based quantifiable performance metrics for analyzing large-scale dynamic networks. As a case study, the developed software will be evaluated on two disparate domains -- fast processing of genomic data on dynamic trees, and cost-effective operation of complex mining engineering applications.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework Implementation: CSSI: CANDY: Cyberinfrastructure for Accelerating Innovation in Network  Dynamics,OAC,2104115,Boyana Norris,norris@cs.uoregon.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Efficient analysis of dynamic networks is highly important in diverse multidisciplinary real-life applications, such as data mining and analytics, social and biological networks, epidemiology, cyber-physical infrastructures, transportation networks, surface mining, and cybersecurity. Although numerous software exists for analyzing static networks, a comprehensive cyberinfrastructure that supports innovative research challenges in large-scale, complex, dynamic networks is lacking. This multi-university proposal addresses this gap by developing a novel platform, called CANDY (Cyberinfrastructure for Accelerating Innovation in Network Dynamics), based on efficient, scalable parallel algorithm design for dynamic networks and high-performance software development with performance optimization. For broader impact and outreach activities, the investigators will (1) collaborate with multidisciplinary research groups to evaluate the effectiveness of the developed platform, algorithms and software tools; (2) host workshops, webinars, and tutorials to educate research community about the cyberinfrastructure; (3) disseminate project outcomes via a dedicated website, keynote and invited talks, demos, and high-quality publications in peer-reviewed journals and conferences; and (4) train next generation data scientists in the development of CANDY platform, by engaging women and underrepresented minority students, including high school students and rural communities in Missouri, Hispanic and African-American communities in Texas, and First Nation (Native American) community in Oregon.This project will develop the first parallel, scalable, extendable, and user-friendly software platform for updating important properties of dynamic networks. It will also provide the requisite functionalities and tools to modify existing algorithms or create new ones, catering to basic, intermediate and advanced users with different levels of expertise. The CANDY cyberinfrastructure platform will be implemented on different architectures, such as distributed memory, shared memory, and graphics processor units providing user-friendly interfaces. Significant research and development innovations include: (1) a novel hierarchical taxonomy of network analysis algorithms that allows for layered specification of parallel algorithms based on multiple parameters; (2) templates for creating new scalable algorithms for dynamic network analysis; (3) algorithms to partition the streaming set of nodes and edges into network snapshots at changing points; and (4) invariant-based quantifiable performance metrics for analyzing large-scale dynamic networks. As a case study, the developed software will be evaluated on two disparate domains -- fast processing of genomic data on dynamic trees, and cost-effective operation of complex mining engineering applications.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
CSSI Elements: DataSwarm: A User-Level Framework for Data Intensive Scientific Applications,OAC,1931348,Douglas Thain,dthain@nd.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","This project creates a capability that will support the construction of large, data intensive scientific applications that must run on top of national cyberinfrastructure, such as large campus clusters, NSF extreme-scale computing facilities, the Open Science Grid, and commercial clouds. The new capability (DataSwarm) brings data requirements and software dependencies to the target cyberinfrastructure systems, and deploys them as and when required, rather than having these requirements pre-installed on the target systems. The motivation comes from applications in high energy physics, molecular dynamics, and quantum chemistry.The main motivation of the work is the challenge of scalable computing frameworks. Based on a prior development by the Principal Investigator (Work Queue), the current project provides technical innovation in three areas: (1) Molecular Task Composition. Molecular task composition is used as an abstraction for the precise construction of tasks that require a custom software environment, large data input, and a scratch data area to capture the outputs. By expressing these aspects explicitly instead of implicitly, the project improves the storage efficiency of large numbers of tasks. (2) In-Situ Data Management. In-situ storage management is performed to offset the increased storage consumption likely to occur under molecular task composition, avoiding unpredictable failures of tasks due to storage exhaustion. (3) Precision Provenance. Precision provenance of both data objects and task components enables the efficient re-use of resources across multiple runs, as well as precise incremental changes to complex workflows.For this project, the three key elements addressed are the software environment, input data, and a scratch data area. These elements are usually independently managed; here, they are bound together to form temporary ""molecules"" for task execution. The three applications included in this project represent three typical types of complex data and complex software dependencies. They include custom late-stage data analysis codes in high energy physics, complex multidimensional optimization, and ensemble molecular dynamics, respectively.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: CSSI Frameworks: SAGE3: Smart Amplified Group Environment for Harnessing the Data Revolution,OAC,2003387,Christopher North,north@cs.vt.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","The Big Data revolution necessitates the use of sophisticated tools such as Artificial Intelligence (AI) and Data Visualization to harness the sheer volume, velocity and variety of datasets that are becoming the norm. However, it is the research community that must make sense of the data being amassed, so cyberinfrastructure must extend to people. SAGE3 (Smart Amplified Group Environment) puts the human in the loop by providing scientists with an intuitive framework that integrates state-of-the-art AI technologies with applications, workflows, smart visualizations and collaboration services to help them access, share, explore and analyze their data, come to conclusions, and make decisions with greater speed, accuracy, comprehensiveness and confidence. SAGE3 augments every step of the scientific discovery enterprise - from quickly summarizing large data, to finding trends and similarities or anomalies among one or more linked datasets, to communicating findings to scientists, public policy and government officials, and the general public, to educating the next-generation workforce. Ultimately, it is the scientists and future scientists who must Harness the Big Data revolution to solve the nation's grand challenge problems that will benefit society as a whole - from studying the diversity of life on Earth, to understanding the Earth and its systems from satellite imagery of its poles, to developing response scenarios for natural disasters such as landslides and pandemics that impact the citizens and economies of the world. SAGE3 development focuses on two fundamental components: AI-enhanced smart services and advanced computing resource orchestration to support reproducible work models for secure collaborative work. SAGE3 amplifies user productivity, providing them with commercially available and open-source AI solutions, which autonomously and transparently analyze data while continually learning and improving through user interactions. SAGE3 makes AI technologies broadly accessible, not just a privilege for the technically savvy. SAGE3 further democratizes AI by using Data Visualization to help interpret and explain AI models so users better understand how AI came to its decisions, which engenders user trust and can help identify potentially prejudiced or biased models.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework Implementation: CSSI: CANDY: Cyberinfrastructure for Accelerating Innovation in Network Dynamics,OAC,2104078,Sajal Das,sdas@mst.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Efficient analysis of dynamic networks is highly important in diverse multidisciplinary real-life applications, such as data mining and analytics, social and biological networks, epidemiology, cyber-physical infrastructures, transportation networks, surface mining, and cybersecurity. Although numerous software exists for analyzing static networks, a comprehensive cyberinfrastructure that supports innovative research challenges in large-scale, complex, dynamic networks is lacking. This multi-university proposal addresses this gap by developing a novel platform, called CANDY (Cyberinfrastructure for Accelerating Innovation in Network Dynamics), based on efficient, scalable parallel algorithm design for dynamic networks and high-performance software development with performance optimization. For broader impact and outreach activities, the investigators will (1) collaborate with multidisciplinary research groups to evaluate the effectiveness of the developed platform, algorithms and software tools; (2) host workshops, webinars, and tutorials to educate research community about the cyberinfrastructure; (3) disseminate project outcomes via a dedicated website, keynote and invited talks, demos, and high-quality publications in peer-reviewed journals and conferences; and (4) train next generation data scientists in the development of CANDY platform, by engaging women and underrepresented minority students, including high school students and rural communities in Missouri, Hispanic and African-American communities in Texas, and First Nation (Native American) community in Oregon.This project will develop the first parallel, scalable, extendable, and user-friendly software platform for updating important properties of dynamic networks. It will also provide the requisite functionalities and tools to modify existing algorithms or create new ones, catering to basic, intermediate and advanced users with different levels of expertise. The CANDY cyberinfrastructure platform will be implemented on different architectures, such as distributed memory, shared memory, and graphics processor units providing user-friendly interfaces. Significant research and development innovations include: (1) a novel hierarchical taxonomy of network analysis algorithms that allows for layered specification of parallel algorithms based on multiple parameters; (2) templates for creating new scalable algorithms for dynamic network analysis; (3) algorithms to partition the streaming set of nodes and edges into network snapshots at changing points; and (4) invariant-based quantifiable performance metrics for analyzing large-scale dynamic networks. As a case study, the developed software will be evaluated on two disparate domains -- fast processing of genomic data on dynamic trees, and cost-effective operation of complex mining engineering applications.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
CSSI: Elements: First Workshop on NSF and DOE High Performance Computing Tools,OAC,1939486,Sameer Shende,sameer@cs.uoregon.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","High Performance Computing (HPC) software has become increasingly complex to install. The complex inter-package dependency can lead to significant loss of productivity. This project will conduct a two-day hands-on workshop that brings together NSF resource providers and system administrators, NSF PIs and HPC application developers, and scientists and facilities administrators from the DOE national laboratories, and industry to develop a strong collaboration on HPC software deployment. The Software Development Kit (SDK) that will be used is available through a containerized distribution as well as Spack - an app store for supercomputers. The workshop will have a significant impact on enabling the delivery of HPC software to NSF and other supercomputing sites. It will better equip application developers in the use of modern software delivery infrastructures including HPC containers and the Spack platform. High Performance Computing (HPC) software has become increasingly complex to install. The complex inter-package dependency can lead to significant loss of productivity. This workshop on HPC tools will bring together NSF resource providers and system administrators, NSF PIs and HPC application developers, and scientists and facilities administrators from the DOE national laboratories, and industry to develop a strong collaboration on HPC. DOEs Exascale Computing Project (ECP) has produced an Extreme-Scale Scientific Software Stack (E4S) of HPC libraries and tools in a Software Development Kit (SDK). This SDK is available through a containerized distribution as well as Spack - an app store for supercomputers. Spack includes recipes for building packages from source code and is the primary means of deploying ECP software. This two-day workshop will focus on the Spack platform and E4S. The expected outcome of this meeting will be actual deployment of the ECP SDK software stack and container-based runtimes on the HPC systems and an understanding of how to develop custom recipes for Spack based builds. The proposed workshop will have a significant impact on enabling the delivery of HPC software to NSF and other supercomputing sites. It will better equip application developers in the use of modern software delivery infrastructures including HPC containers and the Spack platform.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"COLLABORATIVE RESEARCH: EAGER: Towards Building a CyberInfrastructure for Facilitating the Assessment, Dissemination, Discovery, & Reuse of Software and Data Products",OAC,2314202,Ritu Ritu,ritu@wayne.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Over the last several years, the projects funded through the various NSF programs, such as the Cyberinfrastructure for Sustained Scientific Innovation (CSSI), Data Infrastructure Building Blocks (DIBBs), and Software Infrastructure for Sustained Innovation (SI2) programs, have resulted in innovative software and data products with broad societal impacts. Collecting the information on the short-term and long-term impact of these products on their intended user communities in terms of quantifiable metrics can be important for future funding decisions, and hence is in national interest. However, collecting such information can be a challenging task given the diversity of the NSF-funded products, their usage environments, and their target audiences. Additionally, when a product is composed of (or integrated with) other products, it can be difficult to capture the provenance trail of all the embedded products, which impacts the process of gathering the metrics necessary in evaluating their success. Moreover, the knowledge of the entire technology stack used in a product can enable other developers or adopters of that product in analyzing the code reuse and integration cost. When analyzing the feasibility of integrating software products, or interoperating with them, or extending them, it is also important to check the compatibility of their licenses and software stacks so that one can determine if the products can interoperate legally and seamlessly, and if the derived products can be disseminated as intended. It can be time-consuming to carefully review and understand the impact of the licenses of the base products on any derived product, or to check if one product can co-exist or interoperate with another product. Hence, having a central and a publicly accessible infrastructure for (1) tracking the metrics of the NSF-funded products, (2) checking their license and software stack compatibility, and (3) discovering the software stack and its evolution, can be useful for quantifying the societal impacts of the NSF-funded products and in promoting their dissemination. The overarching goal of this project is to develop a software infrastructure for facilitating the assessment, discovery, dissemination, and reuse of publicly accessible software and data products. As a preliminary step towards meeting this goal, this project has initiated research and development activities for prototyping: (1) iTracker: the software infrastructure for tracking the user-defined metrics of products released and deployed on different platforms & computing environments, (2) CompChecker: a license and software-stack compatibility checker for advising the users on the feasibility of integrating or interoperating with existing products, and (3) Discovery Catalog: a prototype of a catalog of NSF-funded products which can display the most recent information captured by iTracker for each product of interest and integrate CompChecker as a feature. The project demonstrates the use of block-chain for securely storing an immutable copy of the metadata related to the cataloged products and this metadata can in turn be useful for tracking the evolution of the products during their life cycle. The project demonstrates the infrastructure required for identifying and promoting the relevant metrics for evaluating different categories of products. The project has the potential of encouraging the developer community to adopt best practices for product dissemination and will likely foster cross-disciplinary collaborations.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF),OAC,1835613,Arthi Jayaraman,arthij@udel.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Elements: Elastica - A software ecosystem for modeling, simulation, design, and control of soft, compliant, and heterogenous structures interacting with their environment",OAC,2209322,Mattia Gazzola,mgazzola@illinois.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","This Cyberinfrastructure for Sustained Scientific Innovation (CSSI) project seeks to establish Elastica as a best-in-class, open source, and ready-to-use ecosystem for the simulation, analysis, design, and control of complex structures made of elastic, slender elements. The goal is to enable researchers to make inroads in three knowledge gaps identified as foundational and of national strategic importance: (1) scarcity of rigorous modeling and engineering design methods for continuum, compliant and configurable structures, devices and robots; (2) paucity of control methods able to effectively coordinate the virtually infinite degrees of freedom that characterize these distributed systems; (3) lack of models and design methods specialized to biomaterials to be incorporated in living biological machines, connecting full circle to the first point. This project will result in a versatile computing framework that tackles a unique set of problems at the intersection of biology, robotics and control, thus increasing competitiveness in manufacturing via advanced computational methods. This effort is complemented by a strong outreach component. Indeed, applications of Elastica to animal locomotion and biological robots have great potential to capture the imagination of a broad, especially younger audience. By highlighting the natural world connection, curiosity and creativity will be fostered, drawing attention to math, physics, biology and advanced computing. Particularly, the paper2tree project that involves local, underrepresented communities in planting trees associated to scientific discoveries, will be leveraged to engage elementary-school students.Within this context, this effort will augment Elastica?s modeling capabilities and solution methods, aiming at tens-of-teraflops to petaflops-grade simulations entailing millions of rods in complex environments. Concurrently, to maximize adoption and impact, a cyberinfrastructure ecosystem of pre/post processing tools and interfaces, organized in easily customizable pipelines, will be created. A structured project plan focused on science, computing, and cyberinfrastructure innovation will deliver: (i) A scalable, versatile, and efficient Cosserat rod solver that incorporates a variety of material models and environmental interface conditions, and that can run on a range of computing architectures from desktops to heterogeneous computing clusters; (ii) Benchmarked and verified discipline-specific modules of prebuilt primitives relevant to address the identified knowledge gaps above; (iii) A suite of both pre/postprocessing tools to assist in setting up domain-specific workflows. Throughout, open science and users/developers support will be core, to accelerate adoption, foster impact and catalyze innovation.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Civil, Mechanical and Manufacturing Innovation within the Directorate for Engineering.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"SI2:SSE: MAtrix, TEnsor, and Deep-Learning Optimized Routines (MATEDOR)",OAC,1740250,Azzam Haidar,haidar@icl.utk.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","A number of scientific software applications from important fields, including applications in deep learning, data mining, astrophysics, image and signal processing, hydrodynamics, and more, do many computations on small matrices (also known as ""tensors"") and using widely available standard linear-algebra software libraries. Scientists are trying to make these applications run faster by running them on advanced high performance computing (HPC) systems, that are heterogeneous systems that use processors of many different types, such as ""accelerators"" - that use of specialized computer hardware to perform some functions more efficiently than standard, general-purpose processors - and ""co-processors"" - that can run certain specialized functions in parallel with the central processor. However, standard linear algebra software libraries cannot make use of these specialized hardware components, and so the scientific applications mentioned above do not become much faster. Many existing linear algebra libraries, including libraries supplied by commercial vendors of computing technology have been tried to no avail. This issue is now critical because advancements in science from important fields are being held back due to the lack of progress in speeding up software. This project will address this through research and development that will create efficient software that can repetitively execute tensor operations grouped together in ""batches"" and which can be written to run very efficiently and quickly on the types of hardware components that exist in HPC systems. In addition to the research and development, several students will be engaged in the project, thus helping develop a critically needed component of the U.S. workforce.The trend in high performance computing (HPC) toward large-scale, heterogeneous systems with GPU accelerators and coprocessors has made the near total absence of linear algebra software for small matrix or tensor operations especially noticeable. Given the fundamental importance of numerical libraries to science and engineering applications of all types, the need for libraries that can perform batched operations on small matrices or tensors has become acute. This MAtrix, TEnsor, and Deep-learning Optimized Routines (MATEDOR) project seeks to provide a solution to this problem by developing a sustainable and portable library for such small computations. Future releases of MATEDOR are expected to have a significant impact on application areas that use small matrices and tensors and need to exploit the power of advanced computing architectures. Such application areas include deep-learning, data mining, metabolic networks, computational fluid dynamics, direct and multi-frontal solvers, image and signal processing, and many more. This team has a proven record of providing software infrastructure that is widely adopted and used, that supports ongoing community contributions, and that becomes incorporated in vendor libraries (e.g., Intel's MKL and NVIDIA's CUBLAS) and other software tools and frameworks (e.g., MATLAB and R). Students will be regularly integrated into the project activities, and this group of PIs has an exceptionally strong record of community outreach, having given numerous performance optimization and software tutorials at conferences and Users Group meetings.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"COLLABORATIVE RESEARCH: EAGER: Towards Building a CyberInfrastructure for Facilitating the Assessment, Dissemination, Discovery, & Reuse of Software and Data Products",OAC,2037661,Ritu Ritu,ritu@wayne.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Over the last several years, the projects funded through the various NSF programs, such as the Cyberinfrastructure for Sustained Scientific Innovation (CSSI), Data Infrastructure Building Blocks (DIBBs), and Software Infrastructure for Sustained Innovation (SI2) programs, have resulted in innovative software and data products with broad societal impacts. Collecting the information on the short-term and long-term impact of these products on their intended user communities in terms of quantifiable metrics can be important for future funding decisions, and hence is in national interest. However, collecting such information can be a challenging task given the diversity of the NSF-funded products, their usage environments, and their target audiences. Additionally, when a product is composed of (or integrated with) other products, it can be difficult to capture the provenance trail of all the embedded products, which impacts the process of gathering the metrics necessary in evaluating their success. Moreover, the knowledge of the entire technology stack used in a product can enable other developers or adopters of that product in analyzing the code reuse and integration cost. When analyzing the feasibility of integrating software products, or interoperating with them, or extending them, it is also important to check the compatibility of their licenses and software stacks so that one can determine if the products can interoperate legally and seamlessly, and if the derived products can be disseminated as intended. It can be time-consuming to carefully review and understand the impact of the licenses of the base products on any derived product, or to check if one product can co-exist or interoperate with another product. Hence, having a central and a publicly accessible infrastructure for (1) tracking the metrics of the NSF-funded products, (2) checking their license and software stack compatibility, and (3) discovering the software stack and its evolution, can be useful for quantifying the societal impacts of the NSF-funded products and in promoting their dissemination. The overarching goal of this project is to develop a software infrastructure for facilitating the assessment, discovery, dissemination, and reuse of publicly accessible software and data products. As a preliminary step towards meeting this goal, this project has initiated research and development activities for prototyping: (1) iTracker: the software infrastructure for tracking the user-defined metrics of products released and deployed on different platforms & computing environments, (2) CompChecker: a license and software-stack compatibility checker for advising the users on the feasibility of integrating or interoperating with existing products, and (3) Discovery Catalog: a prototype of a catalog of NSF-funded products which can display the most recent information captured by iTracker for each product of interest and integrate CompChecker as a feature. The project demonstrates the use of block-chain for securely storing an immutable copy of the metadata related to the cataloged products and this metadata can in turn be useful for tracking the evolution of the products during their life cycle. The project demonstrates the infrastructure required for identifying and promoting the relevant metrics for evaluating different categories of products. The project has the potential of encouraging the developer community to adopt best practices for product dissemination and will likely foster cross-disciplinary collaborations.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: SI2-SSI: Expanding Volunteer Computing,OAC,2039142,Ritu Ritu,ritu@wayne.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Volunteer computing (VC) uses donated computing time consumer devices such as home computers and smartphones to do scientific computing. It has been shown that VC can provide greater computing power, at lower cost, than conventional approaches such as organizational computing centers and commercial clouds. BOINC is the most common software framework for VC. Essentially, donors of computing time simply have to load BOINC on their computer or smartphone, and then register to donate at the BOINC web site. VC provides ""high throughput computing"": handling lots of independent jobs, with performance goals based on the rate of job completion rather than completion time for individual jobs. This type of computing (all known as high-throughput computing) is in great demand in most areas of science. Until now, the adoption of VC has been limited by its structure. For example, VC projects (such as Einstein@home and Rosetta@home) are operated by individual research groups, and volunteers must browse and choose from among many such projects. As a result, there are relatively few VC projects, and volunteers are mostly tech-savvy computer enthusiasts. This project aims to solve these problems using two complementary development efforts: First, it will add BOINC-based VC conduits to two major high-performance computing providers: (a) the Texas Advanced Computing Center, a supercomputer center, and (b) nanoHUB, a web portal for nano science that provides computing capabilities.Also, a unified control interface to VC will be developed, tentatively called Science United, where donors can register. The project will benefit thousands of scientists who use these facilities, and it will create technology that makes it easy for other HPC providers to add their own VC back ends. Also, Science United will provide a simpler interface to BOINC volunteers where they will register to support scientific areas, rather than specific projects. Science United will also serve as an allocator of computing power among projects. Thus, new projects will no longer have to do their own marketing and publicity to recruit volunteers. Finally, the creation of a single VC ""brand"" (i.e Science United) will allow coherent marketing of VC to the public. By creating a huge pool of low-cost computing power that will benefit thousands of scientists, and increasing public awareness of and interest in science, the project plans to establish VC as a central and long-term part of the U.S. scientific cyber infrastructure.Adding VC to an existing HPC facility involves several technical issues, which will be addressed as follows: (1) Packaging science applications (which typically run on Linux cluster nodes) to run on home computers (mostly Windows, some Mac and Linux): the team is developing an approach using VirtualBox and Docker, in which the application and its environment (Linux distribution, libraries, executables) are represented as a set of layers comprising a Docker image, which is then run as a container within a Linux virtual machine on the volunteer device. This has numerous advantages: it reduces the work of packaging applications to near zero; it minimizes network traffic because a given Docker layer is downloaded to a host only once; and it provides a strong security sandbox so that volunteer computers are protected from buggy or malicious applications, (2) File management: Input and output files must be moved between existing private servers and public-facing servers that are accessible to the outside Internet. A file management system will be developed, based on Web RPCs, for this purpose. This system will use content-based naming so that a given file is transferred and stored only once. It also maintains job/file associations so that files can be automatically deleted from the public server when they are no longer needed. (3) Submitting and monitoring jobs: BOINC provides a web interface for efficiently submitting and monitoring large batches of jobs. These were originally developed as part of a system to migrate HTCondor jobs to BOINC. This project is extending it to support the additional requirements of TACC and nanoHUB. Note that these new capabilities are not specific to TACC or nanoHUB: they provide the glue needed to easily add BOINC-based VC to any existing HTC facility. The team is also developing RPC bindings in several languages (Python, C++, PHP). The other component of the project, Science United, is a database-driven web site and an associated web service for the BOINC clients. Science United will control volunteer hosts (i.e. tell them which projects to work for) using BOINC's ""Account Manager"" mechanism, in which the BOINC client on each host periodically contacts Science United and is told what projects to run. Project servers, not Science United, will distribute jobs and files. Science United will define a set of ""keywords"" for science areas (physics, biomedicine, environment, etc.) and for location (country, institution). Projects will be labelled with appropriate keywords. Volunteers will have a yes/no/maybe interface for specifying the types of jobs they want to run. Science United will thus provide a mechanism in which a fraction of total computing capacity can be allocated to a project for a given period. Because total capacity changes slowly over time, this allows near-certain guaranteed allocations. Science United will embody a scheduling system that attempts to enforce allocations, honor volunteer preferences, and maximize throughput. Finally, Science United will do detailed accounting of computing. Volunteer hosts will tell Science United how much work (measured by CPU time and FLOPs, GPU time and FLOPs, and number of jobs) they have done for each project. Science United will maintain historical records of this data for volunteers and projects, and current totals with finer granularity (e.g. for each host/project combination). Finally, Science United will provide web interfaces letting volunteers see their contribution status and history, and letting administrators add projects, control allocations, and view accounting data."
"COLLABORATIVE RESEARCH: EAGER: Towards Building a CyberInfrastructure for Facilitating the Assessment, Dissemination, Discovery, & Reuse of Software and Data Products",OAC,2037656,Subhashini Sivagnanam,sivagnan@sdsc.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Over the last several years, the projects funded through the various NSF programs, such as the Cyberinfrastructure for Sustained Scientific Innovation (CSSI), Data Infrastructure Building Blocks (DIBBs), and Software Infrastructure for Sustained Innovation (SI2) programs, have resulted in innovative software and data products with broad societal impacts. Collecting the information on the short-term and long-term impact of these products on their intended user communities in terms of quantifiable metrics can be important for future funding decisions, and hence is in national interest. However, collecting such information can be a challenging task given the diversity of the NSF-funded products, their usage environments, and their target audiences. Additionally, when a product is composed of (or integrated with) other products, it can be difficult to capture the provenance trail of all the embedded products, which impacts the process of gathering the metrics necessary in evaluating their success. Moreover, the knowledge of the entire technology stack used in a product can enable other developers or adopters of that product in analyzing the code reuse and integration cost. When analyzing the feasibility of integrating software products, or interoperating with them, or extending them, it is also important to check the compatibility of their licenses and software stacks so that one can determine if the products can interoperate legally and seamlessly, and if the derived products can be disseminated as intended. It can be time-consuming to carefully review and understand the impact of the licenses of the base products on any derived product, or to check if one product can co-exist or interoperate with another product. Hence, having a central and a publicly accessible infrastructure for (1) tracking the metrics of the NSF-funded products, (2) checking their license and software stack compatibility, and (3) discovering the software stack and its evolution, can be useful for quantifying the societal impacts of the NSF-funded products and in promoting their dissemination. The overarching goal of this project is to develop a software infrastructure for facilitating the assessment, discovery, dissemination, and reuse of publicly accessible software and data products. As a preliminary step towards meeting this goal, this project has initiated research and development activities for prototyping: (1) iTracker: the software infrastructure for tracking the user-defined metrics of products released and deployed on different platforms & computing environments, (2) CompChecker: a license and software-stack compatibility checker for advising the users on the feasibility of integrating or interoperating with existing products, and (3) Discovery Catalog: a prototype of a catalog of NSF-funded products which can display the most recent information captured by iTracker for each product of interest and integrate CompChecker as a feature. The project demonstrates the use of block-chain for securely storing an immutable copy of the metadata related to the cataloged products and this metadata can in turn be useful for tracking the evolution of the products during their life cycle. The project demonstrates the infrastructure required for identifying and promoting the relevant metrics for evaluating different categories of products. The project has the potential of encouraging the developer community to adopt best practices for product dissemination and will likely foster cross-disciplinary collaborations.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: MATPOWER for Integrated Smart Grid Research and Education,OAC,1931421,Ray Zimmerman,rz10@cornell.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","The electric power system is one of the most fundamental and critical infrastructures underlying modern society, and the economic, environmental and societal impacts of advances in its planning and operations are potentially enormous. MATPOWER is a set of open-source scientific software elements for electric power system simulation, analysis and optimization. A version of MATPOWER is widely used, especially for research and education. This Cyberinfrastructure for Sustained Scientific Innovation (CSSI) Elements award supports the research needed to transition MATPOWER to a new flexible internal modeling architecture. The new capabilities will enable MATPOWER to simulate and optimize integrated smart grid networks consisting of both transmission (balanced) and distribution (unbalanced) systems. These capabilities will serve to expand the scope of MATPOWER's future impact as a successful research tool for designing and analyzing the power systems of the future. As power grids evolve toward more sustainable, economically efficient and environmentally friendly systems, positioning MATPOWER to expand its role as a flexible research and educational tool in this arena of innovation and change has the potential for far reaching and transformative impact both nationally and globally.MATPOWER addresses some of the most fundamental classes of problems in power systems analysis, namely the power flow, optimal power flow, and related problems used to determine the steady state voltages, currents and power flows arising from the interactions among system conditions, operator control actions and the laws of physics. The aim of this work is to broaden and extend MATPOWER's reach as a research-enabling tool for tackling future power systems problems in two specific ways. The first is to restructure the MATPOWER internals around a new unified MATPOWER element model, vastly expanding the range of devices and controls modeled by MATPOWER and further increasing its customizability. The second is to implement the modeling of multiphase unbalanced systems and integrated balanced and unbalanced models. This will expand the scope of MATPOWER?s usefulness to many current and emerging research areas related to the modern smart grid, with its proliferation of decentralized control and distributed energy resources (DER), such as solar PV, plug-in hybrid electric vehicles, local energy storage, and various kinds of actively managed demand.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Element:Software:Enabling Millisecond-Scale Biomolecular Dynamics,OAC,1835838,Erik Santiso,eesantis@ncsu.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Computer simulation methods based on Molecular Dynamics (MD) have been used for decades to understand chemical and biochemical phenomena at the molecular level. MD is a very powerful tool that has enabled scientists to understand the behavior of molecules crucial for life such as proteins and nucleic acids. MD has also been used to understand diseases and develop new drugs. However, MD is limited in both the size of the systems that can be studied and the amount of time that can be simulated. Many complex phenomena relevant to life involve systems too large to study with MD, or require following the system for much longer times. An alternative to traditional MD called discontinuous molecular dynamics (DMD) has been shown to be much more efficient to study biomolecular processes. To date, however, the use of DMD has been limited due to its inability to take advantage of modern parallel computers. This project will develop a next-generation parallel DMD software that will enable the study of complex molecular phenomena involving larger systems and longer time scales. Detailed knowledge of such processes will considerably advance the development of new materials and drugs, and human health. The project team combines the computational and experimental expertise to successfully develop and validate a robust parallel DMD software framework. The software and results will be actively shared both with the computational simulation community and with the scientific and engineering community at large, contributing to the capability, capacity, and cohesiveness of the national cyber-infrastructure ecosystem. Furthermore, the results of this project will be used in outreach efforts geared toward the education and inclusion of minorities traditionally underrepresented in higher STEM education.This project aims to develop an open software framework that enables multi-millisecond dynamic simulations of peptides and peptide-mimetics by implementing a parallel discontinuous molecular dynamics (DMD) package. Unlike current molecular dynamics (MD), which features limited simulation timescales, discontinuous molecular dynamics (DMD) assumes ballistic motion of the particles between interaction events and enables the study of phenomena across much longer time scales. To demonstrate the approach, the project will (1) develop a parallel version of existing serial DMD codes to enable extending simulation times from hundreds of microseconds to several milliseconds; (2) extend and improve the available DMD peptide force field, adding parameters for non-natural peptides and peptoids; and (3) develop software for translating interaction potentials from traditional MD to DMD. The project team possesses the complementary expertise necessary for this project, including coarse-grained models and force fields for complex polymers and peptoids, MD simulation of protein self-assembly and peptide-protein binding processes, synthesis of protein-binding peptides and peptoids, and measurement of thermodynamic and kinetic binding parameters. The tools resulting from this research will allow the scientific and engineering community to model and study very long time-scale phenomena, such as biopolymer folding, aggregation and inhibition of aggregation, fibril formation, and protein-binding. This toolbox shows great promise to not only accelerate innovation in the computational design of biomaterials, but also to impact the molecular simulation community focusing on highly complex systems, up to cell-level dynamics. Notably, this project is ideal for the National Science Foundation's Cyber-infrastructure for Sustained Scientific Innovation (CSSI), as it (i) contributes to the capability, capacity, and cohesiveness of the national cyberinfrastructure ecosystem by providing user-friendly open-source computational tools, (ii) actively engages CI experts and testers of our toolbox, who would potentially be its ultimate users, (iii) advances our current capabilities in developing bioactive peptides and peptoids, (iv) establishes plans and metrics that encourage measurement of progress and certify the quality of shared tools and results, and (iv) devise strategies to combine wide-access with long-term community-driven development and progress.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Frameworks: Garden: A FAIR Framework for Publishing and Applying AI Models for Translational Research in Science, Engineering, Education, and Industry",OAC,2209892,Ian Foster,foster@uchicago.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Harnessing powerful new advances in machine learning (ML) and artificial intelligence (AI) is key to 1) maintaining and building national competitiveness in the sciences and engineering, 2) realizing breakthroughs in health and medicine, 3) enabling the creation of industries of the future, and 4) increasing economic growth and opportunity. Today, researchers are achieving exciting results with these new ML/AI methods in applications ranging from materials discovery, chemistry, and drug discovery to high energy physics, weather prediction, advanced manufacturing, and health. Yet, much work remains. These new methods and results are not easily applied by others due to the specialized expertise and resources needed to understand, develop, share, adapt, test, deploy, and run the resulting ML/AI models. To overcome these barriers to progress, this project seeks to develop methods and tools for constructing and creating Model Gardens, collections of curated and tested ML/AI models linked with the data and computing resources required to advance the work of a specific research community. Such new methods, software, and tools can make it simple for model producers to publish models in forms that are easily consumed by others, and for model consumers to discover published models and integrate them into their applications in academia or industry. The project connects researchers in materials science, physics, and chemistry enabling the establishment of Model Gardens for their communities and empowering key research centers to collect and provide broad access to new methods and models resulting from their work. Further, the project facilitates the connection of aspiring researchers with scientific problems, engaging hundreds of students from diverse backgrounds (including rural community college partners) in learning and contributing to software development, model publication, development of new AI/ML applications, and training of a next-generation ML/AI-empowered workforce through hosted workshops, open office hours, and development of a new engagement platform.This project overcomes the barriers to the dissemination and application of new ML/AI methods by creating a new CSSI framework?the Garden Framework to support the construction and operation of Model Gardens: collections of curated models linked with the data and computing resources required to advance the work of specific communities. By reducing the friction associated with model publication, discovery, access, and deployment; providing for the disciplined and structured organization and linking of data, models, and code; associating appropriate metadata with models to promote reuse and discoverability, and applying quality assessment measures (e.g., automated testing, uncertainty quantification) to support model comparison; supporting the development of communities around specific model classes and research challenges; and permitting easy access to models without (and with) download and installation, established Model Gardens reduce barriers to the use of ML/AI methods and promote the nucleation of communities around specific datasets, methods, and models.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: ELEMENTS: The LROSE Science Gateway LIDAR/RADAR Analysis In The Cloud,AGS,2103785,Michael Dixon,dixon@ucar.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Remote sensing observations from weather radars and lidars consist of a tremendous amount of raw data. Analysis of this data is often time-consuming, slowing the translation of data to scientific insights. The research team, under prior NSF funding, developed the Lidar Radar Open Software Environment (LROSE) which consists of freely available tools to help analyze and visualize data. This award will take the next step by making LROSE available online, eliminating the need for a complicated installation on local computers and taking advantage of data sets on the cloud. The impact of this project will be on all areas of science that make use of lidar and radar data, such as for severe weather and air quality. The online approach will also make it possible for educators to set up exercises and teaching aids for students in a classroom, along with the relevant data and documentation for learning activities.This project builds upon the Lidar Radar Open Software Environment, which is a software toolbox for analyzing radar and lidar data, by developing a fully functional science gateway for LROSE. The research team will integrate LROSE into a web portal environment, thereby allowing ready accessibility to computational resources via web browser interfaces and allowing users to move data directly within the cloud. The intent of this CSSI Elements award is to facilitate access to analysis and data processing tools, reducing barriers to effective science and expanding the user base. The project has two major development components: 1) the underlying infrastructure, including the gateway, the data handling layer and data visualization capabilities, and 2) the scientific software modules that allow users to perform experiments using the data.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: ELEMENTS: The LROSE Science Gateway LIDAR/RADAR Analysis In The Cloud,AGS,2103776,Jennifer DeHart,jcdehart@colostate.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Remote sensing observations from weather radars and lidars consist of a tremendous amount of raw data. Analysis of this data is often time-consuming, slowing the translation of data to scientific insights. The research team, under prior NSF funding, developed the Lidar Radar Open Software Environment (LROSE) which consists of freely available tools to help analyze and visualize data. This award will take the next step by making LROSE available online, eliminating the need for a complicated installation on local computers and taking advantage of data sets on the cloud. The impact of this project will be on all areas of science that make use of lidar and radar data, such as for severe weather and air quality. The online approach will also make it possible for educators to set up exercises and teaching aids for students in a classroom, along with the relevant data and documentation for learning activities.This project builds upon the Lidar Radar Open Software Environment, which is a software toolbox for analyzing radar and lidar data, by developing a fully functional science gateway for LROSE. The research team will integrate LROSE into a web portal environment, thereby allowing ready accessibility to computational resources via web browser interfaces and allowing users to move data directly within the cloud. The intent of this CSSI Elements award is to facilitate access to analysis and data processing tools, reducing barriers to effective science and expanding the user base. The project has two major development components: 1) the underlying infrastructure, including the gateway, the data handling layer and data visualization capabilities, and 2) the scientific software modules that allow users to perform experiments using the data.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Planning Grant:  I/UCRC in Sensory Sciences and Innovation,EEC,1361957,Mary Arpaia,mlarpaia@ucanr.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","The research of the planned I/UCRC for Sensory Sciences and Innovation aims to support basic and applied research in several areas across the spectrum of chemosensory science, providing a focal point for industry-university collaboration. The planned center targets new understanding through multi-disciplinary, cross-cutting research of such topics as how flavors and fragrances influence memory, cognitive function and mood; the full range of benefits of flavors and fragrances in fighting infection (e.g., citral), supporting immune function and protecting food against spoilage (e.g. rosemary oil). The center vision seeks to transform the sensory sciences by integrating the discovery and characterization of taste and flavor molecules, with the study of their physiological and psychological mechanisms, to ultimately understand their effects on human behavior.The flavor and fragrance industry represent a $ 23 B sector of the U.S. economy and is a critical driver of innovation across a broad range of consumer and industrial products from foods, beverages, and medicines, to personal care, home care and cosmetics. The planned I/UCRC?s holistic approach will likely lead to greater insights than the ?piecemeal? approach that is typically practiced in this field. Moreover, in cooperation with industry partners, the Center will establish a blueprint for training students and workers as multi-disciplinary sensory scientists. Partnering with industry will foster the rapid translation of this new knowledge into the next generation of innovative goods and services, resulting in broad societal impacts."
SHF:Small:Test Data Learning for Ultra High Dignostic Resolution,CCF,1218576,Ronald Blanton,blanton@ece.cmu.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Integrated circuits (ICs) are pervasive in all aspects of our lives, used in everything from computers to mobile phones to automobiles. However, fabricating ICs that both work and are reliable is becoming extremely difficult due to the significant complexity inherent in the underlying fabrication processes. It is therefore extremely important to understand the reasons why some ICs fail to operate properly. In this NSF project, new methodologies for locating and characterizing the root cause for IC failure will be developed. Existing approaches typically produce uncertain results because they rely on limited information about the IC design. In this work, additional design information and data extracted from other failing ICs will be holistically used to efficiently derive the reason for IC failure from data measured from failing ICs. Efficiently discovering the root-cause of failure for an IC will enable the design, the fabrication and even the testing of ICs to be significantly improved, helping to ensure the continued advancement of the electronics industry.The principal investigator (PI) is committed to having a broader impact through training a diverse group of undergraduate and graduate researchers. His research group has members from under-represented groups that include women, African Americans, Hispanic Americans, and Native Americans. In addition, as director of the Center for the Silicon System Implementation (CSSI) at Carnegie Mellon University, the PI manages a program that recruits undergrads researchers from various universities (including minority-serving institutions), and the annual convention of the National Society of Black Engineers. This program has been very successful, resulting in the recruitment of many undergraduate researchers, including both women and African-Americans. In the last few years, the PI himself has supervised nine undergraduate researchers, three of which were African-American (two male and one female). In addition, the preliminary work undertaken by this NSF contract is now published and was actually accomplished by one of our African-American researchers. The PI will continue to recruit a diverse group of students, both at the graduate and undergraduate levels, for participation in this project."
EAGER: Statistical Learning in Chip,CCF,1247093,Ronald Blanton,blanton@ece.cmu.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Integrated electronic systems are pervasive in all aspects of our lives, used in everything from computers to mobile phones to automobiles. However, designing and manufacturing systems that both work and are reliable is becoming extremely difficult due to the significant complexity inherent in the underlying technology. In this NSF EAGER project, we will demonstrate how statistical learning in chip (SLIC) can cope with the non-idealities that arise due to imprecise design and fabrication, and the uncertainty that stems from the system's user and operating environment. Specifically, SLIC will enable an integrated system to ""learn"" optimal operating points across various applications so as to maximize performance, and to minimize power consumption. This will be accomplished by developing customized statistical learning algorithms for in-chip implementation that are capable of deriving actionable information from system data produced both on- and off-line. The principal investigator (PI) is committed to having a broader impact through training a diverse group of undergraduate and graduate researchers. His research group has members from under-represented groups that include women, African Americans, Hispanic Americans, and Native Americans. In addition, as director of the Center for the Silicon System Implementation (CSSI) at Carnegie Mellon University, the PI manages a program that recruits undergrads researchers from various universities (including minority-serving institutions), and the annual convention of the National Society of Black Engineers. This program has been very successful, resulting in the recruitment of many undergraduate researchers, including both women and African-Americans. In the last few years, the PI has supervised nine undergraduate researchers, three of which were African-American (two male and one female), and will continue to recruit a diverse group of students, both at the graduate and undergraduate levels, for participation in this project."
NSF Student Travel Grant for 2018 Forum on Infrastructural Resilience to Low-Level Seismicity in Oklahoma,OAC,1849273,Priyank Jaiswal,priyank.jaiswal@okstate.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","This award provides travel support to students from across the country to attend the 2018 Forum on Infrastructural Resilience to Low-level Seismicity, to be held in Stillwater, Oklahoma. Undergraduate and graduate students in data science, geoscience and engineering will be able to take an active role in understanding the interdisciplinary problems and potential solutions.The forum includes an opportunity for hands-on interaction with the latest data and sensing capabilities. Dr. Priyank Jaiswal recently received a Cyberinfrastructure for Sustained Innovation (CSSI) award (#1835371): Oklahoma State University and UCSD are collaborating on the comparative characteristics of earthquakes induced by wastewater injection. The project demonstrates an approach using multi-sensor geoscience and engineering datasets recorded on structures on the Oklahoma State University campus and in the field near the location of the September 2016 Pawnee earthquake.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
HSD: Pathways to Health: Adaptation and Change in the Context of an Oil and Pipeline Project in Chad,BCS,527280,Lori Leonard,ll536@cornell.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","As economic development and other forms of ""modernization"" occur in developing nations, those nations move from a period in which the picture of morbidity and mortality is shaped largely by infectious disease to one in which chronic, so-called ""lifestyle"" conditions predominate. This change has been attributed to shifts in the age structure of the population (the demographic transition), transformations in the structure and technologies of care, and sociocultural factors ranging from urbanization and income growth to the consumption of a high-fat diet and television watching. Existing models of this ""health transition"" lack the specificity required for most public health, health promotion, or social policy purposes, however. While cross-sectional studies have thoroughly described the factors associated with epidemiological regimes that characterize ""pre-transition"" and ""post-transition"" societies, the dynamics of the transition process have been relatively neglected. Previous studies have also failed to adequately account for institutional and organizational features of local ecologies. This research project focuses on a study of health transition in Chad. The research is stimulated by a $3.7 billion oil and pipeline project that is the largest construction project on the African continent. The three principal partners (the World Bank, a consortium of oil companies led by ExxonMobil, and the government of Chad) see the project as a ""modernization"" scheme that will improve development indices and reduce poverty. The ways that people adapt and change in response to the pipeline project and the type of 'modernization' that is expected to accompany it provide the conditions to study the health and nutrition transition processes as they are occurring. This project focuses specifically on one set of relationships implicated in health transition: those linking property regimes and systems of land tenure to agricultural production practices and soil ecology to patterns of household food consumption to nutritional and health status. One of the project's main objectives is to examine how households respond to changes in property regimes, particularly the transition from communal land trusts and collective ownership to individual and private holding, with special emphasis on the implications for agricultural production practices and soil systems. Other objectives are to investigate how changes in agricultural production practices impact household food security and to examine how food security is related to patterns of household food consumption and to the nutritional status of household members. The investigators will use a comparative perspective to examine the specificities of the transition process in three localities that differ in terms of their (1) proximity to the oilfields and pipeline; (2) productive base; (3) level of integration into the cash economy; and (4) access to health care facilities and institutions of governance. The project will incorporate household surveys, anthropometric measures, soil testing and land use surveys, archival research of judicial records, and interviews with local authorities and key stakeholders in the project. This project will integrate disciplinary perspectives from the social sciences (political science, anthropology, economics), the health sciences (public health and medicine), and the natural sciences (ecology and soil science). It will provide interdisciplinary education and training for early-career researchers and students from the U.S. and Chad and will contribute to the development of research capacity in Chad. It will be conducted in collaboration with a Chadian NGO, the Centre de Support en Sante Internationale/Tchad (CSSI/T), and the Department of Natural Resource Sciences and Landscape Architecture at the University of Maryland. The project will contribute fundamental new insights and information regarding the dynamics of the health transition and to the role of institutional and organizational factors in local ecologies (including modes of governance) and how these factors shape the change process. The process of health transition is particularly important to study in the Chadian context because the pipeline project has been called a potential ""model"" for multi-stakeholder and public-private partnerships in low-income countries as well as a ""test case"" for globalization and public health. This project's research findings are expected to improve basic understanding of how globalization, ""modernization,"" and public policy affect the poor. An award resulting from the FY 2005 NSF-wide competition on Human and Social Dynamics (HSD) supports this project. All NSF directorates and offices are involved in the coordinated management of the HSD competition and the portfolio of HSD awards."
Excellence in Research:Single Crystal Growth and Investigation of Novel Exotic Fermion Materials,DMR,1832031,Doyle Temple,datemple@nsu.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Non-technical Summary:The electronic devices we use today operate by controlling the motion of negatively charged electrons, known as an electric current, through a material by applying a voltage. However, electrons possess not only an electric charge but also a property known as their intrinsic spin, which is analogous to a spinning top. This has led to the development of electronic devices that function by exploiting both the charge and the spin of electrons in a new form of electricity known as spintronics. Although still in its infancy, this field of spintronics has far reaching potential in applications such as ultra-low power electronics and quantum computing. Recently, a new class of materials, known as fermion materials, topological insulators and semimetals, or just quantum materials, were discovered which allow electronic currents to be controlled using both charge and spin. As part of this Excellence in Research project, supported by the National Science Foundation, a new, strategic partnership between the Department of Physics at Norfolk State University (NSU) and the 2D Crystal Consortium-Materials Innovation Platform (2DCC-MIP) at Pennsylvania State University has been formed. The collaboration between the two groups leverages their respective unique capabilities in single crystal growth, materials characterization, and modeling into a team focused on the discovery of new Weyl semimetals, which are specific solid state crystals that are good candidates for quantum materials. Additionally, this project also strengthens education and training in the field of growth of crystalline materials, the importance of which was highlighted in the National Research Council's report ""Frontiers in Crystalline Matter."" By providing students with in-depth research training in the NSU Department of Physics and in 2DCC-MIP at Penn State this research has a significant impact on the education, research training, and professional development of at least nine undergraduate physics majors and a graduate student in material science, that are all underrepresented in STEM fields.Technical Summary:Topological fermions such as Dirac and Weyl fermions in condensed matter are not only of fundamental importance, but also carry great promise for information technology applications. Although 3-, 6- and 8-fold fermions have been predicted in a wide range of materials, these predictions are still awaiting experimental verification. The unavailability of single crystal samples of those proposed candidate materials has slowed the progress in this area. This research partnership addresses this challenge by developing a comprehensive strategy to grow and characterize single crystals of the proposed candidate materials. This approach accelerates discoveries of novel topological materials. The new fermion candidate materials which are of interest include: 1) 3-fold fermions Pd3Bi2S2, Ag3SeAu, A4Pn3 (A=Ca, Sr and Ba; Pn=As, Sb and Bi), R4Pn3 (R=La, Ce; Pn =As, Sb and Bi), and ReRh6Ge4 (Re = Y, La and Lu), 2) 6-fold fermions MgPt, PdAsS, K3BiTe3, Mg3Ru2, FeS2 and PtP2, and 3) 8-fold fermions CuBi2O4, PdBi2O4, PdS, CsSn, CsSi, Ta3Sb, MPd3S4(M=rare earth) and Nb3Bi. Materials characterization the research team uses include magnetotransport, quantum oscillation, and ARPES measurements (performed at 2DCCMIP) and XRD, time resolved reflectivity, and transient grating measurements performed at NSU.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: CEDAR: Identifying Sources of Mid-latitude Traveling Ionospheric Disturbances,AGS,1452357,Asti Bhatt,asti.bhatt@sri.com,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Gravity waves (GWs) originating from lower atmospheric sources can lead to large accelerations of the background flow through wave dissipation or breaking. The accelerations produce secondary GWs with longer wavelengths, allowing them to propagate farther upward into the ionosphere. Here, through interaction with the ionospheric plasma, they are visible in the form of optical signatures in red line (630 nm) images, and are often referred to as traveling ionospheric disturbances (TIDs). A broad array of optical imagers will be used to triangulate the lower atmospheric sources for F region GWs, quantifying the GWs by numerically modeling the same optical signatures, and then using this quantification to ray-trace the secondary GWs to reveal their altitude profile of momentum deposition. This effort will connect two critical points on the coupled path: from the lower atmospheric source to momentum deposition in the mid to upper thermosphere. This project will include outreach to students at rural high schools where the instruments will be deployed. It will also provide undergraduate students opportunities to participate in data analysis and interpretation."
CAREER: Mining and Exploring Heterogeneous Information Networks with Social Factors,IIS,1741634,Yizhou Sun,yzsun@cs.ucla.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","Heterogeneous social information networks, such as online social networks, online forums, and digital government, are valuable sources for data analysis. However, most of the current information network studies ignore the social factors involved and treat people and their interactions simply as nodes and links in graphs. This project provides a systematic approach for analyzing such networks that addresses human factor-related questions, recognizing that different types of links have different relevance to a particular question. For example, a ""mentor"" link might be much more relevant to recommending someone to apply for a particular job rather than see a certain movie. This project identifies five fundamental research problems and provides solutions to these problems in heterogeneous social information networks: (1) predicting missing user and link characteristics, (2) identifying personality traits, (3) role detection, (4) prediction of social activities, and (5) recommender systems. Together these provide a way to include social understanding in analysis of networks.The basic approach is to provide probabilistic models that can (1) incorporate guidance in terms of either limited labels or heuristics from domain experts, and (2) automatically select the most critical information in complicated heterogeneous information networks for the target problem. For example, for the user profiling problem of age group prediction, a probabilistic model is designed via defining the probability of a possible label configuration given the network structure and strengths on different types of links. The derived learning algorithm will propagate the labels from only a few users via different types of links, and the strength of each link type will be learned according to the configuration probability of labels on that link type. The intuition is that if the ""classmates"" link type brings two users with similar age together, the algorithm needs to assign the same age group label to the two connected users that are classmates and assigns a higher strength weight to the ""classmates"" link type. The project will develop an integrated network mining system based on Spark and GraphX, to support the proposed algorithms on large-scale networks. This system will be used as a research vehicle for exploring efficient approximations with quality guarantees for the proposed algorithms."
Elements: Open Access Data Generation Engine for Bulk Power System under Extreme Windstorms,OAC,2004658,Ge Ou,gou@ufl.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","The electric power grid is a critical infrastructure that is regularly disrupted by natural disasters. Currently, there is a lack of correlated data on windstorms and the physical and cyber power system infrastructure. This project addresses that gap in knowledge in dealing with high wind disaster events for preventative and restorative resiliency of electric power networks. By integrating multiple data sources into a robust simulation tool, researchers can design new methods to mitigate the impact of hurricanes and other extreme wind events on power system operations. The outcomes of the research would positively impact reliability, resiliency, and delivery of electric power to US population centers.The framework is unique and will enable inter-disciplinary research between atmospheric sciences, civil engineering, and electric power engineering. A preliminary result is provided for the Texas power system (using a digital surrogate model of the physical infrastructure and the Texas synthetic power network) under the impacts of Hurricane Harvey. The calculated damage using the model results in very similar structural damage and power systems impact as the real disaster (the ground truth model). In addition to three deliverable test cases (Texas, New York, and Florida), the project will design automated tools to create new test cases in the future for other researchers. The power system digital surrogate, correlation of datasets, and overall data generation engine will enhance the national cyberinfrastructure ecosystem. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Civil, Mechanical and Manufacturing Innovation (CMMI) and the Division of Electrical, Communications and Cyber Systems (ECCS) within the NSF Directorate for Engineering.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: HDR: Reproducible Visual Analysis of Multivariate Networks with MultiNet,OAC,1835904,Alexander Lex,alex@sci.utah.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","Multivariate networks -- datasets that link together entities that are associated with multiple different variables -- are a critical data representation for a range of high-impact problems, from understanding how our bodies work to uncovering how social media influences society. These data representations are a rich and complex reflection of the multifaceted relationships that exist in the world. Reasoning about a problem using a multivariate network allows an analyst to ask questions beyond those about explicit connectivity alone: Do groups of social-media influencers have similar backgrounds or experiences? Do species that co-evolve live in similar climates? What patterns of cell-types support different types of brain functions? Questions like these require understanding patterns and trends about entities with respect to both their attributes and their connectivity, leading to inferences about relationships beyond the initial network structure. As data continues to become an increasingly important driver of scientific discovery, datasets of networks have also become increasingly complex. These networks capture information about relationships between entities as well as attributes of the entities and the connections. Tools used in practice today provide very limited support for reasoning about networks and are also limited in the how users can interact with them. This lack of support leaves analysts and scientists to piece together workflows using separate tools, and significant amounts of programming, especially in the data preparation step. This project aims fill this critical gap in the existing cyber-infrastructure ecosystem for reasoning about multivariate networks by developing MultiNet, a robust, flexible, secure, and sustainable open-source visual analysis system. MultiNet aims to change the landscape of visual analysis capabilities for reasoning about and analyzing multivariate networks. The web-based tool, along with an underlying plug-in-based framework, will support three core capabilities: (1) interactive, task-driven visualization of both the connectivity and attributes of networks, (2) reshaping the underlying network structure to bring the network into a shape that is well suited to address analysis questions, and (3) leveraging provenance data to support reproducibility, communication, and integration in computational workflows. These capabilities will allow scientists to ask new classes of questions about network datasets, and lead to insights about a wide range of pressing topics. To meet this goal, we will ground the design of MultiNet in four deeply collaborative case studies with domain scientists in biology, neuroscience, sociology, and geology.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Elements: Basil: A Tool for Semi-Automatic Containerization, Deployment, and Execution of Scientific Applications on Cloud Computing and Supercomputing Platforms",OAC,2209946,Ritu Ritu,ritu@wayne.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","The ""containerization"" of software applications future-proofs them, helps in their long-term preservation, makes them portable across different hardware platforms, ensures reproducible results, and makes them convenient to disseminate. Docker and Singularity are two popular software technologies for containerizing scientific applications and are widely supported on different hardware platforms. However, their adoption involves a steep learning curve, especially when it comes to developing secure and optimized images of the applications of interest. A large number of domain-scientists and scholars are usually not formally trained at containerizing their applications with Docker and Singularity, and spend a significant amount of their time in porting their applications to different cloud computing and supercomputing platforms. The process of porting applications having multiple software dependencies and sensitivities to specific software versions can be especially arduous for such users. To assist them, this project is developing BASIL - a tool for semi-automatically containerizing the scientific applications, frameworks, and workflows. This project will deliver BASIL through a web portal, as a command-line tool, and through APIs. BASIL has a broad applicability across multiple domains of deep societal impact such as artificial intelligence, drug discovery, and earthquake engineering. By enabling the preservation of valuable legacy software and making them usable for several years in future, BASIL will save cost and time in software rewriting and software installations, and thus contribute towards advancing the prosperity of the society. The project will result in educational content on ?Introduction to Containerization? and students engaged in the project will develop valuable skills in the areas of national interest such as supercomputing/High Performance Computing (HPC) and cloud computing. BASIL will be the first tool of its kind that can semi-automatically generate secure, optimized, and trustworthy container images with clear information on how to use the images under appropriate licenses. The rules for optimizing the images will be derived from expert knowledge and best practices, such as multi-stage builds and reordering the sequencing of commands to take advantage of caching so that the overall time involved in building the images is reduced. Users of the BASIL tool will provide the recipes for building their applications/workflows in one of the following forms (1) Makefiles/CMakefiles, (2) scripts, (3) commands, or (4) a text-file with predefined keywords and notations using templates provided by the project team. These recipes will be parsed, and Dockerfiles or Singularity definition files will be generated. The parser developed in this project will be another novel contribution of the project. Using a generated Dockerfile or Singularity definition file, a Docker or Singularity image will be built. Next, the image will be scanned for any vulnerabilities, signed, and if the user desires, released in public registries with appropriate licenses. These container images can be tested using the BASIL web portal, and can be pulled to run or deploy on diverse hardware platforms. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Physics at the Information Frontier in the Division of Physics within the Directorate for Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: EXHUME: Extraction for High-Order Unfitted Finite Element Methods,OAC,2103939,David Kamensky,dmkamensky@eng.ucsd.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","Unfitted finite element methods allow for the simulation of physical systems that are difficult if not impossible to simulate using classical finite element methods requiring body-fitted meshes. For instance, unfitted finite element methods can be directly applied to the simulation of physical systems exhibiting a change of domain topology, such as the movement of blood cells through a human capillary or the flow of blood past the heart valves between the four main chambers of the human heart. Unfitted finite element methods also streamline the construction of computational design optimization technologies that optimize the geometry and material layout of an engineered system based on prescribed performance metrics. However, the computer implementation of an unfitted finite element method remains a challenging and time-consuming task even for domain experts. The overarching objective of this project is to construct a novel software library, EXHUME (EXtraction for High-order Unfitted finite element MEthods), to enable the use of classical finite element codes for unfitted finite element analysis. EXHUME will empower a large community of scientists and engineers to employ unfitted finite element methods in their own work, allowing them to carry out biomedical, materials science, and geophysical simulations that have been too expensive or too unstable to realize using classical finite element methods. EXHUME will also improve the fidelity of design optimizations being performed in academia, national laboratories, and industry on a near daily basis.Unfitted finite element methods simplify the finite element solution of PDEs (Partial Differential Equations) on complex and/or deforming domain geometries by relaxing the requirement that the finite element approximation space be defined on a body-fitted mesh whose elements satisfy restrictive shape and connectivity constraints. Early unfitted finite element methods exhibited low-order convergence rates, but recent progress has led to high-order methods. The key ingredient to success of a high-order unfitted finite element method is accurate numerical integration over cut cells (i.e, unfitted elements cut by domain boundaries). EXHUME uses the concept of extraction to express numerical integration over cut cells in terms of basic operations already implemented in typical finite element codes, an integration mesh, and extraction operators expressing unfitted finite element basis functions in terms of canonical shape functions. EXHUME generates integration meshes and extraction operators outside of the confines of a particular finite element code so it may be paired with existing codes with little implementation effort. A key goal of the project is demonstration of EXHUME by connecting it to existing research codes and the popular FEniCS toolchain for finite element analysis. An effort parallel to software development explores accuracy versus efficiency trade-offs associated with 1) approximations made during extraction and 2) novel numerical quadrature schemes for cut cells. The breadth of EXHUME's technical impact is ensured by several factors: 1) the ubiquity of PDEs across nearly all disciplines of science and engineering, 2) the library's interoperability with existing finite element codes, and 3) the generic nature of the EXHUME+FEniCS demonstrative example, which can be applied to arbitrary systems of PDEs. By simplifying the setup of PDE-based computational models, EXHUME+FEniCS enables classroom demonstrations simulating complicated physical scenarios without letting the technical details of numerical methods distract from the scientific principles being taught.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Spatial Ecology Gateway,OAC,2104104,Robert Sinkovits,rssinkovits@ucsd.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Spatial ecology is the study of how landscape characteristics influence the distribution and movement of organisms within their environment. The Spatial Ecology Gateway (SEG) enables researchers, students and wildlife managers to upload biotelemetry data, typically GPS readings, and construct home ranges that allow them to interpret animal space use. Applications of the SEG can include classroom projects, basic research into problems in wildlife ecology, environmental impact studies and mitigation of adverse outcomes such as habitat fragmentation or increased human-wildlife interaction resulting from new development. The SEG insulates users from the underlying computational details so that they can focus on their science rather than mastering the technology. Users of the SEG have the option to generate two-dimensional (2D) or, where applicable, three-dimensional (3D) home ranges. Users are also provided with less computationally intensive tools to perform exploratory analyses.The SEG is built using the HubZero platform, an open-source software platform for building websites that support scientific activities and leverages the Extreme Science and Engineering Discovery Environment (XSEDE) for more computationally demanding tasks. Data can be pulled directly into the SEG from the online community platform Movebank using their REST API. The SEG deploys the Brownian Bridge Movement Model (BBMM), Potential Path Volumes (PPV) and Continuous-Time Movement Modeling (ctmm) methods for home range construction. Tools for exploratory analysis of animal trajectories include net squared displacement, movement path tortuosity and relocation sampling rates versus deployment time. Users can apply basic filtering to their data sets to restrict analyses to specified time ranges or particular animals. The SEG insulates users from having to decide computational details such as choice of resource, number of compute cores or wall clock time. Rather, these decisions are made using logic built into the gateway and based on extensive benchmarking studies.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Biological Infrastructure within the NSF Biosciences Directorate, and by the Division of Information and Intelligent Systems within the NSF Computer and Information Science and Engineering Directorate.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative:RAPID:Leveraging New Data Sources to Analyze the Risk of COVID-19 in Crowded Locations,OAC,2027514,Ashok Srinivasan,asrinivasan@uwf.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","The goal of this project is to create a software infrastructure that will help scientists investigate the risk of the spread of COVID-19 and analyze future epidemics in crowded locations using real-time public webcam videos and location based services (LBS) data. It is motivated by the observation that COVID-19 clusters often arise at sites involving high densities of people. Current strategies suggest coarse scale interventions to prevent this, such as cancellation of activities, which incur substantial economic and social costs. More detailed fine scaled analysis of the movement and interaction patterns of people at crowded locations can suggest interventions, such as changes to crowd management procedures and the design of built environments, that yield social distance without being as disruptive to human activities and the economy. The field of pedestrian dynamics provides mathematical models that can generate such detailed insight. However, these models need data on human behavior, which varies significantly with context and culture. This project will leverage novel data streams, such as public webcams and location based services, to inform the pedestrian dynamics model. Relevant data, models, and software will be made available to benefit other researchers working in this domain, subject to privacy restrictions. The project team will also perform outreach to decision makers so that the scientific insights yield actionable policies contributing to public health. The net result will be critical scientific insight that can generate a transformative impact on the response to the COVID-19 pandemic, including a possible second wave, so that it protects public health while minimizing adverse effects from the interventions.We will accomplish the above work through the following methods and innovations. LBS data can identify crowded locations at a scale of tens of meters and help screen for potential risk by analyzing the long range movement of individuals there. Worldwide video streams can yield finer-grained details of social closeness and other behavioral patterns desirable for accurate modeling. On the other hand, the videos may not be available for potentially high risk locations, nor can they directly answer ?what-if? questions. Videos from contexts similar to the one being modeled will be used to calibrate pedestrian dynamics model parameters, such as walking speeds. Then the trajectories of individual pedestrians will be simulated in the target locations to estimate social closeness. An infection transmission model will be applied to these trajectories to yield estimates of infection spread. This will result in a novel methodology to include diverse real time data into pedestrian dynamics models so that they can quickly and accurately capture human movement patterns in new and evolving situations. The cyberinfrastructure will automatically discover real-time video streams on the Internet and analyze them to determine the pedestrian density, movements, and social distances. The pedestrian dynamics model will be reformulated from the current force-based definition to one that uses pedestrian density and individual speed, both of which can be measured effectively through video analysis. The revised model will be used to produce scientific insight to inform policies, such as steps to mitigate localized outbreaks of COVID-19 and for the systematic reopening, potential re-closing, and permanent changes to economic and social activities.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: HDR: Building the Twenty-First Century Citizen Science Framework to Enable Scientific Discovery Across Disciplines,OAC,1835632,Subhashini Sivagnanam,sivagnan@sdsc.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","A team of experts from five institutions (University of Minnesota, Adler Planetarium, University of Wyoming, Colorado State University, and UC San Diego) links field-based and online analysis capabilities to support citizen science, focusing on three research areas (cell biology, ecology, and astronomy). The project builds on Zooniverse and CitSci.org, leverages the NSF Science Gateways Community Institute, and enhances the quality of citizen science and the experience of its participants.This project creates an integrated Citizen Science Cyberinfrastructure (CSCI) framework that expands the capacity of research communities across several disciplines to use citizen science as a suitable and sustainable research methodology. CSCI produces three improvements to the infrastructure for citizen science already provided by Zooniverse and CitSci.org: - Combining Modes - connecting the process of data collection and analysis; - Smart Assignment - improving the assignment of tasks during analysis; and - New Data Models - exploring the Data-as-Subject model. By treating time series data as data, this model removes the need to create images for classification and facilitates more complex workflows. These improvements are motivated and investigated through three distinct scientific cases: - Biomedicine (3D Morphology of Cell Nucleus). Currently, Zooniverse 'Etch-a-Cell' volunteers provide annotations of cellular components in images from high-resolution microscopy, where a single cell provides a stack containing thousands of sliced images. The Smart Task Assignment capability incorporates this information, so volunteers are not shown each image in a stack where machines or other volunteers have already evaluated some subset of data. - Ecology (Identifying Individual Animals). When monitoring wide-ranging wildlife populations, identification of individual animals is needed for robust estimates of population sizes and trends. This use case combines field collection and data analysis with deep learning to improve results. - Astronomy (Characterizing Lightcurves). Astronomical time series data reveal a variety of behaviors, such as stellar flares or planetary transits. The existing Zooniverse data model requires classification of individual images before aggregation of results and transformation back to refer to the original data. By using the Data-as-Subject model and the Smart Task Assignment capability, volunteers will be able to scan through the entire time series in a machine-aided manner to determine specific light curve characteristics.The team explores the use of recurrent neural networks (RNNs) to determine automated learning architectures best suited to the projects. Of particular interest is how the degree to which neighboring subjects are coupled affects performance. The integration of existing tools, which is based on application programming interfaces (APIs), also facilitates further tool integration. The effort creates a citizen science framework that directly advances knowledge for three science use cases in biomedicine, ecology, and astronomy, and combines field-collected data with data analysis. This has the ability to solve key problems in the individual applications, as well as benefiting the research of the dozens of projects on the Zooniverse platform. It provides benefits to researchers using citizen scientists, and to the nearly 1.6 million citizen scientists themselves.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Research on Learning in Formal and Informal Settings, within the NSF Directorate for Education and Human Resources.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Software: Distributed Workflows for Cyberexperimentation,OAC,1835608,Jelena Mirkovic,mirkovic@isi.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Industry and academic research in computing systems, networking and security suffers from the inability to demonstrate and validate research outcomes, because the process of research consists of complex but largely experiments on distributed, large-scale, networked test-beds. This project will address this issue by enabling rigorous and repeatable experimentation on test-beds by enabling experiments to be defined as executable workflows, that can then be repeatably run, as well as managed. This project will develop and deploy a software framework called Elie. Elie consists of an experiment representation capability, along with several supporting services that make experimentation workflows robust, fault tolerant, easily sharable and reusable. Specifically, Elie consists of: (1) A new experiment representation, called DEW, which encodes experiment behavior and allocation constraints instead of just experiment topology. (2) Capture tools, which capture a researcher's manual actions on the testbed and facilitate script-building, and DEW generation (3) Testbed-integrated version control, (4) An experiment orchestration mechanism, (5) Lightweight failure detection, (6) A GUI, which allows for experiment management throughout an experiment lifecycle. Elie's technologies can be used by researchers as a full environment or as individual tools. Collectively these technologies provide basic testbed support throughout an experiment lifecycle including the design, running, analysis, monitoring, storage and sharing stages. All of Elie's technologies are extensible, and open-sourced, to allow the research community to contribute to their development and customize them to their needs. This project fundamentally differs from the prior work in its goal to co-exist with current testbed experimentation approaches (manual and scripted experimentation) and to be applicable to a broad range of testbeds (via its translation and generation tools). Outcomes of this project will significantly improve ease of testbed experimentation, by offloading tedious, repetitive and detail-sensitive tasks to testbeds. This will shorten experiment duration and improve quality and reliability of results. This work will further make experiments more robust, enable easy sharing and reuse of experiments with minimal user effort, and facilitate repeatability and reproducibility. Overall, such advances in testbed experimentation will enable vertical development (building upon the work of others) in fields, which use testbeds, such as distributed systems, networking and cybersecurity. Sharing and reuse will also improve quality of scientific research in these fields, by enabling creation of larger and more complex experiments built on the shared artifacts of other researchers. The project team will also conduct extensive user outreach and building collaborations with other researchers that work on improving rigor of testbed experimentation, with the goal of building a strong user and developer community for Elie.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Elements:Software:NSCI: Empowering Data-driven Discovery with a Provenance Collection, Management, and Analysis Software Infrastructure",OAC,1835892,Yong Chen,yong.chen@ttu.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Scientific breakthroughs are increasingly powered by advanced computing and data analysis capabilities delivered by high performance computing (HPC) systems. In the meantime, many scientific problems have moved to a level of complexity that the ability of understanding the results, auditing how a result is generated, and reproducing the important experiments or simulation results, is critical to scientists. Enabling such a capability in HPC systems requires a holistic collection, management, and analysis software infrastructure for ""provenance"" data, the metadata that describes the history of a piece of data. Such a software infrastructure does not exist yet, which motivates the proposed software development of a lightweight provenance service. With such a software element, many advanced data management functionalities such as identifying the data sources, parameters, or assumptions behind a given result, auditing data history and usage, or understanding the detailed process that how different input data are transformed into outputs can be possible. Responding to the National Strategic Computing Initiative, this project will provide an attractive software infrastructure to future national HPC systems to improve the productivity of science in complex HPC simulation and analysis cycles. The project team will also recruit underrepresented students, mentor graduate and undergraduate students, integrate results into curriculum, and publish and disseminate results.The lightweight provenance service software on HPC systems will provide: 1) an always-on, background service that automatically and transparently collects and manages provenance for scientific applications, 2) captures comprehensive provenance with accurate causality to support a wide range of use cases, and 3) provides easy-to-use analysis tools for scientists to quickly explore and utilize the provenance. This project will integrate the development, education, and outreach efforts tightly together.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Convergence of Bayesian inverse methods and scientific machine learning in Earth system models through universal differentiable programming,OAC,2103942,Patrick Heimbach,heimbach@utexas.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Understanding and quantifying parameter sensitivity of simulated systems, such as the numerical models of physical systems and mathematical renderings of neural networks, are essential in simulation-based science (SBS) and scientific machine learning (SciML). They are the key ingredients in Bayesian inference and neural network training. Seizing on the opportunity of emerging open-source Earth system model development in the Julia high-level programming language, this project is endowing these open-source models with automatic differentiation (AD) enabled derivative information, making these converging data science and simulation-based science tools available to a much broader research and data science community. Enabling a general-purpose AD framework which can handle both large-scale Earth system models as well as SciML algorithms, such as physics-informed neural networks or neural differential equations, will enable seamless integration of these approaches for hybrid Bayesian inversion and Bayesian machine learning. It merges big data science, in which available data enable model discovery with sparse data science, and the model structure is exploited in the selection of surrogate models representing data-informed subspaces and fulfilling conservation laws. The emerging Julia language engages a new generation of researchers and software engineers, channeling much needed talent into computational science approaches to climate modeling. Through dedicated community outreach programs (e.g., Hackathons, Minisymposia, Tutorials) the project team will be working toward increasing equity, diversity, and inclusion across the participating disciplines.The project is developing a framework for universal differentiable programming and open-source, general-purpose AD that unifies these algorithmic frameworks within Julia programming language. The general-purpose AD framework in Julia leverages the composability of Julia software packages and the differentiable programming approach that underlies many of the SciML and high-performance scientific computing packages. Compared to most current modeling systems targeted for HPC, Julia is ideally suited for heterogeneous parallel computing hardware (e.g., CUDA, ROCm, oneAPI, ARM, PowerPC, x86 64, TPUs). The project is bringing together expertise in AD targeted at Earth system data assimilation in high performance computing environments with SciML expertise. The project team is working with the Julia Computing organization and package developers to ensure sustainability of the developed frameworks. The project?s Earth system flagship applications consist of (i) an open-source, AD-enabled ocean general circulation model that is being developed separately as part of the Climate Modelling Alliance (CliMA), and (2) an open-source, AD-enabled ice flow model. Each of these application frameworks is being made available to the community for science application, in which derivative (gradient or Hessian) information represent key algorithmic enabling tools. These include SciML-based training of surrogate models (data-driven and/or model-informed), parameter and state estimation, data assimilation for model initialization, uncertainty quantification (Hessian-based and gradient-informed MCMC) and quantitative observing system design. Academic and industry partners are involved, who are using the frameworks for developing efficient power grids, personalized precision pharmacometrics, and improved EEG design.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Frameworks: An Interoperable Software Ecosystem for Many-Body Electronic Structure Calculations,OAC,2103991,Feliciano Giustino,fgiustino@oden.utexas.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","The development of advanced materials is a key driver of progress in areas of national strategic importance, such as energy generation, storage, and distribution, wireless communications, and quantum technologies. For example the operation of solar panels, solid-state batteries, touch screens, flat-panel displays, and quantum computer prototypes critically relies on the properties and functionalities of advanced materials down to the scale of individual atoms. Further improving the performance of these materials, as well as designing brand new materials with novel functionalities, requires a detailed understanding of how macroscopic properties, such as the ability to carry electricity, to absorb and emit light, and to store chemical energy, emerge from the elemental composition and the atomic structure of each compound. In this context, simulating materials behavior by solving the fundamental equations of quantum mechanics on supercomputers has become an indispensable complement to experimental research. Today there exists an abundance of high-performance computing software to investigate and predict the properties of materials in their lowest energy state, or ground state. These tools are primarily based on density functional theory, an incredibly successful conceptual paradigm that allows us to find approximate yet accurate solutions of the Schrdinger equation of quantum mechanics for entire materials. While these methods are essential for predicting structural and energetic properties such as thermodynamic phase diagrams, they are not suitable to describe more advanced functional properties such as light-matter interactions, charge transport under electric and magnetic fields, and macroscopic quantum phenomena such as superconductivity. The current project fills this gap by developing a comprehensive software ecosystem to compute and predict functional properties of materials beyond what is currently possible with density functional theory. The cyberinfrastructure supported by this grant will enable the rational design of advanced functional materials at the atomic scale, and will underpin the development of next-generation materials for energy, computing, and quantum technologies. The research program will be tightly integrated with educational activities to promote scientific research in diverse communities. To this end, webinars, schools and hackathons for users and developers will be organized annually.The aim of this project is to create an interoperable software ecosystem to model and design materials at the atomic scale using many-body field-theoretic approaches. Many-body electronic structure methods define a gold standard for accuracy, reliability, and predictive power, but the widespread adoption of these methods in academia and in industry is hindered by the complexity of the underlying theories and algorithms, as well as the lack of broad interoperability and shared data standards. The project expands and combines the complementary strengths of three software packages, EPW, BerkeleyGW, and SternheimerGW, into a user-centric, containerized simulation laboratory with shared data formats and built-in compatibility layers for major density-functional theory codes. This cyberinfrastructure advances understanding of the interplay between electronic and lattice degrees of freedom in advanced materials, and expands the range of properties that can be calculated with predictive accuracy, including: finite-temperature quasiparticle band structures; light absorption and emission spectra; excitons, polarons, and their couplings; superconductivity; carrier transport; and driven quantum systems. Furthermore, this cyberinfrastructure will accelerate future software development by distributing curated, reusable, and interoperable open-source code, and by providing a platform to develop and test new algorithms and software for many-body electronic structure methods. Central to the proposed effort is the training of a diverse, inclusive, and globally competitive STEM workforce cutting across data-driven materials research and cyberinfrastructure development.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate for Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Proposal: Frameworks: Project Tapis: Next Generation Software for Distributed Research,OAC,1931439,Joseph Stubbs,jstubbs@tacc.utexas.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","The goal of a robust cyberinfrastructure (CI) ecosystem is to become a catalyst for discovery and innovation by fostering the development of software frameworks as sustainable production-quality services. Modern science and engineering research increasingly span multiple, geographically distributed data centers and leverage instruments, experimental facilities and a network of national and regional CI. The Tapis framework will enable scientists to accomplish computational and data intensive research in a secure, scalable, and reproducible way allowing scientists to focus on their research instead of the technology needed to accomplish it. Tapis will allow easier implementation, sharing and re-use of complex computational applications, workflows, and infrastructure and enable analysis previously too challenging for researchers. The framework will maximize application portability, allowing flexible scheduling of geographically distributed computational workloads, offer a web-based science-as-a-service to enable multi-facility, decentralized deployments, and provide production-grade support for sensors and streaming data. Tapis will impact multiple science domains, geographic and underrepresented communities with the potential to tackle the world's most important scientific problems spanning astronomy, climate science, medicine, natural hazards, and sustainability science. Education and outreach will include sponsored workshops, hackathons and training materials covering the platform and providing examples to encourage widespread adoption for users across a variety of technical skills and targeting the next generation of young researchers and professionals through immersive workshops and professional development opportunities.Tapis, will be a new platform for distributed computational experiments that leverages NSF's investments in the Agave, Abaco and CHORDS projects. The Tapis software framework will 1) provide production-grade support for sensors and streaming data, 2) maximize application portability, allowing flexible scheduling of computational workloads across geographically distributed providers, and 3) provide science-as-a-service HTTP-based RESTful APIs to enable multi-facility, decentralized deployments that are both secure and scalable. Working alongside a diverse set of domain researchers to drive real-world use cases, Tapis will be the underlying cyberinfrastructure for computational workflows and science gateways. Tapis will leverage containers to maximize application portability, allowing flexible scheduling of computational workloads across geographically distributed providers. The project will achieve this flexibility by introducing execution system capabilities and application requirements throughout the framework. The Jobs service will be run in a distributed manner to take advantage of data locality and, optionally, to schedule jobs on underutilized systems. Tapis will deploy in centralized or distributed configurations using a microservices architecture that includes a novel, decentralized security and authorization kernel. This kernel can be deployed on-premises to retain local control over confidential data. Custom microservices can be plugged into the security kernel to provide new capabilities, resulting in a cyberinfrastructure ecosystem for distributed computing. To effectively execute Tapis, teams from the Texas Advanced Computing Center (TACC), the University of Texas at Austin (UT), and the University of Hawaii (UH) will leverage a long-standing collaboration to support investigator-driven, geographically distributed, data-intensive research.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: NSCI : Computational and data innovation implementing a national community hydrologic modeling framework for scientific discovery,OAC,1835818,Anthony Castronova,acastronova@cuahsi.org,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
NSCI Elements: Software - PFSTRASE - A Parallel FileSystem TRacing and Analysis SErvice to Enhance Cyberinfrastructure Performance and Reliability,OAC,1835135,Richard Evans,rtevans@tacc.utexas.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","This project will develop an open-source software service, the Parallel FileSystem TRacing and Analysis SErvice (PFSTRASE), that improves the reliability and performance of data storage systems for the nation?s largest supercomputers. As simulations and computations represent reality more faithfully they grow commensurately in scale along with the size of the data they consume and generate. To handle the storage and movement of this data, supercomputing systems are built on the backbone of massively parallel data storage systems. Due to their parallel nature these storage systems are capable of moving data at hundreds of times the speed of conventional storage systems, enabling otherwise impractical computations. The performance capabilities these storage systems provide is accompanied by a complexity that results in them often functioning significantly less than optimally and even in some instances failing. This results in wasted computational time and ultimately lost scientific progress. The state of development of tools that could cast light on these problems and improve storage system reliability and performance is inadequate for current and future computing systems. PFSTRASE will fill this gap by continually and automatically monitoring storage system health and performance, providing insights through an easy to use interface that will improve the reliability and performance of storage and supercomputer systems. Parallel filesystems (PFSs) are the most critical high-availability components of High Performance Computing (HPC) architectures, providing input/output (I/O) services to running computations, the environment that users and system services operate in, and storage for applications and data. Because of this central role, failure or performance degradation events in the PFS impact every user of an HPC resource. PFS events must be dealt with quickly and effectively by system administrators; however, there is typically insufficient information to establish precise causal relationships between PFS activity and events, impeding the implementation of timely and targeted remedies. To fill this information gap, an open-source Parallel FileSystem TRacing and Analysis SErvice (PFSTRASE) that traces and analyzes the requisite data to establish causal relationships between PFS activity and both realized and imminent events will be developed. This project will implement the service for the open-source Lustre filesystem, which is the most commonly used PFS at large-scale HPC sites. Loads for specific PFS directory and file operations will be measured and incorporated into the service to construct authentic server load contributions from every job, process, and user. The service?s infrastructure will continuously monitor the entire PFS and generate a real-time, seamless representation that connects contributions of jobs, processes, and users to storage server loads, network bandwidth, and storage capacities. The infrastructure will provide an easily navigable web interface that presents this data, both real-time and historical, in a visual format.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Software: Software Health Monitoring and Improvement Framework,OAC,1835292,Yuanfang Cai,yfcai@cs.drexel.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Software underpins every aspects of modern life, with significant impact in society. Poor quality software can cause huge financial losses, even threatening people's lives. Software quality is even more critical within the scientific community. The reproducibility of research results and sustainability of the research itself, heavily depend on the quality of the software developed by scientists, who usually acquire basics of software programming but are not aware of the best design practices. As a consequence, several existing open access scientific software packages are known to be hard to use and evolve due to their poor quality, as highlighted in recent studies. This project will integrate and enhance recent advances in software issue detection and refactoring techniques, created by the PIs and sponsored by NSF, in order to serve diverse scientific and engineering domains, detecting and fixing software quality issues effectively. This proposal seeks to bridge the gap between software engineering community and other science and engineering community in general. It will provide quantitative comparisons of software projects against an industrial benchmark, enable users to pinpoint software issues responsible for high maintenance costs, visualize the severity of the detected issues, and refactor them using the proposed interactive refactoring framework. The proposed framework will bring together software users and software developers by enabling non software experts to post software challenges for the software community to solve, which will, in turn, boost the research and advances in software research.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Improving the Understanding and Representation of Atmospheric Gravity Waves using High-Resolution Observations and Machine Learning,OAC,2004572,Edwin Gerber,gerber@cims.nyu.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Geophysical gravity waves are a ubiquitous phenomenon in Earth?s atmosphere and ocean, made possible by the interaction of gravity with a stratified, or layered fluid. They are excited in the atmosphere when winds flow over mountains, by thunderstorms and other strong convective systems, and when winter storms intensify. Gravity waves play an important role in the momentum and energy balance of the atmosphere, with direct impacts on surface weather and climate through their effect on the variability of key features of the climate system such as the jet streams and stratospheric polar vortices. These waves present a challenge to weather and climate prediction: waves on scales of 100 meters to 100 kilometers can neither be systematically measured with conventional observational systems, nor properly resolved in global atmospheric models. As a result, these waves must be represented, or approximated, based on the resolved flow that can be directly simulated. Current representations of gravity waves are severely limited by computational necessity and the scarcity of observations, leading to inaccuracies or uncertainties in short term weather and long term climate predictions. The objective of this project is to leverage unprecedented observations from Loon high altitude balloons and use specialized high resolution computer simulations and machine learning techniques to develop accurate, data-informed representation of gravity waves. The outcomes of this project are expected to result in better weather and climate models, thus improving short term forecasts of weather extremes and long term climate change projections, which have substantial societal benefits. Furthermore, the project will support the training of 3 Ph.D. students, 4 postdocs, and 10 undergraduate summer researchers to work at the intersection of atmospheric dynamics, climate modeling, and data science, thus preparing the next generation of scientists for interdisciplinary careers.The project will deliver two key advances. First, it will open up a new data source to constrain gravity wave momentum transport in the atmosphere. Loon LLC has been launching super pressure balloons since 2013 to provide global internet coverage. Very high resolution position, temperature, and pressure observations (taken every 60 seconds) are available from thousands of flights. This provides an unprecedented source of high resolution observations to constrain gravity wave sources and propagation. The project will process the balloon measurements and, in concert with novel high resolution simulations, establish a publicly available dataset to open up a potentially transformational resource for observationally constrained assessment of gravity wave sources, propagation, and breaking. The second transformation will be using machine learning techniques to develop computationally feasible representations of momentum deposition by gravity waves. Current physics-based representations only account for vertical propagation of the waves (i.e., they are one dimensional) and ignore their horizontal propagation. Using the data based on the Loon measurements and high resolution models, one and three dimensional data driven representations will be developed to more accurately and efficiently represent the effects of gravity waves in weather and climate models. These novel representations will be implemented in idealized atmospheric models to study the role of gravity waves in the variability of the extratropical jet streams, the Quasi Biennial Oscillation (a slow variation of the winds in the tropical stratosphere) and the polar vortex of the winter stratosphere, enabling better understanding their response to increased atmospheric greenhouse gas concentrations.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Elements: Software: Autonomous, Robust, and Optimal In-Silico Experimental Design Platform for Accelerating Innovations in Materials Discovery",OAC,1835690,Byung-Jun Yoon,bjyoon@ece.tamu.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Accelerating the development of novel materials that have desirable properties is a critical challenge as it can facilitate advances in diverse fields across science, engineering, and medicine with significant contributions to economic growth. For example, the US Materials Genome Initiative calls for cutting the time for bringing new materials from discovery to deployment by half at a fraction of the cost, by integrating experiments, computer simulations, and data analytics. However, the current prevailing practice in materials discovery relies on trial-and-error experimental campaigns and/or high-throughput screening approaches, which cannot efficiently explore the huge design space to develop materials with the targeted properties. Furthermore, measurements of material composition, structure, and properties often contain considerable errors due to technical limitations in materials synthesis and characterization, making this exploration even more challenging. This project aims to develop a software platform for robust autonomous materials discovery that can shift the current trial-and-error practice to an informatics-driven one that can potentially expedite the discovery of novel materials at substantially reduced cost and time. Throughout the project, the PI and Co-PIs will mentor students and equip them with the skills necessary to tackle interdisciplinary problems that involve materials science, computing, optimization, and artificial intelligence. Research findings in the project will be incorporated into the courses taught by the PI and Co-PIs, thereby enriching the learning experience of students.The objective of this project is to develop an effective in-silico experimental design platform to accelerate the discovery of novel materials. The platform will be built on optimal Bayesian learning and experimental design methodologies that can translate scientific principles in materials, physics, and chemistry into predictive models, in a way that takes model and data uncertainty into account. The optimal Bayesian experimental design framework will enable the collection of smart data that can help exploring the material design space efficiently, without relying on slow and costly trial-and-error and/or high-throughput screening approaches. The developed methodologies will be integrated into MSGalaxy, a modular scientific workflow management system, resulting in an accessible, reproducible, and transparent computational platform for accelerated materials discovery that allows easy and flexible customization as well as synergistic contributions from researchers across different disciplines.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Data: HDR: Nanocomposites to Metamaterials: A Knowledge Graph Framework,OAC,1835782,Wei Chen,weichen@northwestern.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","A team of experts from four universities (Duke, RPI, Caltech and Northwestern) creates an open source data resource for the polymer nanocomposites and metamaterials communities. A broad spectrum of users will be able to query the system, identify materials that may have certain characteristics, and automatically produce information about these materials. The new capability (MetaMine) is based on previous work by the research team in nanomaterials (NanoMine). The effort focuses upon two significant domain problems: discovery of factors controlling the dissipation peak in nanocomposites, and tailored mechanical response in metamaterials motivated by an application to personalize running shoes. The project will significantly improve the representation of data and the robustness with which user communities can identify promising materials applications. By expanding interaction of the nanocomposite and metamaterials communities with curated data resources, the project enables new collaborations in materials discovery and design. Strong connections with the National Institute of Standards and Technology (NIST), the Air Force Research Laboratory (AFRL), and Lockheed Martin facilitate industry and government use of the resulting knowledge base. The project develops an open source Materials Knowledge Graph (MKG) framework. The framework for materials includes extensible semantic infrastructure, customizable user templates, semi-automatic curation tools, ontology-enabled design tools and custom user dashboards. The work generalizes a prototype data resource (NanoMine) previously developed by the researchers, and demonstrates the extensibility of this framework to metamaterials. NanoMine enables annotation, organization and data storage on a wide variety of nanocomposite samples, including information on composition, processing, microstructure and properties. The extensibility will be demonstrated through creation of a MetaMine module for metamaterials, parallel to the NanoMine module for nanocomposites. The frameworks will allow for curation of data sets and end-user discovery of processing-structure-property relationships. The work supports the Materials Genome Initiative by creating an extensible data ecosystem to share and re-use materials data, enabling faster development of materials via robust testing of models and application of analysis tools. The capability will be compatible with the NIST Material Data Curator System, and the team also engages both AFRL and Lockheed Martin to facilitate industry and government use of the resulting knowledge base. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate for Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Framework: Sofware: Collaborative Research: CyberWater -An open and sustainable framework for diverse data and model integration with provenance and access to HPC,OAC,2018500,Yang Zhang,ya.zhang@northeastern.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities. The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery. The models and datasets cover fields such as hydrology, biology, environmental engineering and climate. The project also addresses one of the key issues for extreme-scale computing: scalable file systems. The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI). The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity. The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use. To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts. The project builds upon an existing prototype developed by the lead investigator; basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control. The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI. For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated. The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Scalable Modular Software and Methods for High-Accuracy Materials and Condensed Phase Chemistry Simulation,OAC,1931321,Timothy Berkelbach,tcb2112@columbia.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","How electrons are arranged in materials gives rise to a large variety of different behaviors. We can observe these behaviors and use them in various technologies. However, the prediction of these behaviors is a serious challenge. This makes the successful design of new materials harder. The goal of the Materials Genome Initiative is to use computer simulations to model electrons according to the laws of quantum physics. This will allow researchers to design new materials with desired properties. This project aims to build fast and accurate computer programs which simulate those new materials. These programs combine advances in computer science, quantum chemistry, and condensed-matter physics. They will be implemented in an open-source Python-based community code. This distribution model allows other researchers to use this code and to contribute new features.This research addresses gaps in existing software cyberinfrastructure in quantum materials simulation, by developing novel parallel implementations of low-scaling, high-accuracy methods. In particular, new techniques for mean-field calculations will be developed, which will act as groundwork for periodic coupled-cluster and quantum Monte Carlo methods. State-of-the-art techniques in sparsity and tensor decomposition will be employed to achieve good system-size scaling while retaining accuracy within each of these numerical schemes. Critically, the methods will be developed using efficient high-level software abstractions, implemented as Python-level modules within PySCF that leverage the Cyclops library for massively-parallel execution. The library software infrastructure will also be extended to maximize productivity via source-to-source automatic differentiation, as well as to enable execution of sparse kernels on emerging GPU-based supercomputing architectures. This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure, and the Division of Materials Research and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: NSCI: Computational and data innovation implementing a national community hydrologic modeling framework for scientific discovery,OAC,1835864,Paul Constantine,paul.constantine@colorado.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF),OAC,1835874,Peter Cummings,peter.cummings@vanderbilt.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Building a Collaboration Infrastructure: CyberWater2 -- A Sustainable Data/Model Integration Framework,OAC,2209833,Xu Liang,xuliang@pitt.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Natural hazards, such as coastal and inland flooding caused by Hurricanes and severe drought and its associated wildfire, have been occurring with unprecedented frequency, induced by climate changes that encompass hydrological, biological, environmental, atmospheric, ocean, and other geosciences. Such hazards have caused not only profound damages to our environment and required tremendous efforts to recover, but also cost people's lives. To mitigate these potential disasters, it is a critical time to tackle their associated scientific questions both fundamental and large-scale that impact on the health, resilience, and sustainability of the Earth system we live in. The problems are complex and multidisciplinary, and researchers and practitioners from diverse fields must work together to find solutions. By its nature, Earth system models are comprised of component models ? from land surface, to rivers, coastal regions, ocean, sea ice, and atmosphere, where each component model is coupled with one another. As science advances, a component model or its subsystems may have to be replaced because of new understanding, or because different perspectives must be explored and tested for the credence of different combinations to find the most credible predictions for different conditions at different locations. Such tasks often require substantial efforts and time and can become a bottleneck. This project is aimed at developing a new open-source cyberinfrastructure framework, Cyberwater2, in which model coupling is shifted from the current ""code-coupling"" approach to a new ""information coupling"" approach, and can be configured without writing glue code. This minimizes the need to access and modify each participating model's original code, and removes a major obstacle for large-scale cross-institutional collaborations and scientific investigations across disciplines and geographic boundaries. CyberWater2 is designed for diverse research communities including water, climate, coastal, engineering, and beyond. With our framework, researchers can devote their collaborative energy on problem solving and exploration of new frontiers, while using CyberWater2 to effectively achieve two-way open model couplings across platforms, model parameter calibration, data assimilation, testing/validations/comparisons, etc.The goal of this project is to make it easier to conduct large scale collaboration on complex problems and solve them efficiently, accurately and in-depth by developing a cyberinfrastructure, CyberWatyer2, that (1) significantly eliminates ""glue"" coding for two-way couplings across heterogeneous computing platforms, disciplines, and organizations; (2) automates complex model calibration and facilitates data assimilation processes applicable to various models; (3) supports task-based and in-situ hybrid workflow for greatly improved efficiency on two-way coupling across heterogeneous platforms; (4) provides a CyberWater2 server and web service framework for users in addition to the standalone systems; (5) enables sustainable data access from diverse sources by automatically adapting data agents to the changes (e.g., API interfaces) made to external data sources by providers; and (6) enables automated resource planning with intelligent site recommendation for High Performance Computing (HPC)/Cloud access on demand to maximize users' benefits.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Earth Sciences in the Directorate of Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Building a Collaboration Infrastructure: CyberWater2 -- A Sustainable Data/Model Integration Framework,OAC,2209834,Lan Lin,llin4@bsu.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Natural hazards, such as coastal and inland flooding caused by Hurricanes and severe drought and its associated wildfire, have been occurring with unprecedented frequency, induced by climate changes that encompass hydrological, biological, environmental, atmospheric, ocean, and other geosciences. Such hazards have caused not only profound damages to our environment and required tremendous efforts to recover, but also cost people's lives. To mitigate these potential disasters, it is a critical time to tackle their associated scientific questions both fundamental and large-scale that impact on the health, resilience, and sustainability of the Earth system we live in. The problems are complex and multidisciplinary, and researchers and practitioners from diverse fields must work together to find solutions. By its nature, Earth system models are comprised of component models ? from land surface, to rivers, coastal regions, ocean, sea ice, and atmosphere, where each component model is coupled with one another. As science advances, a component model or its subsystems may have to be replaced because of new understanding, or because different perspectives must be explored and tested for the credence of different combinations to find the most credible predictions for different conditions at different locations. Such tasks often require substantial efforts and time and can become a bottleneck. This project is aimed at developing a new open-source cyberinfrastructure framework, Cyberwater2, in which model coupling is shifted from the current ""code-coupling"" approach to a new ""information coupling"" approach, and can be configured without writing glue code. This minimizes the need to access and modify each participating model's original code, and removes a major obstacle for large-scale cross-institutional collaborations and scientific investigations across disciplines and geographic boundaries. CyberWater2 is designed for diverse research communities including water, climate, coastal, engineering, and beyond. With our framework, researchers can devote their collaborative energy on problem solving and exploration of new frontiers, while using CyberWater2 to effectively achieve two-way open model couplings across platforms, model parameter calibration, data assimilation, testing/validations/comparisons, etc.The goal of this project is to make it easier to conduct large scale collaboration on complex problems and solve them efficiently, accurately and in-depth by developing a cyberinfrastructure, CyberWatyer2, that (1) significantly eliminates ""glue"" coding for two-way couplings across heterogeneous computing platforms, disciplines, and organizations; (2) automates complex model calibration and facilitates data assimilation processes applicable to various models; (3) supports task-based and in-situ hybrid workflow for greatly improved efficiency on two-way coupling across heterogeneous platforms; (4) provides a CyberWater2 server and web service framework for users in addition to the standalone systems; (5) enables sustainable data access from diverse sources by automatically adapting data agents to the changes (e.g., API interfaces) made to external data sources by providers; and (6) enables automated resource planning with intelligent site recommendation for High Performance Computing (HPC)/Cloud access on demand to maximize users' benefits.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Earth Sciences in the Directorate of Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Enriching Scholarly Communication with Augmented Reality,OAC,2209623,Alyssa Goodman,agoodman@cfa.harvard.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Today?s online shoppers can use augmented reality (AR) to aim their smartphones at an empty corner of their living room to see how a particular new lamp might look there, and diners can instantly see a restaurant's menu on their phones just by scanning a QR code posted at their table. This project will leverage the tremendous investments in AR made by the corporate world over the past several years, and the familiar ease of QR codes, to allow astronomers to see and explore the 3D Universe just as easily as they might shop for a new couch. Building on their 2021 success in publishing the first AR-enhanced figure in an American Astronomical Society Journal, the funding from this award will be used to create a robust system allowing any author to publish figures showcasing high-dimensional data in augmented reality environments. No expensive equipment beyond the same smartphones and tablets used by online shoppers will be needed. Astronomers will be able to see and explore their data in ""3D"" by walking around projections of it hovering above flat surfaces, or holding in their hands using AR target devices. Imagine, for example, a jet from a black hole, spewing out material from the center of a simulated galaxy, projected just above a researcher?s kitchen table, etc.Over the course of the project, the team will design, repeatedly test, and ultimately deploy an efficient and effective end-to-end system for embedding augmented reality figures in scholarly journals. By enriching scholarly communication, this new AR-based system is expected to accelerate the pace of scientific discovery. The system created will extend across multiple modular cyberinfrastructure components, including: data format standards; data analysis software; 3D conversion tooling; AR integration pipelines; visual ID encoding infrastructure; and the publication process. The system for authoring and deploying AR figures created and tested under this proposal represents cyberinfrastructure innovation that will ultimately open completely new channels for communication amongst all who rely on effective communication of high-dimensional data.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate for Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Collaborative Research: Frameworks: Interoperable High-Performance Classical, Machine Learning and Quantum Free Energy Methods in AMBER",OAC,2209717,Kenneth Merz,merzjrke@msu.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","With support from the Office of Advanced Infrastructure and the Division of Chemistry at NSF, Professor Merz and his group will work on molecular simulation cyberinfrastructure. Molecular simulations have become an invaluable tool for research and technology development in chemical, pharmaceutical, and materials sciences. With the availability of specialized hardware such as graphics processing units (GPUs), molecular dynamics simulations using classical or molecular mechanical force fields have reached the spatial and temporal scales needed to address important real-world problems in the chemical and biological sciences. Free energy simulations are a particularly important and challenging class of molecular simulations that are critical to gain a predictive understanding of chemical processes. For example, free energy methods can predict the barrier height and rates for chemical reactions, whether a reaction will occur, or how tightly a drug binds to a target. These predictions are extremely valuable for the design of new catalytic agents or drugs. However, the predictive capability of free energy simulations is sensitive to the underlying model that describes the inter-atomic potential energy and forces. Accurate free energy simulations of chemical processes require potential energy models that capture the essential physics and can respond to changes in the chemical environment, but conventional force field models are unsuitable for many processes involving bond breaking and formation as seen, for example, in catalyst design. Consequently, there is great need to extend the scope of free energy methods by enabling the use of a broader range of potential energy models that are more accurate as well as reactive and/or capable of quantum mechanical many-body polarization and charge transfer. The cyberinfrastructure created by this project allows for the routine application of free energy methods, using quantum mechanics, machine learning, reactive and classical potentials to a myriad of important problems that advance the state-of-the art in the biological and chemical sciences. The tools can be applied by a range of scientists to address fundamental problems of national interest, for example, in the design of drugs against zoonotic diseases (e.g., COVID-19), the design of materials with novel functions and in the design of improved batteries. Given the sophistication of the methods employed, education of a diverse pool of chemical, biological and computer scientists to advance this field is essential and is addressed in this project, thereby training the next generation of computational scientists that will form the backbone of the work force of the future. The project develops accurate and efficient free energy software within a powerful new multiscale modeling framework in the AMBER suite of programs for applications in chemistry, biology, and materials science. The multiscale framework enables the design and use of new classes of mixed-method force fields that involve interoperability between several existing and emerging reactive, machine learning and quantum many-body potentials. These potentials have enhanced accuracy, robustness, and predictive capability compared to classical molecular mechanical force fields and enable the study of chemical reactions and catalysis. The cyberinfrastructure supports innovative multi-layered hybrid potentials that can be customized to meet the needs of complex applications in biotechnology development, enzyme design and drug discovery. A robust endpoint ""book-ending"" approach that leverages the GPU-accelerated capability of the AMBER molecular dynamics engine is used to reach these goals. Specifically, the open-source high-performance software for free energy simulations is designed for multi-layered hybrid potentials using combinations of linear-scaling many-body quantum mechanical methods via the GPU-accelerated QUICK package, scalable reactive ReaxFF force fields via the PuReMD package, as well as the recently developed DeepMD-SE, ANAKIN-ME (ANI) and AP-Net families of machine learning potentials. The cyberinfrastructure is built upon the existing high-performance CUDA MD engine in AMBER and extends it to a broad range of GPU-accelerated architectures using industry-standard programming models. Scalability is ensured using innovative parallel algorithms. High impact is achieved by leveraging AMBER's broad user base to expand the scope and success of FE applications. In this way, the project leverages existing recognized capabilities and actively engages a diverse team of collaborators and the broader molecular simulations community. The cyberinfrastructure delivered by the project enables a wide range of new and enhanced applications for a broad community of users in academia, industry, and national laboratories. These applications include drug discovery, enzyme catalysis, and biomaterials design. The AMBER suite of programs has a long-standing extensive worldwide userbase, and is widely used on national production cyberinfrastructure. The enhancement of AMBER as an established, proven sustainable, and widely used package will ensure that the software has a broad impact well beyond the end of the project. The project will also train a diverse population of students and researchers in theory, programming, computational chemistry/biology, computer science, scientific writing, and communication.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Chemistry within the NSF Directorate for Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Software: Software Health Monitoring and Improvement Framework,OAC,2227248,Marouane Kessentini,kessentini@oakland.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Software underpins every aspects of modern life, with significant impact in society. Poor quality software can cause huge financial losses, even threatening people's lives. Software quality is even more critical within the scientific community. The reproducibility of research results and sustainability of the research itself, heavily depend on the quality of the software developed by scientists, who usually acquire basics of software programming but are not aware of the best design practices. As a consequence, several existing open access scientific software packages are known to be hard to use and evolve due to their poor quality, as highlighted in recent studies. This project will integrate and enhance recent advances in software issue detection and refactoring techniques, created by the PIs and sponsored by NSF, in order to serve diverse scientific and engineering domains, detecting and fixing software quality issues effectively. This proposal seeks to bridge the gap between software engineering community and other science and engineering community in general. It will provide quantitative comparisons of software projects against an industrial benchmark, enable users to pinpoint software issues responsible for high maintenance costs, visualize the severity of the detected issues, and refactor them using the proposed interactive refactoring framework. The proposed framework will bring together software users and software developers by enabling non software experts to post software challenges for the software community to solve, which will, in turn, boost the research and advances in software research.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Simulating Autonomous Agents and the Human-Autonomous Agent Interaction,OAC,2209794,Chris Schwarz,chris-schwarz@uiowa.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","This project augments the Chrono computer simulation platform in transformative ways. Chrono's purpose is to predict through simulation the interplay between mechatronic systems, the environment they operate in, and humans with whom they might interact. The open-source simulation platform is slated to become a community-shared virtual investigation tool used to probe competing engineering designs and test hypotheses that would be too dangerous, difficult, or costly to verify through physical experiments. Chrono has been and will continue to be used in multiple fields and disciplines, e.g., terramechanics, astrophysics; soft matter physics; biomechanics; mechanical engineering; civil engineering; industrial engineering; and computer science. Specifically, it is used to engineer the 2023 VIPER lunar rover; relied upon by US Army experts in evaluating its wheeled and tracked vehicle designs; used in the US and Germany in the wind turbine industry; and involved in designing wave energy conversion solutions in Europe. Upon project completion, Chrono will become a simulation engine in Gazebo, which is widely used in robotics research; operate on the largest driving simulator in the US; empower research in the bio-robotics and field-robotics communities; and assist efforts in the broad area of automotive research carried out by a consortium of universities and companies under the umbrella of the Automotive Research Center. The educational impact of this project is threefold: training undergraduate, graduate, and post-doctoral students in a multi-disciplinary fashion that emphasizes advanced computing skills development; anchoring two new courses in autonomous vehicle control and simulation in robotics; and broadening participation in computing through a residential program on the campus of the University of Wisconsin-Madison that engages teachers and students from rural high-schools. Innovation and discovery are fueled by quality data. At its core, this project seeks to increase the share of this data that has simulation as its provenance. In this context, a multi-disciplinary team of 40 researchers augments and validates a physics-based simulation framework that empowers research in autonomous agents (AAs). The AAs operate in complex and unstructured dynamic environments and might engage in two-way interaction with humans or other AAs. This project enables Chrono to generate machine learning training data quickly and inexpensively; facilitates comparison of competing designs for assessing trade-offs; and gauges candidate design robustness via testing in simulation of corner-case scenarios. These tasks are accomplished by upgrading and extending Chrono to leverage recent computational dynamics innovations, e.g., a faster index 3 differential algebraic equations solver; a new approach to solving frictional contact problems; a real-time solver for handling flexible-body dynamics in soft robotics via nonlinear finite element analysis; a best-in-class simulator for terradynamics applications; reliance on just-in-time compiling for producing executables that are both problem- and hardware-optimized; a novel way for using mixed data representations for parsimonious storing of state information; and a scalable multi-agent framework that enables geographically-distributed, over the Internet, real-time simulation of human-AA interaction.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Convergence of Bayesian inverse methods and scientific machine learning in Earth system models through universal differentiable programming,OAC,2103804,Alan Edelman,EDELMAN@MATH.MIT.EDU,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Understanding and quantifying parameter sensitivity of simulated systems, such as the numerical models of physical systems and mathematical renderings of neural networks, are essential in simulation-based science (SBS) and scientific machine learning (SciML). They are the key ingredients in Bayesian inference and neural network training. Seizing on the opportunity of emerging open-source Earth system model development in the Julia high-level programming language, this project is endowing these open-source models with automatic differentiation (AD) enabled derivative information, making these converging data science and simulation-based science tools available to a much broader research and data science community. Enabling a general-purpose AD framework which can handle both large-scale Earth system models as well as SciML algorithms, such as physics-informed neural networks or neural differential equations, will enable seamless integration of these approaches for hybrid Bayesian inversion and Bayesian machine learning. It merges big data science, in which available data enable model discovery with sparse data science, and the model structure is exploited in the selection of surrogate models representing data-informed subspaces and fulfilling conservation laws. The emerging Julia language engages a new generation of researchers and software engineers, channeling much needed talent into computational science approaches to climate modeling. Through dedicated community outreach programs (e.g., Hackathons, Minisymposia, Tutorials) the project team will be working toward increasing equity, diversity, and inclusion across the participating disciplines.The project is developing a framework for universal differentiable programming and open-source, general-purpose AD that unifies these algorithmic frameworks within Julia programming language. The general-purpose AD framework in Julia leverages the composability of Julia software packages and the differentiable programming approach that underlies many of the SciML and high-performance scientific computing packages. Compared to most current modeling systems targeted for HPC, Julia is ideally suited for heterogeneous parallel computing hardware (e.g., CUDA, ROCm, oneAPI, ARM, PowerPC, x86 64, TPUs). The project is bringing together expertise in AD targeted at Earth system data assimilation in high performance computing environments with SciML expertise. The project team is working with the Julia Computing organization and package developers to ensure sustainability of the developed frameworks. The project?s Earth system flagship applications consist of (i) an open-source, AD-enabled ocean general circulation model that is being developed separately as part of the Climate Modelling Alliance (CliMA), and (2) an open-source, AD-enabled ice flow model. Each of these application frameworks is being made available to the community for science application, in which derivative (gradient or Hessian) information represent key algorithmic enabling tools. These include SciML-based training of surrogate models (data-driven and/or model-informed), parameter and state estimation, data assimilation for model initialization, uncertainty quantification (Hessian-based and gradient-informed MCMC) and quantitative observing system design. Academic and industry partners are involved, who are using the frameworks for developing efficient power grids, personalized precision pharmacometrics, and improved EEG design.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Elements: RHAPSODY: Runtime for Heterogeneous Applications, Service Orchestration and DYnamism",OAC,2103986,Matteo Turilli,matteo.turilli@rutgers.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","The end of traditional scaling paradigms coupled with innovations in machine learning are driving unprecedented changes in the formulation of scientific applications as well as the nature of high-performance computing (HPC) software and application ecosystems. As a consequence, scientific applications increasingly depend on heterogeneous components with diverse computational characteristics and performance challenges, coordinated in agile and innovative ways. The Runtime for Heterogeneous APplications, Service Orchestration and DYnamism (RHAPSODY) addresses challenges arising from heterogeneity and need for performance. It will enable the effective and efficient execution of scientific applications at unprecedented scale and on a variety of current and upcoming HPC platforms.RHAPSODY is a scalable, fault-tolerant and portable runtime system to enable scientific communities to innovate by effectively executing applications composed of heterogeneous components on current and upcoming computing platforms. RHAPSODY provides missing runtime capabilities to application and middleware developers, fostering the integration of existing software systems and, ultimately, a more efficient use of existing resources. Together, these contributions make RHAPSODY a tool to advance scientific discovery in multiple domains.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: DeepPDB: An open-source automated framework to enable high-fidelity atomistic simulations in unexplored material space,OAC,2003808,Wissam Saidi,alsaidi@pitt.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","A key requirement to address the world?s energy challenges is the development of new energy-efficient and smart materials. To aid in this process, state-of-the-art and revolutionary computational tools can be used to simulate new materials, thus providing a thorough understanding of their characteristics and behavior. Such materials simulation efforts can drastically reduce the time-to-market of new materials from decades to months. Traditional simulation approaches can accurately predict the behavioral properties of materials both at the smallest possible scale (atomic) and at the macroscopic level, i.e. millimeter or larger. Many critical materials properties are defined and needed at scales ranging from a few nanometers to micrometers, yet simulations are lacking at these levels. While algorithms do exist to simulate properties at these scales, often we lack the fundamental parameters, termed force-fields, for novel materials such as those for next-generation solar cells, batteries and jet turbine alloys. These force-fields are laborious to determine using traditional methods, requiring significant expertise and thus restricted by the human-in-the-loop. The primary goal of the proposed Deep Potential DataBase (DeepPDB) will be to offer an open-source toolkit with the ability to automatically generate estimates of force-fields parameters using advanced empirical-based computational tools. We will also curate and disseminate a validated repository of first-principles datasets and their corresponding potentials for inorganic materials. DeepPDB will serve both the materials science and machine learning communities, by providing the former with critical parameters to solve materials challenges and the latter by benchmark datasets for machine learning development. The resulting synergy will enable artificial intelligence and machine learning to play a greater role in computing critical materials properties for next-generation challenges. DeepPDB will also serve a critical educational objective, allowing the budding of a new generation of materials scientists, who understand how deep learning can be used to solve materials science challenges. DeepPDB aims to build a database of deep neural network potentials (DNP) for the simulation of inorganic materials. In the process DeepPDB will: (1) develop automated workflows that given a target composition, will run the necessary density functional theory (DFT) calculations, train DNPs, validate against metrics imposed by the training data, identify the input-data space with the largest uncertainty and iterate until an optimal DNP is trained; (2) openly disseminate the training DFT data along with the pre-trained DNPs; (3) develop transparent automated validation that encompasses both traditional DNP based methods as well as fully integrated tests that include target metrics. To accomplish this, DeepPDB will build a toolkit based on careful software engineering practices: a combination of feature- and sprint-based development cycles; constant continuous integration using unit-tests and integration tests as milestones; and a database-oriented approach to data and workflow management. The resulting open-source toolkit will serve as a foundational tool to investigate the properties of hitherto-unseen materials at length- and time-scales previously not possible.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
CDS&E: D3SC: Developing A Molecular Mechanics Modeling Platform (MMMP) for Studying Molecular Interactions,CHE,1955260,Junmei Wang,juw79@pitt.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Junmei Wang of The University of Pittsburgh is supported by an award from the Chemical Theory, Models and Computational Methods program in the Division of Chemistry to develop a set of computational tools and build a publicly available platform to facilitate users to study biomolecular systems. The award is cofunded by the Office of Advanced Cyberinfrastructure. High quality molecular mechanics force field (MMFF) parameters are critical to the successful modeling and simulations of various molecular systems. However, users nave to molecular modeling may find it a daunting task to obtain high-quality MMFF parameters and models without assistance. Dr. Wang and his team are conducting research to develop novel software tools, derive MMFF parameters, build high-quality models, and then integrate them into a freely accessible Molecular Mechanics Modeling Platform (MMMP). MMMP will help users from a broad range of disciplines to study molecular mechanisms of biomolecule-ligand interactions and to calculate the binding affinity accurately and efficiently with ease. Researchers from the drug discovery community can employ MMMP to increase the success rate on the discovery of drug candidates for combating a variety of diseases including the Coronavirus Disease 2019 (COVID-19). A major bottleneck for studying novel molecular systems is the availability, accuracy and validation of consistent molecular mechanics parameter sets. Dr. Junmei Wang is developing a Molecular Mechanics Modeling Platform (MMMP) which integrates force field parameters and residue topologies, novel online tools, and Application Programming Interfaces (APIs) to break the bottleneck. He is conducting research to (1) improve the atom type and bond type perception algorithm to handle arbitrary small molecules; (2) develop molecular mechanics model databases for non-standard amino acid/nucleic acid residues and co-crystallized ligands in the Protein Data Bank, and other compounds (3) develop and advance a software tool coined re-Affinity to bridge the the gap between the efficient docking methods and more computer resource-demanding yet more accurate free energy-based methods; (4) develop a physical, efficient and highly transferrable charge model which can significantly improve the accuracy of free energy calculations; and (5) create a user-friendly Graphic User Interface (GUI), ClickFF, which allows users to generate energy profiles, compare force fields, and optimize force field parameters for selected bonded force field parameters with a few clicks.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
CDS&E: Collaborative Research: Deep learning enhanced parallel computations of fluid flow around moving boundaries on binarized octrees,CBET,1953204,Inanc Senocak,senocak@pitt.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Computer simulations of heat and fluid flow find applications in many aspects of science and engineering. Notable examples are aerodynamic design of aircrafts and automobiles, and weather forecasting. These simulations are often computationally expensive, and they are performed on supercomputers. Special methods are used to implement the equations of heat and fluid flow as a simulation software. The end goal is to create an accurate computer code that can make optimal use of available computing power. However, this end goal is becoming challenging on modern extreme-scale supercomputers that deploy a large of number of computing processors to work in parallel. Existing algorithms face performance bottlenecks and do not realize the full potential of a modern supercomputer. The project team will develop new algorithms to overcome this performance bottleneck. The successful completion of this award is expected to result in an open-source heat and fluid flow simulation software. The project team will develop educational tutorials to pique the interest of high-school students in new capabilities of computer simulation and machine learning techniques in science and engineering.The technical objective is to enhance parallel performance of simulations of incompressible fluid flow around moving boundaries. A recently developed binarized octree generation technique will be further developed as an open-source parallel adaptive mesh refinement software infrastructure to solve the fluid flow equations on Cartesian domains with deep levels of mesh adaptations. Machine learning techniques and deep neural nets will be adopted in ways to ease potential bottlenecks that are expected to degrade scalability of parallel computations when large number of processors are deployed in simulations. The project team will develop multiple deep learning algorithms such as convolutional neural networks and generative adversarial networks to learn the fluid flow around complex geometries and apply the learning for rapid and accurate field estimation at arbitrary points. To successfully incorporate the effect of boundary conditions at the interface, conditional generative adversarial networks will be trained on different coarse and fine grids to learn the communication pattern among the blocks.This award by the Division of Chemical, Bioengineering, Environmental and Transport Systems within the NSF Directorate of Engineering is jointly supported by the NSF Office of Advanced Cyberinfrastructure.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: An open source software ecosystem for plasma physics,OAC,1931388,Nicholas Murphy,namurphy@cfa.harvard.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Software is crucial to all areas of modern plasma physics research. Plasma physicists use software for activities such as analyzing data from laboratory experiments and simulating the behavior of plasmas. Research groups often use software developed independently within their own group, which leads to unnecessary duplication of functionality and a lack of interoperability between different software packages. The lack of interoperability is compounded by different groups writing software using different coding styles and conventions. Much of the research software in plasma physics is not openly available to the public, which makes it harder for other scientists to reproduce scientific results. The team will develop PlasmaPy: a community-wide open source software package for plasma physics research and education. PlasmaPy will be written using the freely available Python programming language which is commonly used in related fields like astronomy. PlasmaPy itself will contain the general functionality needed by most plasma physicists, whereas community-developed affiliated software packages will contain more specialized functionality. The team will seek feedback from plasma physicists, hold annual workshops, and actively support new users and contributors.The research team will lead the development of PlasmaPy and affiliated packages to foster the creation of an open source software ecosystem for plasma physics. The PlasmaPy core package will contain functionality needed by plasma physicists across disciplines, whereas affiliated package will contain more specialized functionality. At the beginning of the project, the research team will formalize the software architecture, refactor existing code, improve tests, and improve base data structures to provide a solid foundation for future development. Subsequent code development priorities include a dispersion relation solver for plasma waves and instabilities, the groundwork for a flexible framework for plasma simulation, time series turbulence analysis tools, classes for the analysis of plasma diagnostics, and tools to provide access to atomic and physical data. They will make base data structures compatible with open source packages for data science to enable future data science studies. The research team will actively seek feedback from the plasma physics community, and adjust code development priorities based on this feedback. The team will hold workshops each year and actively support new users and contributors to grow PlasmaPy into a self-sustaining project.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering, the Division of Physics in the Directorate of Mathematical and Physical Sciences, and the Division of Atmospheric and Geospace Sciences in the Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Data: Sustaining Modern Infrastructure For Political And Social Event Data,OAC,1931541,Patrick Brandt,pbrandt@utdallas.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","This project extracts quantitative summaries of political and international conflict events among national and non-state actors across the world by combing news reports across the internet in multiple languages. This generates event data, a machine-coded description of someone doing something to someone else as extracted from news reports. The project focuses on political and social events about conflict and cooperation between governments, individuals, non-governmental organizations, rebel groups, and others. The main goal of this project is to integrate and expand the end-to-end cyberinfrastructure for the robust creation, validation, access, and analysis of political event data by national security, government, academic, and non-governmental actors. A major component of this proposal is to continue to grow the project's engagement with the global event data community. This project extends, produces, and integrates a dynamic, robust system for event data to study sub-national and international conflict processes at a global scale, with applications to the needs of the national security and intelligence communities. Using natural language processing software tools to code event data by annotating the kinds of political events that are of interest to political scientists, international relations scholars, sociologists, and the national security community, the project analyzes contemporaneous news reports in English and Spanish, automatically encodes relevant political events for data analysts, and serves the data along with other open event data via the project websites. The technical challenges include: (1) additional extensions of the multilingual framework to more types of events; (2) smoother updates to political actor dictionaries; (3) robust data querying and linking mechanisms, and analytic tools for the broader research and user community; (4) improved methods for focus location extraction across languages and resolutions. This will improve event data quality and event detection through increased, multi-language comparisons. The multi-lingual extensions of the event encoding software and interface will produce novel methods for detecting and analyzing rare and local events. The proposed database integrations and query optimizations will streamline access to the many open access event datasets that exist, enabling researchers across diverse communities to analyze and compare conflict and political processes. The refinements of the geolocation modules will allow detection of locations from biased training samples, which is an important advancement since some political events, such as human rights violations, tend to occur in locations with low news coverage. The robust and innovative geolocation approaches can be carried over to other domain applications. Scaling the related software and data infrastructure aids the political science, national security and big data research communities. We also will provide robust data linkages across a diverse set of event data from multiple and multilingual news reporting services. The sustainable cyberinfrastructure not only includes the event data coding from news reports, but also analysis tools that include an R package and a thin-client browser-based analysis interface to the data. This sustains the cyberinfrastructure and creates a workforce that is able to work in both science, engineering, national security, and intelligence.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Frameworks: Collaborative Research: Extensible and Community-Driven Thermodynamics, Transport, and Chemical Kinetics Modeling with Cantera: Expanding to Diverse Scientific Domains",OAC,1931389,Richard West,R.West@northeastern.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Modeling and simulation play key enabling roles in aiding and accelerating discovery connecting to energy and chemical research. In applications such as energy storage and conversion, atmospheric chemistry, and catalytic chemical processing, modeling and simulation software helps facilitate technological advances. However, in recent years the available software has not kept pace with the increasing chemical complexity and interdisciplinarity of advanced technology solutions. This project addresses this gap by developing and promoting new state-of-the-art modeling capabilities for diverse scientific fields in the existing Cantera software platform. Cantera is an extensible, open-source framework that enables researchers to study basic science and support new technology development and enables teachers to demonstrate concepts and applications in their classrooms. This project extends Cantera to provide new cross-disciplinary research capabilities and provides a foundation for further community-driven improvements to the Cantera framework. Simultaneous development of the open-source platform and outreach to new user communities will facilitate both fundamental scientific insight and practical technology design and analysis, train the next generation of researchers in both software-development best practices and scientific knowledge, and generate reusable and open educational materials. In addition to work on the framework development, the project includes training of graduate students as well as education, outreach and scientific community engagement activities.This work will develop the Cantera software platform in service of three objectives: (i) extend Cantera?s scientific capabilities to support the development of transformative technologies; (ii) expand Cantera?s user base in fields including electrochemistry, heterogeneous catalysis, and atmospheric chemistry; and (iii) broaden participation in the software?s development and management to improve Cantera?s sustainability and usability. These will be achieved by developing new scientific modeling capabilities, conducting outreach to new user communities, and improving Cantera?s architecture and software development practices. The new scientific modeling capabilities will focus on four content areas: thermodynamics, chemical kinetics, transport, and multi-phase capabilities. Outreach activities, including publications and presentations, conference workshops, and domain-specific software toolkits with examples to demonstrate Cantera operation and functionality, will engage new and existing communities. The project will establish a scientific advisory board, consisting of experts from diverse fields and backgrounds who will help guide software development and outreach. Finally, the architectural and software engineering changes in this work will improve extensibility and interoperability and implement advanced numerical algorithms to enable the application of Cantera to new types of problems. These changes will also make it easier for users to contribute to Cantera, ensure software correctness, and provide new ways of accessing Cantera?s functionality. The resulting software framework will aid in scientific discovery and development of key enabling technologies with broad societal impacts. These impacts include next-generation batteries and fuel cells for clean energy storage and conversion, catalytic and membrane reactors for electrolysis, novel fuel generation, chemical processing, environmentally conscious combustion applications, and understanding and addressing anthropogenic challenges in atmospheric chemistry.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Elements: Software: Roundoff-Error-Free Algorithms for Large-Scale, Sparse Systems of Linear Equations and Optimization",OAC,1835499,Erick Moreno-Centeno,emc@tamu.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Solving systems of linear equations is central to solving problems in numerous applications within healthcare, power generation, national defense, economics, physics, chemistry, mathematics, computer science and engineering. Nowadays, a large number of these critical applications have an ever-increasing need for faster, more reliable, and more accurate solutions. For example, more accurate treatment plans are proven to be less costly, less invasive, safer, and more reliable outcomes for prostate-cancer brachytherapy (placement of radioactive ""seeds"" inside a tumor). Similarly, millions of dollars can be saved, and cleaner energy can be produced, by solving the power generation dispatch optimization problem with more accuracy. Paradoxically, today's state-of-the-art software tools are limited to calculating limited-precision solutions (e.g., treatment plans and power dispatches). This is due in part to the prevalence of computing methods relying on floating-point arithmetic (i.e., arithmetic using truncated decimal numbers). At the same time, real-life problems in a wide range of applications are becoming larger and so more prone to incorrect results due to roundoff errors (errors introduced when truncating the decimal numbers). The primary goal of this project is to design, create, and deploy computational tools to solve large-scale, sparse systems of linear equations and optimization problems without any error at all. Because of the ubiquity of solving systems of linear equations and optimization problems, the outcomes of this project will directly translate in software that is more reliable for applications across academia, industry, and government. Large-scale, sparse systems of linear equations (SLEs) and linear optimization problems (LPs) are routinely solved and the accuracy/correctness of solvers is taken for granted. However, state-of-the-art solvers commonly report incorrect results, some as striking as misclassifying feasible problems as infeasible and vice versa or even failing altogether. Moreover, exactly solving SLEs and LPs is of fundamental importance for applications where fixed-precision standards have been deemed inadequate, including specific applications in healthcare, power generation, biology, combinatorial auctions, and formal verification of mathematical proofs. Therefore, the first objective of this project is to devise efficient algorithms and implement robust software to reliably and exactly solve large-scale, sparse SLEs, free of any roundoff error. This objective will build on our recently devised roundoff-error-free (REF) LU and Cholesky factorizations for dense matrices. The second objective of this project is to devise efficient algorithms and implement robust software to reliably and exactly (REF) solve large-scale, sparse LPs. The specific outcomes of this project include: (1) Devise an efficient REF factorization framework for large-scale sparse matrices, including devising good fill-reducing orderings that consider the bit-size growth of the entries; (2) Devise REF optimization algorithms to exactly solve large-scale, sparse linear programs; (3) Our software will be rigorously tested, with a full 100% test coverage suite and scaffolding code to test loop invariants and data sanity. The software products will be submitted as algorithm papers to the ACM Transactions on Mathematical software, where the code itself, test suite and documentation undergo rigorous peer review. Finally, we will incorporate our solvers into our existing SuiteSparse installations, including all Linux distros with the ultimate goal of being integrated into MATLAB and thus accessible to a wide user base.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Software: NSCI: Efficient GPU Enabled QM/MM Calculations: AMBER Coupled with QUICK,OAC,1835144,Kenneth Merz,merzjrke@msu.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","The over-arching goal of this project is to develop a software cyberinfrastructure aimed at solving important molecular-level problems in catalysis, drug design, and energy conversion. The PI and his collaborators will develop open source software that will enhance the ability to tackle chemical and biological problems using a sustainable model. The PI and his collaborators are collaborating with NVIDIA on this project to help accelerate the development efforts. Finally, the projects undertaken here will train students in formal theory, computer programming, computational chemistry and biology, and manuscript preparation/publication further enhancing the technical workforce in the USA.Combined quantum mechanical/molecular mechanical (QM/MM) models have enabled significant advances in the understanding of chemical reactivity and intermolecular interactions. This approach allows regions of a system where bonds are to be broken and formed to be modeled using accurate QM methods, while the surrounding environment is treated using classical models. The most widely used QM models in QM/MM studies are generally semiempirical, but the most accurate employ density functional theory (DFT), Hartree-Fock (HF) or post HF methods. The shortcoming when using the more accurate methods is the computational expense, which limits the extent of QM/MM molecular dynamics simulations. The performance of QM methods has been greatly improved over the years through algorithmic and hardware improvements. This project will focus on both: for the former the PI will add the ability to handle long-range interactions in QM/MM calculations, add GPU enabled correlated methods and create an electron repulsion interaction (ERI) engine for general use, while for the latter the PI will integrate the GPU enabled Quantum Interaction Computational Kernel (QUICK) program with the Sander and PMEMD molecular dynamics (MD) engines from the AMBER suite of programs. AMBER is one of the most popular simulations packages and has been supported and sustained by the AMBER developer community for approximately 30 years. The developments proposed here will be fully available to the community via AMBERTools, which is released using an open source model (see http://ambermd.org/AmberTools.php). This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Element: Data: HDR: Enabling data interoperability for NSF archives of high-rate real-time GPS and seismic observations of induced earthquakes and structural damage detection in OK,OAC,1835372,Jennifer Haase,jhaase@ucsd.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Recent studies have identified critical differences between earthquakes induced by wastewater injection in tectonically passive regions of oil and gas exploration (such as earthquakes recently experienced in Oklahoma, Texas, and Kansas) and earthquakes in tectonically active environments (such as fault zones in California). This has significant implications for earthquake engineering in the Midwest, where the building inventory was not designed to withstand large earthquakes or cumulative damage due to successive earthquakes. Estimating the level of ground shaking at different frequencies is needed to calculate structural response, understand which structures run higher risks of earthquake damage, and establish criteria for building design and real-time decision making. This project uses recent breakthroughs in real-time GPS data analysis to address challenges limiting the use of real-time GPS and seismic data by the geoscience and engineering communities. The effort develops new capabilities to handle data streams in a manner that is independent of the content and formats of the environmental sensor measurements. Creating these links will have a substantial impact on interoperability among the geodesy, seismology, and earthquake engineering research communities. The project demonstrates the approach using multi-sensor geoscience and engineering datasets recorded on structures on the Oklahoma State University campus and in the field near the location of the Magnitude 5.8 September 2016 Pawnee earthquake. The effort creates new methods for capturing permanent deformation in structures, and a better understanding of building inventory resiliency.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Established Program to Stimulate Competitive Research (EPSCoR).This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Cybershuttle: An end-to-end Cyberinfrastructure Continuum to accelerate Discovery in Science and Engineering,OAC,2209875,Emad Tajkhorshid,tajkhors@illinois.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","Science depends critically on accessing scientific and engineering software, data repositories, storage resources, analytical tools, and a wide range of advanced computing resources, all of which must be integrated into a cohesive scientific research environment. The Cybershuttle project is creating a seamless, secure, and highly usable scientific research environment that integrates all of a scientist?s research tools and data, which may be on the scientist?s laptop, a computing cloud, or a university supercomputer. These research environments can further support scientific research by enabling scientists to share their research with collaborators and the broader scientific community, supporting replicability and reuse. The Cybershuttle team integrates biophysicists, neuroscientists, engineers, and computer scientists into a single team pursuing the project goals with a grounding in cutting-edge research problems such as understanding how spike proteins in viruses work, how the brain functions during sleep, and how artificial intelligence techniques can be applied to modeling engineering materials. To meet its ambitious goals, the project is building on over a decade of experience in developing and operating the open-source Apache Airavata software framework for creating science-centric distributed systems. Cybershuttle is providing a system that can be used as a training ground to educate students in concepts of open-source software development and applied distributed systems, fostering a globally competitive workforce who can move easily between academic and non-academic careers. Cybershuttle is creating a new type of user-facing cyberinfrastructure that will enable seamless access to a continuum of CI resources usable for all researchers, increasing their productivity. The core of the Cybershuttle framework is a hybrid distributed system, based on open-source Apache Airavata software. This system integrates locally deployed agent programs with centrally hosted middleware to enable an end-to-end integration of computational science and engineering research on resources that span users? local resources, centralized university computing and data resources, computational clouds, and NSF-funded, national-scale computing centers. Scientists and engineers access this system using scientific user environments designed from the beginning with the best user-centered design practices. Cybershuttle uses a spiral approach for developing, deploying, and increasing usage and usability, beginning with on-team scientists and expanding to larger scientific communities. The project engages the larger community of scientists, cyberinfrastructure experts, and other stakeholders in the creation and advancement of Cybershuttle through a stakeholder advisory board. Cybershuttle's team includes researchers from Indiana University, the University of Illinois at Urbana-Champaign, the University of California San Diego, the San Diego Supercomputer Center, and the Allen Institute.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Seismic COmputational Platform for Empowering Discovery (SCOPED),OAC,2104052,Carl Tape,ctape@alaska.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Seismology is the most powerful tool for investigating the interior structure of Earth ? from its surface down to the inner core ? and its wide range of processes, including earthquakes, volcanic activity, glacial processes, oceanic and environmental processes, and human-caused processes such as nuclear explosions or hydraulic fracturing in oil and gas exploration. Seismology cannot achieve its greatest potential without harnessing state-of-the-art computing capabilities for the dual purpose of scientific modeling and analysis of rapidly increasing data sets. The SCOPED (Seismic COmputational Platform for Empowering Discovery) project establishes a computing platform that delivers data, computation, and service to the seismological community in a way that promotes education, innovation, and discovery, and enables efficient solutions to outstanding scientific problems in geophysics. By focusing on openly available data, openly available software, and virtual training, SCOPED opens seismological research to a broad range of users. Four research components emphasize openly available software for the purpose of characterizing Earth's subsurface structure and the wide range of natural and man-made events that are recorded by seismometers every day. Training of seismologists is a central focus of the project. SCOPED training workshops (seismoHackweeks) are open to the community. Emphasis on virtual research and training diversifies strategies to engage minority groups entering computational geosciences. The project trains a new generation of seismologists to harness the latest capabilities for processing and modeling large data sets. The SCOPED project establishes cyberinfrastructure that provides fast access to large seismic archives from a suite of containerized open-source computational tools for big data analysis, machine learning, and high-performance simulations. The implementation focuses on four interconnected, compute- and data-intensive research components: seismic imaging of Earth?s interior, waveform modeling of earthquakes and Earth structure, monitoring of Earth structure using ambient noise, and precision monitoring of earthquakes and faults. Each research component is enabled by open-source codes that meet, or aspire to meet, best practices for software development. The project contains several transformative components. First, it offers compute performance for both model- and data-driven seismological problems. Hundreds of terabytes of waveform data are directly accessible both to modelers?for data assimilation problems?and to data scientists for processing, analysis, and exploration. Second, it establishes a direct collaborative link among four teams of seismologists at four institutions and a team of computational scientists at Texas Advanced Computing Center. This unity reflects the necessity of both groups to achieve research-ready codes that can exploit high-performance computing (HPC) and Cloud systems. Third, it establishes a gateway with ready-to-run (or adapt) container images and data as a service for the seismological community. Fourth, it develops computational tools that promote the democratization of HPC/Cloud with cutting-edge data processing and modeling software through their scalability from laptops to HPC or Cloud systems and through their portability with containerization. Finally, although the development of cyberinfrastructure is the main priority, ancillary scientific results from advanced techniques are expected to offer insights into fundamental seismological problems. The project has the potential for discoveries across fields (seismology, Earth science, computer science, data science, material science), as well as societal relevance in the realms of seismic hazard assessment, environmental science, cryosphere, earthquake early warning, energy systems, and geophysical detection of nuclear proliferation.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Element: Data: HDR: Enabling data interoperability for NSF archives of high-rate real-time GPS and seismic observations of induced earthquakes and structural damage detection in OK,OAC,1835371,Priyank Jaiswal,priyank.jaiswal@okstate.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Recent studies have identified critical differences between earthquakes induced by wastewater injection in tectonically passive regions of oil and gas exploration (such as earthquakes recently experienced in Oklahoma, Texas, and Kansas) and earthquakes in tectonically active environments (such as fault zones in California). This has significant implications for earthquake engineering in the Midwest, where the building inventory was not designed to withstand large earthquakes or cumulative damage due to successive earthquakes. Estimating the level of ground shaking at different frequencies is needed to calculate structural response, understand which structures run higher risks of earthquake damage, and establish criteria for building design and real-time decision making. This project uses recent breakthroughs in real-time GPS data analysis to address challenges limiting the use of real-time GPS and seismic data by the geoscience and engineering communities. The effort develops new capabilities to handle data streams in a manner that is independent of the content and formats of the environmental sensor measurements. Creating these links will have a substantial impact on interoperability among the geodesy, seismology, and earthquake engineering research communities. The project demonstrates the approach using multi-sensor geoscience and engineering datasets recorded on structures on the Oklahoma State University campus and in the field near the location of the September 2016 Pawnee earthquake. The effort creates new methods for capturing permanent deformation in structures, and a better understanding of building inventory resiliency.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Established Program to Stimulate Competitive Research (EPSCoR).This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Production quality Ecosystem for Programming and Executing eXtreme-scale Applications (EPEXA),OAC,1931387,Robert Harrison,robert.harrison@stonybrook.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","A team of researchers from three institutions will work collaboratively to design and develop a software framework that implements high-performance methods for irregular and dynamic computations that are poorly supported by current programming paradigms. The framework, titled EPEXA (Ecosystem for Programming and Executing eXtreme Applications), will create a production-quality, general-purpose, community-supported, open-source software ecosystem that attacks the twin challenges of programmer productivity and portable performance for advanced scientific applications on modern high-performance computers. Employing science-driven co-design, the team will transition into production a successful research prototype of a new programming model and accelerate the growth of the community of computer scientists and domain scientists employing these tools for their research. The project bridges the so-called ""valley of death"" between successful proofs of principle to an implementation with enough quality, performance, and community support to motivate application scientists and other researchers to adopt the tools and invest their own effort into the community. In addition to work on the framework development, the project includes training of postdoctoral scholars, graduate and undergraduate students as well as education, outreach and scientific community engagement activities.Specifically, the new powerful data-flow programming model and associated parallel runtime directly address multiple challenges faced by scientists as they attempt to employ rapidly changing computer technologies including current massively-parallel, hybrid, and many-core systems. Both data-intensive and compute-intensive applications are enabled in part by the general programming model and through the ability to target multiple backends or runtime systems. Also enabled is the creation by domain scientists of new domain-specific languages (DSLs) for both shared and distributed-memory computers. EPEXA contributes to the design and development of state-of-the-art software environments that leverage the National Science Foundation's investments in cyberinfrastructure to enable scientific discovery across all disciplines.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Software: Towards Efficient Embedded Data Processing,OAC,1835446,Jignesh Patel,jignesh@cs.wisc.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Embedded databases are ubiquitous, though that fact may not be widely-recognized. Every smart phone has many embedded databases, which implies that billions of people worldwide carry dozens of databases in their phones/pockets every day. Many of these databases are powered by data processing technology that has not kept up with the pace with which the underling hardware in phones have evolved. As a result, data processing is slow, and consumes more energy than needed. The focus of this proposal is on developing new data processing technology for mobile devices that targets a 10X efficiency and performance improvements. The aims of the project go beyond more efficient data processing on phones to also include more efficient processing in embedded environments, which also includes databases running on laptops. Thus, the project aims for a broad impact on database across a spectrum of mobile devices.The technical contributions of this project are in recognizing that modern hardware, even at the ?low-end? which includes mobile phones and laptops, now have multiple processing cores, relatively large amounts of memory, and flash storage. There is a critical need for a new class of embedded data processing systems that can work efficiently, and effectively on such modern mobile platforms. This project aims to build a system, called Hustle, to address this need. The project will design, develop and implement a range of data processing methods, which include predicate-based concurrency control mechanisms, query processing methods that inherently expose and exploit opportunities for intra and inter-operator parallelism, and query optimization methods that target embedded settings.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Scalable Bayesian Software for Interpreting Astronomical Images,OAC,2209720,Jeffrey Regier,regier@umich.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","The BLISS (Bayesian Light Source Separator) Project is an interdisciplinary research effort to develop a software tool that allows astronomers to make use of the latest advances in machine learning. By harnessing these advances, astronomers can rapidly analyze vast quantities of complex data to understand the nature of our universe. This project also engages and educates a wider audience through a workshop series that promotes technical proficiency in software development and machine learning.The software tool, developed as part of this project, will allow astronomers to more easily access Bayesian statistical methods to interpret image data from astronomical surveys. Bayesian methods excel at uncertainty quantification and data integration, two capabilities that will be critical in analyzing the deluge of data produced by next-generation astronomical surveys. One major barrier to the more widespread adoption of Bayesian analysis for interpreting astronomical images is computational: Bayesian inference is notoriously computationally demanding. A second major barrier is social: up to now, novel Bayesian methods have been developed in isolation by statisticians and have rarely been integrated into astronomy workflows because it is unclear to practitioners in either discipline how this can be accomplished. The BLISS Project addresses both these computational and community integration challenges. To overcome the computational challenges, BLISS leverages recent advances in Bayesian inference methodology, including the use of deep learning, variational inference, and GPU acceleration. To ensure immediate and sustainable community use, development of the BLISS is guided by needs identified by domain experts, who are themselves prepared to participate in BLISS's development and are enthusiastic about integrating BLISS into their teams' data analysis workflows.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering, the Division of Mathematical Sciences and the Division of Astronomical Sciences in the Directorate for Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks for Intelligent Adaptive Experimentation: Enhancing and Tailoring Digital Education,OAC,2209819,John Stamper,jstamper@cs.cmu.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","People are constantly learning ? whether formal education of homework problems & videos, or reading websites like Wikipedia. This project develops the Experiments As a Service Infrastructure (EASI), which lowers the barriers to conducting randomized experiments that compare alternative ways of designing digital learning experiences, as well as analyzing the data derived from the systems to rapidly change what future people receive. It does this by bringing together multidisciplinary researchers around the shared problem of testing ideas for improving and personalizing educational resources. The research also advances (1) the science of learning and instruction; (2) methods for analyzing complex educational data, and (3) machine learning algorithms that use data to improve educational experiences. Improving learning and teaching increases people's knowledge and gives them the ability to solve problems they care about, driving their personal and career success and increasing society's human capital.Instructional decisions about digital educational resources impact all students, from practice problems in K12 systems to tutorial webpages in university and community college online courses. The current versions of resources are too infrequently compared against alternative resources, which may provide better learning. With this in mind, the project has the goal of using data to test hypotheses about what is most helpful to students, and then use that data to change the experience for future students. The Experiments-As-a-Service-Infrastructure supports three complementary types of multi-disciplinary, collaborative research. A?Design: the infrastructure helps researchers investigate theories of learning and discover how to improve instruction by designing randomized field experiments on components of real-world digital educational resources. This provides more ecologically valid research on learning and instruction, in subfields of education, psychology, policy and discipline-based education research. B?Analysis: the infrastructure facilitates sophisticated analysis of experiments in the context of large-scale data about student profiles, such as to discover which interventions are effective for different subgroups of students. This can advance the use of innovative data-intensive methods for gaining actionable knowledge in education, learning analytics, educational data mining, and applied statistics. C?Adaptation: the infrastructure enables research into adaptive experimentation by providing a testbed for algorithms that dynamically analyze data from experiments, to enhance learning by presenting future students with whichever version of a resource (condition) is more effective, or to personalize learning by presenting different subgroups of future students with the version of a resource that is most effective for their subgroup. The infrastructure provides a testbed for empirical evaluation of which algorithms enact effective adaptive experimentation in education to inspire the development of new algorithms. Finally, the work aligns many educational communities around the shared problem of enhancing and personalizing education through experimentation and spurs multidisciplinary research by providing extensive support for collaboration and sharing of designs, data, analysis scripts and algorithms while fostering an online community for training and collaborations, to promote high-quality, innovative, impactful experiments.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Open-Source Cyberinfrastructure as a Decision Engine for Socioeconomic Disaster Risk (DESDR),OAC,2103794,Daniel Osgood,deo@iri.columbia.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Natural disasters can have long-lasting financial consequences for vulnerable populations such as farmers, but disparate disaster impacts make the accurate and timely deployment of limited resources such as disaster funding and relief very difficult. This project will develop an open-source suite of cyberinfrastructure tools, called the Decision Engine for Socioeconomic Disaster Risk (DESDR), to fill data voids by collecting and cleaning disaster risk data directly from affected populations. As well, the project will combine those data with satellite data to provide farmers and others with data and tools to make better informed decisions about disaster risk management. Existing approaches for disaster detection, mapping, and prediction primarily rely on satellite and sensor data that are susceptible to errors and do not measure how disasters directly affect regions and individuals. This project will develop open-source software infrastructure employing data from rural populations, and then use the data to build more accurate disaster risk models. The cyberinfrastructure, DESDR, will provide a scalable, customizable data collection platform using mobile messaging services that take local incentives and community norms into account, and will be used to gather data on from affected communities. A data visualization and cleaning platform will enable local partners and researchers to cross-reference the data with satellite and sensor data sources, and to identify and clean data errors. A database architecture will store the data in an interoperable format. And, a web-based interface will provide government agencies, policy makers, researchers, and other stakeholders the ability to interactively create and back-test disaster risk models. DESDR will be disseminated as open-source, ready-to-deploy software to a user community of governmental meteorological agencies, humanitarian program officers, insurers and affiliated agricultural and social scientists. By combining these tools into an integrated, extensible process cyberinfrastructure, DESDR will directly involve vulnerable populations in the design of solutions, enabling disaster risk managers to scale up critical relief programs to reach multitudes of farmers and others while providing an unprecedented voice to the project beneficiaries.This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering, and by the Division of Social and Economic Sciences in the Directorate for Social, Behavioral & Economic Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Improving the Understanding and Representation of Atmospheric Gravity Waves using High-Resolution Observations and Machine Learning,OAC,2005123,Pedram Hassanzadeh,ph25@rice.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Geophysical gravity waves are a ubiquitous phenomenon in Earth?s atmosphere and ocean, made possible by the interaction of gravity with a stratified, or layered fluid. They are excited in the atmosphere when winds flow over mountains, by thunderstorms and other strong convective systems, and when winter storms intensify. Gravity waves play an important role in the momentum and energy balance of the atmosphere, with direct impacts on surface weather and climate through their effect on the variability of key features of the climate system such as the jet streams and stratospheric polar vortices. These waves present a challenge to weather and climate prediction: waves on scales of 100 meters to 100 kilometers can neither be systematically measured with conventional observational systems, nor properly resolved in global atmospheric models. As a result, these waves must be represented, or approximated, based on the resolved flow that can be directly simulated. Current representations of gravity waves are severely limited by computational necessity and the scarcity of observations, leading to inaccuracies or uncertainties in short term weather and long term climate predictions. The objective of this project is to leverage unprecedented observations from Loon high altitude balloons and use specialized high resolution computer simulations and machine learning techniques to develop accurate, data-informed representation of gravity waves. The outcomes of this project are expected to result in better weather and climate models, thus improving short term forecasts of weather extremes and long term climate change projections, which have substantial societal benefits. Furthermore, the project will support the training of 3 Ph.D. students, 4 postdocs, and 10 undergraduate summer researchers to work at the intersection of atmospheric dynamics, climate modeling, and data science, thus preparing the next generation of scientists for interdisciplinary careers.The project will deliver two key advances. First, it will open up a new data source to constrain gravity wave momentum transport in the atmosphere. Loon LLC has been launching super pressure balloons since 2013 to provide global internet coverage. Very high resolution position, temperature, and pressure observations (taken every 60 seconds) are available from thousands of flights. This provides an unprecedented source of high resolution observations to constrain gravity wave sources and propagation. The project will process the balloon measurements and, in concert with novel high resolution simulations, establish a publicly available dataset to open up a potentially transformational resource for observationally constrained assessment of gravity wave sources, propagation, and breaking. The second transformation will be using machine learning techniques to develop computationally feasible representations of momentum deposition by gravity waves. Current physics-based representations only account for vertical propagation of the waves (i.e., they are one dimensional) and ignore their horizontal propagation. Using the data based on the Loon measurements and high resolution models, one and three dimensional data driven representations will be developed to more accurately and efficiently represent the effects of gravity waves in weather and climate models. These novel representations will be implemented in idealized atmospheric models to study the role of gravity waves in the variability of the extratropical jet streams, the Quasi Biennial Oscillation (a slow variation of the winds in the tropical stratosphere) and the polar vortex of the winter stratosphere, enabling better understanding their response to increased atmospheric greenhouse gas concentrations.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research:  Frameworks: Internet of Samples: Toward an Interdisciplinary Cyberinfrastructure for Material Samples,OAC,2004642,Neil Davies,ndavies@moorea.berkeley.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Research frequently uses material samples as a basic element for reference, study, and experimentation in many scientific disciplines, especially in the natural and environmental sciences, material sciences, agriculture, physical anthropology, archaeology, and biomedicine. Observations made on samples collected in the field and in the laboratory constitute a critical data resource for research that addresses grand challenges of our planet's future sustainability, from environmental change; to food, energy, and water resources; to natural hazards and their mitigation; to public health. The large investments of public funds being made to curate huge volumes of samples acquired over decades or even centuries, and to collect and analyze new samples, demand that these samples be openly accessible, easily discoverable, and documented with sufficient information to make them reusable. The current ecosystem of sample and sample data management in the U.S. and globally is highly fragmented across stakeholders, including museums, federal agencies, academic institutions, and individual researchers, with a multitude of institutional and discipline-specific catalogs, practices for sample identification, and protocols for describing samples. The iSamples project is a multi-disciplinary collaboration that will develop a national digital infrastructure to provide services for globally unique, consistent, and convenient identification of material samples; metadata about them; and linking them to other samples, derived data, and research results published in the literature. iSamples builds on previous initiatives to achieve these goals by providing material samples with globally unique, persistent identifiers that reliably link to landing pages with metadata describing the sample and its provenance, and which allow unambiguously linking samples with data and publications. Leveraging significant national investments, iSamples provides the missing link among (i) physical collections (e.g., natural history museums, herbaria, biobanks), (ii) field stations, marine laboratories, long-term ecological research sites, and observatories, and (iii) data repositories and cyberinfrastructure. iSamples delivers enhanced infrastructure for STEM research and education, decision-makers, and the general public. iSamples benefits national security and resource management by offering a means to assure sample provenance, improving scientific reproducibility and demonstrating compliance with ethical standards, national regulations, and international treaties.The Internet of Samples (iSamples) is a multi-disciplinary and multi-institutional project to design, develop, and promote service infrastructure to uniquely, consistently, and conveniently identify material samples, record metadata about them, and persistently link them to other samples and derived digital content, including images, data, and publications. The project will create a flexible and scalable architecture to ensure broad adoption and implementation by diverse stakeholders. iSamples will build upon existing identifier infrastructure such as IGSNs (Global Sample Number;) and ARKs (Archival Resource Keys), but is agnostic to identifier type. Likewise, iSamples will encourage a high-level metadata standard for natural history samples (across biosciences, geosciences, and archaeology), while supporting community-developed metadata standards in specialist domains. Through integration with established discipline-specific infrastructure at the System for Earth Sample Registration SESAR (geoscience), CyVerse (bioscience), and Open Context (archaeology), iSamples will extend existing capabilities, enhance consistency, and expand their reach to serve science and society much more broadly. The project includes three main objectives: 1) Design and develop iSamples infrastructure (iSamples in a Box and iSamples Central); 2) Build four initial implementations of iSamples for adoption and use case testing (Open Context, GEOME, SESAR, and Smithsonian Institution); and 3) Conduct outreach and community engagement to developers, individual researchers, and international organizations concerned with material samples. The project will follow an agile development process that includes community engagement as an important element of creating software requirements and an implementation timeline.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Connecting Researchers in Sharing and Re-Use of Research Data and Software,OAC,2031647,Guenter Waibel,guenter.waibel@ucop.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Open science practices have gained widespread adoption, globally, with the help of federal funding andpublisher policies, as well as the increasing visibility and growing awareness of the value of sharingwork. This has been largely evident in light of the current COVID19 pandemic, with data sharing drivingmany areas of research, and open software resources must evolve to meet the needs of researchers. To meet the emerging demands and growing requirements of the research community who need support for both data and software sharing, Dryad and California Digital Library partnered in 2018 and Dryad and Zenodo partnered in 2019. These partnerships have allowed for the three organizations to re-think the data and publishing processes, explore ways for data curation, software preservation, and for output re-use to be tied together more seamlessly. This project is a one-day, invitational workshop bringing together researchers and adjacent community members with diverse backgrounds to discuss needs, challenges, and priorities for re-using research data and software. The goal of the meeting is to develop pathways for consistent engagement with individuals and groups across the diverse scientific disciplines in order to be connected with and responsive to researchers' needs and goals. Meeting topics include dataset re-use, deposition guidance, curation standards and requirements, integrations and relationships between data and code, and advocacy and adoption. The anticipated outputs are a set of requirements and needs to better enable data and software sharing and re-use.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Frameworks: Bayesian Analysis of Nuclear Dynamics,OAC,2004601,Daniel Phillips,phillid1@ohio.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","Nuclear physicists seek an accurate description of the properties of atomic nuclei, collisions between nuclei, and extreme environments such as the first few seconds of our universe or the interior of a neutron star. These situations involve many particles interacting through complex forces. They?re each described by a number of different models that typically explain accurately results of existing experiments. The models don?t do as well predicting what will happen in future experiments or in environments that are inaccessible here on Earth. The Bayesian Analysis of Nuclear Dynamics (BAND) Framework will use advanced statistical methods to produce forecasts for as-yet-unexplored situations that combine nuclear-physics models in an optimal way. These will be more reliable than the predictions of any individual model. BAND?s forefront computer codes will be widely available and will facilitate the design of nuclear-physics experiments that can deliver the largest gain in understanding. The adoption of BAND?s tools in other sciences dealing with ?model uncertainty? could spur broad scientific innovation. Undergraduate and graduate students working on BAND will gain a broad range of technical skills in data science, machine learning, nuclear physics, and high-performance computing.Nuclear physicists seek a quantitative description of strongly-interacting matter. Sophisticated models of how neutrons and protons interact in the nucleus, extreme environments, and collisions between nuclei have been key to the great progress made towards this goal. These models typically describe extant data well, but often yield divergent predictions for future experiments. The Bayesian Analysis of Nuclear Dynamics (BAND) framework will be a broadly available set of computational tools built through intensive collaboration between statisticians, computer scientists and nuclear physicists. It will combine the results of several models, incorporating prior knowledge and experimental data for each, to produce a full assessment of the uncertainty in nuclear-physics predictions. This will enable quantitative evaluation of the impact of future experiments, accelerating the theory-experiment feedback loop and spurring innovation. It will also help quantify uncertainties for terrestrially inaccessible environments, such as the core of neutron stars or the first microsecond after the Big Bang. Similar challenges are faced by researchers modeling complex dynamics in other sciences, so BAND?s tools will have broad appeal. Undergraduate and graduate students working on BAND will gain expertise in statistical methods and nuclear physics, as well as experience with large-scale computing and machine learning.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Physics at the Information Frontier in the Division of Physics and the CDS&E program in the Division of Mathematical Sciences within the Directorate for Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research:Frameworks: Basic ALgebra LIbraries for Sustainable Technology with Interdisciplinary Collaboration (BALLISTIC),OAC,2004763,James Demmel,demmel@cs.berkeley.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","Scientific software libraries have long provided a large and growing resource for high-quality, reusable software components upon which applications from science and engineering can be rapidly constructed ? with improved robustness, portability, and sustainability. For this, a team of researchers from four collaborating organizations proposed to develop BALLISTIC (Basic ALgebra Libraries for Sustainable Technology with Interdisciplinary Collaboration). The BALLISTIC project, through the leading-edge research it channels into its software deliverables, will lead to the introduction of tools that will simplify the transition to the next generation of extreme-scale computer architectures. The main impact of the project will be to develop, push, and deploy software into the scientific community to make it competitive on a world-wide scale and to contribute to standardization efforts in the area. BALLISTIC has the potential to become the community standard for dense linear algebra and be adopted and/or supported by a large community of users, computing centers, and High-Performance Computing (HPC) vendors. Learning to use numerical libraries is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. BALLISTIC will have a correspondingly large impact on the research and education community, government laboratories, and private industry and support national efforts to build a workforce capable of employing state of the art tools in pursuit of science and engineering discoveries.The goal of BALLISTIC is to create a layered package of software components that is capable of running at every level of the platform deployment pyramid and achieves three complementary objectives: (1) deliver seamless access to the most up-to-date algorithms, numerics, and performance via familiar Sca/LAPACK interfaces, wherever possible; (2) make advanced algorithms, numerics, and performance capabilities available through new interface extensions, wherever necessary; and (3) provide a well-engineered conduit through which new discoveries at the frontiers of research in these areas can be channeled as quickly as possible to all the applications from science and engineering communities that depend on high-performance linear algebra libraries.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research:SHF:Medium:Bringing Python Up to Speed,CCF,1954830,Emery Berger,emery@cs.umass.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","The Python programming language is among today's most popular computer programming languages and is used to write software in a wide variety of domains, from web services to data analysis to machine learning. Unfortunately, Python?s lightweight and flexible nature -- a major source of its appeal -- can cause significant performance and correctness problems - Python programs can suffer slowdowns as high as 60,000x over optimized code written in traditional programming languages like C and C++, and can require an order-of-magnitude more memory. Python's flexible, ?dynamic? features also make its programs error-prone, with many coding errors only being discovered late in development or after deployment. Python?s frequent use as a ""glue language"" -- to integrate and interact with different components written in C or C++ -- exposes many Python programs to the unique dangers of those languages, including susceptibility to memory corruption-based security vulnerabilities. This project aims to remedy these problems by developing new technology for Python in the form of novel performance analysis tools, memory-reduction and speed-improving optimizations (including support for multi-core execution), automated software testing frameworks, and common benchmarks to drive their evaluation.This project will develop (1) performance analysis tools that help Python programmers accurately identify the sources of slowdowns; (2) techniques for automatically identifying code that can be replaced by calls to C/C++ libraries; (3) an approach to unlocking parallelism in Python threads, which currently must execute sequentially due to a global interpreter lock; and (4) automatic techniques to drastically reduce the memory footprints of Python applications. To improve the correctness of Python applications, the project will develop novel automated testing techniques that (1) augment property-based random testing with coverage-guided fuzzing; (2) employ concolic execution for smarter test generation and input minimization; (3) synthesize property-specific generator functions; (4) leverage statistical clustering techniques to reduce duplicated failure-inducing inputs; and (5) leverage parallelism and adaptive scheduling algorithms to increase testing throughput. The project will develop a set of ""bug benchmarks"" -- indeed, a novel benchmark-producing methodology -- to evaluate these techniques. The twin threads of performance and correctness are synergistic and complementary: automatic testing drives performance analysis, while performance optimizations (like parallelism) speed automatic testing.This award is co-funded by the Software & Hardware Foundations Program in the Division of Computer & Computing Foundations, and the NSF Office of Advanced Cyberinfrastructure.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
III: Large: Collaborative Research: Analysis Engineering for Robust End-to-End Data Science,IIS,1856641,Brad Myers,bam@cs.cmu.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","From poor statistical practices leading to retractions of scientific ""discoveries"" to low-level spreadsheet errors subverting high-stakes analyses, failures of data analysis can have catastrophic consequences. The rapid growth of data science practice in the last decade has led to large collaborative efforts to develop new data processing, machine learning, and analytics tools that put more advanced data analysis into the hands of a wider audience of practitioners, from students to scientists to designers. The most dominant tool for data science is code, where cutting-edge algorithms can be applied from an existing libraries. However, as this democratization of data science has lowered the barrier to using advanced methods, safely using these tools under sound statistical practice remains as difficult as ever. To facilitate more robust data science, this project investigates models and tools for analysis engineering by data scientists who write programs. The focus is on the complete end-to-end process of data analysis performed with code: the iterative, and often exploratory, steps that analysts go through to turn data into This project will contribute insights and characterizations of analytic work, novel methods for capturing and analyzing data science activities, and develop new programming tools and visualization methods for authoring and validating analyses. If successful, this project will augment people's ability to conduct and assess data analyses, promoting more robust results and reducing the gap between novice and expert analysts. The findings and tools from the project will be incorporated into educational efforts, including classroom teaching and tutorials and available as open source software integrated into popular analytical environments (e.g., Jupyter).Data analysis is a central activity to scientific research, yet is too often conducted in an undisciplined fashion. This project treats the entire analytic process as our central phenomenon of study. The project will employ mixed methods to study and characterize common analysis practices and pitfalls, including direct observations of data analysts, large-scale analysis of computational notebooks, and instrumentation of analytic programming environments like JupyterLab. The project will contribute new methods for specifying and safeguarding analyses, including domain-specific languages and program synthesis methods to guide users to preferred next steps. It will also explore ""multiverse"" workflows to manage and assess a diversity of analysis decisions. Analogues of debugging and testing tools will be developed to flag problems and perform error analysis, while the capture and visualization of analytic provenance to aid reproducibility, verification, and collaborative review. The work will be evaluated through controlled studies, classroom use, and open-source deployment for wide-scale field use.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Data: Toward Exascale Community Ocean Circulation Modeling,OAC,1835778,Ryan Abernathey,ra2697@columbia.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","This project designs and implements a software framework for handling petabyte-scale datasets; the focus is on global ocean circulation. A team of three universities (Johns Hopkins University, MIT, and Columbia University) builds a unified data system that is capable of delivering global ocean circulation model output at 1 km horizontal resolution. The product will be hosted in an open portal, providing the community with scalable software tools to enable analysis of the dataset. The team will use this data to answer specific questions about mixing and dissipation processes in the ocean. The goal of this effort is the creation and demonstration of a complete and replicable cyberinfrastructure for sharing and analysis of massive simulations. The focus is on high resolution ocean circulation modeling, with software tools that will enable efficient storage. Two major challenges to the study of ocean and climate dynamics are addressed: handling large datasets from high-resolution simulations, and understanding the role of small-scale ocean processes in large-scale ocean/climate systems. Resolving the first challenge would significantly facilitate ongoing and future studies of the ocean/atmosphere/climate system; addressing the second challenge would profoundly improve understanding of ocean/climate dynamics. The project builds a unified data system consisting of high-resolution global ocean circulation simulations, a petascale portal for data sharing, and scalable software tools for interactive analysis. The software framework from this project is expected to handle petascale to exascale datasets for users. Several pre-existing capabilities are leveraged for this project: the JHU regional numerical model of the Spill Jet on the East Greenland continental slope, software from the Pangeo project, the SciServer data-intensive software infrastructure, and lessons learned from the North East Storage Exchange multi-petabyte regional data store. The broader target is next generation simulation software in the geosciences and other disciplines. This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Ocean Sciences and the Integrative and Collaborative Education and Research Program within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
AXS - Enabling Analysis of Petascale Astronomical Datasets,AST,2003196,Mario Juric,mjuric@astro.washington.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Astronomy is being rapidly transformed by the advent of large, automated, digital sky surveys into a field where petabyte tabular data sets are becoming commonplace. Unfortunately, this increase has not been followed by commensurate improvements in the tools and frameworks: we are now limited not by the richness of our datasets, but by an inability to mine them for knowledge. When the most challenging questions of the day demand repeated, complex processing of large information-rich tabular datasets, scalable and stable tools that are easy to use by scientists are crucial. This is a project to develop, package, and deploy the Astronomical eXtensions for Spark (AXS), a scalable open-source astronomical data analysis framework built on Apache Spark. AXS will make it possible for astronomers, including and perhaps especially those who are not data management experts, to devise and execute astronomical big data analyses using industry-standard tools. This will be a transformative increase in the community's ability to extract knowledge from datasets collected at great expense, thus unlocking their value across all areas of astronomy. There will be opportunities for knowledge transfers and partnerships between industry and academia. The techniques used and created will be taught within the astronomy curriculum, and those curriculum materials will be made public. This will improve the competitiveness of astronomy students in careers beyond astronomy, and it will help to develop a globally competitive STEM workforce.AXS will enable astronomers to scale their analysis from a personal laptop to thousands of nodes on either cloud or NSF-supported cyberinfrastructure (CI). This system has already been prototyped, and leverages Spark, a state-of-the-art industry-standard engine for big data processing, to make it possible to query and analyze almost arbitrarily large astronomical catalogs while supporting complex workflows with astronomy-specific operations. The tool will be accompanied by a hosted demonstration service, documentation, and support for deployment on NSF CI resources and public cloud platforms. For long-term sustainability, AXS will be developed in a tight loop with major stakeholders, built on open source tools and processes, and strongly integrated with AstroPy and the PyData stack, which are widely used in astronomy. AXS will also robustly scale to large computational clusters, making both NSF-supported and public CI more accessible to astronomers. Developments by this project will enable other industrial and academic applications, especially those dealing with large, tabular, spatio-temporal datasets indexed on a sphere, such as geospatial analysis.This award by the Division of Astronomical Sciences within the NSF Directorate of Mathematical and Physical Sciences is jointly supported by the NSF Office of Advanced Cyberinfrastructure.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Towards Better Understanding of the Climate System Using a Global Storm-Resolving Model,AGS,2218827,Marat Khairoutdinov,marat.khairoutdinov@stonybrook.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","Weather phenomena come in all spatial scales, from the turbulent up-and-down motions within clouds to frontal systems that span a time zone to the jet streams that circle the globe. Naturally different models are used to capture phenomena at different scales, including Large Eddy Simulation (LES) models with grid spacings of perhaps 10m used to simulate individual clouds or small cloud clusters. LES models are typically applied on a limited domain, perhaps 10km to 100km wide, and the influence of larger scales of motion on the clouds is represented by imposing domain-wide conditions, for instance a single vertical profile of temperature and moisture for the whole domain. The drawback of such simulations is that they fail to capture two-way interactions between small and large scales, for instance the effect of small clouds on the large-scale temperature and moisture profiles. Models that can capture a larger range of scales would thus be quite valuable.One model which has proved quite useful for this purpose is the System for Atmospheric Modeling (SAM), developed by the Principal Investigator (PI) in the early 2000s. SAM has been used as an LES model, for instance in simulations of flow around a building at 1m resolution, but has also been used with grid spacings around 5km to simulate wave motions in a channel domain spanning the tropics. SAM has been a workhorse model for studies of cloud behaviors including the aggregation of convective clouds and the response of clouds to greenhouse gas-induced warming, in particular the extent to which the cloud response intensifies or counteracts the warming.Recently the PI developed a global version of SAM called gSAM, which extends the Cartesian coordinates to spherical coordinates and makes other modifications to represent flow on a global domain. The model inherits all of the features of SAM and also adds an immersed step topography, an advance over previous versions which were more idealized and assumed a flat surface. Another way in which gSAM adds realism is the ability to run simulations starting from observational initial conditions, allowing short-term ""forecasts"", also called hindcasts, of real-world weather system evolution. A recent study used this feature, along with ""nudging"" to reanalysis data, to simulate conditions observed during the SOCRATES field campaign (see AGS-16628674). The study concluded that the formation of cloud ice particles from the shattering of earlier ice particles plays a role in determining the width of clouds, thus regulating the amount of sunlight that reaches the surface of the Southern Ocean.The goal of this award is to further develop gSAM and make it available to the worldwide research community as a resource for weather and climate research. The work includes tasks devoted to improving model behavior near the poles, improving the accuracy and efficiency of radiative transfer calculations using machine learning techniques, improving input/ouput performance, and validating simulations against satellite data. Additional resources are developed to facilitate use and adoption of the model, including a full suite of documentation and tutorials, initial and boundary condition datasets for multiple configurations and resolutions, and model output for several six-month simulations. The model is maintained on GitHub and users can contribute to code development using GitHub repositories. The PI also maintains a model website that tracks publications using the model and provides additional information and resources. Since gSAM is an extension of SAM it is easily configured to run as a limited-domain LES model, thereby continuing to serve the SAM user community.The work has broader impacts due to the power of gSAM as a tool for conducting basic science research on a wide range of topics. One area of particular interest is the interaction between clouds and climate change, as the sensitivies of clouds to a warming climate could affect the amount of warming that occurs. gSAM can also contribute to our understanding of how the intensity of extreme precipitation events is likely to change in a warming world. In both cases gSAM serves to lower the barriers between the research communities studying climate processes on the global scale and cloud properties on the local scale. The project also supports a graduate student, thereby building the next generation scientific workforce.This project is co-funded by a collaboration between the Directorate for Geosciences and Office of Advanced Cyberinfrastructure to support Artificial Intelligence/Machine Learning and open science activities in the geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF),OAC,1835713,Jeffrey Potoff,jpotoff@wayne.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Comprehensive Time Series Data?Analytics for the Prediction of Solar Flares and Eruptions,OAC,1931555,Rafal Angryk,angryk@cs.gsu.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Solar flares are some of the largest explosive events in our solar system. Together with the accompanying eruptions, solar flares have the potential to disrupt the technology we rely on, such as GPS, radars, high-frequency radio communications between aircraft and air traffic control, communication technology that relies on satellites (such as cell phones and Internet), and electricity grid distribution networks. This project aims to, first, improve scientists' understanding of the time-dependent physical and statistical behavior of solar active regions to the point that we can exploit their observations to predict whether and when they will flare, or erupt, in general, and, second, to enable scientists worldwide to perform comparative, reproducible, and data-driven studies on the prediction of solar explosive events. This project, together with its advanced solar flare prediction software infrastructure, will strengthen our nation's efforts to mitigate the potentially catastrophic impact of solar eruptions. Moreover, by helping achieve reliable forecasts of the timing, location, and magnitude of solar flares (already considered as natural disasters), this project will also contribute to mitigating the impact on other types of infrastructure, such as satellites (GPS, Internet, satellite communications), that are critical to not only our national defense but also a broad range of sectors, such as enterprise operations (e.g., oil-drilling) and communications. This award will also invest in the development of a highly educated, diverse, globally competitive STEM workforce trained in an interdisciplinary environment through the educational and research efforts at Georgia State University. The goal of this project is to improve the scientists' understanding of eruptive solar events by re-shaping the state-of-the-art on data mining techniques. This project will build cyberinfrastructure for data-driven scientific research on complex multivariate time-series data sets. This includes public releases of comprehensive benchmark data sets ideal for data mining research on multivariate time series classification, regression, and clustering, as well as open-source software for these applications (i.e., the public release of pre-trained models). While the domain area is focused on solar physics, this software and data sets can benefit other domains that involve event tracking and mining of spatiotemporal trajectories (e.g., security and healthcare applications of monitoring movement, traffic and weather data analyses, and business predictions). The project will also advance research in Space Weather forecasting through the delivery of reproducible data-driven solar flare predictions, inspired by new research directions (most notably, time series analysis at an unprecedented level) and spearheaded by the data-driven, physically interpretable machine learning models. In this direction, the projects will produce publicly available flare forecasting big data benchmark data sets aligning with recommendations by the National Science and Technology Council and being useful to both Data Science and Space Weather communities. Finally, this project will advance Computer Science research in data mining and information retrieval areas through the above cutting-edge work on time series data analysis.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Flexible & Open-Source Models for Materials and Devices,OAC,1931479,Oliviero Andreussi,olivieroandreuss@boisestate.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","The project will develop first principles materials modeling software that can approach multiple length and time scales (multiscale). This software will be capable of modeling systems as complex as entire devices and materials of mesoscopic sizes. Over the course of the project the principal investigators plan to develop an open-source python-based software aimed at standardizing and generalizing multiscale simulations methods. This will enable the use of computer modeling in the design of new compounds, materials and devices. The goals are to render multiscale simulations reproducible and accessible by the broader community. In that context, the project will address the notion of ""lab 2.0"", by which computer simulations replace laboratory experiments in tasks such as materials design and costly combinatorial searches for viable chemical processes. The software will be self-optimized using machine learning and exploit linear workflows approachable by nonexperts. Education and diversity will be promoted by direct participation of underrepresented minorities from high schools and colleges in hackathon workshops and summer research programs.An approach that leverages the long-range multiscale capabilities of continuum models with accurate short-range atomistic descriptions of specific interactions, and that exploits the ideal scalability of quantum-embedding techniques, will be investigated. The main driver of the proposed implementation will be a Python codebase which will carry out the part of current software that is not computationally heavy, but instead is code heavy where many lines of code are needed in typically non-object-oriented languages. This is key to obtain the desired cluster-topology-agnostic workflows. Longstanding problems related to computational scalability and code stiffness will addressed in a three-pronged approach aimed at developing (1) modular tools implementing modules with highly object-oriented codes (e.g., quantum, classical atomistic, and continuum solvers), (2) hybrid tools implementing combinations of modular tools in a way that best exploits high-performance computing architectures, and (3) hyper tools implementing a high-level data-enabled optimization strategy that generates optimal workflows combining several hybrid tools, thereby making the software of broad applicability and accessible to nonexperts. These goals will render multiscale simulations reproducible and accessible by the broader community. The project will address the ""lab 2.0"" paradigm, by which computer simulations replace laboratory experiments in tasks such as materials design and combinatorial searches for viable chemical processes. The resultant software will be self-optimized using machine learning and exploit linear workflows approachable by nonexperts. Education and diversity will include the direct participation of underrepresented minorities from high schools and colleges in hackathon workshops and summer research programs.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Chemistry and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: C++ as a service - rapid software development and dynamic  interoperability with Python and beyond,OAC,1931408,David Lange,david.lange@princeton.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","A key enabler of innovation and discovery for many scientific researchers is the ability to explore data and express ideas quickly as software prototypes. Tools and techniques that reduce the ""time to insight"" are essential to the productivity of researchers. At the same time, massive increases in data volumes and computational needs require a continual focus on maximizing code performance to realize the potential science from novel scientific apparatus. Programming language usability and interoperability are omni-disciplinary issues affecting today's scientific research community. As a result, a common approach across many scientific fields research is for scientists to program in Python, while steering kernels written in C++. This C++ as a service (CaaS) project brings a novel interpretative technology to science researchers through a state-of-the-art C++ execution environment. CaaS will enable both beginners and experts in C++. It enables higher-productivity in development and extends the interactive education and training platform for programming languages. CaaS will enable existing technologies as well as truly new development and analysis approaches. CaaS will directly support grow cyber-capabilities that advance scientific research across a broad range of pursuits.Performance-focused languages, such as C++, are a critical infrastructure component for many scientific fields that have either large computing challenges or the need for low latency for results. The productivity of data scientists can be dramatically increased by an easy to use dynamic programming and development environment, together with a fully featured interoperability layer. The CaaS project provides a dynamic C++ execution environment and enables runtime language interoperability between C++ and other languages, such as Python, through a native-like, dynamic environment. CaaS provides seamless offloading of work in a heterogeneous computing environment, including hardware accelerators, which is more and more often required by today's researchers. These advances will enable researchers to more easily develop in, and use, large C++ codebases that are critical infrastructure components in many scientific fields. CaaS also allows true interoperability with C++ in Jupyter notebooks, and a robust prototyping environment for C++ developments. It encourages analysis and code sharing and facilitates scientific provenance tracking. By reducing the technical burden of development, researchers can focus instead on their scientific productivity. More broadly, notebook-based training in C++, or in a mixed programming environment that includes C++, is a key functionality. Enhancements in technical training will enable national advancements in science, technology, engineering, and mathematics capabilities.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
RAPID: MolSSI COVID-19 Biomolecular Simulation Data and Algorithm Consortium,OAC,2029322,Thomas Crawford,crawdad@vt.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","In response to the growing COVID-19 pandemic, the Molecular Sciences Software Institute (MolSSI) will leverage its position as a neutral commodity resource to help the global computational molecular sciences community quickly provide their scientific data and expertise to address the COVID-19 crisis. The MolSSI is jointly supported by the Office of Advanced Cyberinfrastructure and the Divisions of Chemistry and Materials Research. The centerpieces of this engagement will be (1) a centralized repository for simulation-related data targeting the virus and host proteins and potential pharmaceuticals, and (2) a select set of MolSSI Software Seed Fellowships for Ph.D. students and postdocs targeting COVID-19 related software tools that operate on the data developed in the repository. These two components will enable the biomolecular simulation community to share and utilize key data and other resources to help identify the structural and dynamic characteristics of the host-virus complex to generate potential leads for therapeutics. Although this project is intended to address the acute COVID-19 crisis, in the near term, it also will impact research communities and the next generation of computational molecular scientists in the confrontation and proactive resolution of future world problems.The MolSSI will create and curate a large-scale repository containing: simulation input files (structures, configurations, scripts, Jupyter notebooks) in an organized structure; MD trajectories, analysis tools, and ready models for drug discovery; pointers to preprint servers such as arXiv, bioRxiv, and ChemRxiv on biomolecular simulation research in regards SARS-CoV-2; and DOI services that create citable data. In addition, it will engage the molecular sciences community through a set of Software Fellowships for graduate student and postdocs to carry out software development, such as large-scale MD simulations, design of drug discovery tools such as docking, machine learning for small molecule toxicity predictions, and methods for determining whether new drugs are bioavailable or can be synthesized. Collectively, these resources will speed the identification and development of leads for antiviral drugs, analyzing structural effects of genetic variation in the SARS-CoV-2 virus, and inhibitors that can disrupt protein-protein interactions to viral entry into cells and adherence to surfaces that cause disease spread.This award is being funded by the CARES Act supplemental funds allocated to CISE and MPS.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Data: HDR: Nanocomposites to Metamaterials: A Knowledge Graph Framework,OAC,1835677,Lynda Brinson,cate.brinson@duke.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","A team of experts from four universities (Duke, RPI, Caltech and Northwestern) creates an open source data resource for the polymer nanocomposites and metamaterials communities. A broad spectrum of users will be able to query the system, identify materials that may have certain characteristics, and automatically produce information about these materials. The new capability (MetaMine) is based on previous work by the research team in nanomaterials (NanoMine). The effort focuses upon two significant domain problems: discovery of factors controlling the dissipation peak in nanocomposites, and tailored mechanical response in metamaterials motivated by an application to personalize running shoes. The project will significantly improve the representation of data and the robustness with which user communities can identify promising materials applications. By expanding interaction of the nanocomposite and metamaterials communities with curated data resources, the project enables new collaborations in materials discovery and design. Strong connections with the National Institute of Standards and Technology (NIST), the Air Force Research Laboratory (AFRL), and Lockheed Martin facilitate industry and government use of the resulting knowledge base. The project develops an open source Materials Knowledge Graph (MKG) framework. The framework for materials includes extensible semantic infrastructure, customizable user templates, semi-automatic curation tools, ontology-enabled design tools and custom user dashboards. The work generalizes a prototype data resource (NanoMine) previously developed by the researchers, and demonstrates the extensibility of this framework to metamaterials. NanoMine enables annotation, organization and data storage on a wide variety of nanocomposite samples, including information on composition, processing, microstructure and properties. The extensibility will be demonstrated through creation of a MetaMine module for metamaterials, parallel to the NanoMine module for nanocomposites. The frameworks will allow for curation of data sets and end-user discovery of processing-structure-property relationships. The work supports the Materials Genome Initiative by creating an extensible data ecosystem to share and re-use materials data, enabling faster development of materials via robust testing of models and application of analysis tools. The capability will be compatible with the NIST Material Data Curator System, and the team also engages both AFRL and Lockheed Martin to facilitate industry and government use of the resulting knowledge base. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate for Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science,OAC,1835441,Albert Esterline,esterlin@ncat.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth. Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn. The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above. Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities. Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks. Compositions of these diverse capabilities are rare. Furthermore, many researchers who study networks are not computer scientists. As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming. The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use. What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science. CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software. CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks. The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program. It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions. CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science. Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science,OAC,1835439,Richard Alo,richard.alo@famu.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth. Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn. The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above. Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities. Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks. Compositions of these diverse capabilities are rare. Furthermore, many researchers who study networks are not computer scientists. As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming. The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use. What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science. CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software. CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks. The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program. It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions. CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science. Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: ROCCI: Integrated Cyberinfrastructure for In Situ Lossy Compression Optimization Based on Post Hoc Analysis Requirements,OAC,2104023,Sheng Di,sdi@uchicago.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Today?s simulations and advanced instruments are producing vast volumes of data, presenting a major storage and I/O burden for scientists. Error-bounded lossy compressors, which can significantly reduce the data volume while controlling data distortion with a constant error bound, have been developed for years. However, a significant gap still remains in practice. On the one hand, the impact of the compression errors on scientific research is not well understood, so how to set an appropriate error bound for lossy compression is very challenging. On the other hand, how to select the best fit compression technology and run it automatically in scientific application codes is non-trivial because of strengths and weaknesses of different compression techniques and diverse characteristics of applications and datasets. This project aims to develop a Requirement-Oriented Compression Cyber-Infrastructure (ROCCI) for data-intensive domains such as astrophysics and materials science, which can select and run the best fit lossy compressor automatically at runtime, in terms of user's requirement on their post hoc analysis.The overarching goal of this project is to offer a complete series of automatic functions and services allowing users to transparently run the best fit compressor at runtime during the scientific simulations or data acquisition. This project advances knowledge and understanding with three key thrusts: (1) it builds an efficient layer to interoperate with different lossy compressors and diverse post hoc analysis requirements on data fidelity by leveraging an existing compression adaptor library (LibPressio) and compression assessment library (Z-checker); (2) it develops an efficient engine to determine the best fit compressor with optimized settings based on user?s post-hoc analysis requirements; and (3) it develops a user-friendly infrastructure that integrates compression optimization and execution via the HDF5 dynamic filter mechanism. This project particularly targets cosmology and materials science applications and their specific requirements of using lossy compressors in practice.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
ELEMENTS: DATA: HDR: SWIM to a Sustainable Water Future,OAC,1835897,Natalia Villanueva Rosales,nvillanuevarosales@utep.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","The project develops a system that automates ingestion of data into models, the integration of decoupled models, and the dynamic generation and interpretation of models. The focus is on water resources. The team leverages software development from an ongoing USDA-funded project, and the expertise of an interdisciplinary, international team of scientists and students who are investigating future scenarios of water availability and use in the Middle Rio Grande valley of southern New Mexico, west Texas, and northern Chihuahua (Mexico).This project advances water sustainability research capabilities by creating a Sustainable Water through Integrated Modeling (SWIM) framework that automates ingestion of data into models, facilitates integration of decoupled models, and supports dynamic generation and interpretation of models. The four objectives are to: 1) foster use of water models by stakeholders (non-modelers) through direct participation enabled by a web-based interface and provenance capture; 2) enable seamless model-to-model integration through service-driven data exchange and transformation; 3) develop data- and technology-enabled approaches for reasoning with biophysical and social models; and 4) engage data providers, modelers and stakeholders in conceiving and testing the framework.The research and products of this project contribute to advanced research capabilities on water sustainability. By providing seamless integration of scientific data and models, and generating provenance data to create dynamic user interfaces, the project instills trust in the models generated through participatory analysis. This approach is built around a strong appreciation of the value of stakeholder engagement and alignment to achieving the described goals. The research is carried out by a diverse and experienced team, and will contribute to understanding of how to more effectively conduct convergent research with researchers and stakeholders.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Cyberloop for Accelerated Bionanomaterials Design,OAC,1931304,Ellad Tadmor,tadmor@aem.umn.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","The evolution of biological and materials systems must be understood at many scales in order to achieve groundbreaking advances. Areas that are impacted include the health sciences, materials sciences, energy conversion, sustainability, and overall quality of life. Molecular simulations using complex models and configurations play an increasing role in such efforts. They address the limitations of experiments which study events over very small time and length scales. Such simulations require great expertise due to the complexity of the systems being studied. and the tools being used. This is particularly true for systems containing both inorganic and biological materials. This project will help researchers to quickly set up complex simulations, carry out the simulations with high accuracy, and assess uncertainties in the results. They will help develop the Cyberloop computational infrastructure. Cyberloop will dramatically reduce the time required to perform state-of-the-art simulations. They will also help to educate the next generation of researchers in this important field.Cyberloop will integrate three existing successful platforms for soft matter and solid state simulations (IFF, OpenKIM, and CHARMM-GUI) into a single unified framework. These systems will work together to enable users to set up complex bionanomaterial configurations, select reliable validated force fields, generate input scripts for popular simulation platforms, and assess the uncertainty in the results. The integration of these tools requires a host of technological and scientific innovations including: automated charge assignment protocols and file conversions, expansion of the Interface force field (IFF) to new systems, generation of new surface models, extension of the Open Knowledgebase of Interatomic Models (OpenKIM) to bonded force fields, development of machine learning based force field selection and uncertainty tools, and development of new Nanomaterial Builder and Bionano Builder modules in CHARMM-GUI. Cyberloop fulfils a critical need in the user community to discover and engineer new multi-component bionanomaterials to create the next generation of therapeutics, materials for energy conversion, and ultrastrong composites. The project will facilitate the training of graduate students, undergraduate students, and postdoctoral scholars, including underrepresented and minority students, at the participating institutions to prepare an interdisciplinary scientific workforce with significant experience in cyber-enabled technology. Online educational materials and tutorials will help increase participation in bionanomaterial research across academia and government. This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure, and the Division of Materials Research and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Expeditions: Collaborative Research: Global Pervasive Computational Epidemiology,CCF,1918656,Madhav Marathe,mvm7hz@virginia.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","Infectious diseases cause more than 13 million deaths per year worldwide. Rapid growth in human population and its ability to adapt to a variety of environmental conditions has resulted in unprecedented levels of interaction between humans and other species. This rise in interaction combined with emerging trends in globalization, anti-microbial resistance, urbanization, climate change, and ecological pressures has increased the risk of a global pandemic. Computation and data sciences can capture the complexities underlying these disease determinants and revolutionize real-time epidemiology --- leading to fundamentally new ways to reduce the global burden of infectious diseases that has plagued humanity for thousands of years. This Expeditions project will enable novel implementations of global infectious disease computational epidemiology by advancing computational foundations, engineering principles, theoretical understanding, and novel technologies. The innovative tools developed will provide new analytical capabilities to decision makers and result in improved science-based decision making for epidemic planning and response. They will facilitate enhanced inter-agency and inter-government coordination and outbreak response. The team will work closely with many local, regional, national, and international public health agencies and universities to apply and deploy powerful technologies during epidemic outbreaks that can be expected to occur during the course of the project. International scientific networks linked to a comprehensive postdoctoral, graduate and undergraduate student training program will be established. Educational programs to foster interest in and increase understanding of computational science in addressing the complex societal challenges due to pandemics will also be developed. The team, with partners in Asia, Africa, Europe, and Latin America, will produce multidisciplinary scientists with diverse skills related to public health. The novel implementations of this project will be enabled by the development of a rigorous computational theory of spreading and control processes on dynamic multi-scale, multi-layer (MSML) networks, along with tools from AI, machine learning, and social sciences. New techniques resulting from this research will make it possible to develop and apply large-scale simulations of epidemics and social interactions over MSML networks. These simulations, in turn, will provide fundamentally new insights into how to control epidemics. Pervasive computing technologies will be developed to support disease surveillance and real-time response. The computational advances will also be generalizable; that is, they will be applicable to other areas such as cybersecurity, ecology, economics and social sciences. The project will take into account emerging concerns and constraints that include: preserving privacy of individuals and vulnerable groups, enabling model predictions to be interpreted and explained, developing effective interventions under uncertain and unknown network data, understanding strategic and adversarial behaviors of individual agents, and ensuring fairness of the process across the entire population. The research team includes experts from multiple disciplines and will address these societal concerns and constraints in practical, impactful, and novel ways, including the development of computational tools and techniques to support sound, ethical science-based policy pertaining to public health infectious disease epidemiology. Center for Computational Research in Epidemiology (CoRE) at the University of Virginia will be established as a part of the project. CoRE will develop transformative ways to support real-time epidemiology and facilitate improved outbreak response to benefit the society.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: CRISPS: Cell-Centric Recursive Image Similarity Projection Searching,OAC,2246463,Joshua Agar,jca92@drexel.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Materials scientists use microscopy to determine the structure, order, and periodicity that affect properties. The core challenge is that only a fraction of microscopy data is published. Thus, most of the information from these costly experiments is lost to an abyss. CRISPS - Cell-Centric Recursive Image Similarity Projection Searching ? is a computing infrastructure to make high-value materials microscopy Findable, Accessible, Interoperable, and Reproducible (FAIR). CRISPS is a multitouch, interactive research ""buddy"" that uses artificial intelligence to form associations like the human mind to identify similarities between images. It has permanent, immutable recollection to search and discover collections of scientific images based on labels and associations. CRISPS will be made openly available and will be promoted at conferences, in user facilities, and online to foster a community of developers and users. Public scientific literacy will be enforced through interactive museum exhibits that use CRISPS to explore and discover materials microscopy. The program also supports a first summer research experience for six under-represented persons in STEM.CRISPS is a full-stack software solution for materials microscopy that seamlessly integrates three novel software concepts. 1. DataFed: a federated scientific database for collecting, collating, and searching scientific data and metadata. 2. Schema-Free Search: a tool for cell-centric indexing and searching metadata without schemas. 3. Recursive Image Similarity Projections: a tool to interactively explore image similarity. Each of these efforts is intellectually innovative. DataFed provides an automated, secure, scalable generalized scientific data repository that supports metadata schemas, searches, and provenance graphs. DataFed removes barriers to collaborative science through trusted authentication and secure managed file transfers using GridFTP. Schema-Free Search: an innovative index and ML-tokenization methodology to search unstructured metadata efficiently using ElasticSearch. Scientists will discover schemas and ontologies through an interactive graphical user interface (GUI). Recursive Image Similarity Projections: a collection of deep learning models to conduct automatic symmetry-aware microscopy featurization. When coupled with manifold learning and a GUI, this software tool will enable filtering, similarity exploration, and rapid labeling of materials microscopy. Combining these tools will facilitate creative inquiry into unpublished microscopy, accelerating the discovery of new materials with novel functionalities. CRISPS will be documented and released under a non-restrictive license which allows its reuse, modification, and commercialization. The PIs work with stakeholders in academia and industry to implement CRISPS for typical experiments in optical, electron, and scanning probe microscopies. It provides public access to a 0.5 PB allocation on a DataFed server with indexing and image similarity searching functionality through CRISPS. Interdisciplinary concepts, including scientific data management, search ontologies, and machine learning, are being integrated into courses in materials and computer science; and will be broadly shared through the Lehigh Microscopy School and conference tutorials.This proposal receives funds through the Office of Advanced Cyberinfrastructure in the Computer and Information Science and Engineering Directorate and the Division of Materials Research in the Mathematical and Physical Sciences Directorate.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: GPU-accelerated First-Principles Simulation of Exciton Dynamics in Complex Systems,OAC,2209858,Yosuke Kanai,ykanai@UNC.EDU,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","The development of societally-important energy harvesting approaches, such as photocatalysis or photovoltaics, requires detailed understanding of fast dynamics of electrons coupled with ions in novel materials. Modern supercomputers can help obtain such an understanding using sophisticated quantum-mechanical simulations. However, such a scientific effort requires accurate simulation techniques to be developed and efficient use of the underlying supercomputer hardware is crucial. This project meets these outstanding challenges by implementing novel techniques to describe the quantum-mechanical electron-electron interaction and interactions of electron dynamics with ions. Using and testing these new developments on graphical processing units advances science as it prepares quantum-mechanical simulations for next-generation supercomputers. Applying these advanced simulations to model complex systems of great importance for energy harvesting furthers the computational science community towards the goal of advancing national prosperity and welfare. The project makes these techniques freely available for a broad community, including documentation and tutorials, and trains the next generation of computational researchers through organizing summer schools and workshops.This project leverages a multi-organizational team to benefit from synergies that emerge from two possible solutions to current scientific barriers: Descriptions of exchange and correlation based on a long-range correction and on hybrid functionals that can scale favorably even for large systems with thousands of electrons are implemented and applied. Descriptions of non-adiabatic dynamics are implemented and applied to study long-term dynamics of excitons during which the interaction with the nuclei becomes important. Doing so within a cutting-edge electronic-structure code that runs efficiently on graphics processing units, provides a unique opportunity to compare accuracy, applicability to a broad range of systems, and computational cost. Knowledge on the reliability of these approximations and their computational cost for extended systems of practical relevance, including complex heterogeneous systems like semiconductor-molecule interfaces, are advanced by this research. The efforts include building, increasing, and growing a skilled community especially of US based researchers in a recurrent summer school.This proposal receives funds through the Office of Advanced Cyberinfrastructure in the Computer and Information Science and Engineering Directorate and the Division of Materials Research in the Mathematical and Physical Sciences Directorate.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: TRAnsparency CErtified (TRACE): Trusting Computational Research Without Repeating It,OAC,2209628,Bertram Ludaescher,ludaesch@illinois.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Research communities across the natural and social sciences are increasingly concerned about the transparency and reproducibility of results obtained by computational means. Calls for increased transparency can be found in the policies of peer-reviewed journals and processing pipelines employed in the creation of research data products made available through science gateways, data portals, and statistical agencies. These communities recognize that the integrity of published results and data products is uncertain when it is not possible to trace their lineage or validate their production. Verifying the transparency or reproducibility of computational artifacts?by repeating computations and comparing results?is expensive, time-consuming, and difficult, and may be infeasible if the research products rely on resources that are subject to legitimate restrictions such as the use of sensitive or proprietary data; streaming, transient, or ephemeral data; and large-scale or specialized computational resources available only to approved or authorized users. The TRACE project is addressing this problem through an approach called certified transparency - a trustworthy record of computations signed by the systems within which they were performed. Using TRACE, system owners and operators certify the original execution of a computational workflow that produces findings or data products. By using a TRACE-enabled system, researchers produce transparent computational artifacts that no longer require verification, reducing burden on journal editors and reviewers seeking to ensure reproducibility and transparency of computational results. TRACE presents an innovative and efficient approach to ensuring the transparency of research that uses computational methods, is consistent with the vision outlined by the National Academies, and enables evidence-based policymaking based on transparent and trustworthy science.The central goal of the TRACE project is the development, validation, and implementation of a technical model of certified transparency. This includes a set of infrastructure elements that can be employed by system owners to (1) declare the dimensions of computational transparency supported by their platforms; (2) certify that a specific computational workflow was executed on the platform; and (3) bundle artifacts, records of their execution, technical metadata about their contents, and certify them for dissemination. The first phase of the project focuses on the development of a conceptual model and technical specification that can be used to certify the description of a system, termed a Transparency-Certified System (TRACE system), and the aggregation of artifacts along with records of their execution, termed Transparency-Certified Research Objects (TROs). The second phase focuses on the development of reusable software components implementing the TRACE model and approach. To demonstrate certified transparency, the toolkit is used to TRACE-enable existing platforms including Whole Tale, SKOPE, and the SLURM workload manager. These TRACE-enabled systems produce certified TROs that can be trusted and do not need to be repeated or re-executed to verify that results were obtained as claimed.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Social and Economic Sciences within the Directorate for Social, Behavioral and Economic Sciences; and by the Division of Information and Intelligent Systems within the Directorate for Computer and Information Science and Engineering.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Simulating Autonomous Agents and the Human-Autonomous Agent Interaction,OAC,2209791,Dan Negrut,negrut@wisc.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","This project augments the Chrono computer simulation platform in transformative ways. Chrono's purpose is to predict through simulation the interplay between mechatronic systems, the environment they operate in, and humans with whom they might interact. The open-source simulation platform is slated to become a community-shared virtual investigation tool used to probe competing engineering designs and test hypotheses that would be too dangerous, difficult, or costly to verify through physical experiments. Chrono has been and will continue to be used in multiple fields and disciplines, e.g., terramechanics, astrophysics; soft matter physics; biomechanics; mechanical engineering; civil engineering; industrial engineering; and computer science. Specifically, it is used to engineer the 2023 VIPER lunar rover; relied upon by US Army experts in evaluating its wheeled and tracked vehicle designs; used in the US and Germany in the wind turbine industry; and involved in designing wave energy conversion solutions in Europe. Upon project completion, Chrono will become a simulation engine in Gazebo, which is widely used in robotics research; operate on the largest driving simulator in the US; empower research in the bio-robotics and field-robotics communities; and assist efforts in the broad area of automotive research carried out by a consortium of universities and companies under the umbrella of the Automotive Research Center. The educational impact of this project is threefold: training undergraduate, graduate, and post-doctoral students in a multi-disciplinary fashion that emphasizes advanced computing skills development; anchoring two new courses in autonomous vehicle control and simulation in robotics; and broadening participation in computing through a residential program on the campus of the University of Wisconsin-Madison that engages teachers and students from rural high-schools. Innovation and discovery are fueled by quality data. At its core, this project seeks to increase the share of this data that has simulation as its provenance. In this context, a multi-disciplinary team of 40 researchers augments and validates a physics-based simulation framework that empowers research in autonomous agents (AAs). The AAs operate in complex and unstructured dynamic environments and might engage in two-way interaction with humans or other AAs. This project enables Chrono to generate machine learning training data quickly and inexpensively; facilitates comparison of competing designs for assessing trade-offs; and gauges candidate design robustness via testing in simulation of corner-case scenarios. These tasks are accomplished by upgrading and extending Chrono to leverage recent computational dynamics innovations, e.g., a faster index 3 differential algebraic equations solver; a new approach to solving frictional contact problems; a real-time solver for handling flexible-body dynamics in soft robotics via nonlinear finite element analysis; a best-in-class simulator for terradynamics applications; reliance on just-in-time compiling for producing executables that are both problem- and hardware-optimized; a novel way for using mixed data representations for parsimonious storing of state information; and a scalable multi-agent framework that enables geographically-distributed, over the Internet, real-time simulation of human-AA interaction.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: The Einstein Toolkit ecosystem: Enabling fundamental research in the era of multi-messenger astrophysics,OAC,2227105,Zachariah Etienne,zbetienne@mail.wvu.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","A team of experts from five institutions (University of Illinois Urbana-Champaign, Georgia Institute of Technology, Rochester Institute of Technology, Louisiana State University, and West Virginia University) are collaborating on further development of the Einstein Toolkit, a community-driven, open-source cyberinfrastructure ecosystem providing computational tools supporting research in computational astrophysics, gravitational physics, and fundamental science. The new tools address current and future challenges in gravitational wave source modeling, improve the scalability of the code base, and support an expanded science and user community around the Einstein Toolkit.The Einstein Toolkit is a community-driven suite of research-grade Python codes for performing astrophysics and gravitational wave calculations. The code is open-source, accessible via Conda (an open source package management system) and represents a long-term investment by NSF in providing such computational infrastructure. The software is designed to simulate compact binary stars as sources of gravitational waves. This project focuses on the sustainability of the Einstein Toolkit; specific research efforts center around the development of three new software capabilities for the toolkit: ? CarpetX -- a new mesh refinement driver and interface between AMReX, a software framework containing the functionality to write massively parallel block-structured adaptive mesh refinement (AMR) code, and Cactus, a framework for building a variety of computing applications in science and engineering;? NRPy+ -- a user-friendly code generator based on Python; and ? Canuda -- a new physics library to probe fundamental physics. Integration of graphics processing units (GPUs) will incorporate modern heterogeneous computing devices into the system and will enhance the capability of the toolkit. The end product is sustainable through integration into the Einstein Toolkit, yet also includes an active community maintaining and enhancing the foundational components. Broader impacts are enhanced through training, documentation and a support infrastructure that reduces the barrier to adoption by the community. The team is also creating a science portal with additional educational and showcase resources. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the National Science Foundation's Big Idea activities in Windows on the Universe (WoU).This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: OpenEarthscape - Transformative Cyberinfrastructure for Modeling and Simulation in the Earth-Surface Science Communities,OAC,2104055,Allison Pfeiffer,allison.pfeiffer@wwu.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).The landscape around us changes constantly. Sometimes change is slow: a river bend migrates, soil erodes from a field, a waterfall carves its way upstream. Sometimes change happens fast: a landslide, a flood, a winter storm eroding beaches. To understand, manage, and forecast such processes, scientists rely on computer simulation models. This project develops software tools to make simulation modeling easier, more accessible, and more efficient. Among the products is a website through which researchers and students alike can learn about and experiment with a variety of environmental simulation models, without needing to install anything on their own computers. This web portal takes advantage of a technology that combines text, pictures, and computer code in a single online document. The project also develops improved computer-programming libraries designed to make it easier and more efficient for researchers to create new simulation models. The project contributes computing-skills training for college students enrolled in Colorado-based summer programs that serve traditionally underrepresented student populations. The project also promotes public education in geology, by creating an online animated simulation illustrating how landscapes evolve in response to various geologic events.As the sciences that probe Earth's changing surface become more quantitative and prediction-oriented, they increasingly rely on computational modeling and model-data integration. This project develops OpenEarthscape: an integrated suite of community-developed cyber resources for simulation and model-data integration, focusing on nine high-priority geoscience frontiers. Products and activities include EarthscapeHub: a JupyterHub server providing easy access to models, tools, and libraries; new capacity for creating and sharing reproducible analyses; and major enhancements to current programming libraries for model construction and coupling. OpenEarthscape catalyzes efficiency by building new technology to improve performance and developing an extended version of the Basic Model Interface API standard to address parallel architecture and coupling. OpenEarthscape fosters research productivity with improved library capabilities for data I/O and visualization, and with community resources for efficient software distribution and cross-platform compatibility. Broader impacts include partnership with undergraduate research programs that support traditionally underrepresented student populations, with the project team contributing introductory training in scientific computing. A novel educational element is the OpenEarthscape Simulator: a web-hosted visual simulation of a micro-continent evolving in response to various geologic events. The simulator provides students and the general public with an intriguing visualization of Earthscape dynamics and provides a template for the research community to identify defects in our current understanding.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
ELEMENTS: Anharmonic formalism and codes to calculate thermal transport and phase change from first-principles calculations,OAC,2103989,Keivan Esfarjani,ke4c@virginia.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Most properties of crystalline materials can today be calculated with a high degree of accuracy via computation. Due to the nature of the physical theories, these properties are, however, currently calculated at absolute zero temperature. The purpose of this project is to develop methodologies and computer codes to extend the calculation of elastic and thermodynamic properties to temperatures as high as 1000C or more, where thermal expansion becomes important and structural phase transitions can occur. This work can have a great impact in the aerospace and car industries, which make engines and parts operating at very high temperatures. The tools we will develop enable accurate prediction of stability and thermophysical properties of materials, even if they have not been synthesized in the lab. The results of these calculations will also be included in materials databases and help in our understanding of the behavior of new functional materials. These tools will be freely available to the research community under an open source license so as to engage the community in further development and collaboration. During this project a postdoctoral researcher and a graduate student will be trained in computing, data processing and storage methods. We will also develop modules to teach K-12 students about energy, its conversion, storage and sustainability during summer projects organized by the project lead.The mission of the proposed project is to provide to the materials physics community tools based on a new generation of quantum mechanical methodologies and input from first-principles calculations, to enable advances in two challenging areas: (1) thermodynamic, dielectric, mechanical and thermal transport properties at high temperatures where anharmonic effects become important, and (2) prediction of solid-solid phase transitions as a function of temperature, particularly in multifunctional materials, in which phonons are coupled to electronic degrees of freedom. This approach will be systematic, and applies to real materials enabling quantitative prediction of the above properties. The codes will be tested and validated on non-trivial materials such as transition metal oxides (TMOs) due to those materials having a rich number of phase transitions and emergent multiferroic phases. These materials and their applications in energy and information storage and processing also have a large amount of experimental and theoretical data on their phase transitions available, for validation. In summary, these timely tools will enable materials scientists to predict or understand thermophysical properties of anharmonic, complex and multifunctional materials at arbitrary temperatures with unprecedented accuracy.This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering, with the Division of Materials Research in the Directorate for Mathematical and Physical Sciences also contributing funds.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Simulation-driven Evaluation of Cyberinfrastructure Systems,OAC,2103489,Henri Casanova,henric@hawaii.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Most scientific breakthroughs and discoveries are now preconditioned on performing complex processing of vast amounts of data as conveniently, reliably, and efficiently as possible. This requires high-end interconnected compute and storage resources, as well as software systems to automate the processing on these resources. An enormous amount of effort has been invested in producing such ""cyberinfrastructure"" software systems. And yet, developing and evolving these systems so that they are as efficient as possible, while anticipating future cyberinfrastructure opportunities and needs, is an open challenge. This project transforms the way in which these systems are evaluated, so that their capabilities can be developed and evolved judiciously. The traditional evaluation approach is to observe executions of these systems on real-world hardware resources. Although seemingly natural, this approach suffers from many shortcomings. Instead, this project focuses on simulating these executions. Simulation has tremendous, and untapped, potential for transforming the development cycle of cyberinfrastructure systems. Specifically, this project produces software elements that can be easily integrated into existing and future systems to afford them with simulation capabilities. These capabilities make it possible for developers to put their systems through the wringer and observe their behaviors for arbitrary operating conditions, including ones that go beyond current hardware platforms and scientific applications. Simply put, these capabilities will make it possible to establish a solid experimental science approach for the development of cyberinfrastructure systems that support current and future scientific endeavors that are critical to the development of our society.The cyberinfrastructure has been the object of intensive research and development, resulting in a rich set of interoperable software systems that are used to support science. A key challenge is the development of systems that can execute application workloads efficiently, while anticipating future cyberinfrastructure opportunities and needs. This project aims to transform the way in which these systems are evaluated, so that their capabilities can be evolved based on a sound, quantitative experimental science approach. The traditional evaluation approach is to use full-fledged software stacks to execute application workloads on actual cyberinfrastructure deployments. Unfortunately, this approach suffers from several shortcomings: real-world experiments are time- and labor-intensive, and they are limited to currently available hardware and software configurations. An alternative to real-world experiments that does not suffer from these shortcomings is simulation, i.e., the implementation and use of a software artifact that models the functional and performance behaviors of software and hardware stacks of interest. This project uses simulation to transform the way in which cyberinfrastructure systems are evaluated as part of their long-term development cycles. This is achieved via software elements for enhancing production cyberinfrastructure systems with simulation capabilities so as to enable quantitative evaluation of these systems for arbitrary execution scenarios. Creating these scenarios requires little labor, and executions can be simulated accurately and orders of magnitude faster than their real-world counterparts. Furthermore, simulations are perfectly reproducible and observable. While this approach is general, its effectiveness will be demonstrated by applying it to a number of production systems, namely, workflow management systems. This project capitalizes on the years of development invested in the SimGrid and WRENCH simulation frameworks.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Frameworks: Collaborative Research: ChronoLog: A High-Performance Storage Infrastructure for Activity and Log Workloads,OAC,2104008,Kyle Chard,chard@uchicago.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","Modern computing applications generate massive amounts of data at unprecedented rates. Beyond simply storing data, one increasingly common requirement is to store activity data, also known as log data, which describe things that happen rather than things that are. Activity data are generated by computing systems, scientific instruments, electrical devices, etc. as well as by humans. The fast growing of activity data stresses current data management systems beyond their capability and becomes a known killer performance bottleneck of high-performance computing systems. This project develops ChronoLog, a novel system for organizing and storing activity data effectively and efficiently. ChronoLog leverages modern storage hardware and provides user-focused plugins and easy-to-use interface for productivity. It will benefit a diverse range of communities in various ways, such as enabling better fraud detection in financial transactions, faster and more accurate weather predictions and simulations, reduced time-to-insight for medical and bioengineering data, autonomous computing (e.g., driving), and more secure web and mobile services. ChronoLog uses physical time to provide a synchronization-free data distribution and the total ordering on a log. It first leverages multiple storage tiers, such as storage-class memories (e.g., 3D XPoint) and new flash storage (e.g., NVMe SSDs), to transparently scale the log via log auto-tiering. It then adopts a tunable parallel access model, which offers multiple-writers-multiple-readers (MWMR) semantics and highly concurrent I/O, to fully utilize the multi-tiered storage environment. ChronoLog's innovative design supports high-performance data access via I/O isolation between tails and historical operations, efficient resource utilization with newly developed elastic storage capabilities, and scalability using a novel 3D log distribution. It facilitates data processing pipelining by acting as an authoritative source of strong consistency and with the help of fast append and commit semantics. It can be used as an arbitrator offering a plethora of features such as transactional isolation and atomicity, a consensus engine for consistent replication and indexing services, and a scalable data integration and warehousing solution. ChronoLog and its plugins establish a robust, flexible, and high-performance storage ecosystem that promotes the development of scalable applications and services for high performance computing systems. The project includes a diverse group of collaborators who share a common need for a fundamentally new approach to distributed logging to address their use cases. These close partnerships will strengthen the bonds between academic and applied science, ultimately leading to new applications and driving discovery in domains as diverse as geoscience, cosmology, and astrophysics. Forming these collaborations and integrating students and junior IT professionals will create a well-trained workforce in cyberinfrastructure.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: SciMem: Enabling High Performance Multi-Scale Simulation on Big Memory Platforms,OAC,2104116,Dong Li,dli35@ucmerced.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Increasing system scalability is crucial to improving nation?s computation capabilities for scientific applications. However, some applications often face the scalability challenge from the perspective of memory capacity. This is especially true in multi-scale simulations when handling massive simulation data from different scales. The emerging big memory infrastructures have shown great potential to increase the simulation scale and solve larger numerical problems. However, using big memory architectures for the multi-scale simulation is challenging, because of limited computing capability in the big memory machines and memory heterogeneity introduced by big memory. There is a lack of a software infrastructure that can release the full power of big memory to accelerate multi-scale simulation. This project aims to create a capability and a software package (named SciMem) that enables high performance multi-scale simulation on big memory platforms. The techniques presented offer a path for general use of this structure for a wide variety of applications having a broad impact on science and engineering. There will be impact on the students through their direct involvement with the project and through the integration with the educational activities.The project will enable high performance multi-scale simulations on big memory platforms through more efficient utilization of large and heterogeneous memory machines. Specifically, it will replace computations with pre-computed and stored in memory data on a heterogeneous computing systems. The developed tool, SciMem, will be integrated and tested with the popular parallel molecular dynamics simulator, LAMMPS (Large-scale Atomic/Molecular Massively Parallel Simulator). The developed improvements in the use of computational resources will allow more accurate models of complex physical phenomena to be carried out on the emerging hardware systems. SciMem aims to bring a 10x performance improvement for certain larger-scale multi-scale simulations widely applied in the fields of computational chemistry and material science, e.g., quantum mechanical/molecular mechanical-based molecular dynamics (MD) simulation of catalysis.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Spatiotemporal Analysis of Magnetic Polarity Inversion Lines (STEAMPIL),OAC,2104004,Berkay Aydin,baydin2@gsu.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","Extreme space weather events such as solar flares, coronal mass ejections, energetic proton events and geomagnetic storms can cause massive disruptions in many technologically complex systems, including radio communications, telecommunication and navigation satellites, electrical power systems, or space and even commercial airline flights. This project builds a detection and analysis cyberinfrastructure, and investigates one of the most distinctive features in the solar atmosphere ? magnetic polarity inversion lines, which are hotspots of the most intense eruptive activity. Analyzing these features enables solar physicists to advance understanding of extreme space weather events and provide needed predictive capabilities for space weather forecasters.This project creates an innovative and sustainable software infrastructure to detect, characterize and analyze polarity inversion lines. The first step toward that objective is the identification of polarity inversion lines, and quantitative characterization of these multi-faceted features through image descriptors. In subsequent stages, this project analyzes the time series of these features and descriptors using advanced machine learning and data mining techniques, specifically for improving space weather forecasting capabilities. This includes analyzing the spatiotemporal patterns of emergence and disappearance for polarity inversion lines, selecting and understanding important shape characteristics of these lines pertinent to solar eruptive activity, and creating a prototype eruption forecasting system with discovered precursors. Automatically identifying and analyzing polarity inversion lines has several direct benefits: physically understanding solar magnetic shear layers and the transition from typical non-eruptive active region states to intense, eruptive ones; making contributions to forecasting of solar eruptions; and generating descriptors and measures that can be useful to the study of shear layers in nature.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Solar Terrestrial Physics Program and the Division of Integrative and Collaborative Education and Research within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Basic ALgebra LIbraries for Sustainable Technology with Interdisciplinary Collaboration (BALLISTIC),OAC,2004541,Jack Dongarra,dongarra@icl.utk.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","Scientific software libraries have long provided a large and growing resource for high-quality, reusable software components upon which applications from science and engineering can be rapidly constructed ? with improved robustness, portability, and sustainability. For this, a team of researchers from four collaborating organizations proposed to develop BALLISTIC (Basic ALgebra Libraries for Sustainable Technology with Interdisciplinary Collaboration). The BALLISTIC project, through the leading-edge research it channels into its software deliverables, will lead to the introduction of tools that will simplify the transition to the next generation of extreme-scale computer architectures. The main impact of the project will be to develop, push, and deploy software into the scientific community to make it competitive on a world-wide scale and to contribute to standardization efforts in the area. BALLISTIC has the potential to become the community standard for dense linear algebra and be adopted and/or supported by a large community of users, computing centers, and High-Performance Computing (HPC) vendors. Learning to use numerical libraries is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. BALLISTIC will have a correspondingly large impact on the research and education community, government laboratories, and private industry and support national efforts to build a workforce capable of employing state of the art tools in pursuit of science and engineering discoveries.The goal of BALLISTIC is to create a layered package of software components that is capable of running at every level of the platform deployment pyramid and achieves three complementary objectives: (1) deliver seamless access to the most up-to-date algorithms, numerics, and performance via familiar Sca/LAPACK interfaces, wherever possible; (2) make advanced algorithms, numerics, and performance capabilities available through new interface extensions, wherever necessary; and (3) provide a well-engineered conduit through which new discoveries at the frontiers of research in these areas can be channeled as quickly as possible to all the applications from science and engineering communities that depend on high-performance linear algebra libraries.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Basic ALgebra LIbraries for Sustainable Technology with Interdisciplinary Collaboration (BALLISTIC),OAC,2004850,Julien Langou,julien.langou@ucdenver.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","Scientific software libraries have long provided a large and growing resource for high-quality, reusable software components upon which applications from science and engineering can be rapidly constructed ? with improved robustness, portability, and sustainability. For this, a team of researchers from four collaborating organizations proposed to develop BALLISTIC (Basic ALgebra Libraries for Sustainable Technology with Interdisciplinary Collaboration). The BALLISTIC project, through the leading-edge research it channels into its software deliverables, will lead to the introduction of tools that will simplify the transition to the next generation of extreme-scale computer architectures. The main impact of the project will be to develop, push, and deploy software into the scientific community to make it competitive on a world-wide scale and to contribute to standardization efforts in the area. BALLISTIC has the potential to become the community standard for dense linear algebra and be adopted and/or supported by a large community of users, computing centers, and High-Performance Computing (HPC) vendors. Learning to use numerical libraries is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. BALLISTIC will have a correspondingly large impact on the research and education community, government laboratories, and private industry and support national efforts to build a workforce capable of employing state of the art tools in pursuit of science and engineering discoveries.The goal of BALLISTIC is to create a layered package of software components that is capable of running at every level of the platform deployment pyramid and achieves three complementary objectives: (1) deliver seamless access to the most up-to-date algorithms, numerics, and performance via familiar Sca/LAPACK interfaces, wherever possible; (2) make advanced algorithms, numerics, and performance capabilities available through new interface extensions, wherever necessary; and (3) provide a well-engineered conduit through which new discoveries at the frontiers of research in these areas can be channeled as quickly as possible to all the applications from science and engineering communities that depend on high-performance linear algebra libraries.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Cyberinfrastructure Service for IoT-Based Construction Research and Applications,OAC,2004544,Aaron Costin,aaron.costin@ufl.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Wireless infrastructure is steadily evolving into wireless access for all humans and most devices, from 5G to Internet-of-Things. This widespread access creates the expectation of custom and adaptive services from the personal network to the backbone network. In addition, challenges of scale and interoperability exist across networks, applications and services, both requiring an effective wireless network management infrastructure. At the same time, there has been a rising imperative to capitalize on the current technological advancements to address to the most pressing issues surrounding construction and the built environment to increase health and safety; productivity; and sustainability. This Elements project combines the areas of computer science, electrical engineering, and construction and building to develop a robust cyberinfrastructure (CI) service for construction research, as well as applications that utilize state-of-the-art emerging technologies and software to address the current challenges faced by the construction industry. The major contribution of the project is the development of the IoT-ACRES (IoT-Applied Construction Research and Education Services) system, a central, interoperable framework hub that can incorporate a variety of heterogeneous sensors, technology, software, managed by a software-defined network infrastructure and optimized by machine learning and artificial intelligence techniques. The prototype system will help to augment the works and/or safety manager's ability to detect hazards and subsequently improve safety performance in construction, which is one of the greatest challenges faced by the construction industry. In addition, the framework can be used to increase autonomy in applications that require simultaneous tracking of multiple entities (people, vehicles, equipment, etc.), detecting multiple objects of interests, analyzing real-time biometric data, and making autonomous decisions. Results will be disseminated to industry and research communities through publications and presentations at workshops, training courses and online professional certification programs. The project will also be used as a research, education, and training tool to (1) mentor and teach K-12 students about STEM, and (2) to develop and enhance courses to educate the current and next generations of students, users, and workers, on the latest technology and the latest approaches to cyber security techniques. This project develops a robust cyberinfrastructure (CI) system and service for construction research and applications to address the current challenges faced in the construction industry. The outcomes and services that this proposal aims to provide are 1) a distributed SDN-managed and AI-assisted IoT-based system that can be adapted and extended based on needs of the research and application; 2) identification of the data and data security requirements needed to address the challenges in the construction industry and potential technologies that can provide those data; 3) evaluation of reliable real-time multi-sensor fusion techniques for ruggedness, usability, and limitations of IoT-based components deployed in the dynamic construction environments; 4) robust prototype system for real-time safety monitoring based on the IoT system framework; and 5) recommendations of potential configurations of the system with the appropriate technology and sensors to meet the needs of the application. The empirical data resulting will be delivered through yearly NSF reports on the progress and findings, journal publications of the intellectual merit and scientific findings, and conference proceedings discussing the broader impacts and future research objectives. The framework of the hardware and software, including an instructional manual will also be published. The software will be made available through request via a project website, open source posts, and conference and workshop dissemination. The project will explore the use of various delivery mechanisms, such as NSF's eXtreme Science and Engineering Discovery Environment (XSEDE). The IoT-ACRES will utilize IBM IoT Continuous Engineering and Cloud Computing Servers Cloud (e.g. Amazon AWS) for the data analysis and performance metrics. This novel convergence research project will ultimately advance the development of sustainable CI communities and stewardships of sustainable CI services that can enhance productivity and accelerate innovation in science and engineering. This work will advance practices of safety controls by developing a tool for safety monitoring on construction sites, presented to safety managers with interfaces that visualize, and report real-time safety hazards. Significantly, it will address fundamental research challenges in computer vision and construction management: improving context-based object recognition and tracking; and formalizing rules for integrating visual, textual, biometric data to proactively recognize safety hazards.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: The Einstein Toolkit ecosystem: Enabling fundamental research in the era of multi-messenger astrophysics,OAC,2004157,Steven Brandt,sbrandt@cct.lsu.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","A team of experts from five institutions (University of Illinois Urbana-Champaign, Georgia Institute of Technology, Rochester Institute of Technology, Louisiana State University, and West Virginia University) are collaborating on further development of the Einstein Toolkit, a community-driven, open-source cyberinfrastructure ecosystem providing computational tools supporting research in computational astrophysics, gravitational physics, and fundamental science. The new tools address current and future challenges in gravitational wave source modeling, improve the scalability of the code base, and support an expanded science and user community around the Einstein Toolkit.The Einstein Toolkit is a community-driven suite of research-grade Python codes for performing astrophysics and gravitational wave calculations. The code is open-source, accessible via Conda (an open source package management system) and represents a long-term investment by NSF in providing such computational infrastructure. The software is designed to simulate compact binary stars as sources of gravitational waves. This project focuses on the sustainability of the Einstein Toolkit; specific research efforts center around the development of three new software capabilities for the toolkit: ? CarpetX -- a new mesh refinement driver and interface between AMReX, a software framework containing the functionality to write massively parallel block-structured adaptive mesh refinement (AMR) code, and Cactus, a framework for building a variety of computing applications in science and engineering;? NRPy+ -- a user-friendly code generator based on Python; and ? Canuda -- a new physics library to probe fundamental physics. Integration of graphics processing units (GPUs) will incorporate modern heterogeneous computing devices into the system and will enhance the capability of the toolkit. The end product is sustainable through integration into the Einstein Toolkit, yet also includes an active community maintaining and enhancing the foundational components. Broader impacts are enhanced through training, documentation and a support infrastructure that reduces the barrier to adoption by the community. The team is also creating a science portal with additional educational and showcase resources. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the National Science Foundation's Big Idea activities in Windows on the Universe (WoU).This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Development and Dissemination of a Slurm Simulator,OAC,2004954,Nikolay Simakov,nikolays@buffalo.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","Slurm is an open source resource management and job scheduling system that is widely used on small and large high-performance computing (HPC) systems. Slurm is highly tuneable, with many settings that can significantly influence job throughput, overall system utilization and job wait times. Unfortunately, in many cases it is difficult to judge how modification of these settings will affect the overall performance of the HPC resource. This project develops a prototype version of a Slurm simulator that allows HPC personnel to tune Slurm parameters, to optimize throughput or meet specific workload objectives without impacting an HPC system in production. The proposed Slurm simulator could have impacts far beyond computer science and the field of scheduling. The ability to optimize scheduler parameters on production HPC resources, most of which are significantly oversubscribed, has the potential to dramatically improve job throughput for researchers and reduce the ?time to science?. The simulator would allow many different job scheduling schemes to be rapidly evaluated, and implemented if they are useful or rejected if they are not. Furthermore, the ability to model the impact of various features that impact job execution time (such as network traffic, parallel file system loads and node sharing) can be explored to determine if they merit additional scheduler development work. The developed framework would allow researchers in other science and engineering fields to incorporate their models, and study a range of problems affecting the performance and execution of users' jobs on HPC systems.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Software: Future Proofing the Finite Element Library Deal.II -- Development and Community Building,OAC,2015848,Timo Heister,heister@clemson.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","Partial differential equations (PDEs) are used as mathematical models throughout the natural sciences, engineering, and more recently also in the biomedical and social sciences as well as in finance. Their numerical solution is, consequently, of great relevance in understanding, accurately simulating, and optimizing natural, human, and engineered systems. In many applications, finite element methods (FEM) are the method of choice converting the PDE into finite dimensional, computationally solvable problems. The deal.II project is an open source FEM software library that enables scientists to solve PDEs across many disciplines, that supports simulation and computational discovery in virtually all parts of the sciences and engineering by providing tools to solve essentially all PDEs amenable to the FEM. In this project new capabilities will be added and the user and contributor community expanded to include additional science domains.Deal.II is a project with a thriving, world-wide user and developer community. This project will further enable its community of users and developers, by undertaking specifically for work that can either not be expected of volunteers, or that is necessary to strengthen the long-term independent sustainability of the project. Based on a recent user survey, the following work items in the following four categories will be addressed: 1. Foundational features too large or complicated to be tackled by volunteers: the team will research and implement efficient and scalable approaches to support parallel, adaptive multigrid and hp FEM. 2. Expand documentation and training modules through more tutorial programs and YouTube-hosted video lectures: This will further broaden the reach of the project and extend the education for the computational science community. 3. Continuous integration and packaging infrastructure to better support the pace of development. 4. Support and expand deal.II's thriving communities through a summer school, workshops, hackathons, and careful mentoring of newcomers.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Proposal: Frameworks: Project Tapis: Next Generation Software for Distributed Research,OAC,1931575,Gwen Jacobs,gwenj@hawaii.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","The goal of a robust cyberinfrastructure (CI) ecosystem is to become a catalyst for discovery and innovation by fostering the development of software frameworks as sustainable production-quality services. Modern science and engineering research increasingly span multiple, geographically distributed data centers and leverage instruments, experimental facilities and a network of national and regional CI. The Tapis framework will enable scientists to accomplish computational and data intensive research in a secure, scalable, and reproducible way allowing scientists to focus on their research instead of the technology needed to accomplish it. Tapis will allow easier implementation, sharing and re-use of complex computational applications, workflows, and infrastructure and enable analysis previously too challenging for researchers. The framework will maximize application portability, allowing flexible scheduling of geographically distributed computational workloads, offer a web-based science-as-a-service to enable multi-facility, decentralized deployments, and provide production-grade support for sensors and streaming data. Tapis will impact multiple science domains, geographic and underrepresented communities with the potential to tackle the world's most important scientific problems spanning astronomy, climate science, medicine, natural hazards, and sustainability science. Education and outreach will include sponsored workshops, hackathons and training materials covering the platform and providing examples to encourage widespread adoption for users across a variety of technical skills and targeting the next generation of young researchers and professionals through immersive workshops and professional development opportunities.Tapis, will be a new platform for distributed computational experiments that leverages NSF's investments in the Agave, Abaco and CHORDS projects. The Tapis software framework will 1) provide production-grade support for sensors and streaming data, 2) maximize application portability, allowing flexible scheduling of computational workloads across geographically distributed providers, and 3) provide science-as-a-service HTTP-based RESTful APIs to enable multi-facility, decentralized deployments that are both secure and scalable. Working alongside a diverse set of domain researchers to drive real-world use cases, Tapis will be the underlying cyberinfrastructure for computational workflows and science gateways. Tapis will leverage containers to maximize application portability, allowing flexible scheduling of computational workloads across geographically distributed providers. The project will achieve this flexibility by introducing execution system capabilities and application requirements throughout the framework. The Jobs service will be run in a distributed manner to take advantage of data locality and, optionally, to schedule jobs on underutilized systems. Tapis will deploy in centralized or distributed configurations using a microservices architecture that includes a novel, decentralized security and authorization kernel. This kernel can be deployed on-premises to retain local control over confidential data. Custom microservices can be plugged into the security kernel to provide new capabilities, resulting in a cyberinfrastructure ecosystem for distributed computing. To effectively execute Tapis, teams from the Texas Advanced Computing Center (TACC), the University of Texas at Austin (UT), and the University of Hawaii (UH) will leverage a long-standing collaboration to support investigator-driven, geographically distributed, data-intensive research.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Software: Future Proofing the Finite Element Library Deal.II -- Development and Community Building,OAC,1835452,Timo Heister,heister@clemson.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","Partial differential equations (PDEs) are used as mathematical models throughout the natural sciences, engineering, and more recently also in the biomedical and social sciences as well as in finance. Their numerical solution is, consequently, of great relevance in understanding, accurately simulating, and optimizing natural, human, and engineered systems. In many applications, finite element methods (FEM) are the method of choice converting the PDE into finite dimensional, computationally solvable problems. The deal.II project is an open source FEM software library that enables scientists to solve PDEs across many disciplines, that supports simulation and computational discovery in virtually all parts of the sciences and engineering by providing tools to solve essentially all PDEs amenable to the FEM. In this project new capabilities will be added and the user and contributor community expanded to include additional science domains.Deal.II is a project with a thriving, world-wide user and developer community. This project will further enable its community of users and developers, by undertaking specifically for work that can either not be expected of volunteers, or that is necessary to strengthen the long-term independent sustainability of the project. Based on a recent user survey, the following work items in the following four categories will be addressed: 1. Foundational features too large or complicated to be tackled by volunteers: the team will research and implement efficient and scalable approaches to support parallel, adaptive multigrid and hp FEM. 2. Expand documentation and training modules through more tutorial programs and YouTube-hosted video lectures: This will further broaden the reach of the project and extend the education for the computational science community. 3. Continuous integration and packaging infrastructure to better support the pace of development. 4. Support and expand deal.II's thriving communities through a summer school, workshops, hackathons, and careful mentoring of newcomers.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Software: NSCI: Constitutive Relation Inference Toolkit (CRIKit),OAC,1835792,Tobin Isaac,tisaac@cc.gatech.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","Constitutive relations are mathematical models that describe the way materials respond to local stimuli such as stress or temperature change, and are essential to the study of biological tissues in biomechanics, ice and rock in geosciences, plasmas in high-energy physics and many other science and engineering applications. This project seeks to infer constitutive relations from practical observations without requiring isolation of the material in conventional laboratory experiments, which are often expensive and difficult to apply to volatile materials such as liquid foams or materials such as sea ice that exhibit homogenized behavior only at large scales. The investigators and their students will develop underlying algorithms and the Constitutive Relation Inference Toolkit (CRIKit), a new community software package to leverage recent progress in machine learning and physically-based modeling to infer constitutive relations from noisy, indirect observations, and disseminate the results as citable research products for use in a range of open source and extensible commercial simulation environments. This development will create new opportunities and increase accessibility at the confluence of data science and high-fidelity physical modeling, which the investigators will highlight through community outreach and educational activities.The CRIKit software will integrate parallel partial differential equation (PDE) solvers like FEniCS/dolfin-adjoint with machine learning (ML) packages like TensorFlow to infer constitutive relations from noisy indirect or in-situ observations of material responses. The forward simulation is post-processed to create synthetic observations which are compared to real observations by way of a loss function, which may range from simple least squares to advanced techniques such as ML-based image analysis. This approach results in a nonlinear regression problem for the constitutive relation (formulated to satisfy invariants and free energy compatibility requirements) and relies on well-behaved and efficiently computable gradients provided by PDE solvers using compatible discretizations with adjoint capability. The inference problem exposes parallelism within each forward model and across different experimental realizations and facilitates research in optimization. The research enables constitutive models to be readily updated with new experimental data as well as reproducibility and validation studies. CRIKit's models will improve simulation capability for scientists and engineers by providing ready access to the cutting edge of constitutive modeling.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
CIF21 DIBBs: PD: Cyberinfrastructure Tools for Precision Agriculture in the 21st Century,OAC,1854312,Michela Taufer,taufer@utk.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","This interdisciplinary project applies computer science approaches and computational resources to large multidimensional environmental datasets, and synthesizes this information into finer resolution, spatially explicit products that can be systematically analyzed with other variables. The main emphasis is ecoinformatics, a branch of informatics that analyzes ecological and environmental science variables such as information on landscapes, soils, climate, organisms, and ecosystems. The project focuses on synthesis/computational approaches for producing high-resolution soil moisture datasets, and the pilot application is precision agriculture. The effort combines analytical geospatial approaches, machine learning methods, and high performance computing (HPC) techniques to build cyberinfrastructure tools that can transform how ecoinformatics data is analyzed.The investigators build upon publicly available data collections (soil moisture datasets, soil properties datasets, and topography datasets) to develop: (1) tools based on machine-learning techniques to downscale coarse-grained data to fine-grained datasets of soil moisture information; (2) tools based on HPC techniques to estimate the degree of confidence and the probabilities associated with the temporal intervals within which soil-moisture-base changes, trends, and patterns occur; and (3) data- and user- interfaces integrating data preprocessing to deal with data heterogeneity and inaccuracy, containerized environments to assure portability, and modeling techniques to represent temporal and spatial patterns of soil moisture dynamics. The tools will inform precision agriculture through the generation and use of unique information on soil moisture for the coterminous United States. Accessibility for field practitioners (e.g., local soil moisture information) is made possible through lightweight virtualization, mobile devices, and web applications. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Earth Sciences within the NSF Directorate for Geosciences."
Elements: Kingfisher: Storage Management for Data Federations,OAC,2209645,Brian Bockelman,bbockelman@morgridge.org,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","A cornerstone of translating the raw capacity of a distributed system into an effective source of shared computing power is the methodical management of all the resources. Historically, previous work to manage computations on shared systems across multiple users and workloads has focused on processing resources: CPUs, GPUs, and RAM (main memory). As the amount of research data that needs to be produced and processed is ever increasing, the Kingfisher framework addresses an important missing link in the chain of distributed capacity management: data storage space. Kingfisher provides abstractions that offer researchers and system administrators semantics for managing the sharing of storage spaces across competing workloads. Once storage can be effectively shared, providers across the nation can allocate storage to the commons through entities such as the OSG Consortium?s Open Science Data Federation, enabling researchers to harness the power of distributed computing in support of data intensive applications.This project focuses on the development and integration of the first element of the Kingfisher framework - the LotManager library. To advance the national CI ecosystem, the storage space management capabilities of the LotManager are integrated with an array of distributed software tools. These tools include the widely adopted HTCondor Software Suite and the XRootD software framework which are used by the OSG Consortium to provide data delivery to the nation?s Science and Engineering (S&E) community. The LotManager is designed to facilitate the sharing of a finite amount of storage capacity across users and projects. Sharing of computing capacity, across administrative and physical boundaries, has been proven to advance scientific discovery via Distributed High Throughput Computing (dHTC) at the national and international scales. Such work indicates the value that effective federation of storage capacity can bring to the NSF S&E community. The storage management abstractions and capabilities embodied by LotManager are developed in the context of, and with the feedback from, the OSG's production compute and storage services with real-life experimental data. Consequently, these abstractions and principles result in translational computer science research generally applicable to multi-tenant storage systems beyond the specific system developed in this project.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Frameworks: Collaborative Research: Extensible and Community-Driven Thermodynamics, Transport, and Chemical Kinetics Modeling with Cantera: Expanding to Diverse Scientific Domains",OAC,1931539,Bryan Weber,bryan.weber@uconn.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Modeling and simulation play key enabling roles in aiding and accelerating discovery connecting to energy and chemical research. In applications such as energy storage and conversion, atmospheric chemistry, and catalytic chemical processing, modeling and simulation software helps facilitate technological advances. However, in recent years the available software has not kept pace with the increasing chemical complexity and interdisciplinarity of advanced technology solutions. This project addresses this gap by developing and promoting new state-of-the-art modeling capabilities for diverse scientific fields in the existing Cantera software platform. Cantera is an extensible, open-source framework that enables researchers to study basic science and support new technology development and enables teachers to demonstrate concepts and applications in their classrooms. This project extends Cantera to provide new cross-disciplinary research capabilities and provides a foundation for further community-driven improvements to the Cantera framework. Simultaneous development of the open-source platform and outreach to new user communities will facilitate both fundamental scientific insight and practical technology design and analysis, train the next generation of researchers in both software-development best practices and scientific knowledge, and generate reusable and open educational materials. In addition to work on the framework development, the project includes training of graduate students as well as education, outreach and scientific community engagement activities.This work will develop the Cantera software platform in service of three objectives: (i) extend Cantera?s scientific capabilities to support the development of transformative technologies; (ii) expand Cantera?s user base in fields including electrochemistry, heterogeneous catalysis, and atmospheric chemistry; and (iii) broaden participation in the software?s development and management to improve Cantera?s sustainability and usability. These will be achieved by developing new scientific modeling capabilities, conducting outreach to new user communities, and improving Cantera?s architecture and software development practices. The new scientific modeling capabilities will focus on four content areas: thermodynamics, chemical kinetics, transport, and multi-phase capabilities. Outreach activities, including publications and presentations, conference workshops, and domain-specific software toolkits with examples to demonstrate Cantera operation and functionality, will engage new and existing communities. The project will establish a scientific advisory board, consisting of experts from diverse fields and backgrounds who will help guide software development and outreach. Finally, the architectural and software engineering changes in this work will improve extensibility and interoperability and implement advanced numerical algorithms to enable the application of Cantera to new types of problems. These changes will also make it easier for users to contribute to Cantera, ensure software correctness, and provide new ways of accessing Cantera?s functionality. The resulting software framework will aid in scientific discovery and development of key enabling technologies with broad societal impacts. These impacts include next-generation batteries and fuel cells for clean energy storage and conversion, catalytic and membrane reactors for electrolysis, novel fuel generation, chemical processing, environmentally conscious combustion applications, and understanding and addressing anthropogenic challenges in atmospheric chemistry.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: SHF: Medium: Practical and Rigorous Correctness Checking and Correctness Preservation for Irregular Parallel Programs,CCF,1956106,Ganesh Gopalakrishnan,ganesh@cs.utah.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Many important ? and in some cases, lifesaving ? computations are performed on graph structures consisting of millions of vertices and edges. For example, such graphs might represent medical information, protein interactions, or taxonomies of diseases. Since these graphs tend to be large, they are processed in parallel to fully harness the speed offered by modern computers, which use multicore processors and often general-purpose Graphics Processing Units (GPUs). Unfortunately, parallelizing graph computations is difficult, especially for GPUs,and often leads to accidental uncoordinated accesses known as data races. Data races can be hard to track down as they only sometimes corrupt the result. The project's novelties are the development of scalable and mathematically sound methods for data-race and other bug detection on graph computations. The project's main impact is the elimination of many human programming errors to improve the trust in computations carried out on life-critical and other data.The project develops generic symbolic representations of allowed concurrent operations on primitive data operations. This provides theability to easily boil down new concurrency models into this semantic base to quickly create new analysis tools, thus counteracting verification tool obsolescence. It augments the power of small-scope symbolic-analysis methods with execution-based dynamic-analysis methods that scale to realistic code and data sizes. The project derives real-world case studies from high-performance CUDA and OpenMP implementations of important graph algorithms developed over a decade. The project plans to publicly release the new data-race checking tools as well as verification micro-benchmarks and rigorously verified parallel graph codes. It is also training students whose education is advanced by teaching them modern program analysis methods.This award is co-funded by the Software & Hardware Foundations Program in the Division of Computer & Computing Foundations, and the NSF Office of Advanced Cyberinfrastructure.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF),OAC,1835067,Joern Ilja Siepmann,siepmann@umn.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research:  Elements: Data: HDR: Developing On-Demand Service Module for Mining Geophysical Properties of Sea Ice from High Spatial Resolution Imagery,OAC,1835512,Xin Miao,XinMiao@missouristate.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Sea ice acts as both an indicator and an amplifier of climate change. At present, there are multiple sources of sea ice observations which are obtained from a variety of networks of sensors (in situ, airborne, and space-borne). By developing a smart cyberinfrastructure element for the analysis of high spatial resolution (HSR) remote sensing images over sea ice, the science community is better able to extract important geophysical parameters for climate modeling. The project contributes new domain knowledge to the sea ice community. This is accomplished by integrating HSR images that are spatiotemporally discrete to produce a more rapid and reliable identification of ice types, and by a standardized image processing that allows creating compatible sea ice products. The cyberinfrastructure module is a value-added on-demand web service that can be naturally integrated with existing infrastructure.The key objective is to develop a reliable and efficient on-demand Open Geospatial Consortium-compliant web service, which is capable of extracting accurate geographic knowledge of water, submerged ice, bare ice, melt ponds, deformed 'ridging' ice, ridge shadows, and other information from HSR images with limited human intervention. The embedded spatial-temporal analysis framework provides functions to search, explore, visualize, organize, and analyze the discrete HSR images and other related remote sensing data and field data. The project creates a data and knowledge web service for the Arctic sea ice community by integrating computer vision and machine learning algorithms, computing resources, and HSR image data and other useful datasets. The conceptual model improves data flow, so users would query data, download value-added data, and have more consistent results across various sources of information. This creates new opportunities for scientific analysis that minimizes the investment of time in processing complex and spatiotemporally-discrete HSR imagery. The project includes a strong emphasis on teaching and development of the next-generation workforce through course curricula development, involvement of graduate and undergraduate students in research, and the offering of summer workshops for K-12 teachers (funded by other agencies). The collected images and results of the image analyses will be shared with the public in a timely manner through the NSF Arctic Data Center.This award by the Office of Advanced Cyberinfrastructure is jointly supported by EarthCube and the Office of the Polar Programs Arctic Natural Sciences Program, within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: OpenEarthscape - Transformative Cyberinfrastructure for Modeling and Simulation in the Earth-Surface Science Communities,OAC,2103918,David Gochis,gochis@ucar.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","The landscape around us changes constantly. Sometimes change is slow: a river bend migrates, soil erodes from a field, a waterfall carves its way upstream. Sometimes change happens fast: a landslide, a flood, a winter storm eroding beaches. To understand, manage, and forecast such processes, scientists rely on computer simulation models. This project develops software tools to make simulation modeling easier, more accessible, and more efficient. Among the products is a website through which researchers and students alike can learn about and experiment with a variety of environmental simulation models, without needing to install anything on their own computers. This web portal takes advantage of a technology that combines text, pictures, and computer code in a single online document. The project also develops improved computer-programming libraries designed to make it easier and more efficient for researchers to create new simulation models. The project contributes computing-skills training for college students enrolled in Colorado-based summer programs that serve traditionally underrepresented student populations. The project also promotes public education in geology, by creating an online animated simulation illustrating how landscapes evolve in response to various geologic events.As the sciences that probe Earth's changing surface become more quantitative and prediction-oriented, they increasingly rely on computational modeling and model-data integration. This project develops OpenEarthscape: an integrated suite of community-developed cyber resources for simulation and model-data integration, focusing on nine high-priority geoscience frontiers. Products and activities include EarthscapeHub: a JupyterHub server providing easy access to models, tools, and libraries; new capacity for creating and sharing reproducible analyses; and major enhancements to current programming libraries for model construction and coupling. OpenEarthscape catalyzes efficiency by building new technology to improve performance and developing an extended version of the Basic Model Interface API standard to address parallel architecture and coupling. OpenEarthscape fosters research productivity with improved library capabilities for data I/O and visualization, and with community resources for efficient software distribution and cross-platform compatibility. Broader impacts include partnership with undergraduate research programs that support traditionally underrepresented student populations, with the project team contributing introductory training in scientific computing. A novel educational element is the OpenEarthscape Simulator: a web-hosted visual simulation of a micro-continent evolving in response to various geologic events. The simulator provides students and the general public with an intriguing visualization of Earthscape dynamics and provides a template for the research community to identify defects in our current understanding.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Elements: Basil: A Tool for Semi-Automatic Containerization, Deployment, and Execution of Scientific Applications on Cloud Computing and Supercomputing Platforms",OAC,2314203,Ritu Ritu,ritu@wayne.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","The ""containerization"" of software applications future-proofs them, helps in their long-term preservation, makes them portable across different hardware platforms, ensures reproducible results, and makes them convenient to disseminate. Docker and Singularity are two popular software technologies for containerizing scientific applications and are widely supported on different hardware platforms. However, their adoption involves a steep learning curve, especially when it comes to developing secure and optimized images of the applications of interest. A large number of domain-scientists and scholars are usually not formally trained at containerizing their applications with Docker and Singularity, and spend a significant amount of their time in porting their applications to different cloud computing and supercomputing platforms. The process of porting applications having multiple software dependencies and sensitivities to specific software versions can be especially arduous for such users. To assist them, this project is developing BASIL - a tool for semi-automatically containerizing the scientific applications, frameworks, and workflows. This project will deliver BASIL through a web portal, as a command-line tool, and through APIs. BASIL has a broad applicability across multiple domains of deep societal impact such as artificial intelligence, drug discovery, and earthquake engineering. By enabling the preservation of valuable legacy software and making them usable for several years in future, BASIL will save cost and time in software rewriting and software installations, and thus contribute towards advancing the prosperity of the society. The project will result in educational content on ?Introduction to Containerization? and students engaged in the project will develop valuable skills in the areas of national interest such as supercomputing/High Performance Computing (HPC) and cloud computing. BASIL will be the first tool of its kind that can semi-automatically generate secure, optimized, and trustworthy container images with clear information on how to use the images under appropriate licenses. The rules for optimizing the images will be derived from expert knowledge and best practices, such as multi-stage builds and reordering the sequencing of commands to take advantage of caching so that the overall time involved in building the images is reduced. Users of the BASIL tool will provide the recipes for building their applications/workflows in one of the following forms (1) Makefiles/CMakefiles, (2) scripts, (3) commands, or (4) a text-file with predefined keywords and notations using templates provided by the project team. These recipes will be parsed, and Dockerfiles or Singularity definition files will be generated. The parser developed in this project will be another novel contribution of the project. Using a generated Dockerfile or Singularity definition file, a Docker or Singularity image will be built. Next, the image will be scanned for any vulnerabilities, signed, and if the user desires, released in public registries with appropriate licenses. These container images can be tested using the BASIL web portal, and can be pulled to run or deploy on diverse hardware platforms. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Physics at the Information Frontier in the Division of Physics within the Directorate for Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Software: NSCI: A high performance suite of SVD related solvers for machine learning,OAC,1835821,Andreas Stathopoulos,andreas@cs.wm.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","The accrual of vast amounts of data is one of the defining characteristics of our century. With the help of computers, scientists use this data to make and test hypotheses, draw inferences, predict complex phenomena, and make educated policy decisions. Machine learning (ML) is an area in computer science that uses statistical methods to allow computers to ""learn"" from data, with and without human supervision. Central to the application of machine learning methods is the numerical computation of the Singular Value Decomposition (SVD) of matrices of very large dimension, often larger than a million or even a billion. Since ""off-the-shelf"" algorithms and SVD software, however, cannot handle matrices of very large dimension, iterative methods used in scientific computing are more appropriate. Yet their stringent approximation quality requirements are often excessive for downstream applications, and result in slow execution times. Recently, methods based on randomization have improved execution times, but their implementations relax the approximation quality, often to detrimental levels. This project proposes to develop a software package that unifies randomized and iterative methods with a particular focus on the specific requirements of various ML applications and with high performance optimizations for modern computing platforms. This will allow scientists to analyze significantly larger datasets, ML researchers to study large models that could not be tackled before, and ML service providers to use the new solvers to reduce their operational cost. This project proposes to develop a software package that unifies randomized and iterative methods with a particular focus on the specific requirements of various ML applications and with high performance optimizations for modern computing platforms. This will allow scientists to analyze significantly larger datasets, ML researchers to study large models that could not be tackled before, and ML service providers to use the new solvers to reduce their operational cost. Specifically, the software package builds upon the state-of-the-art eigenvalue/singular value software package PRIMME that integrates cutting-edge iterative methods and high-performance implementations. The development of the package consists of two thrusts: (T1) Unifying state-of-the-art algorithmic techniques including randomized, streaming, and iterative methods, to deliver consistent experience for a diverse range of matrices with different quality requirements, hardware platforms and precisions, and programming environments. (T2) Developing software devices that enable downstream systems and SVD solvers to interoperate so that users can tune and customize solvers without being experts in numeric linear algebra.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Data: HDR: Collaborative Research: Developing an On-Demand Service Module for Mining Geophysical Properties of Sea Ice from High Spatial Resolution Imagery,OAC,1835784,Hongjie Xie,hongjie.xie@utsa.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Sea ice acts as both an indicator and an amplifier of climate change. At present, there are multiple sources of sea ice observations which are obtained from a variety of networks of sensors (in situ, airborne, and space-borne). By developing a smart cyberinfrastructure element for the analysis of high spatial resolution (HSR) remote sensing images over sea ice, the science community is better able to extract important geophysical parameters for climate modeling. The project contributes new domain knowledge to the sea ice community. This is accomplished by integrating HSR images that are spatiotemporally discrete to produce a more rapid and reliable identification of ice types, and by a standardized image processing that allows creating compatible sea ice products. The cyberinfrastructure module is a value-added on-demand web service that can be naturally integrated with existing infrastructure.The key objective is to develop a reliable and efficient on-demand Open Geospatial Consortium-compliant web service, which is capable of extracting accurate geographic knowledge of water, submerged ice, bare ice, melt ponds, deformed 'ridging' ice, ridge shadows, and other information from HSR images with limited human intervention. The embedded spatial-temporal analysis framework provides functions to search, explore, visualize, organize, and analyze the discrete HSR images and other related remote sensing data and field data. The project creates a data and knowledge web service for the Arctic sea ice community by integrating computer vision and machine learning algorithms, computing resources, and HSR image data and other useful datasets. The conceptual model improves data flow, so users would query data, download value-added data, and have more consistent results across various sources of information. This creates new opportunities for scientific analysis that minimizes the investment of time in processing complex and spatiotemporally-discrete HSR imagery. The project includes a strong emphasis on teaching and development of the next-generation workforce through course curricula development, involvement of graduate and undergraduate students in research, and the offering of summer workshops for K-12 teachers (funded by other agencies). The collected images and results of the image analyses will be shared with the public in a timely manner through the NSF Arctic Data Center.This award by the Office of Advanced Cyberinfrastructure is jointly supported by EarthCube and the Office of the Polar Programs Arctic Natural Sciences Program, within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
SI2-SSE: C11Tester: Scaling Testing of C/C++11 Atomics to Real-World Systems,OAC,1740210,Brian Demsky,bdemsky@uci.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","We have long relied on increased raw computing power to drive technological progress. However, processors are now reaching their limits in terms of raw computing power, and continuing progress will require increased productivity in developing parallel software. Fully leveraging the performance of multi-core processors will in many cases require developers to make use of low-level ""atomic"" (or indivisible) operations such as those provided by the C11 and C++11 languages, so that can make very fine-grained optimizations to their code, and take full advantage of the computing power these processors offer them. Unfortunately, using C/C++ atomics is extremely difficult to do correctly and it is very easy to introduce subtle bugs in the use of these constructs. Testing for concurrency bugs in code that uses C/C++11 atomics can be extremely difficult as a bug can depend on the schedule, the state of the processor's memory subsystem, the specific processor, and the compiler. The C11Tester project will develop tools for testing concurrent code that makes use of C/C++11 atomics and make these tools available to both researchers and practitioners.The C/C++11 standard introduced a relaxed memory model with atomic operations into the C and C++ languages. While C/C++11 atomics can provide significant performance benefits, using C/C++11 atomics correctly is extremely difficult. Existing tools such as CDSChecker can only find bugs in small unit tests of concurrent data structures. Bugs can also arise due to the interaction of subtle memory model semantics and the composition of software components. The C11Tester project will develop new techniques for testing and debugging complete concurrent applications that make use of C/C++11 atomics. The C11Tester project will make the following contributions: (1) it will develop new approaches for testing the correctness of concurrent applications, (2) it will develop new approaches for debugging concurrent applications, and (3) it will develop and make available a robust implementation of the approach in the C11Tester tool."
Collaborative Research: Framework: Data: NSCI: HDR: GeoSCIFramework: Scalable Real-Time Streaming Analytics and Machine Learning for Geoscience and Hazards Research,OAC,2219975,Ivan Rodero,ivan.rodero@utah.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","This project develops a real-time processing system capable of handling a large mix of sensor observations. The focus of this system is automation of the detection of natural hazard events using machine learning, as the events are occurring. A four-organization collaboration (UNAVCO, University of Colorado, University of Oregon, and Rutgers University) develops a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research. This work will support rapid analysis and understanding of data associated with hazardous events (earthquakes, volcanic eruptions, tsunamis). This project uses a collaboration between computer scientists and geoscientists to develop a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research. It focuses on the aggregation and integration of a large number of data streams into a coherent system that supports analysis of the data streams in real-time. The framework will offer machine-learning-based tools designed to detect signals of events, such as earthquakes and tsunamis, that might only be detectable when looking at a broad selection of observational inputs. The architecture sets up a fast data pipeline by combining a group of open source components that make big data applications viable and easier to develop. Data sources for the project draw primarily upon the 1500+ sensors from the EarthScope networks currently managed by UNAVCO and the Incorporated Research Institutions for Seismology (IRIS), as well as the Ocean Observatories Initiative (OOI) cabled array data managed by Rutgers University. Machine learning (ML) algorithms will be researched and applied to the tsunami and earthquake use cases. Initially, the project plans to employ an advanced convolutional neural network method in a multi-data environment. The method has only been applied to seismic waveforms, so the project will explore extending the method to a multi-data environment. The approach is expected to be extensible beyond detection and characterization of earthquakes to include the onset of other geophysical signals such as slow-slip events or magmatic intrusion, expanding the potential for new scientific discoveries. The framework is applied to use cases in the Cascadia subduction zone and Yellowstone: these locations combine the expertise of the science team with locations where EarthScope and OOI have the greatest concentration of instruments. The architecture will be transportable and scalable, running in a Docker environment on laptops, local clusters and the cloud. Integral to the project will be development, documentation and training using collaborative online resources such as GitLab and Jupyter Notebooks, and utilizing NSF XSEDE resources to make larger datasets and computational resources more widely available.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program and Division of Earth Sciences within the NSF Directorate for Geosciences, the Big Data Science and Engineering Program within the Directorate for Computer and Information Science and Engineering, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Software: Future Proofing the Finite Element Library Deal.II -- Development and Community Building,OAC,1902308,Timo Heister,heister@clemson.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","Partial differential equations (PDEs) are used as mathematical models throughout the natural sciences, engineering, and more recently also in the biomedical and social sciences as well as in finance. Their numerical solution is, consequently, of great relevance in understanding, accurately simulating, and optimizing natural, human, and engineered systems. In many applications, finite element methods (FEM) are the method of choice converting the PDE into finite dimensional, computationally solvable problems. The deal.II project is an open source FEM software library that enables scientists to solve PDEs across many disciplines, that supports simulation and computational discovery in virtually all parts of the sciences and engineering by providing tools to solve essentially all PDEs amenable to the FEM. In this project new capabilities will be added and the user and contributor community expanded to include additional science domains.Deal.II is a project with a thriving, world-wide user and developer community. This project will further enable its community of users and developers, by undertaking specifically for work that can either not be expected of volunteers, or that is necessary to strengthen the long-term independent sustainability of the project. Based on a recent user survey, the following work items in the following four categories will be addressed: 1. Foundational features too large or complicated to be tackled by volunteers: the team will research and implement efficient and scalable approaches to support parallel, adaptive multigrid and hp FEM. 2. Expand documentation and training modules through more tutorial programs and YouTube-hosted video lectures: This will further broaden the reach of the project and extend the education for the computational science community. 3. Continuous integration and packaging infrastructure to better support the pace of development. 4. Support and expand deal.II's thriving communities through a summer school, workshops, hackathons, and careful mentoring of newcomers.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Frameworks: Developing CyberInfrastructure for Waterborne Antibiotic Resistance Risk Surveillance (CI4-WARS),OAC,2004751,Liqing Zhang,lqzhang@cs.vt.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","CI4-WARS is a cyberinfrastructure (CI) being developed as a key step towards establishing an efficient and integrated network of wastewater treatment plants (WWTPs) incorporating CI for antibiotic resistance (AR) surveillance. AR is the ability of some bacteria to survive antibiotic treatment, a capability encoded by antibiotic resistance genes (ARGs). AR rates are increasing globally, with the US Centers for Disease Control estimating 35,000 related deaths in the US per year in 2019, compared to 23,000 deaths per year in 2013. It is a grand challenge that calls for an interdisciplinary approach to combat its spread. Efficient and effective surveillance is needed to pinpoint where ARGs are spreading among bacteria and to inform ways to stop their spread. Combining next generation DNA sequencing with CI for monitoring patterns in ARG detection WWTPs is a promising and novel way to achieve this. WWTPs aggregate antibiotics excreted by all people in a community undergoing antibiotic therapy, as well as any AR bacteria or ARGs that are present on their skin or eliminated by them. Identifying anomalies in ARG patterns in wastewater could help identify potential outbreaks before they occur, better inform clinical use of antibiotics, and improve treatment practices to prevent release of ARGs to rivers and streams. The objectives of this research are to: (1) Develop and demonstrate the CI4-WARS system using DNA sequencing data and associated metadata collected from a local WWTP; (2) Develop new computational tools to identify ARG occurrence patterns in the DNA sequencing data that indicate the risk of AR spreading and develop and apply new algorithms for identifying anomalies in these indicators that are indicative of emergence of new AR bacteria or outbreaks; and (3) Integrate the developed computational tools into CI4-WARS, establishing it as a one-stop service for surveying, evaluating, communicating, and reporting/alerting indicators of AR risk. Broader impact activities include student and professional training, annual workshops, free online videos, tutorials, and making CI4-WARS freely available on the web to maximize the benefit of CI4-WARS and facilitate adoption by the community.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Frameworks: Collaborative Research: Extensible and Community-Driven Thermodynamics, Transport, and Chemical Kinetics Modeling with Cantera: Expanding to Diverse Scientific Domains",OAC,1931584,Steven Decaluwe,decaluwe@mines.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Modeling and simulation play key enabling roles in aiding and accelerating discovery connecting to energy and chemical research. In applications such as energy storage and conversion, atmospheric chemistry, and catalytic chemical processing, modeling and simulation software helps facilitate technological advances. However, in recent years the available software has not kept pace with the increasing chemical complexity and interdisciplinarity of advanced technology solutions. This project addresses this gap by developing and promoting new state-of-the-art modeling capabilities for diverse scientific fields in the existing Cantera software platform. Cantera is an extensible, open-source framework that enables researchers to study basic science and support new technology development and enables teachers to demonstrate concepts and applications in their classrooms. This project extends Cantera to provide new cross-disciplinary research capabilities and provides a foundation for further community-driven improvements to the Cantera framework. Simultaneous development of the open-source platform and outreach to new user communities will facilitate both fundamental scientific insight and practical technology design and analysis, train the next generation of researchers in both software-development best practices and scientific knowledge, and generate reusable and open educational materials. In addition to work on the framework development, the project includes training of graduate students as well as education, outreach and scientific community engagement activities.This work will develop the Cantera software platform in service of three objectives: (i) extend Cantera?s scientific capabilities to support the development of transformative technologies; (ii) expand Cantera?s user base in fields including electrochemistry, heterogeneous catalysis, and atmospheric chemistry; and (iii) broaden participation in the software?s development and management to improve Cantera?s sustainability and usability. These will be achieved by developing new scientific modeling capabilities, conducting outreach to new user communities, and improving Cantera?s architecture and software development practices. The new scientific modeling capabilities will focus on four content areas: thermodynamics, chemical kinetics, transport, and multi-phase capabilities. Outreach activities, including publications and presentations, conference workshops, and domain-specific software toolkits with examples to demonstrate Cantera operation and functionality, will engage new and existing communities. The project will establish a scientific advisory board, consisting of experts from diverse fields and backgrounds who will help guide software development and outreach. Finally, the architectural and software engineering changes in this work will improve extensibility and interoperability and implement advanced numerical algorithms to enable the application of Cantera to new types of problems. These changes will also make it easier for users to contribute to Cantera, ensure software correctness, and provide new ways of accessing Cantera?s functionality. The resulting software framework will aid in scientific discovery and development of key enabling technologies with broad societal impacts. These impacts include next-generation batteries and fuel cells for clean energy storage and conversion, catalytic and membrane reactors for electrolysis, novel fuel generation, chemical processing, environmentally conscious combustion applications, and understanding and addressing anthropogenic challenges in atmospheric chemistry.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: DeCODER (Democratized Cyberinfrastructure for Open Discovery to Enable Research),OAC,2209865,Christine Kirkpatrick,christine@sdsc.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Scientific research in most scientific fields today involves significant amounts of data as well as software to operate on that data. Furthermore, research increasingly requires data from across a number of sources, perhaps even fields. This is a significant challenge for the scientific community, and better solutions are needed in order to enable new discoveries and accelerate the societal impacts from those discoveries. The approach in this project is to follow in the footsteps of the web and it aims to standardize how scientific data is described, allowing for tools addressing the above challenges, such as search engines for scientific data that not only support discoverability but also facilitate the usage of the data.The DeCODER project will expand and extend the successful EarthCube GeoCODES platform and community to unify data and tool description and re-use across geoscience domains. Building on the NSF CIF21 vision, the EarthCube program was formed to address the technological challenges surrounding data and software within the geosciences. Through extensive interaction with the community this culminated in two key activities around data discovery and reuse. First, the promotion, refinement, and adoption of schema.org to annotate geosciences metadata within distributed repositories so that datasets can be crawled. Second, the promotion and support for the adoption of notebooks to document, share, and reuse software as peer reviewed scholarly objects. A rallying point around these activities was the GeoCODES platform, which allows communities to stand up instances of scientific search engines specific to their domains, while building a community of geoscience data users and developers and, ultimately, reducing the time to science. This project will leverage this effort in the DeCODER platform to enable similar activities and outcomes across scientific communities. This work will continue the endeavor to support the scientific community in the adoption of schema.org and notebooks, facilitating this by providing DeCODER as an open source resource that can be customized by a given scientific community to create lightweight scientific gateways that bring together relevant distributed resources.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
CloudBank: Managed Services to Simplify Cloud Access for Computer Science Research and Education,CNS,1925001,Michael Norman,mlnorman@ucsd.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","The University of California, San Diego's San Diego Supercomputer Center and Information Technology Services Division, the University of Washington's eScience Institute, and the University of California, Berkeley's Division of Data Science will develop and operate CloudBank, a cloud access entity that will help the computer science community access and use public clouds for research and education by delivering a set of managed services designed to simplify access to public clouds. Driven by the profound potential of the public cloud and the associated complexity in using it, CloudBank will serve as an integrated service provider to the research community through a comprehensive set of user-facing and business operations functions. These services will span the spectrum from novice to advanced cloud users, including front line user support, cloud solution consulting, training, and assistance in preparing proposals that include cloud resources. CloudBank will provide innovative financial engineering options that will give researchers more flexible cloud terms tailored for their needs and contribute to the sustainability of CloudBank operations. CloudBank will help NSF by bundling multiple small requests that come directly to NSF into a bulk request to cloud providers, dis-incentivizing more costly direct connections. Through this aggregation and innovative financial contract types, CloudBank will pass along savings to researchers that would otherwise be unavailable to them.CloudBank will provide on-ramp support that reduces researcher cloud adoption pain points such as: managing cost, translating and upgrading research computing environments to an appropriate cloud platform, and learning cloud-based technologies that accelerate and expand research. It will be complemented by a cloud usage monitoring system that gives NSF-funded researchers the ability to easily grant permissions to research group members and students, set spending limits, and recover unused cloud credits. These systems will support multiple cloud vendors, and be accessed via intuitive, easy-to-use user portal that gives users a single point of entry to these functions.The CloudBank project and associated portal software, outreach and training materials, and experience in negotiating and delivering public cloud services will significantly advance the state of the practice and understanding of how to use these resources in computer science research and education. The close collaboration between the CloudBank project, cloud providers, researchers, and students will simultaneously enable new research while providing a unique opportunity to develop and study the operational, technological and business dimensions of fundamentally new model of public/private partnership in the service of the research enterprise.It is a primary objective of CloudBank to broaden the access and impact of cloud computing across the many fields of computer science research and education. The project will reach hundreds of researchers and students through allocated research projects and classes. A far larger group of stakeholders will benefit by CloudBank outreach efforts, such as workshops, publications, the CloudBank Center of Excellence on Cloud-Enabled Research and Education, and the CloudBank Advisory Board. CloudBank offers a long-term vision for service and sustainability that will broaden the impact of public cloud computing across all sciences and help ensure that students entering the workforce and research enterprise will be able to contribute and compete in the global economy.This project is accessible at http://tiny.cc/cloudbankThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Cybershuttle: An end-to-end Cyberinfrastructure Continuum to accelerate Discovery in Science and Engineering,OAC,2209874,Giri Krishnan,gkrishnan@ucsd.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","Science depends critically on accessing scientific and engineering software, data repositories, storage resources, analytical tools, and a wide range of advanced computing resources, all of which must be integrated into a cohesive scientific research environment. The Cybershuttle project is creating a seamless, secure, and highly usable scientific research environment that integrates all of a scientist?s research tools and data, which may be on the scientist?s laptop, a computing cloud, or a university supercomputer. These research environments can further support scientific research by enabling scientists to share their research with collaborators and the broader scientific community, supporting replicability and reuse. The Cybershuttle team integrates biophysicists, neuroscientists, engineers, and computer scientists into a single team pursuing the project goals with a grounding in cutting-edge research problems such as understanding how spike proteins in viruses work, how the brain functions during sleep, and how artificial intelligence techniques can be applied to modeling engineering materials. To meet its ambitious goals, the project is building on over a decade of experience in developing and operating the open-source Apache Airavata software framework for creating science-centric distributed systems. Cybershuttle is providing a system that can be used as a training ground to educate students in concepts of open-source software development and applied distributed systems, fostering a globally competitive workforce who can move easily between academic and non-academic careers. Cybershuttle is creating a new type of user-facing cyberinfrastructure that will enable seamless access to a continuum of CI resources usable for all researchers, increasing their productivity. The core of the Cybershuttle framework is a hybrid distributed system, based on open-source Apache Airavata software. This system integrates locally deployed agent programs with centrally hosted middleware to enable an end-to-end integration of computational science and engineering research on resources that span users? local resources, centralized university computing and data resources, computational clouds, and NSF-funded, national-scale computing centers. Scientists and engineers access this system using scientific user environments designed from the beginning with the best user-centered design practices. Cybershuttle uses a spiral approach for developing, deploying, and increasing usage and usability, beginning with on-team scientists and expanding to larger scientific communities. The project engages the larger community of scientists, cyberinfrastructure experts, and other stakeholders in the creation and advancement of Cybershuttle through a stakeholder advisory board. Cybershuttle's team includes researchers from Indiana University, the University of Illinois at Urbana-Champaign, the University of California San Diego, the San Diego Supercomputer Center, and the Allen Institute.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
S2I2: Impl: The Science Gateways Community Institute (SGCI) for the Democratization and Acceleration of Science,OAC,1547611,Nancy Wilkins-Diehr,wilkinsn@sdsc.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","Science gateways are user-friendly web portals that make advanced computing, data, networking and scientific instrumentation accessible and easily usable by scientists at all levels, including students, thereby revolutionizing how research and education is done in science. For example, scientists are conducting biomedical studies through Galaxy, a science gateway for data intensive biomedical research, as well as engaging citizens in investigating lion density using Snapshot Serengeti, a science gateway for citizen science. By being easily accessible via the Web, science gateways expand and democratize access to supercomputers, telescopes, sensor networks, unique data collections, collaborative spaces that enable the multidisciplinary collaborations needed to solve complex problems, and analysis capabilities. Thus, science gateways expand and broadening participation in science - an important goal of the National Science Foundation (NSF). By increasing participation, science gateways increase the NSF's return on investment in advanced technologies and facilities. The Science Gateways Community Institute (SGCI) will speed the development and application of robust, cost-effective, sustainable gateways to address the needs of scientists and engineers across the sciences. The work of the institute will increase the number as well as the effectiveness and usability of gateways to science and engineering. This will result in broader gateway use and more widespread conduct of science ranging from professionals to citizen scientists, thus, directly amplifying the impact of the SGCI. Further, and very importantly, the Institute's community engagement and exchange activities will, over time, increase the audience for its services, and its partnerships with minority professional organizations will ensure involvement in training and workforce development from underrepresented groups.Science gateways are user-friendly web portals that make advanced computing, data, networking and scientific instrumentation accessible and easily usable by scientists at all levels, including students, thereby revolutionizing how research and education is done in science. Gateways enable scientists to test their assumptions more quickly, providing them more time for deeper thinking about the types of problems that have yet to be solved. In this way, gateways become ""research amplifiers"". They also enable synthetic science - by using modelling and simulation tools powered by high-performance computing - across ecosystems, geographic distances, methodologies, and disciplines. However, and despite the presence of gateways for many years, development of these environments is often done with ad-hoc processes, limiting success, resource efficiency, and long-term impact. Developers of gateways are often unaware that others have solved similar challenges before, and do not know where to turn for advice or expertise. Thus, projects waste money and time re-implementing the more basic functions rather than building the value-added features for their unique audience. Many gateway efforts fail. Some fail early by not understanding how to build communities of users; others fail later by not developing plans for sustainability. The Science Gateways Community Institute (SGCI) has been designed to address the above limitations while providing career paths for gateway developers and for students. The five-component design of the SGCI is the result of several years of studies, including many focus groups and a 5,000-person survey of the research community. Its Incubator component will provide shared expertise in business and sustainability planning, cybersecurity, user interface design, and software engineering practices. The Extended Developer Support component will provide expert developers for up to one year to projects that request assistance as well as demonstrating the potential to achieve impacts on their research communities. The Scientific Software Collaborative component will offer a component-based, open-source, extensible framework for gateway design, integration, and services, including gateway hosting and capabilities for external developers to integrate their own software into Institute offerings. The Community Engagement and Exchange component will provide a forum for communication and shared experiences among gateway developers, within NSF, across federal agencies, and internationally. Finally, with its training programs the Workforce Development component will increase the pipeline of gateway developers, with special emphasis on recruiting underrepresented minorities, and by helping universities form gateway support groups. In short, the work of the institute will increase the number, ease of use, and effective application of gateways to science and engineering, resulting in broader gateway use and more widespread conduct of science ranging from professionals to citizen scientists. The Institute's community engagement and exchange activities over time will increase the audience for its services, and its partnerships with minority professional organizations will ensure involvement in training and workforce development from underrepresented groups."
Collaborative Research: Frameworks: Designing Next-Generation MPI Libraries for Emerging Dense GPU Systems,OAC,1931450,Amitava Majumdar,majumdar@sdsc.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","The extremely high compute and communication capabilities offered by modern Graphics Processing Units (GPUs) and high-performance interconnects have led to the creation of High-Performance Computing (HPC) platforms with multiple GPUs and high-performance interconnects per node. Unfortunately, state-of-the-art production quality implementations of the popular Message Passing Interface (MPI) programming model do not have the appropriate support to deliver the best performance and scalability for applications on such dense GPU systems. These developments in High-End Computing (HEC) technologies and associated middleware issues lead to the following broad challenge: How can existing production quality MPI middleware be enhanced to take advantage of emerging networking technologies to deliver the best possible scale-up and scale-out for HPC and Deep Learning (DL) applications on emerging dense GPU systems? A synergistic and comprehensive research plan, involving computer scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) and computational scientists from the Texas Advanced Computing Center (TACC), and San Diego Supercomputer Center (SDSC) and University of California San Diego (UCSD), is proposed to address the above broad challenges with innovative solutions. The proposed framework will be made available to collaborators and the broader scientific community to understand the impact of the proposed innovations on next-generation HPC and DL frameworks and applications in various science domains. Multiple graduate and undergraduate students will be trained under this project as future scientists and engineers in HPC. The proposed work will enable curriculum advancements via research in pedagogy for key courses in the new Data Science programs at OSU, SDSC and TACC. The established national-scale training and outreach programs at TACC, SDSC and OSC will be used to disseminate the results of this research to XSEDE users. Tutorials and workshops will be organized at PEARC, SC and other conferences to share the research results and experience with the community. The project is aligned with the National Strategic Computing Initiative (NSCI) to advance US leadership in HPC and the recent initiative of the US Government to maintain leadership in Artificial Intelligence (AI.)The proposed innovations include: 1) Designing high-performance and scalable point-to-point, and collective communication operations that fully utilize multiple network adapters and advanced in-network computing features for GPU and CPU buffers within and across nodes; 2) Designing novel datatype processing and unified memory management to improve application performance; 3) Designing CUDA-aware I/O subsystem to accelerate MPI I/O and checkpoint-restart for HPC and DL applications; 4) Designing support for containerized environments to better enable easy deployment of proposed solutions on modern cloud environments; and 5) Carry out integrated development and evaluation to ensure proper integration of proposed designs with the driving applications. The proposed designs will be integrated into the widely-used MVAPICH2 library and made available. The project team members will work closely with internal and external collaborators to facilitate wide deployment and adoption of released software. The proposed solutions will be targeted to enable scale-up and scale-out of the driving science domains (molecular dynamics, lattice QCD, seismology, image classification, and fusion research) on emerging dense GPU platforms. The transformative impact of the proposed development effort is to achieve scalability, performance, and portability out of HPC and DL frameworks and applications to take advantage of emerging dense GPU platforms and hence, leading to significant advancements in science and engineering.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: NSCI : Computational and Data Innovation Implementing a National Community Hydrologic Modeling Framework for Scientific Discovery,OAC,1835855,Ilkay Altintas,altintas@sdsc.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Software: Multidimensional Fast Fourier Transforms on the Path to Exascale,OAC,1835885,Dmitry Pekurovsky,dmitry@sdsc.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","This project will contribute to software available freely to the public, providing solution of an important class of problems in computational science, namely the multidimensional Fast Fourier transforms (FFTs). This software will enable scientists to run certain classes of computer experiments on powerful state-of-the-art supercomputers, known as Exascale machines. A wide range of science fields will benefit from such public software; thus, it will contribute to innumerable discoveries related to fundamental understanding of nature, protecting the environment, efficient energy and transportation, designing new materials and improving medicine. It will also contribute to the computational ecosystem by virtue of being a building block that can be used and reused by other programs. This project also includes an educational effort in training a wide range of supercomputer users, from undergraduate interns to graduate/postdoctoral researchers and faculty. Effort will be made to reach out to underrepresented groups when recruiting the interns.The National Strategic Computing Initiative (NSCI), in particular Strategic Objective 4 ""An Enduring National HPC Ecosystem"", outlines the crucial need for development of foundational algorithms and software for next generation supercomputers, and for easing access to the next-generation compute resources to wide classes of users. Fast Fourier Transforms (FFT) is a ubiquitous tool in scientific simulations, from Computational Fluid Dynamics to plasma physics, astrophysics, ocean modeling, materials research, medical imaging, molecular dynamics and many others. This project will fill the gap in highly efficient software for multidimensional FFTs for use on Exascale platforms. While running full FFT at exa-scale appears prohibitive due to strong interconnect bandwidth dependence, steps must be taken towards this goal due to the importance of the algorithm. Building on the previous work with P3DFFT, an open-source numerical library used by many applications in diverse science fields, this proposal aims to push the envelope in terms of adapting multidimensional FFT to new architectures and aggressively scale its performance, without losing the portability and the practical ease of use. The project will employ both novel and proven state-of-the-art tools, such as auto-tuning, overlap of communication with computation and GPU implementation, which will help reduce the bandwidth bottleneck. In addition, features will be added that will make the software appealing to a wider user base. The software will become an integral part of the National Cyberinfrastructure and will aid in numerous scientific and engineering advances, including in areas such as environmental research, energy, efficient transportation, new drugs and new materials. This project includes training of a new generation of scientific software developers through XSEDE training webinars, conference training workshops and supervision of undergraduate interns. The project plan includes reaching out to underrepresented groups when recruiting undergraduate interns.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Beyond the BLAS: A Framework for Accelerating Computational and Data Science,OAC,2003931,Devin Matthews,damatthews@smu.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Traditional scientific and machine learning high-performance computing software is often cast in terms of a set of fundamental operations, including the linear algebra functionality that underlies many applications. For this reason, research into and development of open-source linear algebra software libraries has been a science infrastructure priority for decades. An emerging trend that has disrupted this field is the recognition that scientific discovery can be made faster and/or more cost efficient by lowering the precision of computations, utilizing non-standard data types, and developing custom computational kernels. The project will leverage insights into how to structure the required software so that the combinatorial explosion in software complexity remains manageable. The outcome of the project will be a modern linear algebra software framework and application-focused libraries that will support future generations of computational applications in academia, at the national labs, and in industry. In addition, the project will impact the training of the next generation of high-performance computing professions and help remove barriers into the field for members of traditionally underrepresented groups.The proposed work will build on previous NSF-sponsored research in order to address the implementation of expanded precision (EP), mixed precision (MP), and mixed domain (MD) algorithms simultaneously in a single software solution. Insights gained from a recent demonstration of MP/MD matrix multiplication will be extended by adding low precision types like float16 and bfloat16 and extended precision types like double-double. The target Basic Linear Algebra Subprograms (BLAS) functionality will be expanded to all level-1, level-2, and level-3 operations which in turn will support new research on how best to exploit MP/MD for LAPACK functionality. The new BLAS-like Library Instantiation Software (BLIS) framework will also be updated to provide the flexibility required to integrate extended dense linear algebra (DLA) operations. This flexible DLA framework will then be used to implement key functionality in computational and data science: tensor contraction and factorization operations important to quantum chemistry (QC) and high-performance primitives for machine learning. As a demonstration, these capabilities will be used to build state-of-the-art QC codes to perform coupled cluster polarization propagator and tensor-factorized coupled cluster calculations with full EP/MP/MD functionality, and the machine learning kernels will be integrated into computer vision and image recognition workflows.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Collaborative Research: Frameworks: Multiphase Fluid-Structure Interaction Software Infrastructure to Enable Applications in Medicine, Biology, and Engineering",OAC,1931516,Boyce Griffith,boyceg@email.unc.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Physical systems in which fluid flows interact with immersed structures are found in a wide range of areas of science and engineering. Such fluid-structure interactions are ubiquitous in biological systems, including blood flow in the heart, the ingestion of food, and mucus transport in the lung. Fluid-structure interaction is also a crucial aspect of new approaches to energy harvesting, such as wave-energy converters that extract energy from the motion of sea or ocean waves, and in advanced approaches to manufacturing, such as 3D printing. This award supports the development of an advanced computer simulation infrastructure for modeling this full range of application areas. Computer models advanced by this project could ultimately lead to improved diagnostics and treatments for human disease, optimized designs of novel approaches to renewable energy, and reduced manufacturing costs through improved production times in 3D printing.This project aims to enhance the IBAMR computer modeling and simulation infrastructure that provides advanced implementations of the immersed boundary (IB) method and its extensions with support for adaptive mesh refinement (AMR). IBAMR is designed to simulate large-scale fluid-structure interaction models on distributed memory-parallel systems. Most current IBAMR models assume that the properties of the fluid are uniform, but many physical systems involve multiphase fluid models with inhomogeneous properties, such as air-water interfaces or the complex fluid environments of biological systems. This project aims to extend recently developed support in IBAMR for treating multiphase flows by improving the accuracy and efficiency of IBAMR's treatment of multiphase Newtonian flows, and also by extending this multiphase flow modeling capability to treat multiphase complex (polymeric) fluid flows, which are commonly encountered in biological systems, and to treat reacting flows with complex chemistry, which are relevant to models of combustion, astrophysics, and additive manufacturing using stereolithography (3D printing). This project also aims to re-engineer IBAMR for massive parallelism, so that it may effectively use very large computational resources in service of applications that require very high fidelity. The project will also develop modules that will facilitate the use of image-derived geometries, and it will develop novel fluid-structure coupling schemes that will facilitate the use of independent fluid and solid solvers. These capabilities are motivated within this project by models of cardiac, gastrointestinal, and lung physiology; renewable energy; and advanced manufacturing. This software will be used in courses developed by the members of the project team. The project also aims to grow the community of IBAMR users by enhancing project documentation and training materials, hosting user group meetings, and offering short courses.This award by the NSF Office of Advanced Cyberinfrastructure is co funded by the Division of Civil, Mechanical, and Manufacturing Innovation to provide enabling tools to advance potentially transformative fundamental research, particularly in biomechanics and mechanobiology.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Cybershuttle: An end-to-end Cyberinfrastructure Continuum to accelerate Discovery in Science and Engineering,OAC,2209872,Suresh Marru,smarru@iu.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","Science depends critically on accessing scientific and engineering software, data repositories, storage resources, analytical tools, and a wide range of advanced computing resources, all of which must be integrated into a cohesive scientific research environment. The Cybershuttle project is creating a seamless, secure, and highly usable scientific research environment that integrates all of a scientist?s research tools and data, which may be on the scientist?s laptop, a computing cloud, or a university supercomputer. These research environments can further support scientific research by enabling scientists to share their research with collaborators and the broader scientific community, supporting replicability and reuse. The Cybershuttle team integrates biophysicists, neuroscientists, engineers, and computer scientists into a single team pursuing the project goals with a grounding in cutting-edge research problems such as understanding how spike proteins in viruses work, how the brain functions during sleep, and how artificial intelligence techniques can be applied to modeling engineering materials. To meet its ambitious goals, the project is building on over a decade of experience in developing and operating the open-source Apache Airavata software framework for creating science-centric distributed systems. Cybershuttle is providing a system that can be used as a training ground to educate students in concepts of open-source software development and applied distributed systems, fostering a globally competitive workforce who can move easily between academic and non-academic careers. Cybershuttle is creating a new type of user-facing cyberinfrastructure that will enable seamless access to a continuum of CI resources usable for all researchers, increasing their productivity. The core of the Cybershuttle framework is a hybrid distributed system, based on open-source Apache Airavata software. This system integrates locally deployed agent programs with centrally hosted middleware to enable an end-to-end integration of computational science and engineering research on resources that span users? local resources, centralized university computing and data resources, computational clouds, and NSF-funded, national-scale computing centers. Scientists and engineers access this system using scientific user environments designed from the beginning with the best user-centered design practices. Cybershuttle uses a spiral approach for developing, deploying, and increasing usage and usability, beginning with on-team scientists and expanding to larger scientific communities. The project engages the larger community of scientists, cyberinfrastructure experts, and other stakeholders in the creation and advancement of Cybershuttle through a stakeholder advisory board. Cybershuttle's team includes researchers from Indiana University, the University of Illinois at Urbana-Champaign, the University of California San Diego, the San Diego Supercomputer Center, and the Allen Institute.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research:Framework:Software:NSCI:Enzo for the Exascale Era (Enzo-E),OAC,1835402,Michael Norman,mlnorman@ucsd.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","The earliest stages of the formation of galaxies and quasars in the universe are soon to be explored with a powerful new generation of ground and space-based observatories. Broad and deep astronomical surveys of the early universe beginning in the next decade using the Large Synoptic Survey Telescope and the James Webb Space Telescope will revolutionize our understanding of the origin of galaxies and quasars, and help constrain the nature of the dark matter which is the dominant matter constituent in the universe. Detailed physical simulations that model the formation of these objects are an indispensible aid to understanding the coming glut of observational data, and to maximize the scientific return of these instruments. In this project, investigators at the Univ. California San Diego, Columbia Univ., Georgia Tech, and Michigan State Univ. are collaborating with the goal of developing a next generation community simulation software framework for the coming generation of supercomputers for cosmological simulations of the young universe. Undergraduate and graduate students will be directly involved in the software development as well as its application to several frontier cosmological research topics. The software framework that will be produced will be disseminated as open source software to enable a much broader range of scientific explorations of astrophysical topics. The project brings together the key developers of the open source Enzo adaptive mesh refinement (AMR) hydrodynamic cosmology code, who will port its software components to a newly developed AMR software framework called Cello. Cello implements the highly scalable array-of-octrees AMR algorithm on top of the powerful Charm++ parallel object system. Designed to be extensible and scalable to millions of processors, the new framework, called Enzo-E, will target exascale high performance computing (HPC) systems of the future. Through this project, the entire Enzo community will have a viable path to exascale simulations of unprecedented size and scope. The investigators have chosen three frontier problems in cosmology to drive the development of the Enzo-E framework: (1) the assembly of the first generation of stars and black holes into the first galaxies; (2) the role of cosmic rays in driving galactic outflows; and (3) the evolution of the intergalactic medium from cosmic dawn to the present day. Annual developer workshops and software releases will keep the broader research community informed and involved in the developments.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Collaborative Research: Frameworks: Multiphase Fluid-Structure Interaction Software Infrastructure to Enable Applications in Medicine, Biology, and Engineering",OAC,1931368,Amneet Pal Bhalla,asbhalla@sdsu.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Physical systems in which fluid flows interact with immersed structures are found in a wide range of areas of science and engineering. Such fluid-structure interactions are ubiquitous in biological systems, including blood flow in the heart, the ingestion of food, and mucus transport in the lung. Fluid-structure interaction is also a crucial aspect of new approaches to energy harvesting, such as wave-energy converters that extract energy from the motion of sea or ocean waves, and in advanced approaches to manufacturing, such as 3D printing. This award supports the development of an advanced computer simulation infrastructure for modeling this full range of application areas. Computer models advanced by this project could ultimately lead to improved diagnostics and treatments for human disease, optimized designs of novel approaches to renewable energy, and reduced manufacturing costs through improved production times in 3D printing.This project aims to enhance the IBAMR computer modeling and simulation infrastructure that provides advanced implementations of the immersed boundary (IB) method and its extensions with support for adaptive mesh refinement (AMR). IBAMR is designed to simulate large-scale fluid-structure interaction models on distributed memory-parallel systems. Most current IBAMR models assume that the properties of the fluid are uniform, but many physical systems involve multiphase fluid models with inhomogeneous properties, such as air-water interfaces or the complex fluid environments of biological systems. This project aims to extend recently developed support in IBAMR for treating multiphase flows by improving the accuracy and efficiency of IBAMR's treatment of multiphase Newtonian flows, and also by extending this multiphase flow modeling capability to treat multiphase complex (polymeric) fluid flows, which are commonly encountered in biological systems, and to treat reacting flows with complex chemistry, which are relevant to models of combustion, astrophysics, and additive manufacturing using stereolithography (3D printing). This project also aims to re-engineer IBAMR for massive parallelism, so that it may effectively use very large computational resources in service of applications that require very high fidelity. The project will also develop modules that will facilitate the use of image-derived geometries, and it will develop novel fluid-structure coupling schemes that will facilitate the use of independent fluid and solid solvers. These capabilities are motivated within this project by models of cardiac, gastrointestinal, and lung physiology; renewable energy; and advanced manufacturing. This software will be used in courses developed by the members of the project team. The project also aims to grow the community of IBAMR users by enhancing project documentation and training materials, hosting user group meetings, and offering short courses.This award by the NSF Office of Advanced Cyberinfrastructure is co funded by the Division of Civil, Mechanical, and Manufacturing Innovation to provide enabling tools to advance potentially transformative fundamental research, particularly in biomechanics and mechanobiology.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Monitoring Earth Surface Deformation with the Next Generation of InSAR Satellites: GMTSAR,OAC,2209807,Xiaohua Xu,xiaohua.xu@austin.utexas.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Understanding the processes of earthquakes, volcanoes and hydrological changes, and their associated hazards is a top priority of the solid earth research community and USGS, due to the potential for societal disruption, financial consequences, and possible loss of life if not prepared for adequately. This requires not only long-term estimates of such processes and their hazards within a socially relevant timeframe, but also an evaluation on the impact of human activities over the Earth?s surface and interior. These estimates and evaluations hinge on the capability of accurately measuring how the Earth?s surface changes and deforms over time. For example, it is important to know how fast the seismic moment is accumulating over the San Andreas fault system, as that will tell us where and when will we be expecting the next destructive earthquake. This requires us to be able to measure the deformation that spans hundreds of kilometers at an accuracy of 0.5 mm/yr with resolution better than 10 km. Interferometric Synthetic Aperture Radar (InSAR) is the best technique for this crucial task, as the current remote sensing satellite observations that inform this technique come with broad-scale coverage, at low-cost, regardless of weather and on a regular basis. However, the upcoming new InSAR missions are raising a new challenge: how to efficiently handle drastically increasing amounts of data (~80 TB per day for the NISAR mission). To answer this challenge, the freely-available InSAR processing software GMTSAR is developing robust and efficient approaches to take full advantage of the satellite-generated data for both scientific research and societal applications. The main innovations of this project are to enable the cloud computing capabilities, transfer to newer generation of programing language, and keep engaging more users to build their own data processing strategies using this software. The developers will ensure that users from across the globe have the support they need for access to state-of-the-art processing techniques, and will continue improving the documentation, example datasets and tutorials to strengthen the foundation for education in the field of space geodesy. Interferometric Synthetic Aperture Radar (InSAR) is a powerful technique for measuring small displacements (1-10 cm) of the surface of the earth including those caused by tectonic loading, earthquakes, volcanoes, landslides, glaciers, ground fluid injection/withdrawal and underground nuclear tests. Over the past decade, a freely available, open-source software has been developed to harness these valuable datasets, which is called GMTSAR. During past investigations, this software has been equipped with the power to capitalize on the freely available ~1200 TB per year of data from Sentinel-1 satellite operated by the European Space Agency, and was provided as a robust research tool to the user base. The upcoming NISAR mission operated by NASA and ISRO, will dramatically increase the amount of available SAR data to over 30,000 TBytes per year. While this is a boon for InSAR science, it presents two main processing hurdles: how can one achieve maximum productivity with these increasing large datasets, while still preserving the accuracy of the measurements and how can one best facilitate broad user access to this trove of data. This project addresses these challenges by (1) enabling GMTSAR to permit rapid processing of very large data sets in a cloud computing environment, and (2) further expanding the usage of these InSAR data in both research and student communities by integrating with Python and streamlining processing modules to simplify user interactions.Facilitating the processing of large InSAR datasets with enhanced GMTSAR software will allow solid earth and cryosphere scientists to utilize the massive InSAR data sets to advance their interdisciplinary investigations, including global observations of volcanoes, estimates of seismic hazard through strain-rate mapping, monitoring urban infrastructure, tracking ice sheet movements, and detecting coastal subsidence. The global reach of InSAR science will be advanced by streamlined processing modules that are accessible to both specialists and students alike. In addition, the improvements we propose to make to specific modules, including the development of routine integration with Global Navigation Satellite System (GNSS) data and the combination of line-of-sight (LOS) InSAR measurements from both right-looking Sentinel-1 and left-looking NISAR satellites, will improve the accuracy of measurements to enable full 3D vector displacement time series analyses.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Seismic COmputational Platform for Empowering Discovery (SCOPED),OAC,2103494,Yinzhi Wang,iwang@tacc.utexas.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Seismology is the most powerful tool for investigating the interior structure of Earth?from its surface down to the inner core?and its wide range of processes, including earthquakes, volcanic activity, glacial processes, oceanic and environmental processes, and human-caused processes such as nuclear explosions or hydraulic fracturing in oil and gas exploration. Seismology cannot achieve its greatest potential without harnessing state-of-the-art computing capabilities for the dual purpose of scientific modeling and analysis of rapidly increasing data sets. The SCOPED (Seismic COmputational Platform for Empowering Discovery) project establishes a computing platform that delivers data, computation, and service to the seismological community in a way that promotes education, innovation, and discovery, and enables efficient solutions to outstanding scientific problems in geophysics. By focusing on openly available data, openly available software, and virtual training, SCOPED opens seismological research to a broad range of users. Four research components emphasize openly available software for the purpose of characterizing Earth's subsurface structure and the wide range of natural and man-made events that are recorded by seismometers every day. Training of seismologists is a central focus of the project. SCOPED training workshops (seismoHackweeks) are open to the community. Emphasis on virtual research and training diversifies strategies to engage minority groups entering computational geosciences. The project trains a new generation of seismologists to harness the latest capabilities for processing and modeling large data sets. The SCOPED project establishes cyberinfrastructure that provides fast access to large seismic archives from a suite of containerized open-source computational tools for big data analysis, machine learning, and high-performance simulations. The implementation focuses on four interconnected, compute- and data-intensive research components: seismic imaging of Earth?s interior, waveform modeling of earthquakes and Earth structure, monitoring of Earth structure using ambient noise, and precision monitoring of earthquakes and faults. Each research component is enabled by open-source codes that meet, or aspire to meet, best practices for software development. The project contains several transformative components. First, it offers compute performance for both model- and data-driven seismological problems. Hundreds of terabytes of waveform data are directly accessible both to modelers?for data assimilation problems?and to data scientists for processing, analysis, and exploration. Second, it establishes a direct collaborative link among four teams of seismologists at four institutions and a team of computational scientists at Texas Advanced Computing Center. This unity reflects the necessity of both groups to achieve research-ready codes that can exploit high-performance computing (HPC) and Cloud systems. Third, it establishes a gateway with ready-to-run (or adapt) container images and data as a service for the seismological community. Fourth, it develops computational tools that promote the democratization of HPC/Cloud with cutting-edge data processing and modeling software through their scalability from laptops to HPC or Cloud systems and through their portability with containerization. Finally, although the development of cyberinfrastructure is the main priority, ancillary scientific results from advanced techniques are expected to offer insights into fundamental seismological problems. The project has the potential for discoveries across fields (seismology, Earth science, computer science, data science, material science), as well as societal relevance in the realms of seismic hazard assessment, environmental science, cryosphere, earthquake early warning, energy systems, and geophysical detection of nuclear proliferation.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: The Einstein Toolkit Ecosystem: Enabling fundamental research in the era of multi-messenger astrophysics,OAC,2114582,Pablo Laguna,pablo.laguna@austin.utexas.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","A team of experts from five institutions (University of Illinois Urbana-Champaign, Georgia Institute of Technology, Rochester Institute of Technology, Louisiana State University, and West Virginia University) are collaborating on further development of the Einstein Toolkit, a community-driven, open-source cyberinfrastructure ecosystem providing computational tools supporting research in computational astrophysics, gravitational physics, and fundamental science. The new tools address current and future challenges in gravitational wave source modeling, improve the scalability of the code base, and support an expanded science and user community around the Einstein Toolkit.The Einstein Toolkit is a community-driven suite of research-grade Python codes for performing astrophysics and gravitational wave calculations. The code is open-source, accessible via Conda (an open source package management system) and represents a long-term investment by NSF in providing such computational infrastructure. The software is designed to simulate compact binary stars as sources of gravitational waves. This project focuses on the sustainability of the Einstein Toolkit; specific research efforts center around the development of three new software capabilities for the toolkit: ? CarpetX -- a new mesh refinement driver and interface between AMReX, a software framework containing the functionality to write massively parallel block-structured adaptive mesh refinement (AMR) code, and Cactus, a framework for building a variety of computing applications in science and engineering;? NRPy+ -- a user-friendly code generator based on Python; and ? Canuda -- a new physics library to probe fundamental physics. Integration of graphics processing units (GPUs) will incorporate modern heterogeneous computing devices into the system and will enhance the capability of the toolkit. The end product is sustainable through integration into the Einstein Toolkit, yet also includes an active community maintaining and enhancing the foundational components. Broader impacts are enhanced through training, documentation and a support infrastructure that reduces the barrier to adoption by the community. The team is also creating a science portal with additional educational and showcase resources. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the National Science Foundation's Big Idea activities in Windows on the Universe (WoU).This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Beyond the BLAS: A framework for accelerating computational and data science,OAC,2003921,Robert van de Geijn,rvdg@cs.utexas.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Traditional scientific and machine learning high-performance computing software is often cast in terms of a set of fundamental operations, including the linear algebra functionality that underlies many applications. For this reason, research into and development of open-source linear algebra software libraries has been a science infrastructure priority for decades. An emerging trend that has disrupted this field is the recognition that scientific discovery can be made faster and/or more cost efficient by lowering the precision of computations, utilizing non-standard data types, and developing custom computational kernels. The project will leverage insights into how to structure the required software so that the combinatorial explosion in software complexity remains manageable. The outcome of the project will be a modern linear algebra software framework and application-focused libraries that will support future generations of computational applications in academia, at the national labs, and in industry. In addition, the project will impact the training of the next generation of high-performance computing professions and help remove barriers into the field for members of traditionally underrepresented groups.The proposed work will build on previous NSF-sponsored research in order to address the implementation of expanded precision (EP), mixed precision (MP), and mixed domain (MD) algorithms simultaneously in a single software solution. Insights gained from a recent demonstration of MP/MD matrix multiplication will be extended by adding low precision types like float16 and bfloat16 and extended precision types like double-double. The target Basic Linear Algebra Subprograms (BLAS) functionality will be expanded to all level-1, level-2, and level-3 operations which in turn will support new research on how best to exploit MP/MD for LAPACK functionality. The new BLAS-like Library Instantiation Software (BLIS) framework will also be updated to provide the flexibility required to integrate extended dense linear algebra (DLA) operations. This flexible DLA framework will then be used to implement key functionality in computational and data science: tensor contraction and factorization operations important to quantum chemistry (QC) and high-performance primitives for machine learning. As a demonstration, these capabilities will be used to build state-of-the-art QC codes to perform coupled cluster polarization propagator and tensor-factorized coupled cluster calculations with full EP/MP/MD functionality, and the machine learning kernels will be integrated into computer vision and image recognition workflows.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Scalable Modular Software and Methods for High-Accuracy Materials and Condensed Phase Chemistry Simulation,OAC,1931328,Garnet Chan,garnetc@caltech.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","How electrons are arranged in materials gives rise to a large variety of different behaviors. We can observe these behaviors and use them in various technologies. However, the prediction of these behaviors is a serious challenge. This makes the successful design of new materials harder. The goal of the Materials Genome Initiative is to use computer simulations to model electrons according to the laws of quantum physics. This will allow researchers to design new materials with desired properties. This project aims to build fast and accurate computer programs which simulate those new materials. These programs combine advances in computer science, quantum chemistry, and condensed-matter physics. They will be implemented in an open-source Python-based community code. This distribution model allows other researchers to use this code and to contribute new features.This research addresses gaps in existing software cyberinfrastructure in quantum materials simulation, by developing novel parallel implementations of low-scaling, high-accuracy methods. In particular, new techniques for mean-field calculations will be developed, which will act as groundwork for periodic coupled-cluster and quantum Monte Carlo methods. State-of-the-art techniques in sparsity and tensor decomposition will be employed to achieve good system-size scaling while retaining accuracy within each of these numerical schemes. Critically, the methods will be developed using efficient high-level software abstractions, implemented as Python-level modules within PySCF that leverage the Cyclops library for massively-parallel execution. The library software infrastructure will also be extended to maximize productivity via source-to-source automatic differentiation, as well as to enable execution of sparse kernels on emerging GPU-based supercomputing architectures. This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure, and the Division of Materials Research and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Frameworks: Collaborative Research: Integrative Cyberinfrastructure for Next-Generation Modeling Science,OAC,2103780,Jerad Bales,jdbales@cuahsi.org,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","This project is designed to support and advance next generation, interdisciplinary science of the complexly interacting societal and natural processes that are critical to human life and well-being. Computational models are powerful scientific tools for understanding these coupled social-natural systems and forecasting their future conditions for evidenced-based planning and policy-making. This project is led by the Network for Computational Modeling in Social and Ecological Sciences (CoMSES.Net). CoMSES.Net's science gateway promotes knowledge sharing among scientists and with the general public, and enables open, online access to sophisticated computational models of social and ecological systems. CoMSES.Net's partners in this project (the Community Surface Dynamics Modeling System and Consortium of Universities for the Advancement of Hydrologic Science) also enable knowledge sharing and provide open, online repositories of models in the earth sciences. This project will enhance these science gateways and create online educational materials to make these critical technologies easier to find, understand, and use for scientists and non-scientists alike. By integrating innovative technology with training and incentives to engage in best practice standards, this project will stimulate innovation and diversity in modeling science. It will enable researchers to build on each other's work and combine it in new ways to address societal and environmental challenges. The cybertools and educational programs developed in the project will be openly accessible not just to research institutions but also to smaller colleges, state and local governments, and a broader audience beyond the science community. The project will give decision-makers and the data scientists who support them access to a larger and more varied toolkit with which to explore potential solutions to societal and environmental policy issues. A long-term aim of the project is to support an evolving ecosystem of diverse, reusable, and combinable models that are transparently accessible to anyone in the world. Sustainable planetary care and management is a challenge that confronts all of humanity, and requires knowledge, histories, methods, perspectives, and engagement of researchers, decision-makers, and private citizens across the country and throughout the world.The project will develop an Integrative Cyberinfrastructure Framework (ICF) to enable innovative next-generation modeling of human and natural systems, and build capacity in modeling science. It will support a set of activities that integrate the human and technological components of cyberinfrastructure. 1) Software tools will be developed that augment model codebases with modern software development scaffolding to facilitate reuse, integration, and validation of model code. 2) The project will provide high-throughput computing (HTC) resources for simultaneously running numerous iterations of models needed to capture stochastic variability, explore a parameter space, and generate alternative scenarios; 3) Online training activities will build expertise and capacity to make effective use of the cybertools and the HTC resources; 4) The ICF will engage a global modeling science community to provide professional incentives that encourage researchers to adopt best practices and catalyze innovative science. Leveraging existing NSF investments, the ICF will be developed and deployed by the Network for Computational Modeling in Social and Ecological Sciences (CoMSES.Net), in partnership with the Community Surface Dynamics Modeling System (CSDMS), Consortium of Universities for the Advancement of Hydrologic Science (CUAHSI), Open Science Grid, Big Data Hub/Spoke network, and Science Gateways Community Institute. Computational models have emerged as powerful scientific tools for understanding coupled social-biogeophysical systems and generating forecasts about future conditions under a range of climate, biogeophysical, and socioeconomic conditions. CoMSES.Net, CSDMS, and CUASI are scientific networks, with online science gateways and code archives that enable open access to computational models for an international community of social, ecological, environmental, and geophysical scientists. However, the full value of accessible, well-documented models only can be realized if their code is also widely reproducible and reusable, with a potential for integration with other models. In order to confront critical challenges for understanding the coupled human and natural systems of today's world, modeling scientists also need HTC environments for upscaling models and exploring high-dimensional parameter spaces inherent in representing these systems. The ICF is designed to meet these challenges. By integrating technology with intellectual capacity-building, the ICF will stimulate innovation and diversity in modeling science by letting creative researchers build on each other's work more readily and combine it in new ways to address societal-environmental challenges we have not yet perceived. The tools and training resources will be openly accessible not just to leading research institutions but also to the many smaller colleges, state and local governments, and a broader audience beyond science. They will provide decision-makers and the data scientists who support them access to a much larger and more varied toolkit with which to explore potential solution spaces to social and environmental policy issues. The proposed ICF is also designed to help transform scientific modeling practice, including incentives that can help early career researchers shift from creating models to solve problems specific to a particular project to models that are also useful for others. The project will help support a future evolving ecosystem of diverse, reusable, and integrable models that are transparently accessible to the broader community.This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering, with the Division of Social and Economic Sciences in the Directorate for Social, Behavioral & Economic Sciences also contributing funds.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements:Software A Scalable Open-Source hp-Adaptive FE Software for Complex Multiphysics Applications,OAC,2103524,Leszek Demkowicz,leszek@oden.utexas.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Computer models can be used to augment, inform, and even replace expensive experimental measurements in science and engineering. However, complex models of engineering applications can quickly exceed computational capability, driving the need for advanced simulation tools. Applications in high-frequency wave simulation--such as submarine sonar (acoustics), fiber optics (electromagnetics), and structural analysis (elastodynamics)--pose a significant challenge for large-scale simulation. This project advances computational modeling capabilities through the development, documentation, and dissemination of a leading-edge simulation software. The effort builds on decades-long research and code development by the investigators and their project collaborators. Distributed as open-source, the software is accessible to the broader scientific community, thereby contributing to fundamental research and education for computer modeling in science and engineering. Furthermore, the project expands the national workforce by training young computational mathematicians at the graduate and postdoctoral levels. The project results are disseminated through conference presentations, workshops and seminars, as well as publications in scientific journals.The hp3D software leverages hybrid MPI/OpenMP parallelism to run efficiently on NSF extreme-scale computing facilities and interfaces with state-of-the-art third-party scientific libraries. In addition to publishing the hp3D code and documentation, this project focuses on the development of a scalable multigrid (MG) solver based on the pre-asymptotically stable discontinuous Petrov-Galerkin (DPG) finite element method. This DPG-MG solver represents a significant advancement in solver technology as 1) the first robust, scalable solver for problems with highly-indefinite operators, such as high-frequency wave propagation; and 2) the first multigrid solver with support for fully anisotropic hp-adaptive hybrid meshes and a reliable built-in error indicator. Serial implementations of the DPG-MG solver have demonstrated near-linear scaling with respect to degrees of freedom in both time and memory; its parallel implementation significantly expands scientific compute capabilities and enables solution of currently intractable problems in 3D wave simulation.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: HDR Elements: Software for a new machine learning based parameterization of moist convection for improved climate and weather prediction using deep learning,OAC,1835863,Michael Pritchard,mspritch@uci.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","This project targets a difficult problem in weather and climate prediction -- the representation of convection. Accurate representation of convection is important, since a majority of current model predictions depend on it. Unraveling the physics involved in convective conditions, clouds and aerosols may take years of modeling to fully understand; however, a set of machine learning techniques, known as ""neural net techniques"", may provide enhanced predictability in the interim, and this project explores their potential.The project develops a Python library enabling the use of machine learning (artificial neural networks) in a broad range of science domains. The focus is on integration of convection and cloud formation within larger-scale climate models, with the Community Earth System Model (CESM) as an initial target. The project develops a new set of machine learning climate model parameterizations to reduce uncertainty in weather and climate predictions. The neural networks will be trained on high-fidelity simulations that explicitly resolve convection. Two types of high-resolution simulations will be used for training the neural networks: 1) an augmented super-parameterized simulation, and 2) a full Global Cloud Resolving Model (GCRM) simulation based on the ICOsahedral Non-hydrostatic (ICON) modelling frameworks provided by the Max Planck Institute, using initial 5km horizontal resolution. The effort has the potential to increase understanding of convection dynamics and processes across scales, and could potentially be implemented to address other scale problems as well, where it is too computationally costly or impractical to represent processes occurring at much finer scales than the main grid resolution.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Cognitasium - Enabling Data-Driven Discoveries in Natural Hazards Engineering,OAC,2103937,Krishna Kumar,krishnak@utexas.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Numerical modeling plays a critical role in assessing and mitigating risks posed by natural hazards, such as the risks to coastal communities from hurricanes and the threats to infrastructure from earthquakes. Accurately predicting these hazards requires modeling the multi-scale nature of these problems, covering a range of physical scales from microscopic to kilometer-scale interactions. Traditional approaches often focus on a particular scale and are incapable of predicting the risks accurately. With the advent of community data repositories such as the DesignSafe CyberInfrastructure, there is an as-yet untapped opportunity to effectively use these large datasets to develop new data-driven models to solve multi-scale problems. Furthermore, assessing the risks of natural hazards involves a complex web of interconnected analyses, which leads to difficulties in tracking the various uncertainties driving the final decision. Tracking the modeling workflow can ensure decision processes are informed and transparent and can help decision-makers define their confidence in model results. To support these needs, new methods are required to automate the workflow tracking and help researchers find and effectively utilize the large datasets in community repositories to develop new theories. Cognitasium, an Artificial Intelligence (AI)-powered cyberinfrastructure, addresses these challenges by automatically extracting the hazard analysis workflows, augmenting large community datasets with relevant information for analysis, and enabling AI models to discover new theories from massive datasets. Cognitasium is being developed as an open-source framework and can be easily adapted to a variety of communities. The project uncovers the possibility of discovering new theories by combining field and experimental data with numerical simulations in large community datasets. The automated tracking of workflows improves the research reproducibility and transparency in risk assessment. The project will transform static data repositories into an active community of users and developers working together to develop new theories. With sustainable software practices and open science strategy, it will support a large community of users beyond natural hazard engineering. The software and tools from the project are generalizable to other fields with massive data requirements and the need for multi-scale models and reduced uncertainties (e.g., physics and health sciences). The project incorporates four specific educational objectives: Inspire future scientists through Code@TACC aimed at enabling high-school students to program, mentor and train undergraduate researchers, facilitate the retention of underrepresented minorities through workshops and training offered at the National Society of Black Engineers and underrepresented colleges, and dissemination through documentation, webinars, and summer institutes. Uncertainties surrounding the modeling process can have important implications for the decision-making process in Natural Hazard Engineering (NHE). Assessing the risks of natural hazards is a complex process involving numerical simulations, integrated field and lab characterization, and uncertainty quantification. The complex web of inter-connected analysis leads to an inability to track workflows and accurately propagate the associated uncertainties, thus impacting the decision-making process. Meanwhile, the emergence of interactive tools such as Jupyter Notebooks has transformed data analysis and exploration. However, the interactive nature of Jupyter has further exasperated the ability to track workflows. Hence, there is an urgent need for automatically extracting workflows to incorporate a data-driven approach to quantify and reduce uncertainty and improve the decision-making process. Cognitasium is a novel machine-learning-powered CI framework for data-driven discoveries in NHE. Cognitasium will become a fundamental component of the NSF-funded DesignSafe CyberInfrastructure offering benefits to a broad community of NHE researchers. The project will: (i) enable end-to-end integration of uncertainty propagation in agile environments through workflow tracking in parameterized Jupyter notebooks, (ii) build knowledge graphs that integrate experimental and field data with numerical analysis to develop new multi-scale models, and (iii) support scalable machine learning to solve complex multi-scale problems with large datasets. The AI framework will improve natural hazard analysis and mitigation of hurricanes, storm surge, and earthquakes through data-driven discoveries. The data-driven CI framework will be generalizable to other fields with massive data and the need for multi-scale models and reduced uncertainties.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
School on Electron-Phonon Physics from First Principles,DMR,2007638,Feliciano Giustino,fgiustino@oden.utexas.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","NONTECHNICAL SUMMARYThis award supports a summer school activity to train graduate students, postdoctoral scholars, faculty members, and research scientists in modern approaches to predictive calculations for materials and particularly their electronic properties. The school is currently planned to take place June 14 to June 20, 2021 at the University of Texas - Austin. The organizers have flexible backup plans including conversion to a virtual summer school, to compensate for Covid-19 related contingencies. The focus of this summer school will be on calculating how electrons interact with oscillations of atoms in the crystal, or phonons starting from fundamental understanding of electrons and atoms, the building blocks of materials. The electron-phonon interaction plays an important in determining the temperature dependence of many electronic and optical properties of solids, and plays a central role in technologically important phenomena, from charge and heat transport to superconductivity and light-driven phase transitions. With rapid progress in materials design and data-driven materials discovery there is a growing need for more advanced computational methods that start from the atomic level together with their implementation in software to describe complex functional properties of materials and materials systems with predictive accuracy. This summer school will bring together expertise from across the nation and the world to introduce participants to advanced first principles methods for calculating electron-phonon physics and related materials properties, including lectures on the quantum mechanical theory of systems of many particles, software implementations, and hands-on training sessions. The school will be followed by a hackathon event guided by experts from the Texas Advanced Computing Center, and devoted to creating and maintaining sustainable cyberinfrastructure. There is currently no specialized training available in this area; this school fills a significant gap in the education of the next generation of physicists, chemists, materials scientists, and engineers.This school will contribute to developing a globally competitive STEM workforce by exposing participants to advanced techniques in computational materials modelling and design. By training participants into best practices in scientific computing and software development, the school will contribute to educating scientists and engineers that will go on to build tomorrow?s cyberinfrastructure. This school will also foster partnerships between academia and industry by delivering training in techniques that can be employed in an industrial setting, and by doing so it will contribute to increasing the economic competitiveness of the United States. Participation of underrepresented minorities, women, and persons with disabilities will be encouraged and prioritizedin order to increase diversity in STEM. An event dedicated to diversity and inclusion will be heldduring the school.TECHNICAL SUMMARYThis award supports a summer school activity to train graduate students, postdoctoral scholars, faculty members, and research scientists in modern approaches to predictive calculations for materials and particularly their electronic properties. The school is currently planned to take place June 14 to June 20, 2021 at the University of Texas - Austin. The organizers have flexible backup plans including conversion to a virtual summer school, to compensate for Covid-19 related contingencies. This school will introduce researchers to state-of-the art techniques for predictive first principles calculations of electronic, optical, and transport properties of materials at finite temperature. By the end of the school participants will be able to compute electron-phonon couplings, band structures including zero-point quantum fluctuations and temperature effects, optical properties including phonon-assisted quantum processes, critical temperature and superconducting gap of phonon mediated superconductors, electron and hole mobilities in semiconductors, the resistivity of metals, and polaronic properties. These properties are essential for designing the materials that will underpin future technology, including solar cells, displays, touch screens, superconducting cables, portable electronics, batteries, and quantum computers. There is currently no specialized training available in this area; this school fills a significant gap in the education of the next generation of physicists, chemists, materials scientists, and engineers.This school will contribute to developing a globally competitive STEM workforce by exposing participants to advanced techniques in computational materials modelling and design. By training participants into best practices in scientific computing and software development, the school will contribute to educating scientists and engineers that will go on to build tomorrow?s cyberinfrastructure. The school will also foster partnership between academia and industry by delivering training in techniques that can be employed in an industrial setting, and by doing so it will contribute to increasing the economic competitiveness of the United States. Participation of underrepresented minorities, women, and persons with disabilities will be encouraged and prioritized in order to increase diversity in STEM. An event dedicated to diversity and inclusion will be held during the school.This award by the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences is jointly supported by the NSF Office of Advanced Cyberinfrastructure in the Directorate of Computer and Information Science and Engineering.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: PASSPP: Provenance-Aware Scalable Seismic Data Processing with Portability,OAC,1931352,Yinzhi Wang,iwang@tacc.utexas.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Most of what we know about the Earth's deep interior comes from the analysis of ground motion data recording seismic waves produced by large earthquakes from instruments around the entire planet. Seismologists have developed a long list of methods to process modern seismic data to ?image? the Earth?s interior. Much of our understanding of Earth's interior has been limited by the resolution of the tools available to construct these ""images"". At present, the massive increase in data volume has pushed the data processing infrastructure of seismology to the breaking point. The inability to handle data of this scale has imposed significant barrier to scientific discoveries, especially for the smaller research groups with limited resources. Aiming to help improve this situation, this project introduces a new data management and processing system that is portable and scalable to run on any platforms from a personal computer to a large-scale supercomputer. By leveraging and integrating sophisticated tools from cloud computing and high-performance computing (HPC) communities, the system can fill in the widening gap between the massive data made available by data centers and the inadequacy of data management and processing capability provided with current tools. Seamless discovery, access, transfer, and processing of data and metadata outside of data centers will become possible for the community. This project will also serve as the foundation to enable novel research utilizing massive data to change the way we study the structure, composition, and evolution of the Earth. This project aims to develop a seismic data management and processing system that is composed of a scalable parallel processing framework based on dataflow computation model, a NoSQL database system centered on document store, and a container-based virtualization environment. The scalable processing component will be based on the iterative map-reduce model using Apache Spark to handle scheduling and flow of data through systems of different scales. The provenance-aware data management will be enabled by managing all data created during processing with MongoDB, including process generated metadata, processed waveform data, processing parameters, and the log outputs. All these core components as well as a script to configure and deploy the framework on different systems will be containerized with Singularity to provide portability. All these components serve the two primary goals of the project: produce a system that will allow common seismology algorithms to run effectively on modern HPC platforms; and provide the means for seismologists with average experience in programming to implement their own algorithms to extend the system. The system will serve as the infrastructure to make data intensive research such as deep learning possible for smaller research groups that usually don't have the necessary manpower to manage and process massive data in a sustainable fashion. By enabling the ability to process massive data collected by increasing number of instruments, it will facilitate the transition of the field into data-intensive paradigm of science discovery.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Designing Next-Generation MPI Libraries for Emerging Dense GPU Systems,OAC,1931354,William Barth,bbarth@tacc.utexas.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","The extremely high compute and communication capabilities offered by modern Graphics Processing Units (GPUs) and high-performance interconnects have led to the creation of High-Performance Computing (HPC) platforms with multiple GPUs and high-performance interconnects per node. Unfortunately, state-of-the-art production quality implementations of the popular Message Passing Interface (MPI) programming model do not have the appropriate support to deliver the best performance and scalability for applications on such dense GPU systems. These developments in High-End Computing (HEC) technologies and associated middleware issues lead to the following broad challenge: How can existing production quality MPI middleware be enhanced to take advantage of emerging networking technologies to deliver the best possible scale-up and scale-out for HPC and Deep Learning (DL) applications on emerging dense GPU systems? A synergistic and comprehensive research plan, involving computer scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) and computational scientists from the Texas Advanced Computing Center (TACC), and San Diego Supercomputer Center (SDSC) and University of California San Diego (UCSD), is proposed to address the above broad challenges with innovative solutions. The proposed framework will be made available to collaborators and the broader scientific community to understand the impact of the proposed innovations on next-generation HPC and DL frameworks and applications in various science domains. Multiple graduate and undergraduate students will be trained under this project as future scientists and engineers in HPC. The proposed work will enable curriculum advancements via research in pedagogy for key courses in the new Data Science programs at OSU, SDSC and TACC. The established national-scale training and outreach programs at TACC, SDSC and OSC will be used to disseminate the results of this research to XSEDE users. Tutorials and workshops will be organized at PEARC, SC and other conferences to share the research results and experience with the community. The project is aligned with the National Strategic Computing Initiative (NSCI) to advance US leadership in HPC and the recent initiative of the US Government to maintain leadership in Artificial Intelligence (AI.)The proposed innovations include: 1) Designing high-performance and scalable point-to-point, and collective communication operations that fully utilize multiple network adapters and advanced in-network computing features for GPU and CPU buffers within and across nodes; 2) Designing novel datatype processing and unified memory management to improve application performance; 3) Designing CUDA-aware I/O subsystem to accelerate MPI I/O and checkpoint-restart for HPC and DL applications; 4) Designing support for containerized environments to better enable easy deployment of proposed solutions on modern cloud environments; and 5) Carry out integrated development and evaluation to ensure proper integration of proposed designs with the driving applications. The proposed designs will be integrated into the widely-used MVAPICH2 library and made available. The project team members will work closely with internal and external collaborators to facilitate wide deployment and adoption of released software. The proposed solutions will be targeted to enable scale-up and scale-out of the driving science domains (molecular dynamics, lattice QCD, seismology, image classification, and fusion research) on emerging dense GPU platforms. The transformative impact of the proposed development effort is to achieve scalability, performance, and portability out of HPC and DL frameworks and applications to take advantage of emerging dense GPU platforms and hence, leading to significant advancements in science and engineering.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Innovating for Edge-to-Edge Climate Services,OAC,2209806,Sergiu Dascalu,dascalus@cse.unr.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Environmental sciences have enormous potential to provide real-time community hazard information because of advances in cyberinfrastructure (CI) and the Internet of Things (IoT). Real-time systems that observe and monitor hydrology, climate, geology, and ecology have historically been difficult to design, implement, and maintain, with challenges ranging from equipment cost to data management. To make it easier for everyone to access the environmental data they need, this project focuses on ?democratizing? portions of an existing regional earthquake and wildfire science network in Nevada by integrating new IoT technologies and cutting-edge cloud-ready CI. Together, these create transformative real-time ?crowd-participating? environmental data services, assembled on a new Nevada Weather ?edge-to-edge? platform, or ?NevWx,? developed as part of the project. These elements become community-centered solutions, in which any individual or organization can easily incorporate new sensors in the NevWx scientific platform. The project?s science application aims to shed new light into temperature patterns in and around mountain communities. This interdisciplinary work contributes new science and engineering knowledge along with new physical resources, which helps move forward the adoption and scalability of IoT in regional research and monitoring networks. Given its emphasis on involving the public and local agencies in science, the project also offers an example for transforming societal engagement with research data networks. By incorporating research results in undergraduate and graduate courses at UNR and in training materials at the Nevada State Climate Office (NSCO), the project also provides substantial and unique educational and workforce development opportunities.To ?democratize? the existing regional science network, this project integrates Low Power Wide Area Network (LPWAN) wireless and IoT-focused streaming data CI to create crowd-participatory environmental hazard data on the new NevWx workflow platform. The software and networking solutions are designed to enable any interested individuals to connect new sensors to the project?s infrastructure, register metadata, and freely access and share the data collected from them. System testing consists of installing temperature sensors and network enhancements in urban and wildland areas in the Lake Tahoe Basin. The main components of this project are: (1) CI research, design & implementation, to facilitate data acquisition from sensors, conduct quality control, and make the data available through a web portal. Specific CI goals are: application of IoT topologies on regional infrastructure; development of containerized microservice-based software architecture to support data collection, storage, processing, and curation; and creation of crowd-participatory data services that incorporate FAIR data principles; (2) Research into how disturbance and development influence temperatures in the Lake Tahoe Basin. This includes measuring urban heat island effects in a mountain community, comparing temperatures in recently burned areas to unburned forest, and tracking freezing levels along a major highway into the Lake Tahoe Basin. Tightly connected with CI research and development, the science applications allow testing the NevWx platform in a range of environments and seasons, while providing real-time information pertinent to public health and safety; and (3) Science stakeholder meetings, which are designed to evaluate their interests and preferences about the proposed sensor data and deployments. Through its modern, flexible and adaptable solutions, available to a wide spectrum of researchers, individuals, and organizations, the project has the potential to advance the current state-of-the-art in CI and environmental sciences, help expand our national cyberinfrastructure on regional networks, and contribute to making significant steps towards ?sensing by the public,? with benefits for many science communities and public service agencies.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Data: NSCI: HDR: GeoSCIFramework: Scalable Real-Time Streaming Analytics and Machine Learning for Geoscience and Hazards Research,OAC,1835791,David Mencin,mencin@unavco.org,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","This project develops a real-time processing system capable of handling a large mix of sensor observations. The focus of this system is automation of the detection of natural hazard events using machine learning, as the events are occurring. A four-organization collaboration (UNAVCO, University of Colorado, University of Oregon, and Rutgers University) develops a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research. This work will support rapid analysis and understanding of data associated with hazardous events (earthquakes, volcanic eruptions, tsunamis). This project uses a collaboration between computer scientists and geoscientists to develop a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research. It focuses on the aggregation and integration of a large number of data streams into a coherent system that supports analysis of the data streams in real-time. The framework will offer machine-learning-based tools designed to detect signals of events, such as earthquakes and tsunamis, that might only be detectable when looking at a broad selection of observational inputs. The architecture sets up a fast data pipeline by combining a group of open source components that make big data applications viable and easier to develop. Data sources for the project draw primarily upon the 1500+ sensors from the EarthScope networks currently managed by UNAVCO and the Incorporated Research Institutions for Seismology (IRIS), as well as the Ocean Observatories Initiative (OOI) cabled array data managed by Rutgers University. Machine learning (ML) algorithms will be researched and applied to the tsunami and earthquake use cases. Initially, the project plans to employ an advanced convolutional neural network method in a multi-data environment. The method has only been applied to seismic waveforms, so the project will explore extending the method to a multi-data environment. The approach is expected to be extensible beyond detection and characterization of earthquakes to include the onset of other geophysical signals such as slow-slip events or magmatic intrusion, expanding the potential for new scientific discoveries. The framework is applied to use cases in the Cascadia subduction zone and Yellowstone: these locations combine the expertise of the science team with locations where EarthScope and OOI have the greatest concentration of instruments. The architecture will be transportable and scalable, running in a Docker environment on laptops, local clusters and the cloud. Integral to the project will be development, documentation and training using collaborative online resources such as GitLab and Jupyter Notebooks, and utilizing NSF XSEDE resources to make larger datasets and computational resources more widely available.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program and Division of Earth Sciences within the NSF Directorate for Geosciences, the Big Data Science and Engineering Program within the Directorate for Computer and Information Science and Engineering, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science,OAC,1835598,Jurij Leskovec,jure@cs.stanford.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth. Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn. The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above. Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities. Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks. Compositions of these diverse capabilities are rare. Furthermore, many researchers who study networks are not computer scientists. As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming. The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use. What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science. CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software. CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks. The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program. It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions. CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science. Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Data: HDR: Developing On-Demand Service Module for Mining Geophysical Properties of Sea Ice from High Spatial Resolution Imagery,OAC,1835507,Chaowei Yang,cyang3@gmu.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Sea ice acts as both an indicator and an amplifier of climate change. At present, there are multiple sources of sea ice observations which are obtained from a variety of networks of sensors (in situ, airborne, and space-borne). By developing a smart cyberinfrastructure element for the analysis of high spatial resolution (HSR) remote sensing images over sea ice, the science community is better able to extract important geophysical parameters for climate modeling. The project contributes new domain knowledge to the sea ice community. This is accomplished by integrating HSR images that are spatiotemporally discrete to produce a more rapid and reliable identification of ice types, and by a standardized image processing that allows creating compatible sea ice products. The cyberinfrastructure module is a value-added on-demand web service that can be naturally integrated with existing infrastructure.The key objective is to develop a reliable and efficient on-demand Open Geospatial Consortium-compliant web service, which is capable of extracting accurate geographic knowledge of water, submerged ice, bare ice, melt ponds, deformed 'ridging' ice, ridge shadows, and other information from HSR images with limited human intervention. The embedded spatial-temporal analysis framework provides functions to search, explore, visualize, organize, and analyze the discrete HSR images and other related remote sensing data and field data. The project creates a data and knowledge web service for the Arctic sea ice community by integrating computer vision and machine learning algorithms, computing resources, and HSR image data and other useful datasets. The conceptual model improves data flow, so users would query data, download value-added data, and have more consistent results across various sources of information. This creates new opportunities for scientific analysis that minimizes the investment of time in processing complex and spatiotemporally-discrete HSR imagery. The project includes a strong emphasis on teaching and development of the next-generation workforce through course curricula development, involvement of graduate and undergraduate students in research, and the offering of summer workshops for K-12 teachers (funded by other agencies). The collected images and results of the image analyses will be shared with the public in a timely manner through the NSF Arctic Data Center.This award by the Office of Advanced Cyberinfrastructure is jointly supported by EarthCube and the Office of the Polar Programs Arctic Natural Sciences Program, within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Machine learning and FPGA computing for real-time applications in big-data physics experiments,OAC,1931561,Eliu Huerta Escudero,elihu@uchicago.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","The cyberinfrastructure needs for gravitational wave astrophysics, high energy physics, and large-scale electromagnetic surveys have rapidly evolved in recent years. The construction and upgrade of the facilities used to enable scientific discovery in these disparate fields of research have led to a common pair of computational grand challenges: (i) datasets with ever-increasing complexity and volume; and (ii) data mining analyses that must be performed in real-time with oversubscribed computational resources. Furthermore, the convergence of gravitational wave astrophysics with electromagnetic and astroparticle surveys, the very birth of Multi-Messenger Astrophysics, has already provided a glimpse of the transformational discoveries that it will enable in years to come. Given the unique potential for scientific discovery with the Large Hadron Collider (LHC) and the combination of the Laser Interferometer Gravitational-wave Observatory (LIGO) and the Large Synoptic Survey Telescope (LSST) for Multi-Messenger Astrophysics, the community needs to accelerate the development and exploitation of deep learning algorithms that will outperform existing approaches. This project serves the national interest, as stated by NSF's mission, by promoting the progress of science. It will push the frontiers of deep learning at scale, demonstrating the versatility and scalability of these methods to accelerate and enable new physics in the big data era. Because these methods are also applicable to many other parts of our national and global economy and society, this work will positively impact many fields. The students and junior scientists to be mentored and trained in this research will interact closely with our industry partners, creating new career opportunities, and strengthening synergies between academia and industry. The team will share the algorithms with the community through open source software repositories, and through our tutorials and workshops the team will train the community regarding software credit and software citation.In this project, the PIs will build upon our recent work developing high quality deep learning algorithms for real-time data analytics of time-series and image datasets, as open source software. This work combines scalable deep learning algorithms, trained with TB-size datasets within minutes using thousands of GPUs/CPUs, with state-of-the-art approaches to endow the predictions of deterministic deep learning models with complete posterior distributions. The team will also investigate the use of Field Programmable Gate Arrays (FPGAs) to accelerate low-latency inference of machine learning algorithms to minimize the demands of future computing, which is a central goal for Multi-Messenger Astrophysics and particle physics. The open source tools to be developed as part of these activities will be readily shared with and adopted by LIGO, LHC, and LSST as core data analytics algorithms that will significantly increase the speed and depth of existing algorithms, enabling new physics while requiring minimal computational resources for real-time inferences analyses. The team will organize deep learning workshops and bootcamps to train students and researchers on how to use and contribute to our framework, creating a wide network of contributors and developers across key science missions. The team will leverage existing open source and interactive model repositories, such as the Data and Learning Hub for Science (DLHub) at Argonne, to reach out to a large cross-section of communities that analyze open datasets from LIGO, LHC, and LSST, and that will benefit from the use of these technologies that require minimal computational resources for inference tasks.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Physics in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: FastTract: Web-Based Exploratory Visualization of Gigapixel Astronomical Images,OAC,2004840,Peter Williams,peter.williams@aas.org,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Like scientists in virtually every field, astronomers are struggling under a""data deluge"". While the ever-larger images obtained by the newestobservatories lead to cutting-edge science, they also overwhelm traditionaltools designed to work with files hundreds or thousands of times smallerthan the state-of-the-art. In particular, astronomers are rapidly losing theability to simply look at the images of the sky that are coming out of theirtelescopes. The FastTract Project will solve this problem by marrying existingWeb-based visualization technologies in the AAS WorldWide Telescope softwaresystem with new features and tools needed to efficiently work with the largeastronomical images of the 2020's, enabling US astronomers to fully exploittheir world-class facilities, in particular the ones that open new Windows onthe Universe. The project team will synthesize the insight gained in thisundertaking to establish the Little Big Data University (LBDU), a learningresource for scientists across disciplines who are struggling to stay afloatin the ""data deluge"". LBDU will follow the Khan Academy model and extensivelyuse interactive environments to help researchers and others learn strategiesfor Harnessing the Data Revolution. These efforts will especially benefitpeople who do not have access to top-tier computational resources, such asthose at small institutions and interested non-specialists.The FastTract Project will create a sustainable cyberinfrastructure (CI)system and associated community of practice that enable exploratory scientificvisualization of large (gigapixel+) astronomical images over the Web. The CIwill build on the NSF-funded AAS WorldWide Telescope (WWT) software system.The research team will extend WWT's image tiling architecture to work withFITS data files, develop the tooling needed for easy creation of such tiles,and produce workflows and tutorials allowing the broad astronomical communityto take ownership of the infrastructure. The team will work with threeNSF-funded partners on specific science applications. Annual workshops willnucleate a core group of project user-contributors and ensure that developmenteffort meets the needs of the broader community. The project will adopt bestpractices in the open-source, open-development paradigm, laying the groundworkfor FastTract infrastructure to be leveraged by non-astronomical communitiesfacing similar data challenges. In parallel, the team will distill itsexpertise to create the Little Big Data University (LBDU), an onlineprofessional development resource aimed at domain scientists who do not intendto become CI specialists. These scientists will learn core strategies forcoping with ever-larger data sets through the empirically successful KhanAcademy model. Telemetry analysis and focus groups will guide curriculumdesign and assess efficacy.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Data: NSCI: HDR: GeoSCIFramework: Scalable Real-Time Streaming Analytics and Machine Learning for Geoscience and Hazards Research,OAC,1835692,Ivan Rodero,ivan.rodero@utah.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","This project develops a real-time processing system capable of handling a large mix of sensor observations. The focus of this system is automation of the detection of natural hazard events using machine learning, as the events are occurring. A four-organization collaboration (UNAVCO, University of Colorado, University of Oregon, and Rutgers University) develops a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research. This work will support rapid analysis and understanding of data associated with hazardous events (earthquakes, volcanic eruptions, tsunamis). This project uses a collaboration between computer scientists and geoscientists to develop a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research. It focuses on the aggregation and integration of a large number of data streams into a coherent system that supports analysis of the data streams in real-time. The framework will offer machine-learning-based tools designed to detect signals of events, such as earthquakes and tsunamis, that might only be detectable when looking at a broad selection of observational inputs. The architecture sets up a fast data pipeline by combining a group of open source components that make big data applications viable and easier to develop. Data sources for the project draw primarily upon the 1500+ sensors from the EarthScope networks currently managed by UNAVCO and the Incorporated Research Institutions for Seismology (IRIS), as well as the Ocean Observatories Initiative (OOI) cabled array data managed by Rutgers University. Machine learning (ML) algorithms will be researched and applied to the tsunami and earthquake use cases. Initially, the project plans to employ an advanced convolutional neural network method in a multi-data environment. The method has only been applied to seismic waveforms, so the project will explore extending the method to a multi-data environment. The approach is expected to be extensible beyond detection and characterization of earthquakes to include the onset of other geophysical signals such as slow-slip events or magmatic intrusion, expanding the potential for new scientific discoveries. The framework is applied to use cases in the Cascadia subduction zone and Yellowstone: these locations combine the expertise of the science team with locations where EarthScope and OOI have the greatest concentration of instruments. The architecture will be transportable and scalable, running in a Docker environment on laptops, local clusters and the cloud. Integral to the project will be development, documentation and training using collaborative online resources such as GitLab and Jupyter Notebooks, and utilizing NSF XSEDE resources to make larger datasets and computational resources more widely available.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program and Division of Earth Sciences within the NSF Directorate for Geosciences, the Big Data Science and Engineering Program within the Directorate for Computer and Information Science and Engineering, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: HDR Elements: Software for a new machine learning based parameterization of moist convection for improved climate and weather prediction using deep learning,OAC,1835769,Pierre Gentine,pg2328@columbia.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","This project targets a difficult problem in weather and climate prediction -- the representation of convection. Accurate representation of convection is important, since a majority of current model predictions depend on it. Unraveling the physics involved in convective conditions, clouds and aerosols may take years of modeling to fully understand; however, a set of machine learning techniques, known as ""neural net techniques"", may provide enhanced predictability in the interim, and this project explores their potential.The project develops a Python library enabling the use of machine learning (artificial neural networks) in a broad range of science domains. The focus is on integration of convection and cloud formation within larger-scale climate models, with the Community Earth System Model (CESM) as an initial target. The project develops a new set of machine learning climate model parameterizations to reduce uncertainty in weather and climate predictions. The neural networks will be trained on high-fidelity simulations that explicitly resolve convection. Two types of high-resolution simulations will be used for training the neural networks: 1) an augmented super-parameterized simulation, and 2) a full Global Cloud Resolving Model (GCRM) simulation based on the ICOsahedral Non-hydrostatic (ICON) modelling frameworks provided by the Max Planck Institute, using initial 5km horizontal resolution. The effort has the potential to increase understanding of convection dynamics and processes across scales, and could potentially be implemented to address other scale problems as well, where it is too computationally costly or impractical to represent processes occurring at much finer scales than the main grid resolution.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Cyber Infrastructure for Shared Algorithmic and Experimental Research in Online Learning,OAC,1931523,Neil Heffernan,nth@wpi.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","This project, RAILKaM, will create new technology that will enable twenty researchers during the grant period to run large-scale field experiments where they study basic principles in education and educational psychology in the context of both K-12 mathematics learning and university Massive Online Open Courses (MOOCs). The experiments will be delivered through adaptive learning technology embedded in learning systems already being used by over 100,000 K-12 students and hundreds of thousands of MOOC learners each year. RAILKaM will also support 75 data scientists in conducting analyses on student data after the fact, using carefully redacted datasets that protect student privacy. In facilitating high-power, replicable experiments with diverse student populations and extensive measurement, this infrastructure increases the efficiency and ease of conducting high-quality educational research in online learning environments, bringing 21st-century research methods to education for the long-term betterment of learner outcomes.This project, RAILKaM, will support researchers in more easily running scaled, highly instrumented studies on education and educational psychology, both in K-12 and university Massive Online Open Courses (MOOCs). RAILKaM will leverage ASSISTments, an online learning platform for middle school mathematics homework and classwork used by more than 100,000 students each year. In addition, RAILKaM will build functionality atop the ASSISTments platform so that educational experiments involving scaffolded problem-solving can be easily built into MOOC courses. ASSISTments will use open source APIs to integrate with MOOCs offered by the University of Pennsylvania, branching capacity for investigation to higher education while enabling richer student interactions and data collection than is typically feasible in MOOC courses. These capacities will enable researchers to run online field experiments to test interventions designed to increase student learning and engagement with a focus on how adaptive learning experiences can be optimized. These experiments will be augmented by rich data collection on learners, extending MOOC log data and ASSISTments data with several indicators of learning and engagement not previously available for research at scale. This project will develop the software infrastructure necessary to conduct experiments and collect enriched data, as well as the social infrastructure necessary to select and refine study ideas while maintaining instructor control over the activities that students experience. The combined software and social infrastructure will enable us to engage with researchers who are interested in these issues but who currently lack the infrastructure, technical capacity, or access to learners necessary to conduct high-powered or complex randomized controlled trials. This infrastructure will help these researchers to improve scientific understanding of the principles of human learning, providing a unique shared resource for learning scientists that will have considerable potential for broader impact.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Collaborative Research: Frameworks: Multiphase Fluid-Structure Interaction Software Infrastructure to Enable Applications in Medicine, Biology, and Engineering",OAC,1931372,Neelesh Patankar,n-patankar@northwestern.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Physical systems in which fluid flows interact with immersed structures are found in a wide range of areas of science and engineering. Such fluid-structure interactions are ubiquitous in biological systems, including blood flow in the heart, the ingestion of food, and mucus transport in the lung. Fluid-structure interaction is also a crucial aspect of new approaches to energy harvesting, such as wave-energy converters that extract energy from the motion of sea or ocean waves, and in advanced approaches to manufacturing, such as 3D printing. This award supports the development of an advanced computer simulation infrastructure for modeling this full range of application areas. Computer models advanced by this project could ultimately lead to improved diagnostics and treatments for human disease, optimized designs of novel approaches to renewable energy, and reduced manufacturing costs through improved production times in 3D printing.This project aims to enhance the IBAMR computer modeling and simulation infrastructure that provides advanced implementations of the immersed boundary (IB) method and its extensions with support for adaptive mesh refinement (AMR). IBAMR is designed to simulate large-scale fluid-structure interaction models on distributed memory-parallel systems. Most current IBAMR models assume that the properties of the fluid are uniform, but many physical systems involve multiphase fluid models with inhomogeneous properties, such as air-water interfaces or the complex fluid environments of biological systems. This project aims to extend recently developed support in IBAMR for treating multiphase flows by improving the accuracy and efficiency of IBAMR's treatment of multiphase Newtonian flows, and also by extending this multiphase flow modeling capability to treat multiphase complex (polymeric) fluid flows, which are commonly encountered in biological systems, and to treat reacting flows with complex chemistry, which are relevant to models of combustion, astrophysics, and additive manufacturing using stereolithography (3D printing). This project also aims to re-engineer IBAMR for massive parallelism, so that it may effectively use very large computational resources in service of applications that require very high fidelity. The project will also develop modules that will facilitate the use of image-derived geometries, and it will develop novel fluid-structure coupling schemes that will facilitate the use of independent fluid and solid solvers. These capabilities are motivated within this project by models of cardiac, gastrointestinal, and lung physiology; renewable energy; and advanced manufacturing. This software will be used in courses developed by the members of the project team. The project also aims to grow the community of IBAMR users by enhancing project documentation and training materials, hosting user group meetings, and offering short courses.This award by the NSF Office of Advanced Cyberinfrastructure is co funded by the Division of Civil, Mechanical, and Manufacturing Innovation to provide enabling tools to advance potentially transformative fundamental research, particularly in biomechanics and mechanobiology.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF),OAC,1835560,Jeremy Palmer,jcpalmer@uh.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: A Deep Neural Network-based Drone (UAS) Sensing System for 3D Crop Structure Assessment,OAC,2104032,Guoyu Lu,guoyulu62@gmail.com,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","This project develops a 3D reconstruction sensing system that can be installed on unmanned aerial systems (UAS), to be used by agricultural researchers, growers, and service providers to assess crop growth. Applying Artificial Intelligence (AI) technology for large scale agriculture reconstruction applications, the sensing system would be able to estimate crop structure for a large coverage area at a much lower cost than current standards that rely on light detection and ranging (LiDAR). The project would develop and refine a deep neural network-based 3D assessment workflow, based solely on a low cost and lightweight 2D LiDAR and color camera configuration. Researchers, growers, and service providers would be able to extract detailed crop structure and forecast yields, based on a 3D time series of crop growth. The technology would provide a less expensive alternative to the current 3D LiDAR sensor approach, and the sensing system could also be applied to related areas such as high-throughput phenotyping and variation estimation of general terrestrial vegetation. Outreach and extension activities are included, to deliver research outcomes to the stakeholders, including agricultural researchers, growers and service providers. PhD students, undergraduates, and high school students will be trained through this project, including a summer activity training high school students through the Rochester Institute of Technology Imaging Science High School Summer Intern Program.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Biological Infrastructure within the NSF Biosciences Directorate, and by the Division of Information and Intelligent Systems within the NSF Computer and Information Science and Engineering Directorate.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative research: Frameworks: The Einstein Toolkit ecosystem: Enabling fundamental research in the era of multi-messenger astrophysics,OAC,2004044,Yosef Zlochower,yosef@astro.rit.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","A team of experts from five institutions (University of Illinois Urbana-Champaign, Georgia Institute of Technology, Rochester Institute of Technology, Louisiana State University, and West Virginia University) are collaborating on further development of the Einstein Toolkit, a community-driven, open-source cyberinfrastructure ecosystem providing computational tools supporting research in computational astrophysics, gravitational physics, and fundamental science. The new tools address current and future challenges in gravitational wave source modeling, improve the scalability of the code base, and support an expanded science and user community around the Einstein Toolkit.The Einstein Toolkit is a community-driven suite of research-grade Python codes for performing astrophysics and gravitational wave calculations. The code is open-source, accessible via Conda (an open source package management system) and represents a long-term investment by NSF in providing such computational infrastructure. The software is designed to simulate compact binary stars as sources of gravitational waves. This project focuses on the sustainability of the Einstein Toolkit; specific research efforts center around the development of three new software capabilities for the toolkit: ? CarpetX -- a new mesh refinement driver and interface between AMReX, a software framework containing the functionality to write massively parallel block-structured adaptive mesh refinement (AMR) code, and Cactus, a framework for building a variety of computing applications in science and engineering;? NRPy+ -- a user-friendly code generator based on Python; and ? Canuda -- a new physics library to probe fundamental physics. Integration of graphics processing units (GPUs) will incorporate modern heterogeneous computing devices into the system and will enhance the capability of the toolkit. The end product is sustainable through integration into the Einstein Toolkit, yet also includes an active community maintaining and enhancing the foundational components. Broader impacts are enhanced through training, documentation and a support infrastructure that reduces the barrier to adoption by the community. The team is also creating a science portal with additional educational and showcase resources. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the National Science Foundation's Big Idea activities in Windows on the Universe (WoU).This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: HDR: Building the Twenty-First Century Citizen Science Framework to Enable Scientific Discovery Across Disciplines,OAC,1835574,Gregory Newman,Gregory.Newman@ColoState.Edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","A team of experts from five institutions (University of Minnesota, Adler Planetarium, University of Wyoming, Colorado State University, and UC San Diego) links field-based and online analysis capabilities to support citizen science, focusing on three research areas (cell biology, ecology, and astronomy). The project builds on Zooniverse and CitSci.org, leverages the NSF Science Gateways Community Institute, and enhances the quality of citizen science and the experience of its participants.This project creates an integrated Citizen Science Cyberinfrastructure (CSCI) framework that expands the capacity of research communities across several disciplines to use citizen science as a suitable and sustainable research methodology. CSCI produces three improvements to the infrastructure for citizen science already provided by Zooniverse and CitSci.org: - Combining Modes - connecting the process of data collection and analysis; - Smart Assignment - improving the assignment of tasks during analysis; and - New Data Models - exploring the Data-as-Subject model. By treating time series data as data, this model removes the need to create images for classification and facilitates more complex workflows. These improvements are motivated and investigated through three distinct scientific cases: - Biomedicine (3D Morphology of Cell Nucleus). Currently, Zooniverse 'Etch-a-Cell' volunteers provide annotations of cellular components in images from high-resolution microscopy, where a single cell provides a stack containing thousands of sliced images. The Smart Task Assignment capability incorporates this information, so volunteers are not shown each image in a stack where machines or other volunteers have already evaluated some subset of data. - Ecology (Identifying Individual Animals). When monitoring wide-ranging wildlife populations, identification of individual animals is needed for robust estimates of population sizes and trends. This use case combines field collection and data analysis with deep learning to improve results. - Astronomy (Characterizing Lightcurves). Astronomical time series data reveal a variety of behaviors, such as stellar flares or planetary transits. The existing Zooniverse data model requires classification of individual images before aggregation of results and transformation back to refer to the original data. By using the Data-as-Subject model and the Smart Task Assignment capability, volunteers will be able to scan through the entire time series in a machine-aided manner to determine specific light curve characteristics.The team explores the use of recurrent neural networks (RNNs) to determine automated learning architectures best suited to the projects. Of particular interest is how the degree to which neighboring subjects are coupled affects performance. The integration of existing tools, which is based on application programming interfaces (APIs), also facilitates further tool integration. The effort creates a citizen science framework that directly advances knowledge for three science use cases in biomedicine, ecology, and astronomy, and combines field-collected data with data analysis. This has the ability to solve key problems in the individual applications, as well as benefiting the research of the dozens of projects on the Zooniverse platform. It provides benefits to researchers using citizen scientists, and to the nearly 1.6 million citizen scientists themselves.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Research on Learning in Formal and Informal Settings, within the NSF Directorate for Education and Human Resources.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: EdgeVPN: Seamless Secure Virtual Networking for Edge and Fog Computing,OAC,2004441,Renato Figueiredo,renato@acis.ufl.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","Edge computing encompasses a variety of technologies that are poised to enable new applications across the Internet that support data capture, storage, processing and communication near the edge of the Internet. Edge computing environments pose new challenges, as devices are heterogeneous, widely distributed geographically, and physically closer to end users, such as mobile and Internet-of-Things (IoT) devices. This project develops EdgeVPN, a software element that addresses a fundamental challenge of networking for edge computing applications: establishing Virtual Private Networks (VPNs) to logically interconnect edge devices, while preserving privacy and integrity of data as it flows through Internet links. More specifically, the EdgeVPN software developed in this project addresses technical challenges in creating virtual networks that self-organize into scalable, resilient systems that can significantly lower the barrier to entry to deploying a private communication fabric in support of existing and future edge applications. There are a wide range of applications that are poised to benefit from EdgeVPN; in particular, this project is motivated by use cases in ecological monitoring and forecasting for freshwater lakes and reservoirs, situational awareness and command-and-control in defense applications, and smart and connected cities. Because EdgeVPN is open-source and freely available to the public, the software will promote progress of science and benefit society at large by contributing to the set of tools available to researchers, developers and practitioners to catalyze innovation and future applications in edge computing.Edge computing applications need to be deployed across multiple network providers, and harness low-latency, high-throughput processing of streams of data from large numbers of distributed IoT devices. Achieving this goal will demand not only advances in the underlying physical network, but also require a trustworthy communication fabric that is easy to use, and operates atop the existing Internet without requiring changes to the infrastructure. The EdgeVPN open-source software developed in this project is an overlay virtual network that allows seamless private networking among groups of edge computing resources, as well as cloud resources. EdgeVPN is novel in how it integrates: 1) a flexible group management and messaging service to create and manage peer-to-peer VPN tunnels grouping devices distributed across the Internet, 2) a scalable structured overlay network topology supporting primitives for unicast, multicast and broadcast, 3) software-defined networking (SDN) as the control plane to support message routing through the peer-to-peer data path, and 4) network virtualization and integration with virtualized compute/storage endpoints with Docker containers to allow existing Internet applications to work unmodified. EdgeVPN self-organizes an overlay topology of tunnels that enables encrypted, authenticated communication among edge devices connected across disparate providers in the Internet, possibly subject to mobility and constraints imposed by firewalls and Network Address Translation, NATs. It builds upon standard SDN interfaces to implement packet manipulation primitives for virtualization supporting the ubiquitous Ethernet and IP-layer protocols.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements:Collaborative Proposal: A task-based code for multiphysics problems in astrophysics at exascale,OAC,1931266,Mark Scheel,scheel@tapir.caltech.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","Upcoming computers will run at exascale, over a hundred times more powerful than typical machines of today. Many algorithms used in current codes will not be able to take advantage of these new machines. The researchers will complete the development of an open-source community code for multi-scale, multi-physics problems in astrophysics and gravitational physics. The code uses transformative algorithms to reach the exascale. The techniques can be applied across discipline boundaries in fluid dynamics, geoscience, plasma physics and nuclear physics and engineering. The development of this new code has been driven by the current deployment of gravitational wave detectors such as LIGO. To fully understand and analyze the signals and waveforms measured with such detectors, it is essential that accurate, robust, and efficient computational tools be available for solving the dynamical Einstein equations over very long time scales. The recent detection of the merger of a neutron star-neutron star merger by LIGO and by a host of electromagnetic telescopes has ushered The extreme energy densities of matter and radiation and the highly dynamic spacetimes of these events probe fundamental physics inaccessible to terrestrial experiments. The new code will be made available as open-source community cyberinfrastructure. The researchers will reach out to other communities within astrophysics (e.g., star formation, space plasma physics) and across discipline boundaries to fluid dynamics, geoscience, plasma physics, nuclear engineering etc. Early-career researchers trained in these techniques are in great demand, both in academia and as highly-skilled members of the industrial STEM workforce. Undergraduates will participate in the research by producing visualizations.The new code uses discontinuous Galerkin methods and task-based parallelism to accomplish its desired goals. This framework will allow the multiphysics applications to be treated both accurately and efficiently on the new architectures of petascale and exascale machines. The code is designed to scale to over a million cores for efficient exploration of the parameter space of potential sources and allowed physics, and for the high-fidelity predictions needed to realize the promise of multi-messenger astrophysics. The code will allow astrophysicists to explore the mechanisms driving core-collapse supernovae and the properties of stellar remnants, to understand electromagnetic transients and gravitational-wave phenomena in compact objects, and to reveal the dense matter equation of state. The two key algorithmic innovations in the code, the discontinuous Galerkin method coupled with task-based parallelism, promise revolutionary impact in other fields relying on numerical solution of partial differential equations at the exascale.This project advances the objectives of ""Windows on the Universe: the Era of Multi-Messenger Astrophysics"", one of the 10 Big Ideas for Future NSF Investments. This project advances also the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Physics and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: NSCI Framework: Software: SCALE-MS - Scalable Adaptive Large Ensembles of Molecular Simulations,OAC,1835449,Matteo Turilli,matteo.turilli@rutgers.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Molecular simulations are becoming important tools in understanding nanoscale processes in science and engineering. Such processes include the motions of proteins and nucleic acids that will enable design of better drugs, the interactions of liquids and metals in photovoltaic and catalytic applications, and the behavior of complex polymers used in industrial materials. Although national cyberinfrastructure investments are increasing raw computational power, the molecular timescales that scientists can simulate are not increasing proportionately. This means that most simulations are significantly shorter than the physical processes they are designed to study. Fortunately, many researchers have developed powerful algorithms that combine multiple simulations to overcome this molecular timescale problem, but these algorithms can still be very difficult to use effectively. This project, called SCALE-MS, will develop computing tools to simplify the process of writing algorithms that use large collections of molecular simulations to simulate the long timescales needed for scientific and industrial understanding. These tools will make it much simpler to have simulations interact adaptively, so simulation results can automatically guide the creation and running of new simulations. By making these complex multi-simulation algorithms easier to create and run, this project will enable users to run existing methods in computational molecular science more easily and make it possible for researchers to create and test new, even more powerful, methods for molecular modeling. This project also brings together researchers from biophysics, chemical engineering, and materials science, combining expertise from multiple simulation fields to develop important new ensemble simulation algorithms. This adaptive ensemble framework will enable communities of molecular simulation users in chemistry, chemical engineering, materials science, and biophysics to more easily exchange advanced methods and best practices. Many aspects of this framework can also be applied to aid societal problems requiring modeling in other domains, such as climate and earthquake modeling and prediction.This project addresses a fundamental need across molecular simulation communities from chemistry to biophysics to materials science: the ability to easily simulate long-timescale phenomena and slowly equilibrating ensembles. Researchers are increasingly developing high-level parallel algorithms that utilize simulation ensembles, loosely coupled molecular simulations that exchange information on a slower time scale than standard parallel computing techniques. However, most existing molecular simulation software cannot express ensemble simulation algorithms in a general manner and execute them at scale. There is thus a need for (i) the ability to express ensemble-based methods in a simple, easy- to-use manner that is agnostic of the underlying simulation code, (ii) support for adaptive and asynchronous execution of ensembles, and (iii) a scalable runtime system that encapsulates the complexity of executing and managing jobs seamlessly on different resources. The project will develop an extensible framework, including a simple high-level API and a sophisticated runtime system, to meet these design objectives on NSF?s production cyberinfrastructure. A key element of this design is the ability to specify ensemble-based patterns of work- and data-flow in a fashion independent of the challenges and complexity of the runtime management of the ensembles. This project will develop a framework consisting of a simple adaptive ensemble API with an underlying runtime platform that enables expression of ensemble simulation methods in a fashion agnostic of the underlying simulation code. This will facilitate design of new ensemble-based methods by the community and enable scientific end users to simply encode complex adaptive workflows. This approach separates the complexity of compute job management from the expression of sophisticated methods. The framework will support adaptive and asynchronous execution of ensembles, removing synchronization blocks that have restricted peta- and exa-scaling of simulation methods. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Chemistry within the NSF Directorate for Mathematical and Physical Sciences and the Division of Chemical, Bioengineering, Environmental, and Transport Systems within the NSF Directorate for Engineering.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: NSCI Framework: Software: SCALE-MS - Scalable Adaptive Large Ensembles of Molecular Simulations,OAC,1835607,Kristen Fichthorn,fichthorn@psu.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Molecular simulations are becoming important tools in understanding nanoscale processes in science and engineering. Such processes include the motions of proteins and nucleic acids that will enable design of better drugs, the interactions of liquids and metals in photovoltaic and catalytic applications, and the behavior of complex polymers used in industrial materials. Although national cyberinfrastructure investments are increasing raw computational power, the molecular timescales that scientists can simulate are not increasing proportionately. This means that most simulations are significantly shorter than the physical processes they are designed to study. Fortunately, many researchers have developed powerful algorithms that combine multiple simulations to overcome this molecular timescale problem, but these algorithms can still be very difficult to use effectively. This project, called SCALE-MS, will develop computing tools to simplify the process of writing algorithms that use large collections of molecular simulations to simulate the long timescales needed for scientific and industrial understanding. These tools will make it much simpler to have simulations interact adaptively, so simulation results can automatically guide the creation and running of new simulations. By making these complex multi-simulation algorithms easier to create and run, this project will enable users to run existing methods in computational molecular science more easily and make it possible for researchers to create and test new, even more powerful, methods for molecular modeling. This project also brings together researchers from biophysics, chemical engineering, and materials science, combining expertise from multiple simulation fields to develop important new ensemble simulation algorithms. This adaptive ensemble framework will enable communities of molecular simulation users in chemistry, chemical engineering, materials science, and biophysics to more easily exchange advanced methods and best practices. Many aspects of this framework can also be applied to aid societal problems requiring modeling in other domains, such as climate and earthquake modeling and prediction.This project addresses a fundamental need across molecular simulation communities from chemistry to biophysics to materials science: the ability to easily simulate long-timescale phenomena and slowly equilibrating ensembles. Researchers are increasingly developing high-level parallel algorithms that utilize simulation ensembles, loosely coupled molecular simulations that exchange information on a slower time scale than standard parallel computing techniques. However, most existing molecular simulation software cannot express ensemble simulation algorithms in a general manner and execute them at scale. There is thus a need for (i) the ability to express ensemble-based methods in a simple, easy- to-use manner that is agnostic of the underlying simulation code, (ii) support for adaptive and asynchronous execution of ensembles, and (iii) a scalable runtime system that encapsulates the complexity of executing and managing jobs seamlessly on different resources. The project will develop an extensible framework, including a simple high-level API and a sophisticated runtime system, to meet these design objectives on NSF?s production cyberinfrastructure. A key element of this design is the ability to specify ensemble-based patterns of work- and data-flow in a fashion independent of the challenges and complexity of the runtime management of the ensembles. This project will develop a framework consisting of a simple adaptive ensemble API with an underlying runtime platform that enables expression of ensemble simulation methods in a fashion agnostic of the underlying simulation code. This will facilitate design of new ensemble-based methods by the community and enable scientific end users to simply encode complex adaptive workflows. This approach separates the complexity of compute job management from the expression of sophisticated methods. The framework will support adaptive and asynchronous execution of ensembles, removing synchronization blocks that have restricted peta- and exa-scaling of simulation methods. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Chemistry within the NSF Directorate for Mathematical and Physical Sciences and the Division of Chemical, Bioengineering, Environmental, and Transport Systems within the NSF Directorate for Engineering.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Software: NCSI: HDR: Building An HPC/HTC Infrastructure For The Synthesis And Analysis Of Current And Future Cosmic Microwave Background Datasets,OAC,1835536,Colin Bischoff,colin.bischoff@uc.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","The photons created in the Big Bang have experienced the entire history of the Universe, and every step in the evolution of the Universe has left its mark on their statistical properties. Observations of these photons have the potential to unlock the secrets of fundamental physics and cosmology, and to provide key insights into the formation and evolution of cosmic structures such as galaxies and galaxy clusters. Since the traces of these processes are so faint, one must gather enormous datasets to be able to detect them above the unavoidable instrumental and environmental noise. This in turn means that one must be able to use the most powerful computing resources available to be able to process the volume of data. These computing resources include both highly localized supercomputers and widely distributed grid and cloud systems. The PI and Co-Is will develop a common computing infrastructure able to take advantage of both types of resource, and demonstrate its suitability for ongoing and planned experiments by adapting the analysis pipelines of four leading Big Bang observatories to run within it. In addition to enabling the full scientific exploitation of these extraordinarily rich data sets, the investigators will mentor students engaged in this research and run summer schools in applied supercomputing.This project seeks to enable the detection of the faintest signals in Cosmic Microwave Background radiation, and in particular the pattern of peaks and troughs in the angular power spectra of its polarization field. In order to obtain these spectra one must first reduce the raw observations to maps of the sky in a way the preserve the correlations in the signal and characterizes the correlation in the noise. While the algorithms to perform this reduction are well-understood, applying them to data sets with quadrillions to quintillions of observations is a very serious computational challenge. The computational resources available to the project to address this include both high performance and high throughput computing systems, and one will need to take advantage of both of them. This project will develop a joint high performance/high throughput computational framework, and deploy within it analysis pipelines currently being fielded by the ongoing Atacama Cosmology Telescope, BICEP/Keck Array, POLARBEAR, and South Pole Telescope experiments. By doing so one will also demonstrate the frameworks efficacy for the planned Simons Observatory and CMB-S4 experiments.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science,OAC,2210266,Geoffrey Fox,gcfexchange@gmail.com,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth. Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn. The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above. Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities. Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks. Compositions of these diverse capabilities are rare. Furthermore, many researchers who study networks are not computer scientists. As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming. The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use. What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science. CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software. CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks. The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program. It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions. CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science. Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Frameworks: Collaborative Research: Extensible and Community-Driven Thermodynamics, Transport, and Chemical Kinetics Modeling with Cantera: Expanding to Diverse Scientific Domains",OAC,1931391,Steven Barrett,sbarrett@mit.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Modeling and simulation play key enabling roles in aiding and accelerating discovery connecting to energy and chemical research. In applications such as energy storage and conversion, atmospheric chemistry, and catalytic chemical processing, modeling and simulation software helps facilitate technological advances. However, in recent years the available software has not kept pace with the increasing chemical complexity and interdisciplinarity of advanced technology solutions. This project addresses this gap by developing and promoting new state-of-the-art modeling capabilities for diverse scientific fields in the existing Cantera software platform. Cantera is an extensible, open-source framework that enables researchers to study basic science and support new technology development and enables teachers to demonstrate concepts and applications in their classrooms. This project extends Cantera to provide new cross-disciplinary research capabilities and provides a foundation for further community-driven improvements to the Cantera framework. Simultaneous development of the open-source platform and outreach to new user communities will facilitate both fundamental scientific insight and practical technology design and analysis, train the next generation of researchers in both software-development best practices and scientific knowledge, and generate reusable and open educational materials. In addition to work on the framework development, the project includes training of graduate students as well as education, outreach and scientific community engagement activities.This work will develop the Cantera software platform in service of three objectives: (i) extend Cantera?s scientific capabilities to support the development of transformative technologies; (ii) expand Cantera?s user base in fields including electrochemistry, heterogeneous catalysis, and atmospheric chemistry; and (iii) broaden participation in the software?s development and management to improve Cantera?s sustainability and usability. These will be achieved by developing new scientific modeling capabilities, conducting outreach to new user communities, and improving Cantera?s architecture and software development practices. The new scientific modeling capabilities will focus on four content areas: thermodynamics, chemical kinetics, transport, and multi-phase capabilities. Outreach activities, including publications and presentations, conference workshops, and domain-specific software toolkits with examples to demonstrate Cantera operation and functionality, will engage new and existing communities. The project will establish a scientific advisory board, consisting of experts from diverse fields and backgrounds who will help guide software development and outreach. Finally, the architectural and software engineering changes in this work will improve extensibility and interoperability and implement advanced numerical algorithms to enable the application of Cantera to new types of problems. These changes will also make it easier for users to contribute to Cantera, ensure software correctness, and provide new ways of accessing Cantera?s functionality. The resulting software framework will aid in scientific discovery and development of key enabling technologies with broad societal impacts. These impacts include next-generation batteries and fuel cells for clean energy storage and conversion, catalytic and membrane reactors for electrolysis, novel fuel generation, chemical processing, environmentally conscious combustion applications, and understanding and addressing anthropogenic challenges in atmospheric chemistry.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Libra: The Modular Software for Nonadiabatic and Quantum Dynamics,OAC,1931366,Alexey Akimov,alexeyak@buffalo.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","This project endeavors to develop a modular open-source software library (Libra) of reusable nonadiabatic and quantum dynamics (NA/QD) algorithms and methods (elements). Sustained progress in scientific endeavors in solar energy, functional, and nanoscale material sciences requires advanced methods and software components that can be used to model the complex dynamics of excited states, including charge and energy transfer. Providing researchers with advanced expert-developed methods for modeling these processes via modular software components can enable new breakthroughs in theoretical and computational chemistry, computationally-enabled and data-driven material sciences, and can foster further exciting innovations in the solar energy materials domain and beyond. The Libra software will enable accurate, reliable, and efficient modeling of excited states dynamics in atomistic systems and should be suitable for the rapid and systematic development of new, improved modeling approaches. The project will contribute to a broader specialized scientific training and will support education and diversity.Over the course of this project the PI will enhance and extend the Libra code with modern nonadiabatic and quantum dynamics methodologies, thereby enabling accurate and reliable modeling of electron and energy transfer dynamics in solar energy materials. The interface of the Libra code with the DFTB+ package will enable modeling excited states dynamics in molecular and periodic systems with 1000+ atoms, thus providing access to new classes of materials that can be studied computationally. The resulting software will enable modeling new types of processes which were computationally-prohibitive to study, such as photoinduced reorganization or exciton trapping in nanoscale systems. The software will enable accounting for the many-body effects in nonadiabatic dynamics, improving the reliability of computational predictions in solar energy materials studies. The software and tools resulting from this project will contribute to fundamental research and rational discovery of novel photovoltaic materials. The large research community of NA/QD users will benefit from the new, enhanced software and online educational materials created in this project. Multiple research groups that develop in-house solutions that did not gain much attention will directly benefit from the Libra library, which will disseminate the expert groups' solutions in a modular, easy-to-use way, and will facilitate their adoption and re-use by others. The project will foster collaborations and help integrate the present-day research efforts and the best software development practices into the computational and materials research communities. Scholars of various levels will be educated during workshops on excited state dynamics.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Production quality Ecosystem for Programming and Executing eXtreme-scale Applications (EPEXA),OAC,1931384,George Bosilca,bosilca@icl.utk.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","A team of researchers from three institutions will work collaboratively to design and develop a software framework that implements high-performance methods for irregular and dynamic computations that are poorly supported by current programming paradigms. The framework, titled EPEXA (Ecosystem for Programming and Executing eXtreme Applications), will create a production-quality, general-purpose, community-supported, open-source software ecosystem that attacks the twin challenges of programmer productivity and portable performance for advanced scientific applications on modern high-performance computers. Employing science-driven co-design, the team will transition into production a successful research prototype of a new programming model and accelerate the growth of the community of computer scientists and domain scientists employing these tools for their research. The project bridges the so-called ""valley of death"" between successful proofs of principle to an implementation with enough quality, performance, and community support to motivate application scientists and other researchers to adopt the tools and invest their own effort into the community. In addition to work on the framework development, the project includes training of postdoctoral scholars, graduate and undergraduate students as well as education, outreach and scientific community engagement activities.Specifically, the new powerful data-flow programming model and associated parallel runtime directly address multiple challenges faced by scientists as they attempt to employ rapidly changing computer technologies including current massively-parallel, hybrid, and many-core systems. Both data-intensive and compute-intensive applications are enabled in part by the general programming model and through the ability to target multiple backends or runtime systems. Also enabled is the creation by domain scientists of new domain-specific languages (DSLs) for both shared and distributed-memory computers. EPEXA contributes to the design and development of state-of-the-art software environments that leverage the National Science Foundation's investments in cyberinfrastructure to enable scientific discovery across all disciplines.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Element: Software: Data-Driven Auto-Adaptive Classification of Cryospheric Signatures as Informants for Ice-Dynamic Models,OAC,1835256,Ute Herzfeld,uch5678@gmail.com,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","The objective of this project is to develop and automatize a connection between Earth observation data and numerical models of Earth system processes. Both collection of Earth observation data from satellites and modeling of physical processes have seen unprecedented advances in recent years. However, data-derived information is not used to inform modeling in a systematic and automated fashion. This creates a bottleneck that is growing with the data revolution. The award supports the development of a software cyberinfrastructure aimed at reducing this bottleneck by automating classification and parameterization. The proposed cyberinfrastructure will be implemented in a general and transportable way, but its functionality will be demonstrated by addressing a concrete open problem in glaciology: the acceleration during a glacier surge, which is characterized by an increase to 100-200 times the flow normal velocity. Glacial accelerations are important, because they constitute the largest uncertainty in sea-level-rise assessment. The team, from University of Colorado will combine their expertise of field work and data collection with their background in software development to generate a high-quality application that will be made available under an open source license to the broader scientific community. The project will engage graduate and undergraduate students in the software development, thus contributing to the development of future generations of scientists and cyberinfrastructure professionals. The results will also be used to inform activities in K-12 schools and other outreach efforts.The proposed data-driven auto-adaptive classification system is expected to provide a tool to the Earth Sciences community that allows it to employ the unprecedented detail in satellite image and SAR data (WordView, Sentinel-1 and Sentinel-2) necessary to extract information on surface properties and processes that were previously indiscernible. In that the classification will automatically adapt to changing conditions in time and space, it will provide a consistent parameterization of spatial processes that can be used to drive numerical simulations of Earth system processes. Thus, a direct connection will be established between data analysis and modeling. In a pilot study, the data-modeling connection will be demonstrated through classification of crevasse patterns, which result from deformation, and optimization of the basal sliding parameter in a three-dimensional model of a glacier surge. Hence the pilot study will advance understanding of ice dynamics. A second application is a sea-ice classification system, aimed to aid in mapping and understanding the changing Arctic sea-ice cover. The planned automated connection between data-driven automated classification and optimization of model parameters is expected to lay the foundation for a transformation of the data-modeling world in Earth sciences, atmospheric and polar sciences. The project will also advance machine learning and spatial statistics through realization of a science-driven approach to computer science and cyberinfrastructure.This award by the NSF Office of Advanced Cyberinfrastructure (OAC) is jointly supported by the Cross-Cutting Program within the NSF Directorate for Geosciences, The OAC Cyberinfrastructure for Emerging Science and Engineering Research (CESER) program and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the OAC.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Transformation-Based High-Performance Computing in Dynamic Languages,OAC,1931577,Andreas Kloeckner,andreask@illinois.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","A key capability in technical computing is the processing of large, regularly-shaped arrays of numbers by a wide variety of different processes. This facility is foundational in, for example, weather prediction, artificial intelligence, and image processing. Correspondingly, modern computing hardware has evolved advanced capabilities for carrying out such computations with high efficiency. Unfortunately, the process of adapting a desired process to a given piece of hardware thus far is costly, laborious, and error-prone. Differences of a factor of 50 in performance between a naive realization and a careful one is the rule, rather than the exception. Loopy, the subject of this project, attacks this problem by using human-guided, automated program rewriting. Loopy has demonstrated application impact in a number of applications ranging from the simulation of natural and engineering phenomena to neuroscience, where it has helped its users achieve higher performance with less effort. The present proposal concerns several important improvements that will contribute to making Loopy more effective and easier to apply, through enlarging the class of programs that Loopy can transform, improving the means by which Loopy represents on-chip communication, and permitting it to realize important basic operations that routinely pose difficulty in efficient implementation. An important component of the effort is making Loopy itself easy to use for its user community, through the realization of an interactive user interface, so that program transformations can be applied with the click of a mouse, rather than by writing computer code. The proposed advances will be demonstrated through a sample workload that is emblematic of many of the computational and software challenges faced in technical computing today.Multidimensional arrays (sometimes called 'tensors') are a foundational data structure for much of scientific computing, with applications ranging from weather prediction to deep learning, to image processing and computational neuroscience. Even the efficient execution of one of the simplest operations on arrays, matrix-matrix multiplication, poses considerable technical challenges on modern computers. Through a polyhedrally-based program transformation tool, the proposed software will provide separation between mathematical intent and the technical challenges of program optimization, allowing each task to be performed by a domain expert. In the proposed project, the PI will develop means for more efficient on-chip communication, code generation for prefix sums, reuse and abstraction in program transformation, increasing the ease of use in transformation discovery and performance analysis, and for expressing array computations in user programs. The PI will validate the proposed techniques through a challenging application with broad applicability. The intellectual merit of the proposed research lies in (1) mapping out and extending the landscape of transformation-based programming from one-off scripts to reusable transform components, (2) the development of a unifying, loop/array-axis-based approach to expressing on-chip communication while reducing redundancy in Loopy?s program representation and transformation, (3) exploring the design space of high-performance languages that establish a close link between execution placement and data placement, (4) the development of an interactive program transform and performance analysis tool, along with the discovery of potential implications for workforce training in high-performance computing, (5) a demonstration that all the developed components can be applied together in a practical and coherent manner. Through graduate and undergraduate teaching as well as mentoring of the students and postdocs supported by this project, the PI contributes to enlarging the talent pool.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF),OAC,1835630,Edward Maginn,ed@nd.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF),OAC,1835593,Eric Jankowski,ericjankowski@boisestate.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Elements: Agricultural Cyber-infrastructure support for Field and Grid Modeling, and Runtime Decision-Making",OAC,2004766,Ratnesh Kumar,rkumar@iastate.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","This project develops science-driven modeling and decision-making algorithms for precision agriculture, with associated cloud-based implementations. This project would increase the potential for more efficient agricultural systems through the development of more accurate and larger scale models, data collection, and ultimately a software infrastructure that farmers could use for real-time monitoring of their crops. The project is derived from scientific questions in sustainable agriculture, sensor networks, decision sciences, data science, and machine learning. The primary innovations are applications of grid-based soil parameters using incomplete data, machine learning-based time series analysis, and optimization problems to estimate the optimal mix of inputs for nutrients and irrigation. These applications are likely to have wide usage among the precision agriculture community. The cyberinfrastructure, MyGeoHub, is a cloud-based service that users can access through web browsers and leverages an existing infrastructure.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Established Program to Stimulate Competitive Research (EPSCoR).This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: FLARE infrastructure for reproducible active learning of Bayesian force fields for ex-machina exascale molecular dynamics,OAC,2003725,Boris Kozinsky,bkoz@seas.harvard.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Much needed progress in technologies for energy storage and conversion relies on our ability to design and understand next-generation key functional materials at the core of these systems. The fundamental physical effects that govern the functions of batteries, catalysts and fuel cells originate at the atomic level. Molecular dynamics simulations are indispensable tools with broad applicability for materials research due to their ability to probe microscopic details of atomic motion and predict thermodynamics, reaction kinetics and ionic diffusivities of many materials. Machine learning approaches are transforming how simulations of complex materials are performed; hence software tools are needed to make this transition faster and smoother. Our main goal is to advance machine learning methods and create software for constructing accurate fast simulation models that contain principled uncertainty of their predictions, which is a highly desirable target in many data science areas beyond atomistic modeling. Principled uncertainty quantification is especially critical for prediction of non-equilibrium dynamics, where rare important events, such as the breaking of bonds or atomic migration, determine the material?s performance but involve atomic configurations that are unlikely to be in a naive unbiased training set. Tools that we aim to develop will accelerate and automate computational research efforts in the fields of catalysis, batteries, thermal coatings, soft structural and functional materials, and actuators, to name a few.Currently available tools for machine learning based force fields are based on non-parametric methods that only provide estimates without uncertainties, require large amounts of training data, and are slow to evaluate for large numbers of atoms of different species. Our goal is to develop community software infrastructure to enable a new paradigm of simulating non-equilibrium dynamics of complex materials, where ML models are automatically trained and dramatically accelerate ab-initio simulations on-the-fly, preserving exact physical symmetries with minimal accuracy loss. We will create and freely disseminate FLARE (Fast Learning of Atomistic Rare Events), a parallelized database-driven automation framework tightly coupling ML model training with high-fidelity DFT computations, using rigorous model uncertainty to guide data acquisition via closed-loop active learning. Specially designed many-body multi-species kernels and tools for systematic hyperparameter optimization will allow the models to be mapped to fast tabulated Bayesian force fields, implemented in the widely used MD software aimed at exascale computing performance. The result will be the ability to perform MD simulations of materials systems of millions of atoms at near-DFT accuracy and with predictive uncertainty. The unique advantages of the proposed infrastructure are (1) the automated training requiring minimal amounts of DFT data, (2) predictions containing principled Bayesian uncertainty, (3) scalable performance of at least 5 orders of magnitude faster than ab-initio molecular dynamics, and (4) ability to record full provenance and reproducibility information of training and prediction workflows.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences and the Division of Chemical, Bioengineering, Environmental and Transport Systems within the NSF Directorate of Engineering.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Collaborative Research: Elements: Development of MuST, A Multiple Scattering Theory based Computational Software for First Principles Approach to Disordered Materials",OAC,1931445,Ka Ming Tam,kmtam@lsu.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","The effect of disorder in materials is of great fundamental and technological interest. Disorder disrupts the periodic arrangement of atoms in perfect materials. It profoundly affects materials properties and can provide a valuable tool in changing and controlling their physical properties. The best-known example is the transistor and other silicon components which are controlled through the introduction of disorder. Quantum mechanical states in semiconductors induced by introducing impurities and disorder can dramatically increase the efficiency of solar cells. There are many other materials of potential technological interest, such as high entropy alloys, dilute magnetic semiconductors, and topological insulators where disorder plays an essential role. Being able to understand, control, and predict the disorder effects in real physical systems is essential for the development of new structural and functional materials for future technological applications. This project involves building computer software that is aimed to enable the study of disorder effects using the principles of quantum mechanics and to accelerate the discovery of materials essential for industry and information technology applications. The creation of a large community of users and developers who will accelerate this process is envisioned. This project supports interdisciplinary training and education of undergraduate and graduate students in the fields of theoretical condensed matter physics and computational materials sciences. The user community of this software will be supported through webinars, discussion groups on social media, and online tutorial materials. This award supports various training and outreach efforts, including an annual ""Beowulf Boot Camp"" and ""Quantum Day"" for high-school students, and undergraduate and graduate research opportunities. The PIs will build upon and unify decades of development of research codes for the first-principles investigation of disordered materials. These codes include the Korringa-Kohn-Rostoker Coherent Potential Approximation, which is the first principles code for the study of random alloys, and the Locally Self-consistent Multiple Scattering code, which can enable the study of extremely large disordered systems from first principles using the largest parallel supercomputers available. Strong disorder effects and Anderson localization have been studied on the model Hamiltonian level using the Typical Medium Dynamical Cluster Approximation (TMDCA). To enable the study the strong disorder effects in real system within the first-principles Locally Self-consistent Multiple Scattering formalism with cluster embedding, the project team will use the typical medium analysis of TMDCA. The software product of this project, MuST, will be made available on GitHub as a self-contained open source package with detailed online documentations. MuST will create a scalable approach for first principles studies of quantum materials that efficiently utilizes petascale and future high-performance computing resources. It will expand the user community by enabling researchers within academia and industry to perform calculations that are presently out of reach for most users. MuST will provide a computational framework for the investigation of phase transitions and electron localization in the presence of disorder in real materials and will also enable the computational study of local chemical correlation effects on the magnetic structure, phase stability, and mechanical properties of high entropy alloys and other disordered structures.This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Collaborative Research: Element: Development of MuST, A Multiple Scattering Theory based Computational Software for First Principles Approach to Disordered Materials",OAC,1931525,Yang Wang,ywg@psc.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","The effect of disorder in materials is of great fundamental and technological interest. Disorder disrupts the periodic arrangement of atoms in perfect materials. It profoundly affects materials properties and can provide a valuable tool in changing and controlling their physical properties. The best-known example is the transistor and other silicon components which are controlled through the introduction of disorder. Quantum mechanical states in semiconductors induced by introducing impurities and disorder can dramatically increase the efficiency of solar cells. There are many other materials of potential technological interest, such as high entropy alloys, dilute magnetic semiconductors, and topological insulators where disorder plays an essential role. Being able to understand, control, and predict the disorder effects in real physical systems is essential for the development of new structural and functional materials for future technological applications. This project involves building computer software that is aimed to enable the study of disorder effects using the principles of quantum mechanics and to accelerate the discovery of materials essential for industry and information technology applications. The creation of a large community of users and developers who will accelerate this process is envisioned. This project supports interdisciplinary training and education of undergraduate and graduate students in the fields of theoretical condensed matter physics and computational materials sciences. The user community of this software will be supported through webinars, discussion groups on social media, and online tutorial materials. This award supports various training and outreach efforts, including an annual ""Beowulf Boot Camp"" and ""Quantum Day"" for high-school students, and undergraduate and graduate research opportunities. The PIs will build upon and unify decades of development of research codes for the first-principles investigation of disordered materials. These codes include the Korringa-Kohn-Rostoker Coherent Potential Approximation, which is the first principles code for the study of random alloys, and the Locally Self-consistent Multiple Scattering code, which can enable the study of extremely large disordered systems from first principles using the largest parallel supercomputers available. Strong disorder effects and Anderson localization have been studied on the model Hamiltonian level using the Typical Medium Dynamical Cluster Approximation (TMDCA). To enable the study the strong disorder effects in real system within the first-principles Locally Self-consistent Multiple Scattering formalism with cluster embedding, the project team will use the typical medium analysis of TMDCA. The software product of this project, MuST, will be made available on GitHub as a self-contained open source package with detailed online documentations. MuST will create a scalable approach for first principles studies of quantum materials that efficiently utilizes petascale and future high-performance computing resources. It will expand the user community by enabling researchers within academia and industry to perform calculations that are presently out of reach for most users. MuST will provide a computational framework for the investigation of phase transitions and electron localization in the presence of disorder in real materials and will also enable the computational study of local chemical correlation effects on the magnetic structure, phase stability, and mechanical properties of high entropy alloys and other disordered structures.This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Collaborative Research:Element: Development of MuST, A Multiple Scattering Theory based Computational Software for First Principles Approach to Disordered Materials",OAC,1931367,Hanna Terletska,Hanna.Terletska@mtsu.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","The effect of disorder in materials is of great fundamental and technological interest. Disorder disrupts the periodic arrangement of atoms in perfect materials. It profoundly affects materials properties and can provide a valuable tool in changing and controlling their physical properties. The best-known example is the transistor and other silicon components which are controlled through the introduction of disorder. Quantum mechanical states in semiconductors induced by introducing impurities and disorder can dramatically increase the efficiency of solar cells. There are many other materials of potential technological interest, such as high entropy alloys, dilute magnetic semiconductors, and topological insulators where disorder plays an essential role. Being able to understand, control, and predict the disorder effects in real physical systems is essential for the development of new structural and functional materials for future technological applications. This project involves building computer software that is aimed to enable the study of disorder effects using the principles of quantum mechanics and to accelerate the discovery of materials essential for industry and information technology applications. The creation of a large community of users and developers who will accelerate this process is envisioned. This project supports interdisciplinary training and education of undergraduate and graduate students in the fields of theoretical condensed matter physics and computational materials sciences. The user community of this software will be supported through webinars, discussion groups on social media, and online tutorial materials. This award supports various training and outreach efforts, including an annual ""Beowulf Boot Camp"" and ""Quantum Day"" for high-school students, and undergraduate and graduate research opportunities. The PIs will build upon and unify decades of development of research codes for the first-principles investigation of disordered materials. These codes include the Korringa-Kohn-Rostoker Coherent Potential Approximation, which is the first principles code for the study of random alloys, and the Locally Self-consistent Multiple Scattering code, which can enable the study of extremely large disordered systems from first principles using the largest parallel supercomputers available. Strong disorder effects and Anderson localization have been studied on the model Hamiltonian level using the Typical Medium Dynamical Cluster Approximation (TMDCA). To enable the study the strong disorder effects in real system within the first-principles Locally Self-consistent Multiple Scattering formalism with cluster embedding, the project team will use the typical medium analysis of TMDCA. The software product of this project, MuST, will be made available on GitHub as a self-contained open source package with detailed online documentations. MuST will create a scalable approach for first principles studies of quantum materials that efficiently utilizes petascale and future high-performance computing resources. It will expand the user community by enabling researchers within academia and industry to perform calculations that are presently out of reach for most users. MuST will provide a computational framework for the investigation of phase transitions and electron localization in the presence of disorder in real materials and will also enable the computational study of local chemical correlation effects on the magnetic structure, phase stability, and mechanical properties of high entropy alloys and other disordered structures.This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Frameworks: Collaborative Proposal: Software Infrastructure for Transformative Urban Sustainability Research,OAC,1931324,Mikhail Chester,Mikhail.Chester@asu.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","The United States is highly urbanized with more than 80% of the population residing in cities. Cities draw from and impact natural resources and ecosystems while utilizing vast, expensive infrastructures to meet economic, social, and environmental needs. The National Science Foundation has invested in several strategic research efforts in the area of urban sustainability, all of which generate, collect, and manage large volumes of spatiotemporal data. Voluminous datasets are also made available in domains such as climate, ecology, health, and census. These data can spur exploration of new questions and hypotheses, particularly across traditionally disparate disciplines, and offer unprecedented opportunities for discovery and innovation. However, the data are encoded in diverse formats and managed using a multiplicity of data management frameworks -- all contributing to a break-down of the observational space that inhibits discovery. A scientist must reconcile not only the encoding and storage frameworks, but also negotiate authorizations to access the data. A consequence is that data are locked in institutional silos, each of which represents only a sliver of the observational space. This project, SUSTAIN (Software for Urban Sustainability to Tailor Analyses over Interconnected Networks), facilitates and accelerates discovery by significantly alleviating data-induced inefficiencies. This effort has deep, far-reaching impact. It transforms urban sustainability science by establishing a community of interdisciplinary researchers and catalyzing their collaborative capacity. Hundreds of researchers from over 150 universities are members of our collaborating organizations and will immediately benefit from SUSTAIN. Domains where spatiotemporal phenomena must be analyzed benefit from this innovative research; the partnership with ESRI and Google Earth amplify the impact of SUSTAIN, giving the project a global reach and enabling international collaborative initiatives. The direct engagement with middle school students in computer science and STEM disciplines has well-known benefits and, combined with graduate training, produces a diverse, globally competitive STEM workforce. SUSTAIN targets transformational capabilities for feature space exploration, hypotheses formulation, and model creation and validation over voluminous, high-dimensional spatiotemporal data. These capabilities are deeply aligned with the urban sustainability community's needs, and they address challenges that preclude effective research. SUSTAIN accomplishes these interconnected goals by enabling holistic visibility of the observational space, interactive visualizations of multidimensional information spaces using overlays, fast evaluation of expressive queries tailored to the needs of the discovery process, generation of custom exploratory datasets, and interoperation with diverse analyses software frameworks - all leading to better science. SUSTAIN fosters deep explorations through its transformative visibility of the federated information space. The project reconciles the fragmentation and diversity of siloed data to provide seamless, unprecedented visibility of the information space. A novel aspect of the project's methodology is the innovative use of the Synopsis, a spatiotemporal sketching algorithm that decouples data and information. The methodology extracts and organizes information from the data and uses the information (or sketches of the data) as the basis for explorations. The project also incorporates a novel algorithm for imputations at the sketch level at myriad spatiotemporal scopes. The effort creates a collaborative community of multidisciplinary researchers to build an enduring software infrastructure for urban sustainability.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative:RAPID:Leveraging New Data Sources to Analyze the Risk of COVID-19 in Crowded Locations.,OAC,2027524,Yung-Hsiang Lu,yunglu@purdue.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","The goal of this project is to create a software infrastructure that will help scientists investigate the risk of the spread of COVID-19 and analyze future epidemics in crowded locations using real-time public webcam videos and location based services (LBS) data. It is motivated by the observation that COVID-19 clusters often arise at sites involving high densities of people. Current strategies suggest coarse scale interventions to prevent this, such as cancellation of activities, which incur substantial economic and social costs. More detailed fine scaled analysis of the movement and interaction patterns of people at crowded locations can suggest interventions, such as changes to crowd management procedures and the design of built environments, that yield social distance without being as disruptive to human activities and the economy. The field of pedestrian dynamics provides mathematical models that can generate such detailed insight. However, these models need data on human behavior, which varies significantly with context and culture. This project will leverage novel data streams, such as public webcams and location based services, to inform the pedestrian dynamics model. Relevant data, models, and software will be made available to benefit other researchers working in this domain, subject to privacy restrictions. The project team will also perform outreach to decision makers so that the scientific insights yield actionable policies contributing to public health. The net result will be critical scientific insight that can generate a transformative impact on the response to the COVID-19 pandemic, including a possible second wave, so that it protects public health while minimizing adverse effects from the interventions.We will accomplish the above work through the following methods and innovations. LBS data can identify crowded locations at a scale of tens of meters and help screen for potential risk by analyzing the long range movement of individuals there. Worldwide video streams can yield finer-grained details of social closeness and other behavioral patterns desirable for accurate modeling. On the other hand, the videos may not be available for potentially high risk locations, nor can they directly answer ?what-if? questions. Videos from contexts similar to the one being modeled will be used to calibrate pedestrian dynamics model parameters, such as walking speeds. Then the trajectories of individual pedestrians will be simulated in the target locations to estimate social closeness. An infection transmission model will be applied to these trajectories to yield estimates of infection spread. This will result in a novel methodology to include diverse real time data into pedestrian dynamics models so that they can quickly and accurately capture human movement patterns in new and evolving situations. The cyberinfrastructure will automatically discover real-time video streams on the Internet and analyze them to determine the pedestrian density, movements, and social distances. The pedestrian dynamics model will be reformulated from the current force-based definition to one that uses pedestrian density and individual speed, both of which can be measured effectively through video analysis. The revised model will be used to produce scientific insight to inform policies, such as steps to mitigate localized outbreaks of COVID-19 and for the systematic reopening, potential re-closing, and permanent changes to economic and social activities.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Framework: Software: Next-Generation Cyberinfrastructure for Large-Scale Computer-Based Scientific Analysis and Discovery,OAC,1835443,Alan Edelman,EDELMAN@MATH.MIT.EDU,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Recent revolutions in data availability have radically altered activities across many fields within science, industry, and government. For instance, contemporary simulations medication properties can require the computational power of entire data centers, and recent efforts in astronomy will soon generate the largest image datasets in history. In such extreme environments, the only viable path forward for scientific discovery hinges on the development and exploitation of next-generation computational cyberinfrastructure of supercomputers and software. The development of this new computational infrastructure demands significant engineering resources, so it is paramount to maximize the infrastructure's potential for high impact and wide adoption across as many technical domains as possible. Unfortunately, despite this necessity, existing development processes often produce software that is limited to specific hardware, or requires additional expertise to use properly, or is overly specialized to a specific problem domain. Such ""single-use"" software tools are limited in scope, leading to underutilization by the wider scientific community. In contrast, this project seeks to develop methods and software for computer-based scientific analysis that are sufficiently powerful, flexible and accessible to (i) enable domain experts to achieve significant advancements within their domains, and (ii) enable innovative use of advanced computational techniques in unexpected scientific, technological and industrial applications. This project will apply these tools to a wide variety of specific scientific challenges faced by various research teams in astronomy, medicine, and energy management. These teams plan on using the proposed work to map out new star systems, develop new life-saving medications, and design new power systems that will deliver more energy to a greater number of homes and businesses at a lower cost than existing systems. Finally, this project will seek to leave a legacy of sustained societal benefit by educating students and practitioners in the broader scientific and engineering communities via exposure to state-of-the-art computational techniques. Through close collaboration with research teams in statistical astronomy, pharmacometrics, power systems optimization, and high-performance computing, this project will deliver cyberinfrastructure that will effectively and effortlessly enable the next generation of computer-based scientific analysis and discovery. To ensure the practical applicability of the developed cyberinfrastructure, the project will focus on three target scientific applications: (i) economically viable decarbonization of electrical power networks, (ii) real-time analysis of extreme-scale astronomical image data, and (iii) pharmacometric modeling and simulation for drug analysis and discovery. While tackling these specific problems will constitute an initial stress test of the proposed cyberinfrastructure, it is the ultimate goal of the project that the developed tools be sufficiently performant, accessible, composable, flexible and adaptable to be applied to the widest possible range of problem domains. To achieve this vision, the project will build and improve various software tools for computational optimization, machine learning, parallel computing, and model-based simulation. Particular attention will be paid to the proposed cyberinfrastructure's composability with new and existing tools for scientific analysis and discovery. The pursuit of these goals will require the design and implementation of new programming language abstractions to allow close integration of high-level language features with low-level compiler optimizations. Furthermore, maximally exploiting proposed cyberinfrastructure will require research into new methods that combine state-of-the-art techniques from optimization, machine learning, and high-performance computing.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF),OAC,1835612,Sharon Glotzer,sglotzer@umich.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements:  Software - Harnessing the InSAR Data Revolution: GMTSAR,OAC,1834807,David Sandwell,dsandwell@ucsd.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","The award supports the development of a software tool aimed at lowering the barriers for the use of Interferometric Synthetics Aperture Radar (InSAR) data. Natural processes such as earthquakes, volcanoes, landslides, and glaciers cause deformation of the surface of the earth that can now be monitored to 10 mm precision globally. These surface measurements provide a new tool for investigating processes in the interior of the Earth. InSAR is a powerful and low-cost way to monitor subsurface magma movement and is especially useful when combined with other tools such as GPS and seismic data. InSAR is also used to understand subsurface fluid movement, such as caused by groundwater withdrawal, geothermal production, or in oil and gas fields. A wide variety of new SAR satellites are currently operating and the US will have an even more capable mission in the near future. These massive data sets need to be transformed into information to promote the progress of science, mitigate natural hazards, and enhance use of underground resources. The software, named GMTSAR will develop robust and sustainable software to take full advantage of the satellite generated data for both science and applications. The main innovation of this project is to develop open and robust software to simplify the InSAR data processing and enable routine processing of thousands of SAR images on state-of-the-art computer facilities. Open distribution of software, and the GMTSAR theoretical basis document provide a foundation for education in the field of space geodesy and ensures availability to a diverse audience.This proposed investigation will use standard software engineering practices to harden the GMTSAR code, improve the geodesy, and make it more accessible to novice and advanced users on a wide array of UNIX platforms. This will be achieved through improved and automated testing, partial redesign and simplification of the work flows, UNAVCO short courses, user feedback, and eventual migration of the code distribution and maintenance to a national facility. Work will be performed by a postdoctoral researcher in collaboration with the GMTSAR and GMT development teams and with assistance from the XSEDE program. The expected outcome is to provide sustainable, open, geodetically-accurate software to move InSAR time series analysis from the intermediate-scale methods published today to large spatial and time scale analyses that are becoming possible using the new data streams.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Frameworks: Cyberinfrastructure for Remote Data Collection with Children,OAC,2209756,Melissa Kline Struhl,mekline@mit.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Lookit/Children Helping Science is a website where families can participate in research with their infants and children from home. In the past, researchers who study how children develop and learn have almost always either invited families to bring their children to a research lab, or else have worked with schools to conduct studies that take place during the school day. Because participating in studies can place a big burden on families and schools, many of these studies have worked with relatively small sample sizes, and often recruit a group of participants who do not represent the diverse experiences and backgrounds of all children. The COVID-19 pandemic has also made it impossible to conduct these kinds of studies for months or years at a time, shutting down much of the scientific that is designed to improve children?s lives. Lookit/Children Helping Science has made it much easier to conduct studies about how infants and children develop: families don?t need to travel anywhere, and they can learn about and decide to participate in a research study (either a video call with a researcher, or a self-paced activity) at a time that works for them. Now, the website is being expanded so that the same tools and procedures for doing remote research about how children?s minds develop (psychological research) can be applied to questions about children?s learning and experience in schools (educational research). To do this, the investigators work with teachers, school administrators, families, and education researchers to learn what else is needed for the website to work in these new contexts. For instance, right now, usually parents or guardians are with their children while they take the study, so they can learn about and decide if their child should participate at the same time. If a teacher is the one to set up the study, then the website will need new ways to let both families and teachers keep track of a child?s information. Studies with infants and children are much harder to carry out than similar studies with adults, but moving this research online can help studies to be better (making fewer mistakes), more flexible for the questions being asked, and more applicable to how children from all backgrounds learn and grow. Researcher often make tools for a specific field like psychology or medicine, but research with children happens across many different topics. By organizing the tools for researchers around the needs of families and children, rather than around the needs of a particular scientific topic, this website will let people from all walks of life participate in studies from many different perspectives, all aiming to improve what we know about how infants and children learn and grow.The proposed cyberinfrastructure is a centralized platform for online psychological and educational research with children, directly supporting goals of the Directorates for Education and Human Resources (EHR) and Social, Behavioral and Economic Sciences (SBE). Many studies with children can be translated into online tasks, either ?unmoderated? and administered entirely by computer, or ?moderated? via video chat with a researcher. The popularity of such approaches has exploded during the COVID pandemic, across nearly every type of research that involves data collection with human participants.The present award builds on the success of the Lookit/Children Helping Science platform with developmental psychology and simultaneously extend to a new domain, education research, that both has common constraints and can benefit from shared resources like online participant pools. In particular, the objectives are: (1) Support additional research types, including educational research conducted in schools and live interactive studies. (2) Accommodate a wider range of researchers and participants by facilitating multilingual studies and providing mobile device support. (3) Improve the quality and reproducibility of research conducted on the platform. (4) Facilitate sustained contributions by researchers who use the platform. Beyond increasing the speed of research, this infrastructure aims to unlock advances in educational research that are more robust and generalize more widely: for example, rather than an intervention being tested in one school in one location, with corresponding limitations regarding generalizing conclusions, the intervention might simultaneously be tested across the country, implemented with perfect consistency at each location. This will increase replicability, generalizability, and the ability to investigate individual differences across a broad range of developmental research. Research with children is essential for many societal aims, including informing pedagogical practices and educational policy. This cyberinfrastructure provides these much-needed tools regardless of geographical location, increasing opportunities for under-resourced researchers and underrepresented groups and expanding opportunities for scientific outreach and study participation among diverse populations. In addition to the impact on the areas of research that the platform will serve after this phase of investment, this work will also provide key insights, opportunities, and tools for digital infrastructure in other domains.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Monitoring Earth Surface Deformation with the Next Generation of InSAR Satellites: GMTSAR,OAC,2209808,David Sandwell,dsandwell@ucsd.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Understanding the processes of earthquakes, volcanoes and hydrological changes, and their associated hazards is a top priority of the solid earth research community and USGS, due to the potential for societal disruption, financial consequences, and possible loss of life if not prepared for adequately. This requires not only long-term estimates of such processes and their hazards within a socially relevant timeframe, but also an evaluation on the impact of human activities over the Earth?s surface and interior. These estimates and evaluations hinge on the capability of accurately measuring how the Earth?s surface changes and deforms over time. For example, it is important to know how fast the seismic moment is accumulating over the San Andreas fault system, as that will tell us where and when will we be expecting the next destructive earthquake. This requires us to be able to measure the deformation that spans hundreds of kilometers at an accuracy of 0.5 mm/yr with resolution better than 10 km. Interferometric Synthetic Aperture Radar (InSAR) is the best technique for this crucial task, as the current remote sensing satellite observations that inform this technique come with broad-scale coverage, at low-cost, regardless of weather and on a regular basis. However, the upcoming new InSAR missions are raising a new challenge: how to efficiently handle drastically increasing amounts of data (~80 TB per day for the NISAR mission). To answer this challenge, the freely-available InSAR processing software GMTSAR is developing robust and efficient approaches to take full advantage of the satellite-generated data for both scientific research and societal applications. The main innovations of this project are to enable the cloud computing capabilities, transfer to newer generation of programing language, and keep engaging more users to build their own data processing strategies using this software. The developers will ensure that users from across the globe have the support they need for access to state-of-the-art processing techniques, and will continue improving the documentation, example datasets and tutorials to strengthen the foundation for education in the field of space geodesy. Interferometric Synthetic Aperture Radar (InSAR) is a powerful technique for measuring small displacements (1-10 cm) of the surface of the earth including those caused by tectonic loading, earthquakes, volcanoes, landslides, glaciers, ground fluid injection/withdrawal and underground nuclear tests. Over the past decade, a freely available, open-source software has been developed to harness these valuable datasets, which is called GMTSAR. During past investigations, this software has been equipped with the power to capitalize on the freely available ~1200 TB per year of data from Sentinel-1 satellite operated by the European Space Agency, and was provided as a robust research tool to the user base. The upcoming NISAR mission operated by NASA and ISRO, will dramatically increase the amount of available SAR data to over 30,000 TBytes per year. While this is a boon for InSAR science, it presents two main processing hurdles: how can one achieve maximum productivity with these increasing large datasets, while still preserving the accuracy of the measurements and how can one best facilitate broad user access to this trove of data. This project addresses these challenges by (1) enabling GMTSAR to permit rapid processing of very large data sets in a cloud computing environment, and (2) further expanding the usage of these InSAR data in both research and student communities by integrating with Python and streamlining processing modules to simplify user interactions.Facilitating the processing of large InSAR datasets with enhanced GMTSAR software will allow solid earth and cryosphere scientists to utilize the massive InSAR data sets to advance their interdisciplinary investigations, including global observations of volcanoes, estimates of seismic hazard through strain-rate mapping, monitoring urban infrastructure, tracking ice sheet movements, and detecting coastal subsidence. The global reach of InSAR science will be advanced by streamlined processing modules that are accessible to both specialists and students alike. In addition, the improvements we propose to make to specific modules, including the development of routine integration with Global Navigation Satellite System (GNSS) data and the combination of line-of-sight (LOS) InSAR measurements from both right-looking Sentinel-1 and left-looking NISAR satellites, will improve the accuracy of measurements to enable full 3D vector displacement time series analyses.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: An Infrastructure for Software Quality and Security Issues Detection and Correction,OAC,2216894,Marouane Kessentini,kessentini@oakland.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Research into more effective software development has the potential to make the infrastructure on which so many aspects of society depend less costly and more secure in the scientific community, industry and government agencies. In particular, the scientific community is proposing millions of scientific software prototypes to enable reproducibility of research results in almost every domain. Scientists may frequently introduce security and quality issues into existing scientific software via their code changes due to their limited experience in software quality and security and the lack of tools for quality and security assessments that can be easily used and integrated in programming environments. Thus, several existing scientific software projects are difficult to 1) extend by scientists due to their poor quality and 2) deploy by industry due to the likelihood of security vulnerabilities and the bad development practices used. Without a unified and easy-to-integrate framework for detecting, fixing, and documenting vulnerability and quality issues in scientific projects, the reusability, extendibility, safe deployment, and technology transfer of scientific projects will remain limited. This project builds a sustainable, community-driven software security and quality analysis framework. These tools enable more scientists to build better software and to transfer their prototypes to industry by following the best software development practices. Its integrated education plan will bring undergraduate and graduate computer science students more awareness and expertise in the evolution of software systems, including security and quality issues.This project develops a framework for detecting, fixing, and documenting security and quality issues. It will continuously monitor the software repository to identify security vulnerabilities and quality issues based on static and dynamic analyses, and then find the best sequence of code changes to prioritize and fix them. The developers can review the recommendations and their impacts in a detailed report and select the code changes that they want to apply. The framework includes a visualization support of the quality and security changes over the evolution of the project. Furthermore, non-expert programmers from the scientific community can use the automatically generated documentation by the framework to understand the severity of the detected issues and necessary code changes to fix them. The project has the potential to revolutionize how developers monitor the evolution of their systems in continuous integration environments by unifying security and quality issues detection and correction and enabling their automated documentation. All tools and methodologies will be empirically evaluated in collaboration with scientists from various domains. These tools will enable more scientists to build better software and transfer their prototypes to industry by following best development practices.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Simulating Autonomous Agents and the Human-Autonomous Agent Interaction,OAC,2209793,Angelantonio Tafuni,atafuni@njit.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","This project augments the Chrono computer simulation platform in transformative ways. Chrono's purpose is to predict through simulation the interplay between mechatronic systems, the environment they operate in, and humans with whom they might interact. The open-source simulation platform is slated to become a community-shared virtual investigation tool used to probe competing engineering designs and test hypotheses that would be too dangerous, difficult, or costly to verify through physical experiments. Chrono has been and will continue to be used in multiple fields and disciplines, e.g., terramechanics, astrophysics; soft matter physics; biomechanics; mechanical engineering; civil engineering; industrial engineering; and computer science. Specifically, it is used to engineer the 2023 VIPER lunar rover; relied upon by US Army experts in evaluating its wheeled and tracked vehicle designs; used in the US and Germany in the wind turbine industry; and involved in designing wave energy conversion solutions in Europe. Upon project completion, Chrono will become a simulation engine in Gazebo, which is widely used in robotics research; operate on the largest driving simulator in the US; empower research in the bio-robotics and field-robotics communities; and assist efforts in the broad area of automotive research carried out by a consortium of universities and companies under the umbrella of the Automotive Research Center. The educational impact of this project is threefold: training undergraduate, graduate, and post-doctoral students in a multi-disciplinary fashion that emphasizes advanced computing skills development; anchoring two new courses in autonomous vehicle control and simulation in robotics; and broadening participation in computing through a residential program on the campus of the University of Wisconsin-Madison that engages teachers and students from rural high-schools. Innovation and discovery are fueled by quality data. At its core, this project seeks to increase the share of this data that has simulation as its provenance. In this context, a multi-disciplinary team of 40 researchers augments and validates a physics-based simulation framework that empowers research in autonomous agents (AAs). The AAs operate in complex and unstructured dynamic environments and might engage in two-way interaction with humans or other AAs. This project enables Chrono to generate machine learning training data quickly and inexpensively; facilitates comparison of competing designs for assessing trade-offs; and gauges candidate design robustness via testing in simulation of corner-case scenarios. These tasks are accomplished by upgrading and extending Chrono to leverage recent computational dynamics innovations, e.g., a faster index 3 differential algebraic equations solver; a new approach to solving frictional contact problems; a real-time solver for handling flexible-body dynamics in soft robotics via nonlinear finite element analysis; a best-in-class simulator for terradynamics applications; reliance on just-in-time compiling for producing executables that are both problem- and hardware-optimized; a novel way for using mixed data representations for parsimonious storing of state information; and a scalable multi-agent framework that enables geographically-distributed, over the Internet, real-time simulation of human-AA interaction.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Proposal: Frameworks: Sustainable Open-Source Quantum Dynamics and Spectroscopy Software,OAC,2103705,Albert DePrince,deprince@chem.fsu.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","With support from the Office of Advanced Cyberinfrastructure and the Division of Chemistry at NSF, Professor Li and his team will work to expand the capabilities of the open-source software platform, Chronus Quantum (ChronusQ), to include time-resolved spectroscopy for supporting quantum chemistry calculations. The new capabilities will include novel computational methods to provide unprecedented capabilities to simulate chemical processes of electrons and nuclei that exhibit quantum behaviors. The physical insights gleaned through applications of ChronusQ underlie the advancement of new technologies that are crucial to sustainable energy, catalysis, quantum computing, and other applications that can immediately impact society. This project provides a mechanism for advanced interdisciplinary education and training in the areas of inorganic, theoretical, physical, and materials chemistry. The collaborative academic research serves as a test ground for identifying and deploying ways that the scientific community as a whole can both increase awareness of the importance of active engagement in professional skill development for graduate students and post-doctoral scholars, and develop tools to facilitate professional development in an academic setting.ChronusQ seamlessly integrates time-dependent quantum mechanical theories, spectral analysis tools, and modular high-performance numerical libraries that are highly parallelized, extensible, reusable, community-driven, and open-sourced. The Team develops in ChronusQ the complete time-dependent quantum description of coupled nuclear and electronic dynamics within the time-dependent density functional theory and equation-of-motion coupled cluster framework. The project enables computational studies of ultrafast time-resolved spectroscopies and simulations of chemical processes in the strongly nonadiabatic regime. Software modules are bolstered by algebraic and integral acceleration engines that can make it feasible to simulate fully quantum mechanically molecular dynamics. The collaborative project advances the theoretical description of quantum dynamics across time scales, bridging the attosecond and subnanosecond regimes, enabling the development of spectroscopic technologies to probe molecular and materials properties with state specificity that is beyond the Born-Oppenheimer approximation.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: OpenEarthscape - Transformative Cyberinfrastructure for Modeling and Simulation in the Earth-Surface Science Communities,OAC,2103815,Nicole Gasparini,ngaspari@tulane.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).The landscape around us changes constantly. Sometimes change is slow: a river bend migrates, soil erodes from a field, a waterfall carves its way upstream. Sometimes change happens fast: a landslide, a flood, a winter storm eroding beaches. To understand, manage, and forecast such processes, scientists rely on computer simulation models. This project develops software tools to make simulation modeling easier, more accessible, and more efficient. Among the products is a website through which researchers and students alike can learn about and experiment with a variety of environmental simulation models, without needing to install anything on their own computers. This web portal takes advantage of a technology that combines text, pictures, and computer code in a single online document. The project also develops improved computer-programming libraries designed to make it easier and more efficient for researchers to create new simulation models. The project contributes computing-skills training for college students enrolled in Colorado-based summer programs that serve traditionally underrepresented student populations. The project also promotes public education in geology, by creating an online animated simulation illustrating how landscapes evolve in response to various geologic events.As the sciences that probe Earth's changing surface become more quantitative and prediction-oriented, they increasingly rely on computational modeling and model-data integration. This project develops OpenEarthscape: an integrated suite of community-developed cyber resources for simulation and model-data integration, focusing on nine high-priority geoscience frontiers. Products and activities include EarthscapeHub: a JupyterHub server providing easy access to models, tools, and libraries; new capacity for creating and sharing reproducible analyses; and major enhancements to current programming libraries for model construction and coupling. OpenEarthscape catalyzes efficiency by building new technology to improve performance and developing an extended version of the Basic Model Interface API standard to address parallel architecture and coupling. OpenEarthscape fosters research productivity with improved library capabilities for data I/O and visualization, and with community resources for efficient software distribution and cross-platform compatibility. Broader impacts include partnership with undergraduate research programs that support traditionally underrepresented student populations, with the project team contributing introductory training in scientific computing. A novel educational element is the OpenEarthscape Simulator: a web-hosted visual simulation of a micro-continent evolving in response to various geologic events. The simulator provides students and the general public with an intriguing visualization of Earthscape dynamics and provides a template for the research community to identify defects in our current understanding.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Frameworks: Collaborative Research: ChronoLog: A High-Performance Storage Infrastructure for Activity and Log Workloads,OAC,2104013,Xian-He Sun,sun@iit.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","Modern computing applications generate massive amounts of data at unprecedented rates. Beyond simply storing data, one increasingly common requirement is to store activity data, also known as log data, which describe things that happen rather than things that are. Activity data are generated by computing systems, scientific instruments, electrical devices, etc. as well as by humans. The fast growing of activity data stresses current data management systems beyond their capability and becomes a known killer performance bottleneck of high-performance computing systems. This project develops ChronoLog, a novel system for organizing and storing activity data effectively and efficiently. ChronoLog leverages modern storage hardware and provides user-focused plugins and easy-to-use interface for productivity. It will benefit a diverse range of communities in various ways, such as enabling better fraud detection in financial transactions, faster and more accurate weather predictions and simulations, reduced time-to-insight for medical and bioengineering data, autonomous computing (e.g., driving), and more secure web and mobile services. ChronoLog uses physical time to provide a synchronization-free data distribution and the total ordering on a log. It first leverages multiple storage tiers, such as storage-class memories (e.g., 3D XPoint) and new flash storage (e.g., NVMe SSDs), to transparently scale the log via log auto-tiering. It then adopts a tunable parallel access model, which offers multiple-writers-multiple-readers (MWMR) semantics and highly concurrent I/O, to fully utilize the multi-tiered storage environment. ChronoLog's innovative design supports high-performance data access via I/O isolation between tails and historical operations, efficient resource utilization with newly developed elastic storage capabilities, and scalability using a novel 3D log distribution. It facilitates data processing pipelining by acting as an authoritative source of strong consistency and with the help of fast append and commit semantics. It can be used as an arbitrator offering a plethora of features such as transactional isolation and atomicity, a consensus engine for consistent replication and indexing services, and a scalable data integration and warehousing solution. ChronoLog and its plugins establish a robust, flexible, and high-performance storage ecosystem that promotes the development of scalable applications and services for high performance computing systems. The project includes a diverse group of collaborators who share a common need for a fundamentally new approach to distributed logging to address their use cases. These close partnerships will strengthen the bonds between academic and applied science, ultimately leading to new applications and driving discovery in domains as diverse as geoscience, cosmology, and astrophysics. Forming these collaborations and integrating students and junior IT professionals will create a well-trained workforce in cyberinfrastructure.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Framework: An A+ Framework for Multimessenger Astrophysics Discoveries through Real-Time Gravitational Wave Detection,OAC,2103662,Chad Hanna,crh184@psu.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","The first direct detection of ripples in space, known as gravitational waves, by the NSF-funded LIGO (Laser Interferometer Gravitational-wave Observatory) project in 2015 opened a new window on the universe and provided an unprecedented ability to study distant astronomical phenomena that could otherwise not be seen with conventional telescopes. The subsequent 2017 detection of merging neutron stars, through gravitational waves with LIGO combined with the light detected by conventional telescopes, opened a new era whereby scientists hope to routinely study the universe using information analogous to both sight and sound. This project will directly enable future detections of gravitational waves through the development of robust signal processing software and an ecosystem of cyberinfrastructure services designed to analyze LIGO data in real time. The program will involve a diverse group of undergraduate students, graduate students, postdoctoral researchers, computational scientists, and faculty in transformative science. This work contributes to the national cyberinfrastructure as a core data-producing component for astronomy and will be relied upon by thousands of scientists globally as they progress the state of knowledge through the study of black holes, neutron stars, fundamental physics, and the evolution of the Universe.With the goal of making new coincident gravitational-wave and electromagnetic observations commonplace, this project targets the development of a software framework for the real-time discovery of gravitational waves with the world-wide network of gravitational-wave detectors including LIGO, Virgo, and KAGRA. With this project, the investigators intend to provide a sustainable community-driven framework supporting current gravitational-wave detectors while developing new infrastructure for the LIGO A+ upgrade in about 2025. The team will develop a real-time gravitational wave processing framework around the following themes: 1) accelerating the pace of discovery and dissemination of results, 2) advancing the use of machine learning and artificial intelligence in production gravitational-wave astronomy, 3) improving scientific robustness and reproducibility, and 4) increasing adoption of the developed software and services. The framework will be used to create libraries, applications and services for real-time calibrated strain data, real-time data quality information and a quick-response gravitational-wave search for merging neutron stars and black holes, all of which will culminate in daily gravitational-wave discoveries released publicly. This framework will contribute gravitational-wave discovery services operating in a high-availability mode with the goal of greater than 99% uptime. A host of scientific metrics will be developed into a real-time test infrastructure to ensure that gravitational-wave alerts are accurate and robust throughout software development and release cycles. This research will have a far-reaching impact on several scientific disciplines with new gravitational-wave and multi-messenger astrophysics discoveries and it will impact society through a gradual change in the shared knowledge about the universe. Beyond these general societal impacts, the project personnel intend to directly weave training and participation broadening activities into their research through 1) training and broadening participation in the research community via quarterly workshops, 2) providing professional development opportunities for the project personnel through training seminars, and 3) engaging and educating the next generation of scientists in the geographic community with a summer school for high school students.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Windows on the Universe NSF Big Idea program, the Physics at the Information Frontier (PIF) program in the Division of Physics (PHY), and the Division of Astronomical Sciences (AST).This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements:  Towards a Robust Cyberinfrastructure for NLP-based Search and Discoverability over Scientific Literature,OAC,2104025,James Pustejovsky,pustejovsky@gmail.com,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","This project creates an open platform for accessing and mining information from scientific texts that provides access to an array of software, computing resources, and publication data. Current search technologies typically find many relevant documents, but do not extract and organize the information content of these documents or suggest new scientific hypotheses based on this organized content. Natural Language Processing (NLP) strategies are a recognized means to approach this problem, and this project develops the cyberinfrastructure to support sophisticated search and retrieval from scientific publications, use and augmentation of facilities for advanced and well-established natural language processing and machine learning tools, and extraction and aggregation of data from scientific publications. The project leverages two NSF-funded projects: the Language Applications (LAPPS) Grid, which has already proven to be an effective platform for development of NLP applications; and University of Wisconsin?s xDD (formerly, GeoDeepDive), a scalable, dependable infrastructure capable of rapidly growing a digital library of scientific publications, currently including over 13 million documents from multiple distributed commercial and open-access providers. The effort significantly enhances the value of these existing NSF-funded infrastructures by providing access to services for mining scientific publications and lowering the barriers to entry resulting from licensing, redistribution, and intellectual property issues. Scientists may perform large-scale text retrieval and mining using the University of Wisconsin?s high performance computing (HPC) infrastructure through a web-based interface. Iterative domain adaptation capabilities allow scientists to easily adapt existing services to specialized areas without configuring or installing additional components. The potential impact of the cyberinfrastructure is applicable to any community that relies on computational tools for mining large textual datasets, including researchers in sociology, psychology, economics, education, linguistics, digital media, and the humanities.This project extends the LAPPS Grid to provide access to UW-xDD?s collection of scientific publications and UW?s High Performance Computing facilities, as well as means to rapidly adapt existing, well-established natural language processing and machine learning software tools to new domains and evaluate results. The LAPPs Grid provides a large collection of NLP tools from a wide variety of sources exposed as web services, together with multiple commonly used resources and a front-end document retrieval engine currently configured to access PubMed/PubMedCentral as well as nightly updates of the CORD-19 dataset. The LAPPS Grid is open source, and can be run from the web, on a user?s laptop or desktop, in the cloud, or as a self-contained docker image when it is necessary to protect sensitive or licensed data, when there is no network connection available, or for deployment on remote HPC facilities. All tools and resources can be used interoperably, eliminating the effort required to convert input and output formats to use a set of tools or resources together. xDD is one of the world?s largest single repositories of scientific publications that spans all domains of knowledge, incorporates new documents automatically and updates API endpoints every hour. xDD has accumulated millions of documents from multiple commercial and open-access publishers (over 13M publications). The xDD infrastructure is an integral part of the developing UW-COSMOS pipeline, which consists of a suite of services supporting document processing, including ingestion and parsing of PDFs; extraction of individual document objects such as text sections, figures, tables, and captions; and recall, which creates searchable Anserini and ElasticSearch indexes on the contexts and objects to enable retrieval of information. Specific project activities include implementing efficient retrieval and analysis of xDD?s vast holdings of scientific publications; extending the NLP capabilities of the LAPPS Grid for scientific publication mining and domain adaptation; developing full interoperability between the Grid and xDD/COSMOS; scaling LAPPS Grid services to handle the very large textual datasets available from UW-xDD; and surveying visualization techniques and integrating them into the Grid.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Division of Information and Intelligent Systems within the Directorate for Computer and Information Science and Engineering, and the NSF Public Access program.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: A Self-tuning Anomaly Detection Service,OAC,2103799,Samuel Madden,madden@csail.mit.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Finding and understanding anomalous behavior in data is important in many applications. A large number of anomaly detection algorithms exist, and it can be difficult to determine which algorithm is best suited to a particular domain. And once an algorithm is selected, users must tune many parameters manually to get the algorithm to perform well; this requires in-depth knowledge of the machine learning process and an understanding of the trade-offs among different algorithms to select the best performing approach. To address these difficulties, this team develops a package that can test a range of unsupervised anomaly detection techniques on a dataset, explore options to identify best-fit, and classify anomalies with higher accuracy than manual tuning.The project will automatically test a range of unsupervised anomaly techniques on a data set, extract knowledge from the combined detection results to reliably distinguish between anomalies and normal data, and use this knowledge as labels to train an anomaly classifier; the goal is to classify anomalies with an accuracy higher than what is achievable by thorough manual tuning. The approach can be applied across of a range of data types and domains. The resulting cyberinfrastructure provides tuning-free anomaly detection capabilities while making it easy to incorporate domain-specific requirements. It enables scientists and engineers having little experience with anomaly detection techniques to steer the anomaly detection process with domain expertise. Evaluation of the unsupervised anomaly detection package will use data sets and partnerships with collaborators from the Massachusetts General Hospital/Harvard Medical School, Cyber Security research, and Signify (formerly Philips Lighting) to ensure that the utility and usability of the package is verified throughout the development process. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Division of Information and Intelligent Systems within the Directorate for Computer and Information Science and Engineering.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research:  Frameworks: Internet of Samples: Toward an Interdisciplinary Cyberinfrastructure for Material Samples,OAC,2004815,David Vieglais,vieglais@ku.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Research frequently uses material samples as a basic element for reference, study, and experimentation in many scientific disciplines, especially in the natural and environmental sciences, material sciences, agriculture, physical anthropology, archaeology, and biomedicine. Observations made on samples collected in the field and in the laboratory constitute a critical data resource for research that addresses grand challenges of our planet's future sustainability, from environmental change; to food, energy, and water resources; to natural hazards and their mitigation; to public health. The large investments of public funds being made to curate huge volumes of samples acquired over decades or even centuries, and to collect and analyze new samples, demand that these samples be openly accessible, easily discoverable, and documented with sufficient information to make them reusable. The current ecosystem of sample and sample data management in the U.S. and globally is highly fragmented across stakeholders, including museums, federal agencies, academic institutions, and individual researchers, with a multitude of institutional and discipline-specific catalogs, practices for sample identification, and protocols for describing samples. The iSamples project is a multi-disciplinary collaboration that will develop a national digital infrastructure to provide services for globally unique, consistent, and convenient identification of material samples; metadata about them; and linking them to other samples, derived data, and research results published in the literature. iSamples builds on previous initiatives to achieve these goals by providing material samples with globally unique, persistent identifiers that reliably link to landing pages with metadata describing the sample and its provenance, and which allow unambiguously linking samples with data and publications. Leveraging significant national investments, iSamples provides the missing link among (i) physical collections (e.g., natural history museums, herbaria, biobanks), (ii) field stations, marine laboratories, long-term ecological research sites, and observatories, and (iii) data repositories and cyberinfrastructure. iSamples delivers enhanced infrastructure for STEM research and education, decision-makers, and the general public. iSamples benefits national security and resource management by offering a means to assure sample provenance, improving scientific reproducibility and demonstrating compliance with ethical standards, national regulations, and international treaties.The Internet of Samples (iSamples) is a multi-disciplinary and multi-institutional project to design, develop, and promote service infrastructure to uniquely, consistently, and conveniently identify material samples, record metadata about them, and persistently link them to other samples and derived digital content, including images, data, and publications. The project will create a flexible and scalable architecture to ensure broad adoption and implementation by diverse stakeholders. iSamples will build upon existing identifier infrastructure such as IGSNs (Global Sample Number;) and ARKs (Archival Resource Keys), but is agnostic to identifier type. Likewise, iSamples will encourage a high-level metadata standard for natural history samples (across biosciences, geosciences, and archaeology), while supporting community-developed metadata standards in specialist domains. Through integration with established discipline-specific infrastructure at the System for Earth Sample Registration SESAR (geoscience), CyVerse (bioscience), and Open Context (archaeology), iSamples will extend existing capabilities, enhance consistency, and expand their reach to serve science and society much more broadly. The project includes three main objectives: 1) Design and develop iSamples infrastructure (iSamples in a Box and iSamples Central); 2) Build four initial implementations of iSamples for adoption and use case testing (Open Context, GEOME, SESAR, and Smithsonian Institution); and 3) Conduct outreach and community engagement to developers, individual researchers, and international organizations concerned with material samples. The project will follow an agile development process that includes community engagement as an important element of creating software requirements and an implementation timeline.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research:  Frameworks: Internet of Samples: Toward an Interdisciplinary Cyberinfrastructure for Material Samples,OAC,2004562,Ramona Walls,rwalls@iplantcollaborative.org,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Research frequently uses material samples as a basic element for reference, study, and experimentation in many scientific disciplines, especially in the natural and environmental sciences, material sciences, agriculture, physical anthropology, archaeology, and biomedicine. Observations made on samples collected in the field and in the laboratory constitute a critical data resource for research that addresses grand challenges of our planet's future sustainability, from environmental change; to food, energy, and water resources; to natural hazards and their mitigation; to public health. The large investments of public funds being made to curate huge volumes of samples acquired over decades or even centuries, and to collect and analyze new samples, demand that these samples be openly accessible, easily discoverable, and documented with sufficient information to make them reusable. The current ecosystem of sample and sample data management in the U.S. and globally is highly fragmented across stakeholders, including museums, federal agencies, academic institutions, and individual researchers, with a multitude of institutional and discipline-specific catalogs, practices for sample identification, and protocols for describing samples. The iSamples project is a multi-disciplinary collaboration that will develop a national digital infrastructure to provide services for globally unique, consistent, and convenient identification of material samples; metadata about them; and linking them to other samples, derived data, and research results published in the literature. iSamples builds on previous initiatives to achieve these goals by providing material samples with globally unique, persistent identifiers that reliably link to landing pages with metadata describing the sample and its provenance, and which allow unambiguously linking samples with data and publications. Leveraging significant national investments, iSamples provides the missing link among (i) physical collections (e.g., natural history museums, herbaria, biobanks), (ii) field stations, marine laboratories, long-term ecological research sites, and observatories, and (iii) data repositories and cyberinfrastructure. iSamples delivers enhanced infrastructure for STEM research and education, decision-makers, and the general public. iSamples benefits national security and resource management by offering a means to assure sample provenance, improving scientific reproducibility and demonstrating compliance with ethical standards, national regulations, and international treaties.The Internet of Samples (iSamples) is a multi-disciplinary and multi-institutional project to design, develop, and promote service infrastructure to uniquely, consistently, and conveniently identify material samples, record metadata about them, and persistently link them to other samples and derived digital content, including images, data, and publications. The project will create a flexible and scalable architecture to ensure broad adoption and implementation by diverse stakeholders. iSamples will build upon existing identifier infrastructure such as IGSNs (Global Sample Number;) and ARKs (Archival Resource Keys), but is agnostic to identifier type. Likewise, iSamples will encourage a high-level metadata standard for natural history samples (across biosciences, geosciences, and archaeology), while supporting community-developed metadata standards in specialist domains. Through integration with established discipline-specific infrastructure at the System for Earth Sample Registration SESAR (geoscience), CyVerse (bioscience), and Open Context (archaeology), iSamples will extend existing capabilities, enhance consistency, and expand their reach to serve science and society much more broadly. The project includes three main objectives: 1) Design and develop iSamples infrastructure (iSamples in a Box and iSamples Central); 2) Build four initial implementations of iSamples for adoption and use case testing (Open Context, GEOME, SESAR, and Smithsonian Institution); and 3) Conduct outreach and community engagement to developers, individual researchers, and international organizations concerned with material samples. The project will follow an agile development process that includes community engagement as an important element of creating software requirements and an implementation timeline.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research : Elements : Extending the physics reach of LHCb by developing and deploying  algorithms for a fully GPU-based first trigger stage,OAC,2004645,Mike Williams,mwill@mit.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","The development of the Standard Model (SM) of particle physics is a major intellectual achievement. The validity of this model was further confirmed by the discovery of the Higgs boson at the Large Hadron Collider (LHC) at CERN. However, the Standard Model leaves open many questions, including why matter dominates over anti-matter in the Universe and the properties of dark matter. Most explanations require new phenomena, which we call Beyond the Standard Model Physics (BSM), and which the LHCb experiment at CERN has been designed to explore. The LHC is the premier High Energy Physics particle accelerator in the world and is currently operating at the CERN laboratory near Geneva Switzerland, one of the foremost facilities for addressing these BSM questions. The LHCb experiment is one of four large experiments at the LHC and is designed to study in detail the decays of hadrons containing b or c quarks. The goal is to identify the existence of new physics beyond the Standard Model by examining the properties of hadrons containing these quarks. The new physics, or new forces, can be manifest by particles, as yet to be discovered, whose presence would modify decay rates and CP violating asymmetries of hadrons containing the b and c quarks, allowing new phenomena to be observed indirectly - or via direct observation of new force-carrying particles. The data sets collected by the LHC experiments are some of the largest in the world. For example, the sensor arrays of the LHCb experiment, in which both PIs participate, produce about 100 TB/s and close to a zettabyte per year. Even after drastic data-reduction performed by custom-built read-out electronics, the data volume is still about 10 exabytes per year. Such large data sets cannot be stored indefinitely; therefore, all high energy physics (HEP) experiments employ a second data-reduction scheme executed in real time by a data-ingestion system - referred to as a trigger system in HEP - to decide whether each event is to be persisted for future analysis or permanently discarded. The primary goal of this project is developing and deploying software that will maximize the performance of the LHCb trigger system - running its first processing stage on GPUs - so that the full physics discovery potential of LHCb is realized.The LHCb detector is being upgraded for Run 3 (which will start to record data in 2022), when the trigger system will need to process 25 exabytes per year. Currently, only 0.3 of the 10 exabytes per year processed by the trigger is analyzed using high-level computing algorithms; the rest is discarded prior to this stage using simple algorithms executed on FPGAs. To significantly extend its physics reach in Run 3, LHCb plans to process the entire 25 exabytes each year using high-level computing algorithms. The PIs propose running the entire first trigger-processing stage on GPUs, which has zero (likely negative) net cost, and frees up all of the CPU resources for the second processing stage. The LHCb trigger makes heavy use of machine learning (ML) algorithms, which will need to be reoptimized both for Run 3 conditions but also for usage on GPUs. The specific objectives of this proposal are developing: GPU-based versions of the primary trigger-selection algorithms, which make heavy usage of ML; GPU-based calorimeter-clustering and electron-identification algorithms, likely using ML; and the infrastructure required to deploy ML algorithms within the GPU-based trigger framework. These advances will make it possible to explore many potential explanations for dark matter, e.g., dark photon decays, and the matter/anti-matter asymmetry of our universe using data that would be otherwise inaccessible due to trigger-system limitations.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Advanced Lossless and Lossy Compression Algorithms for netCDF Datasets in Earth and Engineering Sciences (CANDEE),OAC,2004993,Charles Zender,zender@uci.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Data compression is used to store and transmit digital data such as music, television, and satellite measurements more efficiently by reducing storage space and download times. The compression software broker that this project provides will facilitate the adoption of modern compression techniques in many branches of science. Compressors come in two flavors: lossless, those that perfectly preserve the original information; and lossy, those that irretrievably discard parts of the ""signal"" to further improve compression. Modern lossless and lossy compression improvements in efficiency, speed, and fidelity, are striking and will benefit critical research areas by permitting researchers to simulate, store, and analyze phenomena such as stellar evolution, chemical reactions, and hurricane formation at finer detail than before, with no extra storage costs. Since digital storage consumes power, better compression also reduces power consumption and associated greenhouse gas emissions. This project will develop the software infrastructure necessary for scientific researchers to seamlessly shift their applications to produce and use data stored with state-of-the-art lossless techniques, and by new lossy techniques that are more accurate than any others.The two most widely-used self-describing dataset storage formats, HDF5 and netCDF4, support by default only one patent unencumbered lossless compression format, the venerable DEFLATE algorithm standardized in the 1990s. Our project will develop a dynamic and extensible software library of modern COmpressors and DECompressors (codecs) for scientific data called the Community Codec Repository (CCR). We will populate the CCR with cutting-edge open-source compression technology, including the LZ4, Facebook's Zstandard, and Google's Snappy codecs, and will implement default netCDF support for the CCR. Sequential lossy-then-lossless compression improves both the size and speed of compression/decompression yet is currently tedious to perform. We will implement a user-friendly method to ""chain"" codecs into sequential operations in memory (no intermediate files required) in our widely used netCDF Operators software package. We will also produce a new precision-preserving lossy codec, Granular Bit Grooming, that has unsurpassed compression ratio and statistical accuracy. Technical success will be evaluated by the size and speed improvements of compressing a prototypical geoscience/engineering ""big data"" project, the Coupled Model Intercomparison Project version 6 (CMIP6).This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: An open source software ecosystem for plasma physics,OAC,1931429,Stephen Vincena,vincena@physics.ucla.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Software is crucial to all areas of modern plasma physics research. Plasma physicists use software for activities such as analyzing data from laboratory experiments and simulating the behavior of plasmas. Research groups often use software developed independently within their own group, which leads to unnecessary duplication of functionality and a lack of interoperability between different software packages. The lack of interoperability is compounded by different groups writing software using different coding styles and conventions. Much of the research software in plasma physics is not openly available to the public, which makes it harder for other scientists to reproduce scientific results. The team will develop PlasmaPy: a community-wide open source software package for plasma physics research and education. PlasmaPy will be written using the freely available Python programming language which is commonly used in related fields like astronomy. PlasmaPy itself will contain the general functionality needed by most plasma physicists, whereas community-developed affiliated software packages will contain more specialized functionality. The team will seek feedback from plasma physicists, hold annual workshops, and actively support new users and contributors.The research team will lead the development of PlasmaPy and affiliated packages to foster the creation of an open source software ecosystem for plasma physics. The PlasmaPy core package will contain functionality needed by plasma physicists across disciplines, whereas affiliated package will contain more specialized functionality. At the beginning of the project, the research team will formalize the software architecture, refactor existing code, improve tests, and improve base data structures to provide a solid foundation for future development. Subsequent code development priorities include a dispersion relation solver for plasma waves and instabilities, the groundwork for a flexible framework for plasma simulation, time series turbulence analysis tools, classes for the analysis of plasma diagnostics, and tools to provide access to atomic and physical data. They will make base data structures compatible with open source packages for data science to enable future data science studies. The research team will actively seek feedback from the plasma physics community, and adjust code development priorities based on this feedback. The team will hold workshops each year and actively support new users and contributors to grow PlasmaPy into a self-sustaining project.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering, the Division of Physics in the Directorate of Mathematical and Physical Sciences, and the Division of Atmospheric and Geospace Sciences in the Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Machine learning and FPGA computing for real-time applications in big-data physics experiments,OAC,1931469,Erotokritos Katsavounidis,kats@mit.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","The cyberinfrastructure needs for gravitational wave astrophysics, high energy physics, and large-scale electromagnetic surveys have rapidly evolved in recent years. The construction and upgrade of the facilities used to enable scientific discovery in these disparate fields of research have led to a common pair of computational grand challenges: (i) datasets with ever-increasing complexity and volume; and (ii) data mining analyses that must be performed in real-time with oversubscribed computational resources. Furthermore, the convergence of gravitational wave astrophysics with electromagnetic and astroparticle surveys, the very birth of Multi-Messenger Astrophysics, has already provided a glimpse of the transformational discoveries that it will enable in years to come. Given the unique potential for scientific discovery with the Large Hadron Collider (LHC) and the combination of the Laser Interferometer Gravitational-wave Observatory (LIGO) and the Large Synoptic Survey Telescope (LSST) for Multi-Messenger Astrophysics, the community needs to accelerate the development and exploitation of deep learning algorithms that will outperform existing approaches. This project serves the national interest, as stated by NSF's mission, by promoting the progress of science. It will push the frontiers of deep learning at scale, demonstrating the versatility and scalability of these methods to accelerate and enable new physics in the big data era. Because these methods are also applicable to many other parts of our national and global economy and society, this work will positively impact many fields. The students and junior scientists to be mentored and trained in this research will interact closely with our industry partners, creating new career opportunities, and strengthening synergies between academia and industry. The team will share the algorithms with the community through open source software repositories, and through our tutorials and workshops the team will train the community regarding software credit and software citation.In this project, the PIs will build upon our recent work developing high quality deep learning algorithms for real-time data analytics of time-series and image datasets, as open source software. This work combines scalable deep learning algorithms, trained with TB-size datasets within minutes using thousands of GPUs/CPUs, with state-of-the-art approaches to endow the predictions of deterministic deep learning models with complete posterior distributions. The team will also investigate the use of Field Programmable Gate Arrays (FPGAs) to accelerate low-latency inference of machine learning algorithms to minimize the demands of future computing, which is a central goal for Multi-Messenger Astrophysics and particle physics. The open source tools to be developed as part of these activities will be readily shared with and adopted by LIGO, LHC, and LSST as core data analytics algorithms that will significantly increase the speed and depth of existing algorithms, enabling new physics while requiring minimal computational resources for real-time inferences analyses. The team will organize deep learning workshops and bootcamps to train students and researchers on how to use and contribute to our framework, creating a wide network of contributors and developers across key science missions. The team will leverage existing open source and interactive model repositories, such as the Data and Learning Hub for Science (DLHub) at Argonne, to reach out to a large cross-section of communities that analyze open datasets from LIGO, LHC, and LSST, and that will benefit from the use of these technologies that require minimal computational resources for inference tasks.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Physics in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Frameworks: Re-Engineering Galaxy for Performance, Scalability and Energy Efficiency",OAC,1931531,Mahmut Kandemir,mtk2@psu.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Biomedical research is an important branch of science that deals with the problem of studying biological processes and identifying, preventing and curing diseases. This research forms the pathway to the discovery of new medicines as well as new therapies. As such, biomedical research is crucial to advance the national health and prosperity. Given the geographically distributed research groups and biomedical labs, collaborative science plays a very important role in biomedical research. Galaxy is an open source, web-based framework that is extensively used by more than 20,000 researchers world-wide for conducting research in many application domains, the most prominent of which is biomedical research. It provides a web-based environment using which scientists perform various computational analyses on their data, exchange results from these analyses, explore new research concepts, facilitate student training, and preserve their results for future use. Galaxy currently runs on a large variety of high-performance computing (HPC) platforms including local clusters, supercomputers in national labs, public datacenters and Cloud. Unfortunately, while most of these systems supplement conventional CPUs with significant accelerator capabilities (in the form of Graphical Processing Units (GPUs) and/or Field-Programmable Gate Arrays (FPGAs)), the current Galaxy implementation does not take advantage of these powerful accelerators. This project enhances the Galaxy framework so that it can take full advantage of the tremendous computational capabilities offered by GPUs and FPGAs. By doing so, the important applications running under Galaxy experiences significant speedups, thereby accelerating scientific discoveries. This project consists of four complementary tasks, which follow a logistic progression as follows: Task-I focuses on redesigning existing Galaxy tools with GPU/FPGA support and integrate them to Galaxy tool-chains; Task-II provides containerization support for the tools and accelerator-aware orchestration for running Galaxy on cloud platforms; Task-III implements specific policy driven scheduling schemes for Task-I and Task-II; and finally, Task-IV redesigns Galaxy storage to speed up execution and reduce bottlenecks related to data transfer. The proposed enhancements to Galaxy enables the integration of innovation with discovery by providing a state-of-the art experimental platform to a larger community of researchers across several disciplines. On the broader impact and outreach/educational front, this project impacts the performance and energy efficiency of Galaxy tools and applications and improves the productivity of a typical Galaxy user tremendously; that is, the main beneficiaries of this project are thousands of members of existing Galaxy Community. However, this project also (i) helps existing GPU and FPGA based (non-Galaxy) applications start using Galaxy, thereby taking full advantage of all existing toolsets within the framework, (ii) enables Galaxy tools to take better advantage of emerging cluster scheduling capabilities, and (iii) creates a synergy with concurrent Galaxy related efforts and existing infrastructure efforts the PIs are involved with, to further expedite scientific discoveries. As such, this proposed system support will have a broad societal impact via the enhanced Galaxy system support. On the education side, the project involves under-represented groups in computer science as well as in bio-informatics, outreach to undergraduates, various K-12 related activities (Science-U, CSATS, VIEW), and engagement with researchers in other disciplines (e.g., natural language processing, image processing, drug discovery and cosmology) via a workshop open to the Galaxy community.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
III: Large: Collaborative Research: Analysis Engineering for Robust End-to-End Data Science,IIS,1900991,Arvind Satyanarayan,arvindsatya@mit.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","From poor statistical practices leading to retractions of scientific ""discoveries"" to low-level spreadsheet errors subverting high-stakes analyses, failures of data analysis can have catastrophic consequences. The rapid growth of data science practice in the last decade has led to large collaborative efforts to develop new data processing, machine learning, and analytics tools that put more advanced data analysis into the hands of a wider audience of practitioners, from students to scientists to designers. The most dominant tool for data science is code, where cutting-edge algorithms can be applied from an existing libraries. However, as this democratization of data science has lowered the barrier to using advanced methods, safely using these tools under sound statistical practice remains as difficult as ever. To facilitate more robust data science, this project investigates models and tools for analysis engineering by data scientists who write programs. The focus is on the complete end-to-end process of data analysis performed with code: the iterative, and often exploratory, steps that analysts go through to turn data into This project will contribute insights and characterizations of analytic work, novel methods for capturing and analyzing data science activities, and develop new programming tools and visualization methods for authoring and validating analyses. If successful, this project will augment people's ability to conduct and assess data analyses, promoting more robust results and reducing the gap between novice and expert analysts. The findings and tools from the project will be incorporated into educational efforts, including classroom teaching and tutorials and available as open source software integrated into popular analytical environments (e.g., Jupyter).Data analysis is a central activity to scientific research, yet is too often conducted in an undisciplined fashion. This project treats the entire analytic process as our central phenomenon of study. The project will employ mixed methods to study and characterize common analysis practices and pitfalls, including direct observations of data analysts, large-scale analysis of computational notebooks, and instrumentation of analytic programming environments like JupyterLab. The project will contribute new methods for specifying and safeguarding analyses, including domain-specific languages and program synthesis methods to guide users to preferred next steps. It will also explore ""multiverse"" workflows to manage and assess a diversity of analysis decisions. Analogues of debugging and testing tools will be developed to flag problems and perform error analysis, while the capture and visualization of analytic provenance to aid reproducibility, verification, and collaborative review. The work will be evaluated through controlled studies, classroom use, and open-source deployment for wide-scale field use.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: An open source software ecosystem for plasma physics,OAC,1931393,David Schaffner,dschaffner@brynmawr.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Software is crucial to all areas of modern plasma physics research. Plasma physicists use software for activities such as analyzing data from laboratory experiments and simulating the behavior of plasmas. Research groups often use software developed independently within their own group, which leads to unnecessary duplication of functionality and a lack of interoperability between different software packages. The lack of interoperability is compounded by different groups writing software using different coding styles and conventions. Much of the research software in plasma physics is not openly available to the public, which makes it harder for other scientists to reproduce scientific results. The team will develop PlasmaPy: a community-wide open source software package for plasma physics research and education. PlasmaPy will be written using the freely available Python programming language which is commonly used in related fields like astronomy. PlasmaPy itself will contain the general functionality needed by most plasma physicists, whereas community-developed affiliated software packages will contain more specialized functionality. The team will seek feedback from plasma physicists, hold annual workshops, and actively support new users and contributors.The research team will lead the development of PlasmaPy and affiliated packages to foster the creation of an open source software ecosystem for plasma physics. The PlasmaPy core package will contain functionality needed by plasma physicists across disciplines, whereas affiliated package will contain more specialized functionality. At the beginning of the project, the research team will formalize the software architecture, refactor existing code, improve tests, and improve base data structures to provide a solid foundation for future development. Subsequent code development priorities include a dispersion relation solver for plasma waves and instabilities, the groundwork for a flexible framework for plasma simulation, time series turbulence analysis tools, classes for the analysis of plasma diagnostics, and tools to provide access to atomic and physical data. They will make base data structures compatible with open source packages for data science to enable future data science studies. The research team will actively seek feedback from the plasma physics community, and adjust code development priorities based on this feedback. The team will hold workshops each year and actively support new users and contributors to grow PlasmaPy into a self-sustaining project.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering, the Division of Physics in the Directorate of Mathematical and Physical Sciences, and the Division of Atmospheric and Geospace Sciences in the Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements:Collaborative Proposal: A task-based code for multiphysics problems in astrophysics at exascale,OAC,1931280,Saul Teukolsky,saul@astro.cornell.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","Upcoming computers will run at exascale, over a hundred times more powerful than typical machines of today. Many algorithms used in current codes will not be able to take advantage of these new machines. The researchers will complete the development of an open-source community code for multi-scale, multi-physics problems in astrophysics and gravitational physics. The code uses transformative algorithms to reach the exascale. The techniques can be applied across discipline boundaries in fluid dynamics, geoscience, plasma physics and nuclear physics and engineering. The development of this new code has been driven by the current deployment of gravitational wave detectors such as LIGO. To fully understand and analyze the signals and waveforms measured with such detectors, it is essential that accurate, robust, and efficient computational tools be available for solving the dynamical Einstein equations over very long time scales. The recent detection of the merger of a neutron star-neutron star merger by LIGO and by a host of electromagnetic telescopes has ushered The extreme energy densities of matter and radiation and the highly dynamic spacetimes of these events probe fundamental physics inaccessible to terrestrial experiments. The new code will be made available as open-source community cyberinfrastructure. The researchers will reach out to other communities within astrophysics (e.g., star formation, space plasma physics) and across discipline boundaries to fluid dynamics, geoscience, plasma physics, nuclear engineering etc. Early-career researchers trained in these techniques are in great demand, both in academia and as highly-skilled members of the industrial STEM workforce. Undergraduates will participate in the research by producing visualizations.The new code uses discontinuous Galerkin methods and task-based parallelism to accomplish its desired goals. This framework will allow the multiphysics applications to be treated both accurately and efficiently on the new architectures of petascale and exascale machines. The code is designed to scale to over a million cores for efficient exploration of the parameter space of potential sources and allowed physics, and for the high-fidelity predictions needed to realize the promise of multi-messenger astrophysics. The code will allow astrophysicists to explore the mechanisms driving core-collapse supernovae and the properties of stellar remnants, to understand electromagnetic transients and gravitational-wave phenomena in compact objects, and to reveal the dense matter equation of state. The two key algorithmic innovations in the code, the discontinuous Galerkin method coupled with task-based parallelism, promise revolutionary impact in other fields relying on numerical solution of partial differential equations at the exascale.This project advances the objectives of ""Windows on the Universe: the Era of Multi-Messenger Astrophysics"", one of the 10 Big Ideas for Future NSF Investments. This project advances also the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Physics and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Frameworks: Collaborative Proposal: Software Infrastructure for Transformative Urban Sustainability Research,OAC,1931335,Amir AghaKouchak,amir.a@uci.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","The United States is highly urbanized with more than 80% of the population residing in cities. Cities draw from and impact natural resources and ecosystems while utilizing vast, expensive infrastructures to meet economic, social, and environmental needs. The National Science Foundation has invested in several strategic research efforts in the area of urban sustainability, all of which generate, collect, and manage large volumes of spatiotemporal data. Voluminous datasets are also made available in domains such as climate, ecology, health, and census. These data can spur exploration of new questions and hypotheses, particularly across traditionally disparate disciplines, and offer unprecedented opportunities for discovery and innovation. However, the data are encoded in diverse formats and managed using a multiplicity of data management frameworks -- all contributing to a break-down of the observational space that inhibits discovery. A scientist must reconcile not only the encoding and storage frameworks, but also negotiate authorizations to access the data. A consequence is that data are locked in institutional silos, each of which represents only a sliver of the observational space. This project, SUSTAIN (Software for Urban Sustainability to Tailor Analyses over Interconnected Networks), facilitates and accelerates discovery by significantly alleviating data-induced inefficiencies. This effort has deep, far-reaching impact. It transforms urban sustainability science by establishing a community of interdisciplinary researchers and catalyzing their collaborative capacity. Hundreds of researchers from over 150 universities are members of our collaborating organizations and will immediately benefit from SUSTAIN. Domains where spatiotemporal phenomena must be analyzed benefit from this innovative research; the partnership with ESRI and Google Earth amplify the impact of SUSTAIN, giving the project a global reach and enabling international collaborative initiatives. The direct engagement with middle school students in computer science and STEM disciplines has well-known benefits and, combined with graduate training, produces a diverse, globally competitive STEM workforce. SUSTAIN targets transformational capabilities for feature space exploration, hypotheses formulation, and model creation and validation over voluminous, high-dimensional spatiotemporal data. These capabilities are deeply aligned with the urban sustainability community's needs, and they address challenges that preclude effective research. SUSTAIN accomplishes these interconnected goals by enabling holistic visibility of the observational space, interactive visualizations of multidimensional information spaces using overlays, fast evaluation of expressive queries tailored to the needs of the discovery process, generation of custom exploratory datasets, and interoperation with diverse analyses software frameworks - all leading to better science. SUSTAIN fosters deep explorations through its transformative visibility of the federated information space. The project reconciles the fragmentation and diversity of siloed data to provide seamless, unprecedented visibility of the information space. A novel aspect of the project's methodology is the innovative use of the Synopsis, a spatiotemporal sketching algorithm that decouples data and information. The methodology extracts and organizes information from the data and uses the information (or sketches of the data) as the basis for explorations. The project also incorporates a novel algorithm for imputations at the sketch level at myriad spatiotemporal scopes. The effort creates a collaborative community of multidisciplinary researchers to build an enduring software infrastructure for urban sustainability.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: RADICAL-Cybertools: Middleware Building Blocks for NSF's Cyberinfrastructure Ecosystem.,OAC,1931512,Shantenu Jha,shantenu.jha@rutgers.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","This project builds upon a previously funded middleware development effort by the Research in Advanced Distributed Cyberinfrastructure and Applications Laboratory (RADICAL) at Rutgers University; the previously developed middleware building blocks were known as RADICAL-Cybertools (RCT). In the current project, the team pursues a targeted set of developments, driven by the need to scale the number of software components, user, and supported platforms; and improve performance, engineering processes, and sustainability. The resulting capabilities will serve scientific applications in multiple domains, including software engineering, chemical physics, materials science, health science, climate science, drug discovery and particle physics.This project builds upon a prior prototype investment, which developed a pilot system for leadership-class HPC machines, and a Python implementation of SAGA, a distributed computing standard. The current effort is organized around three activities: - Extending RCT functionality to reliably support a range of novel applications at scale (examples include tightly coupling traditional HPC simulations with machine-learning methods); - Enhancing RCT to be ready to support new NSF systems, such as the Frontera supercomputing system and other new systems; - Prototyping a new component: a campaign manager for computational resource management. Data-driven approaches will be used to improve software development, engineering, and life-cycle management, and to enhance the long-term sustainability of RCT and the supported communities. The project includes use cases that are representative examples of the growing community that RCT engages and supports, such as the ATLAS high-energy physics project and the QCArchive project enabling large-scale force-field construction and physical property prediction.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Framework: Software: NSCI: Collaborative Research: Hermes: Extending the HDF Library to Support Intelligent I/O Buffering for Deep Memory and Storage Hierarchy Systems,OAC,1835764,Xian-He Sun,sun@iit.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","Modern high performance computing (HPC) applications generate massive amounts of data. However, the performance improvement of disk based storage systems has been much slower than that of memory, creating a significant Input/Output (I/O) performance gap. To reduce the performance gap, storage subsystems are under extensive changes, adopting new technologies and adding more layers into the memory/storage hierarchy. With a deeper memory hierarchy, the data movement complexity of memory systems is increased significantly, making it harder to utilize the potential of the deep memory and storage hierarchy (DMSH) design. As we move towards the exascale era, I/O bottleneck is a must to solve performance bottleneck facing the HPC community. DMSHs with multiple levels of memory/storage layers offer a feasible solution but are very complex to use effectively. Ideally, the presence of multiple layers of storage should be transparent to applications without having to sacrifice I/O performance. There is a need to enhance and extend current software systems to support data access and movement transparently and effectively under DMSHs. Hierarchical Data Format (HDF) technologies are a set of current I/O solutions addressing the problems in organizing, accessing, analyzing, and preserving data. HDF5 library is widely popular within the scientific community. Among the high level I/O libraries used in DOE labs, HDF5 is the undeniable leader with 99% of the share. HDF5 addresses the I/O bottleneck by hiding the complexity of performing coordinated I/O to single, shared files, and by encapsulating general purpose optimizations. While HDF technologies, like other existing I/O middleware, are not designed to support DMSHs, its wide popularity and its middleware nature make HDF5 an ideal candidate to enable, manage, and supervise I/O buffering under DMSHs. This project proposes the development of Hermes, a heterogeneous aware, multi tiered, dynamic, and distributed I/O buffering system that will significantly accelerate I/O performance. This project proposes to extend HDF technologies with the Hermes design. Hermes is new, and the enhancement of HDF5 is new. The deliveries of this research include an enhanced HDF5 library, a set of extended HDF technologies, and a group of general I/O buffering and memory system optimization mechanisms and methods. We believe that the combination of DMSH I/O buffering and HDF technologies is a reachable practical solution that can efficiently support scientific discovery. Hermes will advance HDF5 core technology by developing new buffering algorithms and mechanisms to support 1) vertical and horizontal buffering in DMSHs: here vertical means access data to/from different levels locally and horizontal means spread/gather data across remote compute nodes; 2) selective buffering via HDF5: here selective means some memory layer, e.g. NVMe, only for selected data; 3) dynamic buffering via online system profiling: the buffering schema can be changed dynamically based on messaging traffic; 4) adaptive buffering via Reinforcement Learning: by learning the application's access pattern, we can adapt prefetching algorithms and cache replacement policies at runtime. The development Hermes will be translated into high quality dependable software and will be released with the core HDF5 library.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Framework: Software: Collaborative Research: CyberWater-An open and sustainable framework for diverse data and model integration with provenance and access to HPC,OAC,1835602,Lan Lin,llin4@bsu.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities. The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery. The models and datasets cover fields such as hydrology, biology, environmental engineering and climate. The project also addresses one of the key issues for extreme-scale computing: scalable file systems. The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI). The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity. The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use. To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts. The project builds upon an existing prototype developed by the lead investigator; basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control. The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI. For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated. The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Framework: Software: Collaborative Research: CyberWater: An open and sustainable framework for diverse data and model integration with provenance and access to HPC,OAC,1835338,Ibrahim Demir,ibrahim-demir@uiowa.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities. The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery. The models and datasets cover fields such as hydrology, biology, environmental engineering and climate. The project also addresses one of the key issues for extreme-scale computing: scalable file systems. The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI). The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity. The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use. To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts. The project builds upon an existing prototype developed by the lead investigator; basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control. The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI. For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated. The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: NSCI : Computational and data innovation implementing a national community hydrologic modeling framework for scientific discovery,OAC,1835794,Laura Condon,lecondon@email.arizona.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Data: Toward Exascale Community Ocean Circulation Modeling,OAC,1835618,Christopher Hill,cnh@mit.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","This project designs and implements a software framework for handling petabyte-scale datasets; the focus is on global ocean circulation. A team of three universities (Johns Hopkins University, MIT, and Columbia University) builds a unified data system that is capable of delivering global ocean circulation model output at 1 km horizontal resolution. The product will be hosted in an open portal, providing the community with scalable software tools to enable analysis of the dataset. The team will use this data to answer specific questions about mixing and dissipation processes in the ocean. The goal of this effort is the creation and demonstration of a complete and replicable cyberinfrastructure for sharing and analysis of massive simulations. The focus is on high resolution ocean circulation modeling, with software tools that will enable efficient storage. Two major challenges to the study of ocean and climate dynamics are addressed: handling large datasets from high-resolution simulations, and understanding the role of small-scale ocean processes in large-scale ocean/climate systems. Resolving the first challenge would significantly facilitate ongoing and future studies of the ocean/atmosphere/climate system; addressing the second challenge would profoundly improve understanding of ocean/climate dynamics. The project builds a unified data system consisting of high-resolution global ocean circulation simulations, a petascale portal for data sharing, and scalable software tools for interactive analysis. The software framework from this project is expected to handle petascale to exascale datasets for users. Several pre-existing capabilities are leveraged for this project: the JHU regional numerical model of the Spill Jet on the East Greenland continental slope, software from the Pangeo project, the SciServer data-intensive software infrastructure, and lessons learned from the North East Storage Exchange multi-petabyte regional data store. The broader target is next generation simulation software in the geosciences and other disciplines. This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Ocean Sciences and the Integrative and Collaborative Education and Research Program within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Framework: Software: Collaborative Research: CyberWater--An open and sustainable framework for diverse data and model integration with provenance and access to HPC,OAC,1835785,Xu Liang,xuliang@pitt.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities. The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery. The models and datasets cover fields such as hydrology, biology, environmental engineering and climate. The project also addresses one of the key issues for extreme-scale computing: scalable file systems. The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI). The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity. The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use. To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts. The project builds upon an existing prototype developed by the lead investigator; basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control. The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI. For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated. The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research:Framework:Software:NSCI:Enzo for the Exascale Era (Enzo-E),OAC,1835426,Brian O'Shea,oshea@msu.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","The earliest stages of the formation of galaxies and quasars in the universe are soon to be explored with a powerful new generation of ground and space-based observatories. Broad and deep astronomical surveys of the early universe beginning in the next decade using the Large Synoptic Survey Telescope and the James Webb Space Telescope will revolutionize our understanding of the origin of galaxies and quasars, and help constrain the nature of the dark matter which is the dominant matter constituent in the universe. Detailed physical simulations that model the formation of these objects are an indispensible aid to understanding the coming glut of observational data, and to maximize the scientific return of these instruments. In this project, PIs at the Univ. California San Diego, Columbia Univ., Georgia Tech, and Michigan State Univ. are collaborating with the goal of developing a next generation community simulation software framework for the coming generation of supercomputers for cosmological simulations of the young universe. Undergraduate and graduate students will be directly involved in the software development as well as its application to several frontier cosmological research topics. The software framework that will be produced will be disseminated as open source software to enable a much broader range of scientific explorations of astrophysical topics. The project brings together the key developers of the open source Enzo adaptive mesh refinement (AMR) hydrodynamic cosmology code, who will port its software components to a newly developed AMR software framework called Cello. Cello implements the highly scalable array-of-octrees AMR algorithm on top of the powerful Charm++ parallel object system. Designed to be extensible and scalable to millions of processors, the new framework, called Enzo-E, will target exascale high performance computing (HPC) systems of the future. Through this project, the entire Enzo community will have a viable path to exascale simulations of unprecedented size and scope. The PIs have chosen three frontier problems in cosmology to drive the development of the Enzo-E framework: (1) the assembly of the first generation of stars and black holes into the first galaxies; (2) the role of cosmic rays in driving galactic outflows; and (3) the evolution of the intergalactic medium from cosmic dawn to the present day. Annual developer workshops and software releases will keep the broader research community informed and involved in the developments.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: DeCODER (Democratized Cyberinfrastructure for Open Discovery to Enable Research),OAC,2209864,Tao Wen,twen08@syr.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Scientific research in most scientific fields today involves significant amounts of data as well as software to operate on that data. Furthermore, research increasingly requires data from across a number of sources, perhaps even fields. This is a significant challenge for the scientific community, and better solutions are needed in order to enable new discoveries and accelerate the societal impacts from those discoveries. The approach in this project is to follow in the footsteps of the web and it aims to standardize how scientific data is described, allowing for tools addressing the above challenges, such as search engines for scientific data that not only support discoverability but also facilitate the usage of the data.The DeCODER project will expand and extend the successful EarthCube GeoCODES platform and community to unify data and tool description and re-use across geoscience domains. Building on the NSF CIF21 vision, the EarthCube program was formed to address the technological challenges surrounding data and software within the geosciences. Through extensive interaction with the community this culminated in two key activities around data discovery and reuse. First, the promotion, refinement, and adoption of schema.org to annotate geosciences metadata within distributed repositories so that datasets can be crawled. Second, the promotion and support for the adoption of notebooks to document, share, and reuse software as peer reviewed scholarly objects. A rallying point around these activities was the GeoCODES platform, which allows communities to stand up instances of scientific search engines specific to their domains, while building a community of geoscience data users and developers and, ultimately, reducing the time to science. This project will leverage this effort in the DeCODER platform to enable similar activities and outcomes across scientific communities. This work will continue the endeavor to support the scientific community in the adoption of schema.org and notebooks, facilitating this by providing DeCODER as an open source resource that can be customized by a given scientific community to create lightweight scientific gateways that bring together relevant distributed resources.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Enriching Scholarly Communication with Augmented Reality,OAC,2209624,Michelle Borkin,m.borkin@northeastern.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Today?s online shoppers can use augmented reality (AR) to aim their smartphones at an empty corner of their living room to see how a particular new lamp might look there, and diners can instantly see a restaurant's menu on their phones just by scanning a QR code posted at their table. This project will leverage the tremendous investments in AR made by the corporate world over the past several years, and the familiar ease of QR codes, to allow astronomers to see and explore the 3D Universe just as easily as they might shop for a new couch. Building on their 2021 success in publishing the first AR-enhanced figure in an American Astronomical Society Journal, the funding from this award will be used to create a robust system allowing any author to publish figures showcasing high-dimensional data in augmented reality environments. No expensive equipment beyond the same smartphones and tablets used by online shoppers will be needed. Astronomers will be able to see and explore their data in ""3D"" by walking around projections of it hovering above flat surfaces, or holding in their hands using AR target devices. Imagine, for example, a jet from a black hole, spewing out material from the center of a simulated galaxy, projected just above a researcher?s kitchen table, etc.Over the course of the project, the team will design, repeatedly test, and ultimately deploy an efficient and effective end-to-end system for embedding augmented reality figures in scholarly journals. By enriching scholarly communication, this new AR-based system is expected to accelerate the pace of scientific discovery. The system created will extend across multiple modular cyberinfrastructure components, including: data format standards; data analysis software; 3D conversion tooling; AR integration pipelines; visual ID encoding infrastructure; and the publication process. The system for authoring and deploying AR figures created and tested under this proposal represents cyberinfrastructure innovation that will ultimately open completely new channels for communication amongst all who rely on effective communication of high-dimensional data.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate for Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Discrete Simulation of Flexible Structures and Soft Robots,OAC,2209782,Mohammad Khalid Jawed,khalidjm@seas.ucla.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","From carbon nanotubes to human-size soft robots, flexible and deformable structures are present throughout the next generation of promising engineering disciplines. However, simulation of these mechanical systems is often slow, and simulation software is challenging to use. In addition, there is little support for simulating flexible structures in common robotics research and education software, limiting the use of intelligent soft robots to experts only. On the other hand, the computer graphics community has developed advanced software for simulating flexible structures like hair and fur. Recent research has shown these computer graphics approaches can accurately simulate soft robots and flexible structures faster than real-time. This project develops an easy-to-use open-source software platform for these fast physics-based simulations of flexible structures, and incorporates the software into the national cyberinfrastructure ecosystem. This software, DiSMech, can be used by researchers of all ages to investigate the mechanics of slender structures, autonomy for soft robots, and breakthrough designs for deformable machines.The objective of this work is to develop a discrete differential geometry (DDG) simulation environment into a widely-available software package capable of modeling soft and flexible structures. The DDG approach enables low-dimensional modeling of slender rods and flexible shells combined into arbitrary shapes, establishing a practical but still physically accurate contrast to computationally expensive finite element analysis (FEA) techniques. This work first develops a core software package for DiSMech that adapts prior work to meet the standard for national cyberinfrastructure: maintainable, extensible, and with a robust user interface. Next, a virtual testbed for a wide class of soft and flexible robots is built by incorporating DiSMech into an existing robotics software suite. The project team will use the combined software framework with a machine learning approach to develop a locomotion strategy for example soft robots. Finally, add-ons to DiSMech will incorporate machine learning alongside the DDG-based physics models for even faster simulations, demonstrating the research potential for this software in uncovering underlying physical phenomena. By advancing DDG-based physics simulations to capture a wide range of soft and flexible structures, with a computational speed sufficient for learned robot control, all in an easy-to-use interface, DiSMech addresses an important gap in the national cyberinfrastructure.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Divisions of Civil, Mechanical and Manufacturing Innovation and Electrical, Communications and Cyber Systems in the Directorate of Engineering.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Discrete Simulation of Flexible Structures and Soft Robots,OAC,2209783,Andrew Sabelhaus,apsabelhaus@cmu.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","From carbon nanotubes to human-size soft robots, flexible and deformable structures are present throughout the next generation of promising engineering disciplines. However, simulation of these mechanical systems is often slow, and simulation software is challenging to use. In addition, there is little support for simulating flexible structures in common robotics research and education software, limiting the use of intelligent soft robots to experts only. On the other hand, the computer graphics community has developed advanced software for simulating flexible structures like hair and fur. Recent research has shown these computer graphics approaches can accurately simulate soft robots and flexible structures faster than real-time. This project develops an easy-to-use open-source software platform for these fast physics-based simulations of flexible structures, and incorporates the software into the national cyberinfrastructure ecosystem. This software, DiSMech, can be used by researchers of all ages to investigate the mechanics of slender structures, autonomy for soft robots, and breakthrough designs for deformable machines.The objective of this work is to develop a discrete differential geometry (DDG) simulation environment into a widely-available software package capable of modeling soft and flexible structures. The DDG approach enables low-dimensional modeling of slender rods and flexible shells combined into arbitrary shapes, establishing a practical but still physically accurate contrast to computationally expensive finite element analysis (FEA) techniques. This work first develops a core software package for DiSMech that adapts prior work to meet the standard for national cyberinfrastructure: maintainable, extensible, and with a robust user interface. Next, a virtual testbed for a wide class of soft and flexible robots is built by incorporating DiSMech into an existing robotics software suite. The project team will use the combined software framework with a machine learning approach to develop a locomotion strategy for example soft robots. Finally, add-ons to DiSMech will incorporate machine learning alongside the DDG-based physics models for even faster simulations, demonstrating the research potential for this software in uncovering underlying physical phenomena. By advancing DDG-based physics simulations to capture a wide range of soft and flexible structures, with a computational speed sufficient for learned robot control, all in an easy-to-use interface, DiSMech addresses an important gap in the national cyberinfrastructure.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Divisions of Civil, Mechanical and Manufacturing Innovation and Electrical, Communications and Cyber Systems in the Directorate of Engineering.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Collaborative Research: Frameworks: Interoperable High-Performance Classical, Machine Learning and Quantum Free Energy Methods in AMBER",OAC,2209718,Darrin York,Darrin.York@rutgers.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","With support from the Office of Advanced Infrastructure and the Division of Chemistry at NSF, Professor Merz and his group will work on molecular simulation cyberinfrastructure. Molecular simulations have become an invaluable tool for research and technology development in chemical, pharmaceutical, and materials sciences. With the availability of specialized hardware such as graphics processing units (GPUs), molecular dynamics simulations using classical or molecular mechanical force fields have reached the spatial and temporal scales needed to address important real-world problems in the chemical and biological sciences. Free energy simulations are a particularly important and challenging class of molecular simulations that are critical to gain a predictive understanding of chemical processes. For example, free energy methods can predict the barrier height and rates for chemical reactions, whether a reaction will occur, or how tightly a drug binds to a target. These predictions are extremely valuable for the design of new catalytic agents or drugs. However, the predictive capability of free energy simulations is sensitive to the underlying model that describes the inter-atomic potential energy and forces. Accurate free energy simulations of chemical processes require potential energy models that capture the essential physics and can respond to changes in the chemical environment, but conventional force field models are unsuitable for many processes involving bond breaking and formation as seen, for example, in catalyst design. Consequently, there is great need to extend the scope of free energy methods by enabling the use of a broader range of potential energy models that are more accurate as well as reactive and/or capable of quantum mechanical many-body polarization and charge transfer. The cyberinfrastructure created by this project allows for the routine application of free energy methods, using quantum mechanics, machine learning, reactive and classical potentials to a myriad of important problems that advance the state-of-the art in the biological and chemical sciences. The tools can be applied by a range of scientists to address fundamental problems of national interest, for example, in the design of drugs against zoonotic diseases (e.g., COVID-19), the design of materials with novel functions and in the design of improved batteries. Given the sophistication of the methods employed, education of a diverse pool of chemical, biological and computer scientists to advance this field is essential and is addressed in this project, thereby training the next generation of computational scientists that will form the backbone of the work force of the future. The project develops accurate and efficient free energy software within a powerful new multiscale modeling framework in the AMBER suite of programs for applications in chemistry, biology, and materials science. The multiscale framework enables the design and use of new classes of mixed-method force fields that involve interoperability between several existing and emerging reactive, machine learning and quantum many-body potentials. These potentials have enhanced accuracy, robustness, and predictive capability compared to classical molecular mechanical force fields and enable the study of chemical reactions and catalysis. The cyberinfrastructure supports innovative multi-layered hybrid potentials that can be customized to meet the needs of complex applications in biotechnology development, enzyme design and drug discovery. A robust endpoint ""book-ending"" approach that leverages the GPU-accelerated capability of the AMBER molecular dynamics engine is used to reach these goals. Specifically, the open-source high-performance software for free energy simulations is designed for multi-layered hybrid potentials using combinations of linear-scaling many-body quantum mechanical methods via the GPU-accelerated QUICK package, scalable reactive ReaxFF force fields via the PuReMD package, as well as the recently developed DeepMD-SE, ANAKIN-ME (ANI) and AP-Net families of machine learning potentials. The cyberinfrastructure is built upon the existing high-performance CUDA MD engine in AMBER and extends it to a broad range of GPU-accelerated architectures using industry-standard programming models. Scalability is ensured using innovative parallel algorithms. High impact is achieved by leveraging AMBER's broad user base to expand the scope and success of FE applications. In this way, the project leverages existing recognized capabilities and actively engages a diverse team of collaborators and the broader molecular simulations community. The cyberinfrastructure delivered by the project enables a wide range of new and enhanced applications for a broad community of users in academia, industry, and national laboratories. These applications include drug discovery, enzyme catalysis, and biomaterials design. The AMBER suite of programs has a long-standing extensive worldwide userbase, and is widely used on national production cyberinfrastructure. The enhancement of AMBER as an established, proven sustainable, and widely used package will ensure that the software has a broad impact well beyond the end of the project. The project will also train a diverse population of students and researchers in theory, programming, computational chemistry/biology, computer science, scientific writing, and communication.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Chemistry within the NSF Directorate for Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Bifrost - A CPU/GPU Pipeline Framework for High Throughput Data Acquisition and Analysis,OAC,2103707,Gregory Taylor,gbtaylor@unm.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Modern computers, including cell phones and tablets, have sophisticated Graphics Processing Units (GPUs) that render the beautiful graphic displays in games. We are developing software that takes advantage of these same GPUs for capturing and processing data from astronomical telescopes. This allows us to benefit from all the years of effort spent developing these powerful computational tools. This software, known as Bifrost, is currently in use at the Long Wavelength Array (LWA), a radio telescope for exploration of a broad scientific portfolio ranging from the study of Cosmic Dawn when the first stars and galaxies lit up the Universe, to understanding the properties of the Earth's ionosphere. We are actively developing Bifrost to make it both more powerful and easier to use for other telescopes. Eventually we aim for Bifrost to be available as a more general purpose framework that can be applied to research projects beyond astronomy. About 5 years ago we adopted a commodity equipment design for the second LWA station (LWA-SV) which makes use of computing servers with GPUs to handle the data capture, beamforming, and correlation at the station level. Previously these functions were taken on by dedicated hardware referred to as the Digital Processor. However, this custom-hardware design was expensive to build and maintain, lacks flexibility, and cannot be easily replicated for future LWA stations. In contrast the commodity approach is easier to maintain, much more flexible and expandable, and can be readily adapted to new LWA stations. We are engaged in a concentrated effort to improve the underpinnings of Bifrost. This involves increasing the data rates that Bifrost is capable of handling, improving the application programming interface, and providing tools to make it easier for users to develop and test new pipelines. Through this award we are working with collaborators to incorporate Bifrost in telescopes and instruments currently under development. The availability of Bifrost will increase the scientific return of not only radio astronomy but also other areas where high throughput data processing is needed.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: SHF: Medium: Bringing Python Up to Speed,CCF,1955610,Leonidas Lampropoulos,leonidas@umd.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","The Python programming language is among today's most popular computer programming languages and is used to write software in a wide variety of domains, from web services to data analysis to machine learning. Unfortunately, Python?s lightweight and flexible nature -- a major source of its appeal -- can cause significant performance and correctness problems - Python programs can suffer slowdowns as high as 60,000x over optimized code written in traditional programming languages like C and C++, and can require an order-of-magnitude more memory. Python's flexible, ?dynamic? features also make its programs error-prone, with many coding errors only being discovered late in development or after deployment. Python?s frequent use as a ""glue language"" -- to integrate and interact with different components written in C or C++ -- exposes many Python programs to the unique dangers of those languages, including susceptibility to memory corruption-based security vulnerabilities. This project aims to remedy these problems by developing new technology for Python in the form of novel performance analysis tools, memory-reduction and speed-improving optimizations (including support for multi-core execution), automated software testing frameworks, and common benchmarks to drive their evaluation.This project will develop (1) performance analysis tools that help Python programmers accurately identify the sources of slowdowns; (2) techniques for automatically identifying code that can be replaced by calls to C/C++ libraries; (3) an approach to unlocking parallelism in Python threads, which currently must execute sequentially due to a global interpreter lock; and (4) automatic techniques to drastically reduce the memory footprints of Python applications. To improve the correctness of Python applications, the project will develop novel automated testing techniques that (1) augment property-based random testing with coverage-guided fuzzing; (2) employ concolic execution for smarter test generation and input minimization; (3) synthesize property-specific generator functions; (4) leverage statistical clustering techniques to reduce duplicated failure-inducing inputs; and (5) leverage parallelism and adaptive scheduling algorithms to increase testing throughput. The project will develop a set of ""bug benchmarks"" -- indeed, a novel benchmark-producing methodology -- to evaluate these techniques. The twin threads of performance and correctness are synergistic and complementary: automatic testing drives performance analysis, while performance optimizations (like parallelism) speed automatic testing.This award is co-funded by the Software & Hardware Foundations Program in the Division of Computer & Computing Foundations, and the NSF Office of Advanced Cyberinfrastructure.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Enabling Accurate Thermal Transport Calculations in LAMMPS,OAC,1931436,Christopher Wilmer,wilmer@pitt.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Computational thermal transport research is critical to the development of new materials that can address challenging energy and environmental problems. Molecular dynamics (MD) simulations are used extensively to study thermal transport in materials. One of the most widely used MD software packages is the Large-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS). It is the primary aim of this project to create and carefully implement improved thermal transport calculation methods in LAMMPS. This problem is made challenging by the fact that this software has hundreds of thousands of users and the solution must be merged into the core LAMMPS code, as opposed to offered as a modular ""plug-in"". The three objectives of the project are: (1) to implement a corrected heat flux computation for all supported many-body potentials in LAMMPS, (2) to identify the types of molecular systems most affected by the changed heat flux computations, and (3) educate the LAMMPS community on how to implement heat flux in new potentials correctly as well as train new scientists to contribute professional-quality code to the LAMMPS code base. This research will, among other broad impacts, enable large-scale computational screening of materials to accurately predict their thermal properties, which fulfills one of the key goals of the Materials Genome Initiative (MGI). Furthermore, it is an innovative, scalable, reusable software component that supports training for the broad LAMMPS user community as well as general workforce development via training to undergraduates, and ensures the new software capacities are widely available via the open-source LAMMPS package. Additionally, the project provides professional software engineering training to graduate and undergraduate students via a highly-trained resident software developer and resources from the Center for Research Computing at the University of Pittsburgh. Due to the large user base and open source nature of LAMMPS, the research is expected to have a broad impact; these software innovations will be widely available across both industry and academia.The most common MD technique to compute thermal conductivity, the Green-Kubo method, yields incorrect results in LAMMPS for the majority of molecular simulations. Although the ramifications of the error in the heat flux for the thousands of papers already published using LAMMPS has yet to be fully determined, preliminary data indicates that for liquid hydrocarbons, the heat flux through a many-body potential can be underreported by 95%, leading to a total error of the heat flux of 22%. Unless correct thermal transport calculations can be achieved and implemented for this widely used and highly optimized software package, research and development of materials with novel thermal properties will be significantly hindered. There is no widely available MD code that is able to correctly compute heat flux. The one exception is for molecular systems where only pair-wise interactions exist (which excludes all molecules with greater than two atoms), in which case the current LAMMPS implementation gives correct results. The heat flux computation problem in LAMMPS was identified more than four years ago, but the lack of a correct implementation in any widely used MD software package speaks to the challenge of finding and dedicating software engineers to do this important work. This project addresses the theory, implementation pathway, and validation strategy for an expansive re-implementation of heat flux computations in LAMMPS for many-body potentials.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Flexible & Open-Source Models for Materials and Devices,OAC,1931473,Michele Pavanello,m.pavanello@rutgers.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","The project will develop first principles materials modeling software that can approach multiple length and time scales (multiscale). This software will be capable of modeling systems as complex as entire devices and materials of mesoscopic sizes. Over the course of the project the principal investigators plan to develop an open-source python-based software aimed at standardizing and generalizing multiscale simulations methods. This will enable the use of computer modeling in the design of new compounds, materials and devices. The goals are to render multiscale simulations reproducible and accessible by the broader community. In that context, the project will address the notion of ""lab 2.0"", by which computer simulations replace laboratory experiments in tasks such as materials design and costly combinatorial searches for viable chemical processes. The software will be self-optimized using machine learning and exploit linear workflows approachable by nonexperts. Education and diversity will be promoted by direct participation of underrepresented minorities from high schools and colleges in hackathon workshops and summer research programs.An approach that leverages the long-range multiscale capabilities of continuum models with accurate short-range atomistic descriptions of specific interactions, and that exploits the ideal scalability of quantum-embedding techniques, will be investigated. The main driver of the proposed implementation will be a Python codebase which will carry out the part of current software that is not computationally heavy, but instead is code heavy where many lines of code are needed in typically non-object-oriented languages. This is key to obtain the desired cluster-topology-agnostic workflows. Longstanding problems related to computational scalability and code stiffness will addressed in a three-pronged approach aimed at developing (1) modular tools implementing modules with highly object-oriented codes (e.g., quantum, classical atomistic, and continuum solvers), (2) hybrid tools implementing combinations of modular tools in a way that best exploits high-performance computing architectures, and (3) hyper tools implementing a high-level data-enabled optimization strategy that generates optimal workflows combining several hybrid tools, thereby making the software of broad applicability and accessible to nonexperts. These goals will render multiscale simulations reproducible and accessible by the broader community. The project will address the ""lab 2.0"" paradigm, by which computer simulations replace laboratory experiments in tasks such as materials design and combinatorial searches for viable chemical processes. The resultant software will be self-optimized using machine learning and exploit linear workflows approachable by nonexperts. Education and diversity will include the direct participation of underrepresented minorities from high schools and colleges in hackathon workshops and summer research programs.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Chemistry and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Software: NSCI: A Quantum Electromagnetics Simulation Toolbox (QuEST) for Active Heterogeneous Media by Design,OAC,1835267,Carlo Piermarocchi,carlo@pa.msu.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Designing novel optical materials with enhanced properties would impact many areas of science and technology, leading to new lasers, better components for photonics, and to a deeper understanding of how light interacts with matter. This project will develop software that simulates how light would propagate in yet to be made complex optical materials. The final product will be a software toolbox that computes the dynamics of each individual light emitter in the materials rather than calculating an average macroscopic field. This toolbox will permit the engineering and optimization of optical properties by combining heterogeneous components at the nanoscale. The software will be disseminated widely to enable scientists worldwide to conduct research on this area and will provide a blueprint for broader applications to magnetic materials and ultrasound acoustics. Two graduate students will be engaged in this research and will be trained in interdisciplinary topics encompassing fundamental physics, mathematics, materials science, and software engineering.Three innovative modules will be implemented and tested in the software toolbox: (i) A module based on Time Domain Accelerated Integrated Methods. These methods rely on the separation into near field and far field terms in the interaction between optically active centers and introduce a hierarchical structure that can be computationally exploited. This module will dramatically reduce computational costs, leading to simulations of realistic systems with millions of optical emitters. (ii) A stochastic optimization module that maximizes materials functionalities based on geometrical and compositional distribution of the emitters in the medium. This optimization module will simulate in parallel several virtual samples and will guide the computational effort towards optimal materials. (iii) A rationalized representation of electromagnetic field localization based on the novel mathematical concept of landscape functions, which effectively reduces the eigenvalue problem associated to localization into a static boundary condition problem. This computationally efficient approach provides approximated eigenvalues and quickly identifies the sub-regions of the system that support electromagnetic field localization.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: NSCI : Computational and data innovation implementing a national community hydrologic modeling framework for scientific discovery,OAC,1835569,David Tarboton,davidtarboton@usu.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: A task-based code for multiphysics problems in astrophysics at exascale,OAC,2209656,Mark Scheel,scheel@tapir.caltech.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","Current and upcoming computers will run at exascale, over a hundred times more powerful than typical machines of today. Many algorithms used in current codes will not be able to take advantage of these new machines. The researchers will complete the development of an open-source community code for multi-scale, multi-physics problems in astrophysics and gravitational physics. The code uses transformative algorithms to reach the exascale. The techniques can be applied across discipline boundaries in fluid dynamics, geoscience, plasma physics and nuclear physics and engineering. The development of this new code has been driven by the current deployment of gravitational wave detectors such as LIGO. To fully understand and analyze the signals and waveforms measured with such detectors, it is essential that accurate, robust, and efficient computational tools be available for solving the dynamical Einstein equations over very long time scales. The recent detection of the merger of a neutron star-neutron star merger by LIGO and by a host of electromagnetic telescopes has ushered in the era of multi-messenger astronomy. The extreme energy densities of matter and radiation and the highly dynamic spacetimes of these events probe fundamental physics inaccessible to terrestrial experiments. The new code will be made available as open-source community cyberinfrastructure. The researchers will reach out to other communities within astrophysics (e.g., star formation, space plasma physics) and across discipline boundaries to fluid dynamics, geoscience, plasma physics, nuclear engineering etc. Young researchers trained in these techniques are in great demand, both in academia and as highly-skilled members of the industrial STEM workforce. Undergraduates will participate in the research by producing visualizations.The new code uses discontinuous Galerkin methods and task-based parallelism to accomplish its desired goals. This framework will allow the multi-physics applications to be treated both accurately and efficiently on the new architectures of petascale and exascale machines. The code is designed to scale to over a million cores for efficient exploration of the parameter space of potential sources and allowed physics, and for the high-fidelity predictions needed to realize the promise of multi-messenger astronomy. The code will allow astrophysicists to explore the mechanisms driving core-collapse supernovae and the properties of stellar remnants, to understand electromagnetic transients and gravitational-wave phenomena in compact objects, and to reveal the dense matter equation of state. The two key algorithmic innovations in the code, the discontinuous Galerkin method coupled with task-based parallelism, promise revolutionary impact in other fields relying on numerical solution of partial differential equations at the exascale.This project advances the objectives of ""Windows on the Universe: the Era of Multi-Messenger Astrophysics"", one of the 10 Big Ideas for Future NSF Investments. This project advances also the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Physics and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: A task-based code for multiphysics problems in astrophysics at exascale,OAC,2209655,Saul Teukolsky,saul@astro.cornell.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","Current and upcoming computers will run at exascale, over a hundred times more powerful than typical machines of today. Many algorithms used in current codes will not be able to take advantage of these new machines. The researchers will complete the development of an open-source community code for multi-scale, multi-physics problems in astrophysics and gravitational physics. The code uses transformative algorithms to reach the exascale. The techniques can be applied across discipline boundaries in fluid dynamics, geoscience, plasma physics and nuclear physics and engineering. The development of this new code has been driven by the current deployment of gravitational wave detectors such as LIGO. To fully understand and analyze the signals and waveforms measured with such detectors, it is essential that accurate, robust, and efficient computational tools be available for solving the dynamical Einstein equations over very long time scales. The recent detection of the merger of a neutron star-neutron star merger by LIGO and by a host of electromagnetic telescopes has ushered in the era of multi-messenger astronomy. The extreme energy densities of matter and radiation and the highly dynamic spacetimes of these events probe fundamental physics inaccessible to terrestrial experiments. The new code will be made available as open-source community cyberinfrastructure. The researchers will reach out to other communities within astrophysics (e.g., star formation, space plasma physics) and across discipline boundaries to fluid dynamics, geoscience, plasma physics, nuclear engineering etc. Young researchers trained in these techniques are in great demand, both in academia and as highly-skilled members of the industrial STEM workforce. Undergraduates will participate in the research by producing visualizations.The new code uses discontinuous Galerkin methods and task-based parallelism to accomplish its desired goals. This framework will allow the multi-physics applications to be treated both accurately and efficiently on the new architectures of petascale and exascale machines. The code is designed to scale to over a million cores for efficient exploration of the parameter space of potential sources and allowed physics, and for the high-fidelity predictions needed to realize the promise of multi-messenger astronomy. The code will allow astrophysicists to explore the mechanisms driving core-collapse supernovae and the properties of stellar remnants, to understand electromagnetic transients and gravitational-wave phenomena in compact objects, and to reveal the dense matter equation of state. The two key algorithmic innovations in the code, the discontinuous Galerkin method coupled with task-based parallelism, promise revolutionary impact in other fields relying on numerical solution of partial differential equations at the exascale.This project advances the objectives of ""Windows on the Universe: the Era of Multi-Messenger Astrophysics"", one of the 10 Big Ideas for Future NSF Investments. This project advances also the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Physics and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: An Infrastructure for Software Quality and Security Issues Detection and Correction,OAC,2103596,Marouane Kessentini,kessentini@oakland.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Research into more effective software development has the potential to make the infrastructure on which so many aspects of society depend less costly and more secure in the scientific community, industry and government agencies. In particular, the scientific community is proposing millions of scientific software prototypes to enable reproducibility of research results in almost every domain. Scientists may frequently introduce security and quality issues into existing scientific software via their code changes due to their limited experience in software quality and security and the lack of tools for quality and security assessments that can be easily used and integrated in programming environments. Thus, several existing scientific software projects are difficult to 1) extend by scientists due to their poor quality and 2) deploy by industry due to the likelihood of security vulnerabilities and the bad development practices used. Without a unified and easy-to-integrate framework for detecting, fixing, and documenting vulnerability and quality issues in scientific projects, the reusability, extendibility, safe deployment, and technology transfer of scientific projects will remain limited. This project builds a sustainable, community-driven software security and quality analysis framework. These tools enable more scientists to build better software and to transfer their prototypes to industry by following the best software development practices. Its integrated education plan will bring undergraduate and graduate computer science students more awareness and expertise in the evolution of software systems, including security and quality issues.This project develops a framework for detecting, fixing, and documenting security and quality issues. It will continuously monitor the software repository to identify security vulnerabilities and quality issues based on static and dynamic analyses, and then find the best sequence of code changes to prioritize and fix them. The developers can review the recommendations and their impacts in a detailed report and select the code changes that they want to apply. The framework includes a visualization support of the quality and security changes over the evolution of the project. Furthermore, non-expert programmers from the scientific community can use the automatically generated documentation by the framework to understand the severity of the detected issues and necessary code changes to fix them. The project has the potential to revolutionize how developers monitor the evolution of their systems in continuous integration environments by unifying security and quality issues detection and correction and enabling their automated documentation. All tools and methodologies will be empirically evaluated in collaboration with scientists from various domains. These tools will enable more scientists to build better software and transfer their prototypes to industry by following best development practices.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: HDR: Building the Twenty-First Century Citizen Science Framework to Enable Scientific Discovery Across Disciplines,OAC,1835530,Lucy Fortson,fortson@physics.umn.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","A team of experts from five institutions (University of Minnesota, Adler Planetarium, University of Wyoming, Colorado State University, and UC San Diego) links field-based and online analysis capabilities to support citizen science, focusing on three research areas (cell biology, ecology, and astronomy). The project builds on Zooniverse and CitSci.org, leverages the NSF Science Gateways Community Institute, and enhances the quality of citizen science and the experience of its participants.This project creates an integrated Citizen Science Cyberinfrastructure (CSCI) framework that expands the capacity of research communities across several disciplines to use citizen science as a suitable and sustainable research methodology. CSCI produces three improvements to the infrastructure for citizen science already provided by Zooniverse and CitSci.org: - Combining Modes - connecting the process of data collection and analysis; - Smart Assignment - improving the assignment of tasks during analysis; and - New Data Models - exploring the Data-as-Subject model. By treating time series data as data, this model removes the need to create images for classification and facilitates more complex workflows. These improvements are motivated and investigated through three distinct scientific cases: - Biomedicine (3D Morphology of Cell Nucleus). Currently, Zooniverse 'Etch-a-Cell' volunteers provide annotations of cellular components in images from high-resolution microscopy, where a single cell provides a stack containing thousands of sliced images. The Smart Task Assignment capability incorporates this information, so volunteers are not shown each image in a stack where machines or other volunteers have already evaluated some subset of data. - Ecology (Identifying Individual Animals). When monitoring wide-ranging wildlife populations, identification of individual animals is needed for robust estimates of population sizes and trends. This use case combines field collection and data analysis with deep learning to improve results. - Astronomy (Characterizing Lightcurves). Astronomical time series data reveal a variety of behaviors, such as stellar flares or planetary transits. The existing Zooniverse data model requires classification of individual images before aggregation of results and transformation back to refer to the original data. By using the Data-as-Subject model and the Smart Task Assignment capability, volunteers will be able to scan through the entire time series in a machine-aided manner to determine specific light curve characteristics.The team explores the use of recurrent neural networks (RNNs) to determine automated learning architectures best suited to the projects. Of particular interest is how the degree to which neighboring subjects are coupled affects performance. The integration of existing tools, which is based on application programming interfaces (APIs), also facilitates further tool integration. The effort creates a citizen science framework that directly advances knowledge for three science use cases in biomedicine, ecology, and astronomy, and combines field-collected data with data analysis. This has the ability to solve key problems in the individual applications, as well as benefiting the research of the dozens of projects on the Zooniverse platform. It provides benefits to researchers using citizen scientists, and to the nearly 1.6 million citizen scientists themselves.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Research on Learning in Formal and Informal Settings, within the NSF Directorate for Education and Human Resources.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: ALE-AMR Framework and the PISALE Codebase,OAC,2005259,Alice Koniges,koniges@hawaii.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","The solution of partial differential equations (PDEs) on modern high performance computing (HPC) platforms is essential to the continued success of research and modeling for a wide variety of areas of importance to the national interest. This project will make available software for modeling with PDEs. It will also apply the code for simulations of complex groundwater flow processes in Hawaiian islands characterized by highly heterogeneous volcanic rocks and dynamic interaction between freshwater and seawater. Roughly half the population in the US lives near coastal areas where groundwater supplies much of the domestic, agricultural, and industrial water supply. Almost all of Hawaii?s domestic water use is pumped from volcanic aquifer systems since the islands are completely surrounded by the Pacific Ocean. In Hawaii?s groundwater resources, freshwater accumulates on top of the denser underlying saltwater, making it highly susceptible to anthropogenic activities and saltwater intrusion induced by possible sea water and volcanic events. It is essential that Hawaii?s groundwater resources are properly modeled and managed for sustainable use. Island-scale numerical groundwater flow modeling with PDEs on HPC will play an important role in predicting the sustainable yields for the volcanic aquifer systems and planning groundwater resources management. The software in this project will be used both for applications and as a nexus for student involvement in HPC. Curriculum material associated with the project will be developed and offered at university level groundwater modeling classes. The PDE software developed, distributed, and applied in this project uses an innovative combination of advanced mathematical techniques for the solution of PDEs including parallel software tools to dynamically adapt the grids and special Lagrangian-flow methods that allow for solution of equations that can reproduce the sharp freshwater-seawater interface observed in sea water monitoring locations. The software is particularly appropriate for applications with equations that can be couched in a conservation law form. Source terms of these applications can be included using complex numerical techniques including the ability to handle anisotropic tensors components.The software is based on techniques of ALE (Arbitrary Lagrangian Eulerian Methods) with AMR (Adaptive Mesh Refinement) to create a publicly available sustainable branch of the software known as PISALE for Pacific Island Structured-amr with ALE. In addition to the subsurface flow and transport application in Hawaiian aquifers, the project will provide capability for collaborative research in a variety of fields that require efficient solution of PDEs on advanced HPC architectures.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Hydrologic Sciences Program, part of the Division of Earth Sciences, within the NSF Directorate of Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Data: NSCI: HDR: GeoSCIFramework: Scalable Real-Time Streaming Analytics and Machine Learning for Geoscience and Hazards Research,OAC,1835661,Diego Melgar,dmelgarm@uoregon.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","This project develops a real-time processing system capable of handling a large mix of sensor observations. The focus of this system is automation of the detection of natural hazard events using machine learning, as the events are occurring. A four-organization collaboration (UNAVCO, University of Colorado, University of Oregon, and Rutgers University) develops a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research. This work will support rapid analysis and understanding of data associated with hazardous events (earthquakes, volcanic eruptions, tsunamis). This project uses a collaboration between computer scientists and geoscientists to develop a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research. It focuses on the aggregation and integration of a large number of data streams into a coherent system that supports analysis of the data streams in real-time. The framework will offer machine-learning-based tools designed to detect signals of events, such as earthquakes and tsunamis, that might only be detectable when looking at a broad selection of observational inputs. The architecture sets up a fast data pipeline by combining a group of open source components that make big data applications viable and easier to develop. Data sources for the project draw primarily upon the 1500+ sensors from the EarthScope networks currently managed by UNAVCO and the Incorporated Research Institutions for Seismology (IRIS), as well as the Ocean Observatories Initiative (OOI) cabled array data managed by Rutgers University. Machine learning (ML) algorithms will be researched and applied to the tsunami and earthquake use cases. Initially, the project plans to employ an advanced convolutional neural network method in a multi-data environment. The method has only been applied to seismic waveforms, so the project will explore extending the method to a multi-data environment. The approach is expected to be extensible beyond detection and characterization of earthquakes to include the onset of other geophysical signals such as slow-slip events or magmatic intrusion, expanding the potential for new scientific discoveries. The framework is applied to use cases in the Cascadia subduction zone and Yellowstone: these locations combine the expertise of the science team with locations where EarthScope and OOI have the greatest concentration of instruments. The architecture will be transportable and scalable, running in a Docker environment on laptops, local clusters and the cloud. Integral to the project will be development, documentation and training using collaborative online resources such as GitLab and Jupyter Notebooks, and utilizing NSF XSEDE resources to make larger datasets and computational resources more widely available.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program and Division of Earth Sciences within the NSF Directorate for Geosciences, the Big Data Science and Engineering Program within the Directorate for Computer and Information Science and Engineering, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: SENSORY: Software Ecosystem for kNowledge diScOveRY - a data-driven framework for soil moisture applications,OAC,2103836,Rodrigo Vargas,rvargas@udel.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Tools for gathering soil moisture data (such as in situ soil sensors and satellites) have differing capabilities. In situ soil moisture data has fine-grained spatial and high temporal resolution, but is only available in limited areas; satellite data is available globally, but is more coarse in resolution. Existing software tools for studying the dynamic characteristics of soil moisture data are limited in their ability to model soil moisture at multiple spatial and temporal scales, and these limitations hamper scientists? ability to address urgent practical problems such as wildfire management and food and water security. Accurate gathering and effective modeling of soil moisture data are essential to address pressing environmental challenges. This interdisciplinary project designs, builds, and shares a data-driven software ecosystem for soil moisture applications. This software ecosystem models and predicts soil moisture at scales suitable to support studies in forestry, precision agriculture, and earth surface hydrology.This project connects multi-disciplinary advances across the scientific community (such as generating datasets at scale and supporting cloud-based cyberinfrastructures) to develop a data-driven software ecosystem for analyzing, visualizing, and extracting knowledge from the growing data collections (from fine-grained, in situ soil sensor information to coarse-grained, global satellite measurements) and releasing this knowledge to applications in environmental sciences. Specifically, this project (a) develops scalable methodologies to integrate and analyze soil moisture data at multiple spatial and temporal scales; (b) implements a data-driven software ecosystem to access complex information and provide basic and applied knowledge to inform researchers and stakeholders interested in soil moisture dynamics (scientists, educators, government agencies, policy makers); and (c) builds cyberinfrastructures to support discovery on cloud platforms, lowering resource barriers to improve accessibility and interoperability.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Hydrologic Sciences Program, the Division of Earth Sciences, and the Division of Integrative and Collaborative Education and Research within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Scaling MetPy to Big Data Workflows in  Meteorology and Climate Science,OAC,2103682,Ryan May,rmay@ucar.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","MetPy is a Python-based software package for atmospheric science; it provides modern, well-tested, domain-specific software tools for reading data formats, performing calculations, and creating visualizations. MetPy builds upon an extensive set of community-developed scientific Python tools, and leverages technologies such as continuous integration, automated documentation generation, screencasts, and Jupyter notebook tutorials. This project advances MetPy to address some current limitations in supported data formats, scalability, and run-time performance; it makes MetPy more suitable for working on much larger datasets, frequently encountered in climate science and ensemble-based modeling studies. Addressing these areas allows MetPy to continue to be a powerful tool for Python users in the atmospheric sciences for both small and large datasets. When equipped with modern and well-engineered tools, researchers will be able to better utilize the large amounts of data available in a more time-efficient way.This project has three main goals: enabling efficient access to big and small datasets, enabling access to cloud-based datasets, and creating training resources for using MetPy with big data. In tackling these challenges, the project is creating a benchmark suite for MetPy, to quantify the current performance of MetPy in important workflows as well as the performance improvements that occur as a result of further development. Using performance profiling tools, bottlenecks in MetPy are identified and optimized using Python performance tools like Cython and Numba. MetPy is also being refactored to work better with the Dask library, which provides facilities for distributed computing in Python and would allow MetPy to work more effectively with large datasets. The World Meteorological Organization (WMO) has made GRIB (GRIdded Binary) its standard format for gridded model output, and BUFR (Binary Universal Form for the Representation of meteorological data) the standard format to encode meteorological observational data. MetPy enhancements developed in this project will enable reading of additional datasets used in large cloud-based data holdings, such as GRIB and BUFR. All of this work will be featured in additional, freely available training materials in MetPy?s online documentation.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Physical and Dynamic Meteorology Program and the Division of Integrative and Collaborative Education and Research within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Improving the Understanding and Representation of Atmospheric Gravity Waves using High-Resolution Observations and Machine Learning,OAC,2004492,Aditi Sheshadri,aditi_sheshadri@stanford.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Geophysical gravity waves are a ubiquitous phenomenon in Earth?s atmosphere and ocean, made possible by the interaction of gravity with a stratified, or layered fluid. They are excited in the atmosphere when winds flow over mountains, by thunderstorms and other strong convective systems, and when winter storms intensify. Gravity waves play an important role in the momentum and energy balance of the atmosphere, with direct impacts on surface weather and climate through their effect on the variability of key features of the climate system such as the jet streams and stratospheric polar vortices. These waves present a challenge to weather and climate prediction: waves on scales of 100 meters to 100 kilometers can neither be systematically measured with conventional observational systems, nor properly resolved in global atmospheric models. As a result, these waves must be represented, or approximated, based on the resolved flow that can be directly simulated. Current representations of gravity waves are severely limited by computational necessity and the scarcity of observations, leading to inaccuracies or uncertainties in short term weather and long term climate predictions. The objective of this project is to leverage unprecedented observations from Loon high altitude balloons and use specialized high resolution computer simulations and machine learning techniques to develop accurate, data-informed representation of gravity waves. The outcomes of this project are expected to result in better weather and climate models, thus improving short term forecasts of weather extremes and long term climate change projections, which have substantial societal benefits. Furthermore, the project will support the training of 3 Ph.D. students, 4 postdocs, and 10 undergraduate summer researchers to work at the intersection of atmospheric dynamics, climate modeling, and data science, thus preparing the next generation of scientists for interdisciplinary careers.The project will deliver two key advances. First, it will open up a new data source to constrain gravity wave momentum transport in the atmosphere. Loon LLC has been launching super pressure balloons since 2013 to provide global internet coverage. Very high resolution position, temperature, and pressure observations (taken every 60 seconds) are available from thousands of flights. This provides an unprecedented source of high resolution observations to constrain gravity wave sources and propagation. The project will process the balloon measurements and, in concert with novel high resolution simulations, establish a publicly available dataset to open up a potentially transformational resource for observationally constrained assessment of gravity wave sources, propagation, and breaking. The second transformation will be using machine learning techniques to develop computationally feasible representations of momentum deposition by gravity waves. Current physics-based representations only account for vertical propagation of the waves (i.e., they are one dimensional) and ignore their horizontal propagation. Using the data based on the Loon measurements and high resolution models, one and three dimensional data driven representations will be developed to more accurately and efficiently represent the effects of gravity waves in weather and climate models. These novel representations will be implemented in idealized atmospheric models to study the role of gravity waves in the variability of the extratropical jet streams, the Quasi Biennial Oscillation (a slow variation of the winds in the tropical stratosphere) and the polar vortex of the winter stratosphere, enabling better understanding their response to increased atmospheric greenhouse gas concentrations.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: AMR-H: Adaptive multi-resolution high-order solver for multiphase compressible flows on heterogeneous platforms,OAC,2103509,Sanjiva Lele,lele@stanford.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","In the past decades, computational research has enabled high-fidelity simulations of complex fluid dynamics problems. However, the new generation of high performance computing architectures present significant challenges in portability and, more importantly, parallel performance of complex science application software. This work develops a multi-purpose computational fluid dynamics solver for high-fidelity high-order adaptive resolution simulations. It leverages the state-of-the-art high performance programming models, Legion and Kokkos, which guarantees the performance portability on various existing and upcoming high performance computing platforms. Additionally, it integrates the commonly used numerical and physical modules, with an easy-to-use programming interface for users. As a significant benefit, the researchers can maintain their focus on physical modeling and not require a deep understanding of code design for new high-performance hardware. The educational and community outreach elements of the project will develop a growing community of computational scientists and engineers who are educated to exploit the power of task-level parallelism and enable a new era in high-fidelity computational science.This project develops a general computational framework combining high-order, high accuracy, solution-adaptive discretizations of partial differential equations (with emphasis on flows of non-ideal fluids) tailored to the physics they represent. The discretization is optimized for high resolving efficiency, allows optimal use of the computational degrees of freedom and high utilization of the computer resources due to its high arithmetic intensity and data locality. Adaptive mesh refinement in combination with high-order multi-resolution compact scheme allows for easy pre-processing and meshing for complex-geometry problems. Co-designing the numerical framework with new developments in the Legion framework would allow for automated, optimized runtime scheduling of tasks involving computational kernels and data movement across memory hierarchies. This, combined with efficient leveraging of Kokkos, would free the computational scientist/engineer from hardware specific programming models and allow exascale computations on heterogeneous computers. It will enable first of its kind simulations of compressible multiphase flow phenomena in turbulent flow regimes for retrograde fluids on exascale platforms.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Software: NSCI: HDR: Building An HPC/HTC Infrastructure For The Synthesis And Analysis Of Current And Future Cosmic Microwave Background Datasets,OAC,1835865,Julian Borrill,borrill@berkeley.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","The photons created in the Big Bang have experienced the entire history of the Universe, and every step in the evolution of the Universe has left its mark on their statistical properties. Observations of these photons have the potential to unlock the secrets of fundamental physics and cosmology, and to provide key insights into the formation and evolution of cosmic structures such as galaxies and galaxy clusters. Since the traces of these processes are so faint, one must gather enormous datasets to be able to detect them above the unavoidable instrumental and environmental noise. This in turn means that one must be able to use the most powerful computing resources available to be able to process the volume of data. These computing resources include both highly localized supercomputers and widely distributed grid and cloud systems. The PI and Co-Is will develop a common computing infrastructure able to take advantage of both types of resource, and demonstrate its suitability for ongoing and planned experiments by adapting the analysis pipelines of four leading Big Bang observatories to run within it. In addition to enabling the full scientific exploitation of these extraordinarily rich data sets, the investigators will mentor students engaged in this research and run summer schools in applied supercomputing.This project seeks to enable the detection of the faintest signals in Cosmic Microwave Background radiation, and in particular the pattern of peaks and troughs in the angular power spectra of its polarization field. In order to obtain these spectra one must first reduce the raw observations to maps of the sky in a way the preserve the correlations in the signal and characterizes the correlation in the noise. While the algorithms to perform this reduction are well-understood, applying them to data sets with quadrillions to quintillions of observations is a very serious computational challenge. The computational resources available to the project to address this include both high performance and high throughput computing systems, and one will need to take advantage of both of them. This project will develop a joint high performance/high throughput computational framework, and deploy within it analysis pipelines currently being fielded by the ongoing Atacama Cosmology Telescope, BICEP/Keck Array, POLARBEAR, and South Pole Telescope experiments. By doing so one will also demonstrate the frameworks efficacy for the planned Simons Observatory and CMB-S4 experiments.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Advancing Data Science and Analytics for Water (DSAW),OAC,1931278,Anthony Castronova,acastronova@cuahsi.org,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Scientific challenges in hydrology and water resources such as understanding impacts of variable climate, sustainability of water supply with population growth and land use change, and impacts of hydrologic change on ecosystems and humans are increasingly data intensive. The volume of data produced by environmental scientists to study hydrologic systems requires advanced software tools for effective data visualization, analysis, and modeling. Scientists spend much of their time accessing, organizing, and preparing datasets for analyses, which can be a barrier to efficient analyses and hinders scientific inquiries and advances. This project will develop new software that will enhance scientists' ability to apply advanced data visualization and analysis methods (collectively referred to as ""data science"" methods) in the hydrology and water resources domain. The project will promote standardized software tools and data formats to help scientists enhance the consistency, share-ability, and reproducibility of the analyses they perform - all of which are important in building trust in scientific results. The software developed in the project will make data loading and organization for analysis easier, reducing the time spent by scientists in choosing appropriate data structures and writing computer code to read and parse data. It will enable users to automatically retrieve data from the HydroShare system, which is a hydrology domain data repository, as well as from important national water data sources like the United States Geological Survey's National Water Information System. The software will automatically load data from these sources into standardized and high performance data structures targeted to specific scientific data types and that integrate with visualization, analysis, and other data science capabilities commonly used by scientists in the hydrology and water resources domains. The project will also reduce the technical burden for water scientists associated with creating a computational environment within which to execute their analyses by installing and maintaining the Python packages developed within the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI) HydroShare-linked JupyterHub environment. Finally, the project will demonstrate the functionality and use of the software by producing a set of educational modules based on real water-data science applications that provide a specific mechanism for delivering the software to the community and promoting its use in classroom and research environments.Scientific and related management challenges in the water domain are inherently multi-disciplinary, requiring synthesis of data of multiple types from multiple domains. Many data manipulation, visualization, and analysis tasks performed by water scientists are difficult because (1) datasets are becoming larger and more complex; (2) standard data formats for common data types are not always agreed upon, and, when they are, they are not always mapped to an efficient structure for visualization and/or analysis within an analytical environment; and (3) water scientists generally lack training in data intensive scientific methods that would enable them to use new and existing tools to efficiently tackle large and complex datasets. This project will advance Data Science and Analytics for Water (DSAW) by developing: (1) an advanced object data model that maps common water-related data types to high performance data structures within the object-oriented Python language and analytical environment based upon standard file, data, and content types established by the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI) HydroShare system; (2) two new Python packages that enable users to write Python code for automating retrieval of desired water data, loading it into high performance memory objects specified by the object data model designed in the project, and performing analysis in a reproducible way that can be shared, collaborated around, and formally published for reuse. The project will use domain-specific data science applications to demonstrate how the new Python packages can be paired with the powerful data science capabilities of existing Python packages like Pandas, numpy, and scikit-learn to develop advanced analytical workflows within cloud and desktop environments. The project aims to extend the data access, collaboration, and archival capabilities of the HydroShare data and model repository and promote its use as a platform for reproducible water-data science. The project also aims to overcome barriers associated with accessing, organizing, and preparing datasets for data science intensive analyses. Overcoming these barriers will be an enabler for transforming scientific inquiries and advancing application of data science methods in the hydrology and water resources domains.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative:Elements:RUI:Cyberinfrastructure for Pedestrian Dynamics-Based Analysis of Infection Propagation Through Air Travel,OAC,1931511,Ashok Srinivasan,asrinivasan@uwf.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","When people congregate - for example, at entertainment events, in crowds, and airplanes - they come into close contact with each other and can spread infectious diseases. The Disney World measles outbreak in 2016 is a prominent example. Air travel, in particular, is a leading factor in the spread of infections, and there have been several outbreaks of serious diseases that spread during air travel, such as SARS, H1N1 influenza, and tuberculosis. Public health policies and procedures for crowd management, boarding airplanes, etc. can help in mitigating the spread of disease, if these policies are science-based. The spread of directly transmitted diseases is governed by the movement patterns of people because the movement can bring an infected person close to others. The science of ""pedestrian dynamics"" provides mathematical models that can accurately simulate the movement of individuals in a crowd. These models allow scientists to understand how different policies, such as boarding procedures on planes, can prevent, or make worse, the transmission of infections. This project seeks to develop a novel software that will provide a variety of pedestrian dynamics models, infection spread models, as well as data so that scientists can analyze the effect of different mechanisms on the spread of directly transmitted diseases in crowded areas. The initial focus of this project is on air travel. However, the software can be extended to a broader scope of applications in movement analysis and epidemiology, such as in theme parks and sports venues. The project team is working closely with decision makers in airports, public health agencies, and the airline industry. This collaboration will lead to practical applications of this science that will improve public health. This project and the software will educate a wide range of scientists as well as students, in particular, students from under-represented groups, as well as professionals working in the public health fields.This project seeks to develop a novel software that will provide a variety of pedestrian dynamics models, infection spread models, as well as data so that scientists can analyze the effect of different mechanisms on the spread of directly transmitted diseases in crowded areas. The initial focus of this project is on air travel. However, the software can be extended to a broader scope of applications in movement analysis and epidemiology, such as in theme parks and sports venues. Development of the proposed software will involve several innovations. It will include a novel phylogeography model that links fine-scale human movement data with virus genetic information to more accurately model geographic diffusion of viruses. New models for pedestrian movement will enable modeling of complex human movement patterns. A recommendation system for the choice of pedestrian dynamics models and a domain specific language for the input of policies and human behaviors will enhance usability by researchers in diverse fields. Community building initiatives will catalyze inter-disciplinary research to ensures the long-term sustainability of the project through a critical mass of contributors and users.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative:Elements:Cyberinfrastructure for Pedestrian Dynamics-Based Analysis of Infection Propagation Through Air Travel,OAC,1931483,Sirish Namilae,namilaes@erau.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","When people congregate - for example, at entertainment events, in crowds, and airplanes - they come into close contact with each other and can spread infectious diseases. The Disney World measles outbreak in 2016 is a prominent example. Air travel, in particular, is a leading factor in the spread of infections, and there have been several outbreaks of serious diseases that spread during air travel, such as SARS, H1N1 influenza, and tuberculosis. Public health policies and procedures for crowd management, boarding airplanes, etc. can help in mitigating the spread of disease, if these policies are science-based. The spread of directly transmitted diseases is governed by the movement patterns of people because the movement can bring an infected person close to others. The science of ""pedestrian dynamics"" provides mathematical models that can accurately simulate the movement of individuals in a crowd. These models allow scientists to understand how different policies, such as boarding procedures on planes, can prevent, or make worse, the transmission of infections. This project seeks to develop a novel software that will provide a variety of pedestrian dynamics models, infection spread models, as well as data so that scientists can analyze the effect of different mechanisms on the spread of directly transmitted diseases in crowded areas. The initial focus of this project is on air travel. However, the software can be extended to a broader scope of applications in movement analysis and epidemiology, such as in theme parks and sports venues. The project team is working closely with decision makers in airports, public health agencies, and the airline industry. This collaboration will lead to practical applications of this science that will improve public health. This project and the software will educate a wide range of scientists as well as students, in particular, students from under-represented groups, as well as professionals working in the public health fields.This project seeks to develop a novel software that will provide a variety of pedestrian dynamics models, infection spread models, as well as data so that scientists can analyze the effect of different mechanisms on the spread of directly transmitted diseases in crowded areas. The initial focus of this project is on air travel. However, the software can be extended to a broader scope of applications in movement analysis and epidemiology, such as in theme parks and sports venues. Development of the proposed software will involve several innovations. It will include a novel phylogeography model that links fine-scale human movement data with virus genetic information to more accurately model geographic diffusion of viruses. New models for pedestrian movement will enable modeling of complex human movement patterns. A recommendation system for the choice of pedestrian dynamics models and a domain specific language for the input of policies and human behaviors will enhance usability by researchers in diverse fields. Community building initiatives will catalyze inter-disciplinary research to ensures the long-term sustainability of the project through a critical mass of contributors and users.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Data: U-Cube: A Cyberinfrastructure for Unified and Ubiquitous Urban Canopy Parameterization,OAC,1835739,Daniel Aliaga,aliaga@cs.purdue.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Urban canopy parameters (UCPs) can be used in model simulations to study the health and behavior of a city, determine the ability to sustain a growing population, and study potential impacts of extreme weather events. The ability to identify and compute urban canopy parameters has been a missing element in city models; this project develops that capability for use in city design and analysis, integrating weather models and remote sensing data to infer a 3D model of cities of various sizes. The project deploys innovative science-based analysis tools within an extensible, broadly-available cyberinfrastructure portal, allowing users to ingest satellite imagery and other geographic information system (GIS) data to calculate urban canopy parameters. The cyberinfrastructure would improve urban modeling and planning, particularly for extreme weather events. The tools and high-performance computing and storage resources would be usable by other researchers through a portal. Potential beneficiaries include smaller and disadvantaged cities and countries without the resources for urban characterization and modeling necessary for such urban planning. There are also plans to transfer the results of this research to communities beyond college students -- to local teachers and secondary students and museums, and to the GIS urban planning user communities at local, state, and international levels.The project develops cyberinfrastructure which would use a novel inverse modeling approach incorporating satellite images, social science and urban zonal data, to infer a 3D model of a city from which urban canopy parameters could be derived for use in simulation models. The focus is on weather modeling, urban parameterization and a desire to better understand sustainable urbanization. The main cyberinfrastructure products will be 3D urban models and UCP values for urban locations. These UCP parameters will be used for fine-scale urban weather modeling, and evaluation of various classification techniques and simulation models in an integrated portal. The approach differs from prior work that relied on simple urban canopy models, either tuned for a large metropolis or assuming that all cities are the same. The team uses a cyberinfrastructure platform at Purdue (HubZERO) and the Geospatial Data Analysis Building Blocks (GABBs), a suite of software modules developed during a previously funded NSF Data Infrastructure project. The resulting platform can be deployed using Amazon Web Services, extending built-in geospatial data capabilities and providing a scalable CI solution. This platform can be used by researchers to test predictive models or deploy applications that have been developed. The team has cultivated relationships with the research communities and stakeholders relevant to the proposed research. Through the World Urban Database and Access Portal Tools (WUDAPT) project -- a community-based project to gather a census of cities around the world -- the team is already connected to the urban planning community globally. The project will improve urban weather modeling accuracy and increase availability of and access to the new techniques, capabilities and dedicated cyberinfrastructure. The results have the potential to support city officials and urban planners, especially in regions with the fastest rate of urbanization and/or those in developing countries, where access to computational resources is likely to be limited. This award by the NSF Office of Advanced Cyberinfrastructure will be jointly supported by the Division of Chemical, Bioengineering, Environmental, and Transport Systems, within the NSF Directorate for Engineering; and the Division of Atmospheric and Geospace Sciences and the Integrative and Collaborative Education and Research (ICER) Program, within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Can Empirical SE be Adapted to Computational Science?,OAC,1931425,Timothy Menzies,timm@ieee.org,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Today the computer is just as important a tool for chemists as the test tube. For example, the 2013 Nobel Prize was awarded to chemists using computer models to explore very fast chemical reactions during photosynthesis. Other scientific areas where software is used intensively are astronomy, astrophysics, chemistry, weather prediction, economics, genomics, molecular biology, oceanography, physics, political science, and many other engineering fields. It is important to ensure the quality of these software-driven fields since its results accelerate global innovations by improving quality and quantity of computational scientific studies. But many software developers in this area have not formally studied computer science or software engineering. This proposal will create SEnTRY, a workbench containing methods adapted from empirical software engineering, that would help bridge the skill gap via automatic agents by suggesting to developers when they should investigate or redo part of their code. Software is used intensively in scientific areas such as astronomy, astrophysics, chemistry, weather prediction, economics, genomics, molecular biology, oceanography, physics, political science, and many other engineering fields. It is important to ensure the quality of these software-driven fields since its results accelerate global innovations by improving quality and quantity of computational scientific studies. But many software developers in this area have not formally studied computer science or software engineering. This proposal will create SEnTRY, a workbench containing methods adapted from empirical software engineering, that would help bridge the skill gap via automatic agents by suggesting to developers when they should investigate or redo part of their code. To achieve these goals, methods developed for traditional kinds of software must be extensively adapted for computational science. For example, language models describing software defects must be created, especially for the computational science community; test case prioritization algorithms must be re-tuned to appropriately prioritize ""tests"" that are really ""tests of scientific concepts""; and static code analysis warnings have to be re-engineered to manage the kinds of software tools used within the computational science community. To that end, this project will apply data miners, hyperparameter optimizers and active learning to project data from the computational science community. When successful, SEnTRY will reduce the associated cost (time, money, etc.) required to handle many of the large and more tedious aspects of software development. This will free up more time of the computational scientists, and allow them to focus on core scientific issues. As an additional benefit, SEnTRY will also ensure the reproducibility and credibility of the computational science researches which, in turn, will naturally encourage more adoption of current work as well as adaptation and innovation in future work.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative:Elements:Cyberinfrastructure for Pedestrian Dynamics-Based Analysis of Infection Propagation Through Air Travel,OAC,1931560,Matthew Scotch,Matthew.Scotch@asu.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","When people congregate - for example, at entertainment events, in crowds, and airplanes - they come into close contact with each other and can spread infectious diseases. The Disney World measles outbreak in 2016 is a prominent example. Air travel, in particular, is a leading factor in the spread of infections, and there have been several outbreaks of serious diseases that spread during air travel, such as SARS, H1N1 influenza, and tuberculosis. Public health policies and procedures for crowd management, boarding airplanes, etc. can help in mitigating the spread of disease, if these policies are science-based. The spread of directly transmitted diseases is governed by the movement patterns of people because the movement can bring an infected person close to others. The science of ""pedestrian dynamics"" provides mathematical models that can accurately simulate the movement of individuals in a crowd. These models allow scientists to understand how different policies, such as boarding procedures on planes, can prevent, or make worse, the transmission of infections. This project seeks to develop a novel software that will provide a variety of pedestrian dynamics models, infection spread models, as well as data so that scientists can analyze the effect of different mechanisms on the spread of directly transmitted diseases in crowded areas. The initial focus of this project is on air travel. However, the software can be extended to a broader scope of applications in movement analysis and epidemiology, such as in theme parks and sports venues. The project team is working closely with decision makers in airports, public health agencies, and the airline industry. This collaboration will lead to practical applications of this science that will improve public health. This project and the software will educate a wide range of scientists as well as students, in particular, students from under-represented groups, as well as professionals working in the public health fields.This project seeks to develop a novel software that will provide a variety of pedestrian dynamics models, infection spread models, as well as data so that scientists can analyze the effect of different mechanisms on the spread of directly transmitted diseases in crowded areas. The initial focus of this project is on air travel. However, the software can be extended to a broader scope of applications in movement analysis and epidemiology, such as in theme parks and sports venues. Development of the proposed software will involve several innovations. It will include a novel phylogeography model that links fine-scale human movement data with virus genetic information to more accurately model geographic diffusion of viruses. New models for pedestrian movement will enable modeling of complex human movement patterns. A recommendation system for the choice of pedestrian dynamics models and a domain specific language for the input of policies and human behaviors will enhance usability by researchers in diverse fields. Community building initiatives will catalyze inter-disciplinary research to ensures the long-term sustainability of the project through a critical mass of contributors and users.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Frameworks: MUSES, Modular Unified Solver of the Equation of State",OAC,2103680,Nicolas Yunes,nyunes@illinois.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","The recent detection of X-rays from hot spots on the surface of rotating neutron stars and the observation of the gravitational waves emitted when neutron stars collide hold the promise of revealing the mysteries of nuclear astrophysical phenomena in a previously inaccessible regime. Meanwhile, in the laboratory, heavy-ion collision experiments, akin to miniature neutron star mergers, can reveal complementary information about the properties of matter at extreme temperatures and densities. A core group of research teams at the University of Illinois Urbana-Champaign, the National Center for Supercomputing Applications, the University of Houston, Kent State University, and several other auxiliary institutions are creating a new cyberinfrastructure that can rapidly and efficiently describe nuclear matter across vastly different densities and temperatures. This open cyberinfrastructure allows users to understand the nature of the building blocks of matter through comparisons with data from NASA?s Neutron Star Interior Composition Explorer, the National Science Foundation?s Laser Interferometer Gravitational-wave Observatory, the Department of Energy?s fixed target Solenoid Tracker at the Relativistic Heavy Ion Collider, and other international facilities. The project provides outreach to the community through an online science portal that makes high-end computations more approachable for end users with additional tutorials, extended showcases, and detailed documentation.The research plan creates a cyberinfrastructure, MUSES (Modular Unified Solver of the EoS), that provides various scientific communities with novel tools to help answer critical interdisciplinary questions in nuclear astrophysics, gravitational wave astrophysics, and heavy-ion experiments. The investigators are creating modern, efficient and parallelizable code to generate equation-of-state modules in different regimes of density, pressure, and temperature in either 2, 3, or 4 dimensions. These modules are then integrated into a single, standardized global calculational engine that allows the fast generation of equations-of-state across the entire phase diagram of quantum chromodynamics ? the fundamental theory of strong interactions. The project creates a web interface with a collection of application programming interfaces that allows external users to run the MUSES cyberinfrastructure remotely and deploy it in high-performance computing clusters. In particular, the plug-and-play operability of MUSES allows external users to select parameters in the equation-of-state package to run a given set of modules and generate a global EoS with a chosen set of observable byproducts. This project advances the goals of the Division of Physics and the Office of Advanced Cyberinfrastructure of the National Science Foundation.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Windows on the Universe NSF Big Idea program, the Physics at the Information Frontier (PIF) program in the Division of Physics (PHY), and the Division of Astronomical Sciences (AST).This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Framework: Software: Collaborative Research: CyberWater-An open and sustainable framework for diverse data and model integration with provenance and access to HPC,OAC,1835592,Anthony Castronova,acastronova@cuahsi.org,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities. The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery. The models and datasets cover fields such as hydrology, biology, environmental engineering and climate. The project also addresses one of the key issues for extreme-scale computing: scalable file systems. The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI). The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity. The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use. To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts. The project builds upon an existing prototype developed by the lead investigator; basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control. The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI. For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated. The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Elements: NSCI-Software -- A General and Effective B-Spline R-Matrix Package for Charged-Particle and Photon Collisions with Atoms, Ions, and Molecules",OAC,1834740,Klaus Bartschat,klaus.bartschat@drake.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","This project concerns the development and subsequent distribution of a suite of computer codes that can accurately describe the interaction of charged particles (mostly electrons) and light (mostly lasers and synchrotrons) with atoms and ions. The results are of importance for the understanding of fundamental collision dynamics, and they also fulfill the urgent practical need for accurate atomic data to model the physics of stars, plasmas, lasers, and planetary atmospheres. With the rapid advances currently seen in computational resources, such studies can now be conducted for realistic systems, as opposed to idealized models. In particular, it has become possible to describe very complex targets, such as transition metals and other open-shell systems. Examples include the excited states of the inert gases beyond helium, as well as neutral and lowly-ionized iron. These systems are of significant importance for plasma diagnostics and astrophysics, respectively. The source code will be made publicly available. The project will support a post-doctoral researcher. A website devoted to user-developer interaction will be developed and maintained together with the neccessary code documentation and training materials.The numerical calculations will be based upon the non-perturbative R-matrix (close-coupling) method. A particular strength of the implementation pursued in this project is the use of a highly flexible B-spline basis with non-orthogonal orbital sets. The major advantage of the approach compared to traditional methods is the fact that an accurate target description can be achieved with a much-reduced configuration-interaction (CI) expansion if the orthogonality requirements on the individual orbitals are relaxed. This is critical for complex targets, where the valence orbitals in particular are known to be strongly term-dependent. Using a sufficiently small but still accurate enough CI expansion for the target states is essential for the feasibility and quality of the subsequent collision calculation.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research : Elements : Extending the physics reach of LHCb by developing and deploying algorithms for a fully GPU-based  first trigger stage,OAC,2004364,Michael Sokoloff,mike.sokoloff@uc.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","The development of the Standard Model (SM) of particle physics is a major intellectual achievement. The validity of this model was further confirmed by the discovery of the Higgs boson at the Large Hadron Collider (LHC) at CERN. However, the Standard Model leaves open many questions, including why matter dominates over anti-matter in the Universe and the properties of dark matter. Most explanations require new phenomena, which we call Beyond the Standard Model Physics (BSM), and which the LHCb experiment at CERN has been designed to explore. The LHC is the premier High Energy Physics particle accelerator in the world and is currently operating at the CERN laboratory near Geneva Switzerland, one of the foremost facilities for addressing these BSM questions. The LHCb experiment is one of four large experiments at the LHC and is designed to study in detail the decays of hadrons containing b or c quarks. The goal is to identify the existence of new physics beyond the Standard Model by examining the properties of hadrons containing these quarks. The new physics, or new forces, can be manifest by particles, as yet to be discovered, whose presence would modify decay rates and CP violating asymmetries of hadrons containing the b and c quarks, allowing new phenomena to be observed indirectly - or via direct observation of new force-carrying particles. The data sets collected by the LHC experiments are some of the largest in the world. For example, the sensor arrays of the LHCb experiment, in which both PIs participate, produce about 100 TB/s and close to a zettabyte per year. Even after drastic data-reduction performed by custom-built read-out electronics, the data volume is still about 10 exabytes per year. Such large data sets cannot be stored indefinitely; therefore, all high energy physics (HEP) experiments employ a second data-reduction scheme executed in real time by a data-ingestion system - referred to as a trigger system in HEP - to decide whether each event is to be persisted for future analysis or permanently discarded. The primary goal of this project is developing and deploying software that will maximize the performance of the LHCb trigger system - running its first processing stage on GPUs - so that the full physics discovery potential of LHCb is realized.The LHCb detector is being upgraded for Run 3 (which will start to record data in 2022), when the trigger system will need to process 25 exabytes per year. Currently, only 0.3 of the 10 exabytes per year processed by the trigger is analyzed using high-level computing algorithms; the rest is discarded prior to this stage using simple algorithms executed on FPGAs. To significantly extend its physics reach in Run 3, LHCb plans to process the entire 25 exabytes each year using high-level computing algorithms. The PIs propose running the entire first trigger-processing stage on GPUs, which has zero (likely negative) net cost, and frees up all of the CPU resources for the second processing stage. The LHCb trigger makes heavy use of machine learning (ML) algorithms, which will need to be reoptimized both for Run 3 conditions but also for usage on GPUs. The specific objectives of this proposal are developing: GPU-based versions of the primary trigger-selection algorithms, which make heavy usage of ML; GPU-based calorimeter-clustering and electron-identification algorithms, likely using ML; and the infrastructure required to deploy ML algorithms within the GPU-based trigger framework. These advances will make it possible to explore many potential explanations for dark matter, e.g., dark photon decays, and the matter/anti-matter asymmetry of our universe using data that would be otherwise inaccessible due to trigger-system limitations.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Framework: Software: Collaborative Research: CyberWater--An open and sustainable framework for diverse data and model integration with provenance and access to HPC,OAC,1835817,Yao Liang,yliang@cs.iupui.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities. The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery. The models and datasets cover fields such as hydrology, biology, environmental engineering and climate. The project also addresses one of the key issues for extreme-scale computing: scalable file systems. The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI). The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity. The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use. To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts. The project builds upon an existing prototype developed by the lead investigator; basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control. The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI. For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated. The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: NSCI : Computational and data innovation implementing a national community hydrologic modeling framework for scientific discovery,OAC,1835704,Catherine Olschanowsky,cathie@cs.boisestate.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Flexible & Open-Source Models for Materials and Devices,OAC,2306967,Oliviero Andreussi,olivieroandreuss@boisestate.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","The project will develop first principles materials modeling software that can approach multiple length and time scales (multiscale). This software will be capable of modeling systems as complex as entire devices and materials of mesoscopic sizes. Over the course of the project the principal investigators plan to develop an open-source python-based software aimed at standardizing and generalizing multiscale simulations methods. This will enable the use of computer modeling in the design of new compounds, materials and devices. The goals are to render multiscale simulations reproducible and accessible by the broader community. In that context, the project will address the notion of ""lab 2.0"", by which computer simulations replace laboratory experiments in tasks such as materials design and costly combinatorial searches for viable chemical processes. The software will be self-optimized using machine learning and exploit linear workflows approachable by nonexperts. Education and diversity will be promoted by direct participation of underrepresented minorities from high schools and colleges in hackathon workshops and summer research programs.An approach that leverages the long-range multiscale capabilities of continuum models with accurate short-range atomistic descriptions of specific interactions, and that exploits the ideal scalability of quantum-embedding techniques, will be investigated. The main driver of the proposed implementation will be a Python codebase which will carry out the part of current software that is not computationally heavy, but instead is code heavy where many lines of code are needed in typically non-object-oriented languages. This is key to obtain the desired cluster-topology-agnostic workflows. Longstanding problems related to computational scalability and code stiffness will addressed in a three-pronged approach aimed at developing (1) modular tools implementing modules with highly object-oriented codes (e.g., quantum, classical atomistic, and continuum solvers), (2) hybrid tools implementing combinations of modular tools in a way that best exploits high-performance computing architectures, and (3) hyper tools implementing a high-level data-enabled optimization strategy that generates optimal workflows combining several hybrid tools, thereby making the software of broad applicability and accessible to nonexperts. These goals will render multiscale simulations reproducible and accessible by the broader community. The project will address the ""lab 2.0"" paradigm, by which computer simulations replace laboratory experiments in tasks such as materials design and combinatorial searches for viable chemical processes. The resultant software will be self-optimized using machine learning and exploit linear workflows approachable by nonexperts. Education and diversity will include the direct participation of underrepresented minorities from high schools and colleges in hackathon workshops and summer research programs.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Chemistry and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative:RAPID: Leveraging New Data Sources to Analyze the Risk of COVID-19 in Crowded Locations,OAC,2027529,Matthew Scotch,Matthew.Scotch@asu.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","The goal of this project is to create a software infrastructure that will help scientists investigate the risk of the spread of COVID-19 and analyze future epidemics in crowded locations using real-time public webcam videos and location based services (LBS) data. It is motivated by the observation that COVID-19 clusters often arise at sites involving high densities of people. Current strategies suggest coarse scale interventions to prevent this, such as cancellation of activities, which incur substantial economic and social costs. More detailed fine scaled analysis of the movement and interaction patterns of people at crowded locations can suggest interventions, such as changes to crowd management procedures and the design of built environments, that yield social distance without being as disruptive to human activities and the economy. The field of pedestrian dynamics provides mathematical models that can generate such detailed insight. However, these models need data on human behavior, which varies significantly with context and culture. This project will leverage novel data streams, such as public webcams and location based services, to inform the pedestrian dynamics model. Relevant data, models, and software will be made available to benefit other researchers working in this domain, subject to privacy restrictions. The project team will also perform outreach to decision makers so that the scientific insights yield actionable policies contributing to public health. The net result will be critical scientific insight that can generate a transformative impact on the response to the COVID-19 pandemic, including a possible second wave, so that it protects public health while minimizing adverse effects from the interventions.We will accomplish the above work through the following methods and innovations. LBS data can identify crowded locations at a scale of tens of meters and help screen for potential risk by analyzing the long range movement of individuals there. Worldwide video streams can yield finer-grained details of social closeness and other behavioral patterns desirable for accurate modeling. On the other hand, the videos may not be available for potentially high risk locations, nor can they directly answer ?what-if? questions. Videos from contexts similar to the one being modeled will be used to calibrate pedestrian dynamics model parameters, such as walking speeds. Then the trajectories of individual pedestrians will be simulated in the target locations to estimate social closeness. An infection transmission model will be applied to these trajectories to yield estimates of infection spread. This will result in a novel methodology to include diverse real time data into pedestrian dynamics models so that they can quickly and accurately capture human movement patterns in new and evolving situations. The cyberinfrastructure will automatically discover real-time video streams on the Internet and analyze them to determine the pedestrian density, movements, and social distances. The pedestrian dynamics model will be reformulated from the current force-based definition to one that uses pedestrian density and individual speed, both of which can be measured effectively through video analysis. The revised model will be used to produce scientific insight to inform policies, such as steps to mitigate localized outbreaks of COVID-19 and for the systematic reopening, potential re-closing, and permanent changes to economic and social activities.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: NSCI Framework: Software: SCALE-MS - Scalable Adaptive Large Ensembles of Molecular Simulations,OAC,1835720,Michael Shirts,michael.shirts@colorado.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Molecular simulations are becoming important tools in understanding nanoscale processes in science and engineering. Such processes include the motions of proteins and nucleic acids that will enable design of better drugs, the interactions of liquids and metals in photovoltaic and catalytic applications, and the behavior of complex polymers used in industrial materials. Although national cyberinfrastructure investments are increasing raw computational power, the molecular timescales that scientists can simulate are not increasing proportionately. This means that most simulations are significantly shorter than the physical processes they are designed to study. Fortunately, many researchers have developed powerful algorithms that combine multiple simulations to overcome this molecular timescale problem, but these algorithms can still be very difficult to use effectively. This project, called SCALE-MS, will develop computing tools to simplify the process of writing algorithms that use large collections of molecular simulations to simulate the long timescales needed for scientific and industrial understanding. These tools will make it much simpler to have simulations interact adaptively, so simulation results can automatically guide the creation and running of new simulations. By making these complex multi-simulation algorithms easier to create and run, this project will enable users to run existing methods in computational molecular science more easily and make it possible for researchers to create and test new, even more powerful, methods for molecular modeling. This project also brings together researchers from biophysics, chemical engineering, and materials science, combining expertise from multiple simulation fields to develop important new ensemble simulation algorithms. This adaptive ensemble framework will enable communities of molecular simulation users in chemistry, chemical engineering, materials science, and biophysics to more easily exchange advanced methods and best practices. Many aspects of this framework can also be applied to aid societal problems requiring modeling in other domains, such as climate and earthquake modeling and prediction.This project addresses a fundamental need across molecular simulation communities from chemistry to biophysics to materials science: the ability to easily simulate long-timescale phenomena and slowly equilibrating ensembles. Researchers are increasingly developing high-level parallel algorithms that utilize simulation ensembles, loosely coupled molecular simulations that exchange information on a slower time scale than standard parallel computing techniques. However, most existing molecular simulation software cannot express ensemble simulation algorithms in a general manner and execute them at scale. There is thus a need for (i) the ability to express ensemble-based methods in a simple, easy- to-use manner that is agnostic of the underlying simulation code, (ii) support for adaptive and asynchronous execution of ensembles, and (iii) a scalable runtime system that encapsulates the complexity of executing and managing jobs seamlessly on different resources. The project will develop an extensible framework, including a simple high-level API and a sophisticated runtime system, to meet these design objectives on NSF?s production cyberinfrastructure. A key element of this design is the ability to specify ensemble-based patterns of work- and data-flow in a fashion independent of the challenges and complexity of the runtime management of the ensembles. This project will develop a framework consisting of a simple adaptive ensemble API with an underlying runtime platform that enables expression of ensemble simulation methods in a fashion agnostic of the underlying simulation code. This will facilitate design of new ensemble-based methods by the community and enable scientific end users to simply encode complex adaptive workflows. This approach separates the complexity of compute job management from the expression of sophisticated methods. The framework will support adaptive and asynchronous execution of ensembles, removing synchronization blocks that have restricted peta- and exa-scaling of simulation methods. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Chemistry within the NSF Directorate for Mathematical and Physical Sciences and the Division of Chemical, Bioengineering, Environmental, and Transport Systems within the NSF Directorate for Engineering.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Software Elements: NUPACK: Molecular Programming in the Cloud,OAC,1835414,Niles Pierce,niles@caltech.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Life is orchestrated by programmable biomolecules (DNA, RNA, and proteins) that interact within complex molecular machines and biological circuits to grow, regulate, and repair organisms. These biological proofs-of-principle inspire diverse engineering efforts within the new fields of molecular programming, nucleic acid nanotechnology, and synthetic biology. Over the coming decades, these fields are poised to generate transformative programmable molecular and cellular technologies addressing challenges to science and society ranging from neuroscience and development, to diagnosis and treatment, and from renewable energy to sustainable manufacturing. To support these engineering efforts, the PI is engaged in a multi-decade effort to develop NUPACK (Nucleic Acid Package), a growing software suite for analyzing and designing nucleic acid structures, devices, and systems. Launched in 2007, NUPACK usage has grown to the point where the NUPACK compute resource is frequently overwhelmed by the research community. With the proposed work, the NUPACK web application will be re-architected from the ground up to run in the cloud, enabling the resource to scale dynamically in response to spikes in researcher demand and to growth year-over-year. The NUPACK user interface will be substantially expanded to allow users to harness next-generation analysis and design tools. Additionally, the re-architected web application will benefit from a complete re-write of the NUPACK scientific code base (moving from NUPACK 3.2 to 4.0) to achieve dramatic computational speed-ups and exploit enhanced physical models. With NUPACK in the cloud, users will be able to perform calculations far beyond current capabilities both in terms of scale and scientific scope, enabling exploration of a growing frontier of programmable molecular technologies.NUPACK is a growing software suite for the analysis and design of nucleic acid structures, devices, and systems serving the needs of researchers in the emerging disciplines of molecular programming, nucleic acid nanotechnology, and synthetic biology. NUPACK algorithms are unique in treating complex and test tube ensembles containing arbitrary numbers of interacting strand species, providing crucial tools for capturing concentration effects essential to analyzing and designing the intermolecular interactions that are a hallmark of these new fields. Usage has increased to the point where the NUPACK compute cluster is frequently overwhelmed. With the proposed work, the NUPACK web application will be re-architected to enable deployment on the cloud, containerizing the dozens to thousands of jobs that are launched by a single click, and enabling the scale of the resource to vary dynamically minute-to-minute and year-over-year. To move to a sustainable model for NUPACK compute hardware and engineering support, NUPACK user accounts will be created that enable users to view and retrieve old jobs, to seamlessly pay for the cloud compute cycles that are used for their jobs, and to provide incremental support for the NUPACK Software Engineer proportional to their usage of this non-profit academic resource. The user interface will be substantially expanded to allow users to harness the new capabilities of the enhanced NUPACK backend, including kinetic analysis for complex and test tube ensembles, kinetic design for test tube ensembles, equilibrium design for large-scale pseudo-knotted structures in test tube ensembles, and use of new computationally parameterized physical models generated for custom experimental conditions. The re-architected web application will also benefit from a complete re-write of the NUPACK scientific code base, featuring improved implementations, reduced-complexity algorithms, overflow-safe evaluation algebras, and expanded physical models.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: NSCI Framework. Software: SCALE-MS - Scalable Adaptive Large Ensembles of Molecular Simulations,OAC,1835780,Peter Kasson,kasson@virginia.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Molecular simulations are becoming important tools in understanding nanoscale processes in science and engineering. Such processes include the motions of proteins and nucleic acids that will enable design of better drugs, the interactions of liquids and metals in photovoltaic and catalytic applications, and the behavior of complex polymers used in industrial materials. Although national cyberinfrastructure investments are increasing raw computational power, the molecular timescales that scientists can simulate are not increasing proportionately. This means that most simulations are significantly shorter than the physical processes they are designed to study. Fortunately, many researchers have developed powerful algorithms that combine multiple simulations to overcome this molecular timescale problem, but these algorithms can still be very difficult to use effectively. This project, called SCALE-MS, will develop computing tools to simplify the process of writing algorithms that use large collections of molecular simulations to simulate the long timescales needed for scientific and industrial understanding. These tools will make it much simpler to have simulations interact adaptively, so simulation results can automatically guide the creation and running of new simulations. By making these complex multi-simulation algorithms easier to create and run, this project will enable users to run existing methods in computational molecular science more easily and make it possible for researchers to create and test new, even more powerful, methods for molecular modeling. This project also brings together researchers from biophysics, chemical engineering, and materials science, combining expertise from multiple simulation fields to develop important new ensemble simulation algorithms. This adaptive ensemble framework will enable communities of molecular simulation users in chemistry, chemical engineering, materials science, and biophysics to more easily exchange advanced methods and best practices. Many aspects of this framework can also be applied to aid societal problems requiring modeling in other domains, such as climate and earthquake modeling and prediction.This project addresses a fundamental need across molecular simulation communities from chemistry to biophysics to materials science: the ability to easily simulate long-timescale phenomena and slowly equilibrating ensembles. Researchers are increasingly developing high-level parallel algorithms that utilize simulation ensembles, loosely coupled molecular simulations that exchange information on a slower time scale than standard parallel computing techniques. However, most existing molecular simulation software cannot express ensemble simulation algorithms in a general manner and execute them at scale. There is thus a need for (i) the ability to express ensemble-based methods in a simple, easy- to-use manner that is agnostic of the underlying simulation code, (ii) support for adaptive and asynchronous execution of ensembles, and (iii) a scalable runtime system that encapsulates the complexity of executing and managing jobs seamlessly on different resources. The project will develop an extensible framework, including a simple high-level API and a sophisticated runtime system, to meet these design objectives on NSF?s production cyberinfrastructure. A key element of this design is the ability to specify ensemble-based patterns of work- and data-flow in a fashion independent of the challenges and complexity of the runtime management of the ensembles. This project will develop a framework consisting of a simple adaptive ensemble API with an underlying runtime platform that enables expression of ensemble simulation methods in a fashion agnostic of the underlying simulation code. This will facilitate design of new ensemble-based methods by the community and enable scientific end users to simply encode complex adaptive workflows. This approach separates the complexity of compute job management from the expression of sophisticated methods. The framework will support adaptive and asynchronous execution of ensembles, removing synchronization blocks that have restricted peta- and exa-scaling of simulation methods. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Chemistry within the NSF Directorate for Mathematical and Physical Sciences and the Division of Chemical, Bioengineering, Environmental, and Transport Systems within the NSF Directorate for Engineering.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Cyber Infrastructure for Shared Algorithmic and Experimental Research in Online Learning,OAC,1931419,Ryan Baker,ryanshaunbaker@gmail.com,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","This project, RAILKaM, will create new technology that will enable twenty researchers during the grant period to run large-scale field experiments where they study basic principles in education and educational psychology in the context of both K-12 mathematics learning and university Massive Online Open Courses (MOOCs). The experiments will be delivered through adaptive learning technology embedded in learning systems already being used by over 100,000 K-12 students and hundreds of thousands of MOOC learners each year. RAILKaM will also support 75 data scientists in conducting analyses on student data after the fact, using carefully redacted datasets that protect student privacy. In facilitating high-power, replicable experiments with diverse student populations and extensive measurement, this infrastructure increases the efficiency and ease of conducting high-quality educational research in online learning environments, bringing 21st-century research methods to education for the long-term betterment of learner outcomes.This project, RAILKaM, will support researchers in more easily running scaled, highly instrumented studies on education and educational psychology, both in K-12 and university Massive Online Open Courses (MOOCs). RAILKaM will leverage ASSISTments, an online learning platform for middle school mathematics homework and classwork used by more than 100,000 students each year. In addition, RAILKaM will build functionality atop the ASSISTments platform so that educational experiments involving scaffolded problem-solving can be easily built into MOOC courses. ASSISTments will use open source APIs to integrate with MOOCs offered by the University of Pennsylvania, branching capacity for investigation to higher education while enabling richer student interactions and data collection than is typically feasible in MOOC courses. These capacities will enable researchers to run online field experiments to test interventions designed to increase student learning and engagement with a focus on how adaptive learning experiences can be optimized. These experiments will be augmented by rich data collection on learners, extending MOOC log data and ASSISTments data with several indicators of learning and engagement not previously available for research at scale. This project will develop the software infrastructure necessary to conduct experiments and collect enriched data, as well as the social infrastructure necessary to select and refine study ideas while maintaining instructor control over the activities that students experience. The combined software and social infrastructure will enable us to engage with researchers who are interested in these issues but who currently lack the infrastructure, technical capacity, or access to learners necessary to conduct high-powered or complex randomized controlled trials. This infrastructure will help these researchers to improve scientific understanding of the principles of human learning, providing a unique shared resource for learning scientists that will have considerable potential for broader impact.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Community-Based Weather and Climate Simulation With a Global Storm-Resolving Model,OAC,2004973,Andrew Gettelman,andrew@ucar.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","Global Earth System Models (ESMs) use mathematical equations to simulate both weather and climate. ESMs include the dynamics of the atmosphere, oceans, land surface, ice, and vegetation. They can be used to make predictions of use to the public and policymakers. Today?s ESMs use coarse grids with cells about 100 km wide. Important weather systems like thunderstorms are too small to be simulated with such grids. One way to improve ESMs is to use finer grids that can directly simulate thunderstorms, but such models can only be run on very powerful computers. This project, called EarthWorks, will create an ESM capable of resolving storms by taking advantage of recent developments in high performance computing. EarthWorks will also use artificial intelligence to improve and speed up the model, and state-of-the-art methods to limit the amount of data produced as the model runs. The EarthWorks ESM will be built by spinning off and modifying a copy of the most recent version of the widely used Community Earth System Model. The modified model will represent the atmosphere, the oceans, and the land surface on a single very high-resolution grid, with grid cells about 4 km wide. It will have improved forecast skill, and produce more realistic simulations of past, present, and future climates. The project will make the model and its output openly available for use by all scientists.The open-source Community Earth System Model (CESM) is both developed and applied to scientific problems by a large community of researchers. It is critical infrastructure for the U.S. climate research community. In the atmosphere and ocean components of the CESM, the adiabatic terms of the partial differential equations that express conservation of mass, momentum, and thermodynamic energy are solved numerically using what is called a dynamical core. Atmosphere and ocean models also include parametric representations, called parameterizations, that are designed to include the effects of storm and cloud processes that occur on scales too small to be represented on the model's grid. Despite decades of work by many scientists, today's parameterizations are still problematic and limit the utility of ESMs for many applications of societal relevance. Fortunately, recent advances in computer power have made it possible to parameterize less, by using grid spacings on the order of a few kilometers over the entire globe. These ""global storm-resolving models"" (GSRMs) can only be run on today's fastest computers. GSRMs are under very active development at a dozen or so modeling centers around the world. Unfortunately, however, the current formulation of the CESM prevents it from being run as a GSRM. This project, called EarthWorks, will create a new, openly available GSRM by spinning off and intensively modifying a copy of the CESM. To accomplish this goal, the researchers will use recently developed and closely related dynamical cores for the atmosphere and ocean. All components of the model will use the same very high-resolution grid. This high resolution will make it possible to eliminate the particularly troublesome parameterization of deep cumulus convection (i.e., thunderstorms), and thereby reduce systematic biases that plague current ESMs. Earthworks will exploit the pre-exascale and exascale technologies now being brought to market by high performance computing vendors. The new exascale ESM will run the most computationally intensive components on powerful graphics processor units (GPUs), and exploit node-level task parallelism to execute the rest of the model asynchronously. The component model codes are close to completion and are currently being tested on GPUs. EarthWorks will use a simplified component-coupling approach, incorporate machine learning where feasible, and leverage lossy compression techniques and parallel I/O tools to deal with the enormous data volumes that will be generated as the model runs. The completed model will be simple, powerful, and well documented. The project will apply it to pressing scientific problems in both numerical weather prediction and climate simulation. The model and its input datasets will be made openly available to the broad research community, via GitHub.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Data Driven Autonomous Thermodynamic and Kinetic Model Builder for Microstructural Simulations,OAC,2209423,Katsuyo Thornton,kthorn@umich.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Materials with improved properties can dramatically impact sustainability, human welfare, and national prosperity. As an example, a stronger material can reduce the weight of vehicles and can therefore reduce energy consumption and pollution. Properties of materials frequently depend on their microstructures (features in materials at scales of one micrometer to hundreds of micrometers). Thermodynamic free energy (providing the driving force for evolution) and kinetic parameters (providing how quickly the evolution can occur) together govern how a material evolves at the microscale. This project develops algorithms and software that automate the extraction of the thermodynamics and kinetic information using artificial intelligence to enable simulation of microstructure evolution for complex mixtures of metals. The AI-enabled Microstructure Model BuildER (AMMBER) harvests and harnesses data ranging from first-principles calculations, experimental micrographs and associated natural language text, and thermodynamic databases, as well as custom user input. It then produces input to microstructure evolution models that facilitate the fundamental understanding needed to gain control of the microstructure and resulting material properties. The demonstration of its capability is planned for commercially important alloys (nickel-aluminum-based and aluminum-copper-based alloys), as well as the corresponding high-entropy alloys (alloys with five or more components with near equimolar fractions). AMMBER contributes to the software infrastructure for simulation-based material discovery and development within the context of the Material Genome Initiative. Training activities, including training workshops for the community to learn about the software and the theory behind it and integration into the undergraduate and graduate thermodynamics and kinetics courses, provide opportunities for education and professional development. Nickel-aluminum-based and aluminum-copper-based alloys are key materials in the aerospace and automobile industries, and thus the results are expected to have a direct impact on manufacturing. The goal of this project is to develop an artificial intelligence framework for the autonomous determination of input parameters for phase-field models based on a variety of data sources to establish constraints on the model parameters. The AI-enabled Microstructure Model BuildER (AMMBER) leverages automated data-stream pipelines to collect, curate, and tabulate disparate data sources spanning first-principles calculations, experimental micrographs, and associated natural language text, thermodynamic databases, and custom user input. Then, advanced optimization algorithms iteratively optimize phase-field parameters such that the resulting models reproduce known microstructural characteristics (e.g., the phase fraction and characteristic length scale as a function of time). These models can then be used to simulate the microstructural evolution of materials over a range of conditions that are relevant to engineering and manufacturing. The demonstration of AMMBER involves commercially important Ni-Al-based and Al-Cu-based alloys, some of which contain more than five components, leading to a complicated high-dimensional parameter space in which thermodynamic and kinetic model parameters must be optimized. The application to high-entropy alloys, which contain near equimolar amounts of five or more components, provides a ground for new scientific discoveries. By automating the time-consuming initial model parameterization, AMMBER reduces the human bottleneck of materials modeling and paves the way to increased throughput of phase-field simulations. AMMBER complements existing Materials Genome Initiative (MGI) efforts, and it leverages and integrates into existing computational materials research communities built around tools such as open-source phase-field software (PRISMS-PF, MOOSE), an integrated computational materials engineering framework (PRISMS), CALPHAD tools (ESPEI, Thermo-Calc), and a dissemination platform (nanoHUB). The training workshops and integration of the computational tools and research findings into classrooms facilitate community interaction and engagement. Ni-Al-based and Al-Cu-based alloys are key materials in the aerospace and automobile industries, and thus the results are expected to have a direct impact on manufacturing.This proposal receives funds through the Office of Advanced Cyberinfrastructure in the Computer and Information Science and Engineering Directorate and the Division of Materials Research in the Mathematical and Physical Sciences Directorate.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks for Intelligent Adaptive Experimentation: Enhancing and Tailoring Digital Education,OAC,2209823,Steven Ritter,sritter@carnegielearning.com,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","People are constantly learning ? whether formal education of homework problems & videos, or reading websites like Wikipedia. This project develops the Experiments As a Service Infrastructure (EASI), which lowers the barriers to conducting randomized experiments that compare alternative ways of designing digital learning experiences, as well as analyzing the data derived from the systems to rapidly change what future people receive. It does this by bringing together multidisciplinary researchers around the shared problem of testing ideas for improving and personalizing educational resources. The research also advances (1) the science of learning and instruction; (2) methods for analyzing complex educational data, and (3) machine learning algorithms that use data to improve educational experiences. Improving learning and teaching increases people's knowledge and gives them the ability to solve problems they care about, driving their personal and career success and increasing society's human capital.Instructional decisions about digital educational resources impact all students, from practice problems in K12 systems to tutorial webpages in university and community college online courses. The current versions of resources are too infrequently compared against alternative resources, which may provide better learning. With this in mind, the project has the goal of using data to test hypotheses about what is most helpful to students, and then use that data to change the experience for future students. The Experiments-As-a-Service-Infrastructure supports three complementary types of multi-disciplinary, collaborative research. A?Design: the infrastructure helps researchers investigate theories of learning and discover how to improve instruction by designing randomized field experiments on components of real-world digital educational resources. This provides more ecologically valid research on learning and instruction, in subfields of education, psychology, policy and discipline-based education research. B?Analysis: the infrastructure facilitates sophisticated analysis of experiments in the context of large-scale data about student profiles, such as to discover which interventions are effective for different subgroups of students. This can advance the use of innovative data-intensive methods for gaining actionable knowledge in education, learning analytics, educational data mining, and applied statistics. C?Adaptation: the infrastructure enables research into adaptive experimentation by providing a testbed for algorithms that dynamically analyze data from experiments, to enhance learning by presenting future students with whichever version of a resource (condition) is more effective, or to personalize learning by presenting different subgroups of future students with the version of a resource that is most effective for their subgroup. The infrastructure provides a testbed for empirical evaluation of which algorithms enact effective adaptive experimentation in education to inspire the development of new algorithms. Finally, the work aligns many educational communities around the shared problem of enhancing and personalizing education through experimentation and spurs multidisciplinary research by providing extensive support for collaboration and sharing of designs, data, analysis scripts and algorithms while fostering an online community for training and collaborations, to promote high-quality, innovative, impactful experiments.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements:  RAD Discoveries for Fundamental Physics,OAC,2209917,Georgia Karagiorgi,georgia@nevis.columbia.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","This project supports a diverse team of physicists and computer scientists to leverage recent advancements in AI and newly available particle detector capabilities in order to develop new computationally- and energy-efficient technology that is capable of detecting rare and unpredictable patterns in data with unprecedented intelligence. Such technology finds wide applicability in industry, medicine, national security, but also fundamental physics research. The latter is a key focus of the project, where the developed infrastructure enables new searches for yet-undiscovered physics phenomena using state-of-the-art particle detectors at national and international particle accelerator facilities. These searches represent a major departure from the methods that scientists have been applying for decades in particle physics experiments to discover new phenomena, in that they are no longer confined within an existing and limited understanding of how the Universe works, or a prior expectation for how rare and unpredictable patterns in data may manifest. The developed technology allows the data itself, rather than prior knowledge, to guide new discoveries about how Nature works at its most fundamental level, and aims to revolutionize the way new discoveries are made.This project aims to transform fundamental discoveries in particle and astro-particle physics by developing new Cyberinfrastructure (CI) for real-time anomaly detection (?RAD?). The newly developed CI enables searches that can uncover ?unknown? physics through rare, unpredictable phenomena. Traditionally, discoveries of rare processes through particle and astro-particle experiments have relied on scientists? ability to accurately predict new physics phenomena, and to subsequently selectively look for them within the data by using algorithms trained on predicted unknowns. In a new era of scientific discovery, driven by unprecedented data statistics and Artificial Intelligence (AI) advances, scientists instead may now use AI-powered tools that let the data guide expectation to selectively identify rare and unpredictable signatures that may lie within the data itself, and which may be signatures of new fundamental physics phenomena in nature. The project brings together physicists and computer scientists at Columbia and Princeton, including students, post-doctoral researchers, and CI professionals, and additional collaborators at other US institutions and national labs, to develop anomaly detection algorithms that are executable on resource constrained platforms, commonly employed as on-detector data processing devices at particle physics experiments. Through targeted developments and demonstrations at existing particle detector facilities, the ultimate goal of this project is to develop advanced CI that is energy-efficient, reliable, scalable, and expandable, and thus applicable to trigger systems of current and future generation experiments in the intensity, energy, and cosmic frontiers of particle physics. By design, the delivered CI is capable of fundamental discoveries that are beyond what is envisioned with current trigger frameworks, and has the potential of revolutionizing the way particle and astro-particle physics discoveries are made. The developed CI promises to be broadly applicable to other high-throughput data-selection applications that require real-time, intelligent data processing for the purposes of discovering anomalies in data. These applications can range from neuroscience, to medical imaging, to satellite imaging, cybersecurity, etc.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Physics at the Information Frontier in the Division of Physics within the Directorate for Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Frameworks: Collaborative Research: Integrative Cyberinfrastructure for Next-Generation Modeling Science,OAC,2103905,C Michael Barton,Michael.barton@asu.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","This project is designed to support and advance next generation, interdisciplinary science of the complexly interacting societal and natural processes that are critical to human life and well-being. Computational models are powerful scientific tools for understanding these coupled social-natural systems and forecasting their future conditions for evidenced-based planning and policy-making. This project is led by the Network for Computational Modeling in Social and Ecological Sciences (CoMSES.Net). CoMSES.Net's science gateway promotes knowledge sharing among scientists and with the general public, and enables open, online access to sophisticated computational models of social and ecological systems. CoMSES.Net's partners in this project (the Community Surface Dynamics Modeling System and Consortium of Universities for the Advancement of Hydrologic Science) also enable knowledge sharing and provide open, online repositories of models in the earth sciences. This project will enhance these science gateways and create online educational materials to make these critical technologies easier to find, understand, and use for scientists and non-scientists alike. By integrating innovative technology with training and incentives to engage in best practice standards, this project will stimulate innovation and diversity in modeling science. It will enable researchers to build on each other's work and combine it in new ways to address societal and environmental challenges. The cybertools and educational programs developed in the project will be openly accessible not just to research institutions but also to smaller colleges, state and local governments, and a broader audience beyond the science community. The project will give decision-makers and the data scientists who support them access to a larger and more varied toolkit with which to explore potential solutions to societal and environmental policy issues. A long-term aim of the project is to support an evolving ecosystem of diverse, reusable, and combinable models that are transparently accessible to anyone in the world. Sustainable planetary care and management is a challenge that confronts all of humanity, and requires knowledge, histories, methods, perspectives, and engagement of researchers, decision-makers, and private citizens across the country and throughout the world.The project will develop an Integrative Cyberinfrastructure Framework (ICF) to enable innovative next-generation modeling of human and natural systems, and build capacity in modeling science. It will support a set of activities that integrate the human and technological components of cyberinfrastructure. 1) Software tools will be developed that augment model codebases with modern software development scaffolding to facilitate reuse, integration, and validation of model code. 2) The project will provide high-throughput computing (HTC) resources for simultaneously running numerous iterations of models needed to capture stochastic variability, explore a parameter space, and generate alternative scenarios; 3) Online training activities will build expertise and capacity to make effective use of the cybertools and the HTC resources; 4) The ICF will engage a global modeling science community to provide professional incentives that encourage researchers to adopt best practices and catalyze innovative science. Leveraging existing NSF investments, the ICF will be developed and deployed by the Network for Computational Modeling in Social and Ecological Sciences (CoMSES.Net), in partnership with the Community Surface Dynamics Modeling System (CSDMS), Consortium of Universities for the Advancement of Hydrologic Science (CUAHSI), Open Science Grid, Big Data Hub/Spoke network, and Science Gateways Community Institute. Computational models have emerged as powerful scientific tools for understanding coupled social-biogeophysical systems and generating forecasts about future conditions under a range of climate, biogeophysical, and socioeconomic conditions. CoMSES.Net, CSDMS, and CUASI are scientific networks, with online science gateways and code archives that enable open access to computational models for an international community of social, ecological, environmental, and geophysical scientists. However, the full value of accessible, well-documented models only can be realized if their code is also widely reproducible and reusable, with a potential for integration with other models. In order to confront critical challenges for understanding the coupled human and natural systems of today's world, modeling scientists also need HTC environments for upscaling models and exploring high-dimensional parameter spaces inherent in representing these systems. The ICF is designed to meet these challenges. By integrating technology with intellectual capacity-building, the ICF will stimulate innovation and diversity in modeling science by letting creative researchers build on each other's work more readily and combine it in new ways to address societal-environmental challenges we have not yet perceived. The tools and training resources will be openly accessible not just to leading research institutions but also to the many smaller colleges, state and local governments, and a broader audience beyond science. They will provide decision-makers and the data scientists who support them access to a much larger and more varied toolkit with which to explore potential solution spaces to social and environmental policy issues. The proposed ICF is also designed to help transform scientific modeling practice, including incentives that can help early career researchers shift from creating models to solve problems specific to a particular project to models that are also useful for others. The project will help support a future evolving ecosystem of diverse, reusable, and integrable models that are transparently accessible to the broader community.This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering, with the Division of Social and Economic Sciences in the Directorate for Social, Behavioral & Economic Sciences also contributing funds.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Cyberinfrastructure for spin and charge transport calculation of partially disordered alloys,OAC,2103958,Michael Widom,widom@andrew.cmu.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Metallic alloys are ubiquitous in high technology, in industry, and even in our households. Alloys, which form when two or more chemical species are combined to create a single metallic phase, offer the chance to improve upon the properties of pure metallic elements. Some of the properties that can be altered through alloying include mechanical strength, magnetism, melting temperature, and oxidation resistance. This project focuses on the property of electrical conductivity, by developing computer codes to evaluate the quantum mechanical scattering of electrons off of atoms. To accurately predict the scattering we must know where the atoms are located, and in an alloy that means understanding how the different chemical species arrange in space. Often these arrangements are random, but even if the elements are randomly distributed, there will be correlations in the positions of certain species relative to others as a result of chemical bonding preferences. The code will contain features that enable it to predict these correlations and reveal how the correlations influence the conductivity. In addition to developing computer codes, this project will develop a user base of scientists interested and able to run the code and to contribute to its further development. Outreach to high school students and their teachers will enhance the pipeline of prospective scientists. Inclusion of scattering theory in college and graduate level courses taught by the PIs will prepare Physics and Materials Science students to understand and apply the codes. Presentations at scientific society conferences will inform the existing community of the capabilities, while workshops and webinars and webinars will provide specific training for active users. Electronic density functional theory (DFT) has flourished as a practical tool for calculating energies, forces and electronic structure; its use is now widespread both in basic science and in engineering. Charge and spin transport calculation is a capability that has not yet reached the broader user community, partly because codes that incorporate these effects are not widely available and partly because these properties are highly sensitive to the degree of crystalline order. Basic knowledge of the degree of order is often lacking, as it can be temperature dependent, and thermal effects are not captured by most DFT codes. To address this need, a code will be developed that is easy to use (capable of running on a desktop computer), that can predict the degree of chemical order or disorder as a function of temperature, and that can calculate the resulting charge and spin conductivities. This will be achieved by building upon innovations in electronic structure calculation, coupled with methods of statistical mechanics to address thermal disorder. Specifically, we will modify the Coherent Potential Approximation (CPA) to incorporate the effects of short range order by unifying the resulting total energies with the Cluster Variation Method (CVM) to predict temperature dependent disorder. The modified CPA will express the total energy as a function of interatomic correlation functions, while the CVM will express the entropy in the same terms, allowing the determination of correlations that balance the energy against the entropy. This approach to density functional theory employs multiple scattering as implemented in the public domain code MuST. This method determines the electronic Green?s functions, and consequently it integrates naturally with the Kubo and Greenwood formulas for charge and spin conductivity. This internally consistent combination of approximations will achieve both high accuracy and high performance. This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering, with the Division of Materials Research in the Directorate for Mathematical and Physical Sciences also contributing funds.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Seismic COmputational Platform for Empowering Discovery (SCOPED),OAC,2103741,Felix Waldhauser,felixw@ldeo.columbia.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Seismology is the most powerful tool for investigating the interior structure of Earth?from its surface down to the inner core?and its wide range of processes, including earthquakes, volcanic activity, glacial processes, oceanic and environmental processes, and human-caused processes such as nuclear explosions or hydraulic fracturing in oil and gas exploration. Seismology cannot achieve its greatest potential without harnessing state-of-the-art computing capabilities for the dual purpose of scientific modeling and analysis of rapidly increasing data sets. The SCOPED (Seismic COmputational Platform for Empowering Discovery) project establishes a computing platform that delivers data, computation, and service to the seismological community in a way that promotes education, innovation, and discovery, and enables efficient solutions to outstanding scientific problems in geophysics. By focusing on openly available data, openly available software, and virtual training, SCOPED opens seismological research to a broad range of users. Four research components emphasize openly available software for the purpose of characterizing Earth's subsurface structure and the wide range of natural and man-made events that are recorded by seismometers every day. Training of seismologists is a central focus of the project. SCOPED training workshops (seismoHackweeks) are open to the community. Emphasis on virtual research and training diversifies strategies to engage minority groups entering computational geosciences. The project trains a new generation of seismologists to harness the latest capabilities for processing and modeling large data sets. The SCOPED project establishes cyberinfrastructure that provides fast access to large seismic archives from a suite of containerized open-source computational tools for big data analysis, machine learning, and high-performance simulations. The implementation focuses on four interconnected, compute- and data-intensive research components: seismic imaging of Earth?s interior, waveform modeling of earthquakes and Earth structure, monitoring of Earth structure using ambient noise, and precision monitoring of earthquakes and faults. Each research component is enabled by open-source codes that meet, or aspire to meet, best practices for software development. The project contains several transformative components. First, it offers compute performance for both model- and data-driven seismological problems. Hundreds of terabytes of waveform data are directly accessible both to modelers?for data assimilation problems?and to data scientists for processing, analysis, and exploration. Second, it establishes a direct collaborative link among four teams of seismologists at four institutions and a team of computational scientists at Texas Advanced Computing Center. This unity reflects the necessity of both groups to achieve research-ready codes that can exploit high-performance computing (HPC) and Cloud systems. Third, it establishes a gateway with ready-to-run (or adapt) container images and data as a service for the seismological community. Fourth, it develops computational tools that promote the democratization of HPC/Cloud with cutting-edge data processing and modeling software through their scalability from laptops to HPC or Cloud systems and through their portability with containerization. Finally, although the development of cyberinfrastructure is the main priority, ancillary scientific results from advanced techniques are expected to offer insights into fundamental seismological problems. The project has the potential for discoveries across fields (seismology, Earth science, computer science, data science, material science), as well as societal relevance in the realms of seismic hazard assessment, environmental science, cryosphere, earthquake early warning, energy systems, and geophysical detection of nuclear proliferation.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Collaborative Research: Community-driven Environment of AI-powered Noise Reduction Services for Materials Discovery from Electron Microscopy Data,OAC,2104105,Peter Crozier,crozier@asu.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","The goal of this project is to create cyberinfrastructure (CI) powered by artificial intelligence (AI) for sustained innovation in materials science. Deep understanding of materials is critical for progress in technologies related to energy, communication, construction, transportation and human health. The revolutionary progress of deep learning has been enabled by the availability of open-source AI models and open-access benchmark databases. However, the existing codebases and datasets relevant to image processing focus mostly on photographic images. In order to promote the sustained development of AI technology that can have significant impact in materials science, it is critical to provide data and AI models that are tailored to this domain. The developed CI will address this need by providing software to process images obtained from electron-microscopes, a technique enabling atoms to be visualized, and has the potential to enable transformative breakthroughs in varied and important areas of materials science. The CI is explicitly designed to foster the growth of a sustainable community of users and developers of AI technology at the intersection of the materials and data science communities, and to empower materials scientists to simulate their own datasets and develop their own AI models for scientific discovery. The developed AI-powered CI will therefore enable transformative progress in atomic-level understanding of materials, which will have broader impacts in health, energy, environment, and biotechnology. The CI environment will contribute to training materials scientists in AI technology, connecting them to the AI community, and providing software, data, and support materials to initiate them in AI-powered research. Educational and outreach plans are designed to facilitate interactions between the materials science and AI communities. Outreach activities specifically targeted to the general public, and to high-school teachers and their students, will expose them to materials science, electron microscopy, and AI. The project is committed to providing opportunities to women and underrepresented groups and will prioritize diversity in collaboration with the NYU Center for Data Science diversity committee.Developing a fundamental understanding of atomic level structure and dynamics is critical for transformative advances in materials science. Aberration-corrected transmission electron microscopy is a primary tool to accomplish this goal. Unfortunately, the information content of microscopy data may be severely limited by poor signal-to-noise ratios. This is particularly true for radiation sensitive materials and experiments where high time resolution is required to investigate dynamic kinetic processes. AI methodology can exploit prior information about material structure by training deep neural nets with extensive simulations. These approaches may significantly outperform existing state-of-the-art methods, especially for non-periodic structures, including defects, interfaces, and surfaces. The developed CI will provide AI noise reduction services which will yield immediate advances and impacts for zeolites, metal organic frameworks, protein-material interfaces, liquid phase nucleation and growth, liquid-solid interfaces, and fluxional behavior in catalytic nanoparticles. In addition, the project will advance methodology for the design of AI-oriented CI. The CI is strategically designed to create a holistic environment for the use and development of AI technology in a specific scientific domain. It will attract domain scientists with little AI expertise, by providing software where the AI technology is transparent to the end user. Exposure to the technology will motivate the scientific community to design and train their own models, which will be facilitated by the open-source codebase in the AI repository. The open-access database combined with the repository will attract AI practitioners with little domain expertise, by giving them access to well-curated data and a clear specification of the relevant AI tasks. These services will be jump-started and supported through multiple educational and outreach activities.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: HPN-SSH,OAC,2004012,Christopher Rapier,rapier@psc.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","The transfer of large data sets between computing resources is an integral component of the scientific workflow. Multiple tools have been developed to aid in this task but, despite significant performance bottlenecks, secure shell (SSH) based tools like secure copy protocol (SCP) and secure file transfer protocol (SFTP) remain popular due to ubiquity, ease of use, and minimal administrative burdens. HPN-SSH, the project tool, was initially developed in 2004 to address this need. This award will allow the PIs to expand on the original work of HPN-SSH and address new challenges created by advances in computer technology and the needs of users. They will incorporate hardware accelerated encryption; efficiently use modern CPUs by dynamically sizing the number of threads; accelerate the cryptographic workflow by allowing it to work in parallel; investigate making the default open source secure shell OpenSSH cipher use multiple cores; create a ?resume on failure? feature enabling users to restart transfers from the point of failure; and incorporate networking metrics to aid in troubleshooting and performance analysis of HPN-SSH. HPN-SSH will support and enhance research efforts across a wide range of scientific domains by lowering the costs of entry to big data and remote computation without compromising security or functionality. These benefits will extend to business and industry, educational communities, and the general public as well. The transfer of large data sets between computing resources is an integral component of the scientific workflow. Multiple tools have been developed to aid in this task but, despite significant performance bottlenecks, SSH based tools remain popular due to ubiquity, ease of use, and minimal associated costs. To address these bottlenecks we developed HPN-SSH; a series of patches that enable high performance throughput for the OpenSSH application. These patches were initially released in 2004 and have become widely used throughout the research, academic, financial, and technology communities. This award gives the PIs opportunity to foster innovative development in HPN-SSH that will benefit the community by significantly increasing performance. With this grant the PIs will: incorporate on-die hardware accelerated encryption in their multithreaded AES counter cipher; efficiently use multicore CPUs by dynamically sizing the number of threads; introduce pipelining and parallelization into the cryptographic workflow; investigate the parallelization of the default OpenSSH cipher CHACHA20; create a ?resume on failure? feature enabling users to restart transfers from the point of failure; and incorporate inline network telemetry to aid in troubleshooting and performance analysis. This work will also advance the field of computer science through the development and improvement of parallelization methods to enhance the performance of cryptographic routines. As most widely used cryptographic libraries and methods are highly serial in nature they are unable to take advantage of multicore processors. As processor speed has remained relatively stable over the past ten years we must distribute the cryptographic workload over multiple cores in order to significantly increase throughput. HPN-SSH will democratize access and extend the reach of the national cyberinfrastructure by lowering the costs of entry without compromising security or functionality. These benefits will extend to business and industry, educational communities, and the general public.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
CDS&E: Collaborative Research: Deep learning enhanced parallel computations of fluid flow around moving boundaries on binarized octrees,CBET,1953222,Amir Barati Farimani,barati@cmu.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Computer simulations of heat and fluid flow find applications in many aspects of science and engineering. Notable examples are aerodynamic design of aircrafts and automobiles, and weather forecasting. These simulations are often computationally expensive, and they are performed on supercomputers. Special methods are used to implement the equations of heat and fluid flow as a simulation software. The end goal is to create an accurate computer code that can make optimal use of available computing power. However, this end goal is becoming challenging on modern extreme-scale supercomputers that deploy a large of number of computing processors to work in parallel. Existing algorithms face performance bottlenecks and do not realize the full potential of a modern supercomputer. The project team will develop new algorithms to overcome this performance bottleneck. The successful completion of this award is expected to result in an open-source heat and fluid flow simulation software. The project team will develop educational tutorials to pique the interest of high-school students in new capabilities of computer simulation and machine learning techniques in science and engineering. The technical objective is to enhance parallel performance of simulations of incompressible fluid flow around moving boundaries. A recently developed binarized octree generation technique will be further developed as an open-source parallel adaptive mesh refinement software infrastructure to solve the fluid flow equations on Cartesian domains with deep levels of mesh adaptations. Machine learning techniques and deep neural nets will be adopted in ways to ease potential bottlenecks that are expected to degrade scalability of parallel computations when large number of processors are deployed in simulations. The project team will develop multiple deep learning algorithms such as convolutional neural networks and generative adversarial networks to learn the fluid flow around complex geometries and apply the learning for rapid and accurate field estimation at arbitrary points. To successfully incorporate the effect of boundary conditions at the interface, conditional generative adversarial networks will be trained on different coarse and fine grids to learn the communication pattern among the blocks. This award by the Division of Chemical, Bioengineering, Environmental and Transport Systems within the NSF Directorate of Engineering is jointly supported by the NSF Office of Advanced Cyberinfrastructure.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Models and tools for on-line design and simulations for DNA and RNA nanotechnology,OAC,1931487,Petr Sulc,psulc@asu.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","DNA and RNA nanotechnology are rapidly developing fields that use DNA and RNA molecules as basic building blocks for constructing self-assembled nanoscale structures and devices. Promising applications include novel materials for diagnostics, drug delivery, nanophotonics, biophysical studies, and protein structural biology, as well as devices to perform molecular computation. Development of these structures is currently done with a trial-and-error approach, which is costly and time consuming because each new design has to be experimentally tested and gradually optimized until the desired structure is achieved. Computer simulations can provide insight into the assembly and function of these systems and greatly simplify and accelerate the design process, as well as guide understanding of the nanostructure function. However, computer simulations require molecular simulation expertise and high performance computing infrastructures that are not readily accessible to experimental groups. In this project, we will develop a new web server to provide users with online automatized tools that can be used to design, simulate, and analyze the properties of DNA and RNA nanostructures. The web server will also contain a repository of previously reported nanostructures so that researchers can easily access and use existing designs and adapt them for their use. We will also develop new models for hybrid DNA/RNA and protein-DNA/RNA nanostructures, thereby extending the ability of computational design and verification to larger and more complex nanostructures. This project will benefit the public by creating a highly efficient integrated platform to store, edit, design and computationally analyze nanostructures, thus providing a common resource to groups working in nanotechnology while simultaneously expanding access to these nanostructures to researchers in other fields. The net result will be to speed up and integrate the development of nanotechnology, simplify the design process, and facilitate the extension of biomolecular nanostructures to practical applications. The tools will also be impactful for teaching the simulation methods and for outreach activities, where students will be taught the principles of simulation and self-assembly by using the tools to design their own nanostructures. Despite significant progress in the development of experimental methods that allow for assembly and characterization of the DNA and RNA nanostructures, the lack of easy-to-use software tools for design of nanostructures still remain a major bottleneck for wider adoption of the DNA and RNA nanotechnology in other related fields. Novel methods for structure design and verification are needed for the field to reach its full potential. Furthermore, the nascent field of hybrid DNA-protein and RNA-protein nanotechnology currently lacks efficient coarse-grained tools that would allow for the simulation. This proposal will consist of 1) Creation of a web-based platform for the interactive design of DNA/RNA nanostructures, along with a publicly accessible webserver for simulations and analysis of nucleic acid nanotechnology; 2) Creation of a public online repository of successfully assembled and verified DNA and RNA nanostructures from the field, where researchers will be able to share and edit published designs, allowing easy sharing and extensions of nanostructures; 3) Extension of our previously developed models of DNA and RNA to include coarse-grained representation of proteins and DNA-RNA hybrids, thus enabling the design of hybrid protein-DNA/RNA nanomaterials. The project will require development of new methods for nanostructure visualizations and analysis coupled with interactive high performance simulations on GPU cards, and parametrization of new models on new experimental data of hybrid nanostructures. The proposed research will have significant societal and educational impact. The user-friendly design tool will be easily incorporated into both the graduate and undergraduate curriculum. We will also incorporate the tools also into our outreach activities among high school students and teacher. The online design tool will be used for crowd-sourced science for web-based nanostructure design competitions.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
RUI: Framework: Data - An Open Semantic Data Framework for Data-Driven Discovery,OAC,1835643,Stuart Chalk,schalk@unf.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","The project makes contributions to the ease of annotating, sharing, and searching heterogeneous data sets. It focuses upon undergraduate training, emphasizing data science capabilities applied to a range of science problems. The project enables aggregation, search, and inference with heterogeneous datasets using a structured framework allowing data and metadata to be linked by encoding the framework as a JavaScript Object Notation (JSON) for Linked Data (JSON-LD) document. The approach builds on existing developments such as the Scientific Data (SciData) framework and associated ontology that has been developed by the PI, and Shape Constraint Language (SHACL) shapes to provide efficient searching, browsing, and visualization of data. The result extends existing approaches to link data and metadata and make data easily discoverable. The activity emphasizes Research in Undergraduate Institutions (RUI), training more than 30 undergraduates, graduate students and a post-doctoral student in the application of data science techniques to an array of science problems.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research:Framework:Software:NSCI:Enzo for the Exascale Era (Enzo-E),OAC,1835509,Greg Bryan,gbryan@astro.columbia.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","The earliest stages of the formation of galaxies and quasars in the universe are soon to be explored with a powerful new generation of ground and space-based observatories. Broad and deep astronomical surveys of the early universe beginning in the next decade using the Large Synoptic Survey Telescope and the James Webb Space Telescope will revolutionize our understanding of the origin of galaxies and quasars, and help constrain the nature of the dark matter which is the dominant matter constituent in the universe. Detailed physical simulations that model the formation of these objects are an indispensible aid to understanding the coming glut of observational data, and to maximize the scientific return of these instruments. In this project, investigators at the Univ. California San Diego, Columbia Univ., Georgia Tech, and Michigan State Univ. are collaborating with the goal of developing a next generation community simulation software framework for the coming generation of supercomputers for cosmological simulations of the young universe. Undergraduate and graduate students will be directly involved in the software development as well as its application to several frontier cosmological research topics. The software framework that will be produced will be disseminated as open source software to enable a much broader range of scientific explorations of astrophysical topics. The project brings together the key developers of the open source Enzo adaptive mesh refinement (AMR) hydrodynamic cosmology code, who will port its software components to a newly developed AMR software framework called Cello. Cello implements the highly scalable array-of-octrees AMR algorithm on top of the powerful Charm++ parallel object system. Designed to be extensible and scalable to millions of processors, the new framework, called Enzo-E, will target exascale high performance computing (HPC) systems of the future. Through this project, the entire Enzo community will have a viable path to exascale simulations of unprecedented size and scope. The investigators have chosen three frontier problems in cosmology to drive the development of the Enzo-E framework: (1) the assembly of the first generation of stars and black holes into the first galaxies; (2) the role of cosmic rays in driving galactic outflows; and (3) the evolution of the intergalactic medium from cosmic dawn to the present day. Annual developer workshops and software releases will keep the broader research community informed and involved in the developments.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Discrete Simulation of Flexible Structures and Soft Robots,OAC,2209784,Carmel Majidi,cmajidi@andrew.cmu.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","From carbon nanotubes to human-size soft robots, flexible and deformable structures are present throughout the next generation of promising engineering disciplines. However, simulation of these mechanical systems is often slow, and simulation software is challenging to use. In addition, there is little support for simulating flexible structures in common robotics research and education software, limiting the use of intelligent soft robots to experts only. On the other hand, the computer graphics community has developed advanced software for simulating flexible structures like hair and fur. Recent research has shown these computer graphics approaches can accurately simulate soft robots and flexible structures faster than real-time. This project develops an easy-to-use open-source software platform for these fast physics-based simulations of flexible structures, and incorporates the software into the national cyberinfrastructure ecosystem. This software, DiSMech, can be used by researchers of all ages to investigate the mechanics of slender structures, autonomy for soft robots, and breakthrough designs for deformable machines.The objective of this work is to develop a discrete differential geometry (DDG) simulation environment into a widely-available software package capable of modeling soft and flexible structures. The DDG approach enables low-dimensional modeling of slender rods and flexible shells combined into arbitrary shapes, establishing a practical but still physically accurate contrast to computationally expensive finite element analysis (FEA) techniques. This work first develops a core software package for DiSMech that adapts prior work to meet the standard for national cyberinfrastructure: maintainable, extensible, and with a robust user interface. Next, a virtual testbed for a wide class of soft and flexible robots is built by incorporating DiSMech into an existing robotics software suite. The project team will use the combined software framework with a machine learning approach to develop a locomotion strategy for example soft robots. Finally, add-ons to DiSMech will incorporate machine learning alongside the DDG-based physics models for even faster simulations, demonstrating the research potential for this software in uncovering underlying physical phenomena. By advancing DDG-based physics simulations to capture a wide range of soft and flexible structures, with a computational speed sufficient for learned robot control, all in an easy-to-use interface, DiSMech addresses an important gap in the national cyberinfrastructure.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Divisions of Civil, Mechanical and Manufacturing Innovation and Electrical, Communications and Cyber Systems in the Directorate of Engineering.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Frameworks: Large Scale Atmospheric Research Using an Integrated WRF Modeling, Visualization, and Verification Container Framework (I-WRF)",OAC,2209711,Richard Knepper,rich.knepper@cornell.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","The Cornell University and National Center for Atmospheric Research I-WRF Project is developing and deploying an integrated multi-container framework for the Weather Research and Forecasting (WRF) model to enable multi-node containerized simulations complete with verification and visualization capabilities. I-WRF lowers the bar for multidisciplinary researchers who wish to use WRF in parallel on multiple platforms, ranging from desktops to clouds and supercomputers. By enabling easier and more effective use of this set of applications, I-WRF makes atmospheric modeling accessible to a broader range of researchers (atmospheric scientists, civil engineers, agricultural scientists, etc.) who no longer must set up and maintain a complex suite of software applications on their own. The portability, traceability, and ease of deploying the multi-node containerized framework supports the urgent need for more and better simulations of possible future climate states signaled by the UN Climate Change Conference and the establishment of the NSF Engineering Research Visioning Alliance on The Role of Engineering in Addressing Climate Change.I-WRF application containers incorporate software tools developed mainly at NCAR. These include the WRF model and Model Evaluation Tools (METplus), as well as visualization and analysis tools that are enhanced for interactive investigations. I-WRF containers are optimized to run on multiple platforms and to enable parallel analysis of large domains with a high level of spatial discretization. They will be freely available at a container repository and discoverable with common tools. Adopters can use the I-WRF container components to build and modify their own containers, thus saving time and increasing overall container availability and sharing within and between scientific communities. To prove container validity, scaling studies will be performed on a range of use cases. These include examining the evolution of renewable energy generation in a changing climate, the effect of land use and climate change on severe weather events, and the relation between air quality and human morbidity and mortality. Three postdocs will engage in the use case studies. The ability to run I-WRF multi-node simulations on desktops will overcome current obstacles in training students to use WRF at NCAR and in university course curricula.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: TRAnsparency CErtified (TRACE): Trusting Computational Research Without Repeating It,OAC,2209629,Lars Vilhuber,lars.vilhuber@cornell.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Research communities across the natural and social sciences are increasingly concerned about the transparency and reproducibility of results obtained by computational means. Calls for increased transparency can be found in the policies of peer-reviewed journals and processing pipelines employed in the creation of research data products made available through science gateways, data portals, and statistical agencies. These communities recognize that the integrity of published results and data products is uncertain when it is not possible to trace their lineage or validate their production. Verifying the transparency or reproducibility of computational artifacts?by repeating computations and comparing results?is expensive, time-consuming, and difficult, and may be infeasible if the research products rely on resources that are subject to legitimate restrictions such as the use of sensitive or proprietary data; streaming, transient, or ephemeral data; and large-scale or specialized computational resources available only to approved or authorized users. The TRACE project is addressing this problem through an approach called certified transparency - a trustworthy record of computations signed by the systems within which they were performed. Using TRACE, system owners and operators certify the original execution of a computational workflow that produces findings or data products. By using a TRACE-enabled system, researchers produce transparent computational artifacts that no longer require verification, reducing burden on journal editors and reviewers seeking to ensure reproducibility and transparency of computational results. TRACE presents an innovative and efficient approach to ensuring the transparency of research that uses computational methods, is consistent with the vision outlined by the National Academies, and enables evidence-based policymaking based on transparent and trustworthy science.The central goal of the TRACE project is the development, validation, and implementation of a technical model of certified transparency. This includes a set of infrastructure elements that can be employed by system owners to (1) declare the dimensions of computational transparency supported by their platforms; (2) certify that a specific computational workflow was executed on the platform; and (3) bundle artifacts, records of their execution, technical metadata about their contents, and certify them for dissemination. The first phase of the project focuses on the development of a conceptual model and technical specification that can be used to certify the description of a system, termed a Transparency-Certified System (TRACE system), and the aggregation of artifacts along with records of their execution, termed Transparency-Certified Research Objects (TROs). The second phase focuses on the development of reusable software components implementing the TRACE model and approach. To demonstrate certified transparency, the toolkit is used to TRACE-enable existing platforms including Whole Tale, SKOPE, and the SLURM workload manager. These TRACE-enabled systems produce certified TROs that can be trusted and do not need to be repeated or re-executed to verify that results were obtained as claimed.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Social and Economic Sciences within the Directorate for Social, Behavioral and Economic Sciences; and by the Division of Information and Intelligent Systems within the Directorate for Computer and Information Science and Engineering.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Simulating Autonomous Agents and the Human-Autonomous Agent Interaction,OAC,2209796,William Whittaker,red@ri.cmu.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","This project augments the Chrono computer simulation platform in transformative ways. Chrono's purpose is to predict through simulation the interplay between mechatronic systems, the environment they operate in, and humans with whom they might interact. The open-source simulation platform is slated to become a community-shared virtual investigation tool used to probe competing engineering designs and test hypotheses that would be too dangerous, difficult, or costly to verify through physical experiments. Chrono has been and will continue to be used in multiple fields and disciplines, e.g., terramechanics, astrophysics; soft matter physics; biomechanics; mechanical engineering; civil engineering; industrial engineering; and computer science. Specifically, it is used to engineer the 2023 VIPER lunar rover; relied upon by US Army experts in evaluating its wheeled and tracked vehicle designs; used in the US and Germany in the wind turbine industry; and involved in designing wave energy conversion solutions in Europe. Upon project completion, Chrono will become a simulation engine in Gazebo, which is widely used in robotics research; operate on the largest driving simulator in the US; empower research in the bio-robotics and field-robotics communities; and assist efforts in the broad area of automotive research carried out by a consortium of universities and companies under the umbrella of the Automotive Research Center. The educational impact of this project is threefold: training undergraduate, graduate, and post-doctoral students in a multi-disciplinary fashion that emphasizes advanced computing skills development; anchoring two new courses in autonomous vehicle control and simulation in robotics; and broadening participation in computing through a residential program on the campus of the University of Wisconsin-Madison that engages teachers and students from rural high-schools. Innovation and discovery are fueled by quality data. At its core, this project seeks to increase the share of this data that has simulation as its provenance. In this context, a multi-disciplinary team of 40 researchers augments and validates a physics-based simulation framework that empowers research in autonomous agents (AAs). The AAs operate in complex and unstructured dynamic environments and might engage in two-way interaction with humans or other AAs. This project enables Chrono to generate machine learning training data quickly and inexpensively; facilitates comparison of competing designs for assessing trade-offs; and gauges candidate design robustness via testing in simulation of corner-case scenarios. These tasks are accomplished by upgrading and extending Chrono to leverage recent computational dynamics innovations, e.g., a faster index 3 differential algebraic equations solver; a new approach to solving frictional contact problems; a real-time solver for handling flexible-body dynamics in soft robotics via nonlinear finite element analysis; a best-in-class simulator for terradynamics applications; reliance on just-in-time compiling for producing executables that are both problem- and hardware-optimized; a novel way for using mixed data representations for parsimonious storing of state information; and a scalable multi-agent framework that enables geographically-distributed, over the Internet, real-time simulation of human-AA interaction.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: The Einstein Toolkit ecosystem: Enabling fundamental research in the era of multi-messenger astrophysics,OAC,2004879,Roland Haas,rhaas@illinois.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","A team of experts from five institutions (University of Illinois Urbana-Champaign, Georgia Institute of Technology, Rochester Institute of Technology, Louisiana State University, and West Virginia University) are collaborating on further development of the Einstein Toolkit, a community-driven, open-source cyberinfrastructure ecosystem providing computational tools supporting research in computational astrophysics, gravitational physics, and fundamental science. The new tools address current and future challenges in gravitational wave source modeling, improve the scalability of the code base, and support an expanded science and user community around the Einstein Toolkit.The Einstein Toolkit is a community-driven suite of research-grade Python codes for performing astrophysics and gravitational wave calculations. The code is open-source, accessible via Conda (an open source package management system) and represents a long-term investment by NSF in providing such computational infrastructure. The software is designed to simulate compact binary stars as sources of gravitational waves. This project focuses on the sustainability of the Einstein Toolkit; specific research efforts center around the development of three new software capabilities for the toolkit: ? CarpetX -- a new mesh refinement driver and interface between AMReX, a software framework containing the functionality to write massively parallel block-structured adaptive mesh refinement (AMR) code, and Cactus, a framework for building a variety of computing applications in science and engineering;? NRPy+ -- a user-friendly code generator based on Python; and ? Canuda -- a new physics library to probe fundamental physics. Integration of graphics processing units (GPUs) will incorporate modern heterogeneous computing devices into the system and will enhance the capability of the toolkit. The end product is sustainable through integration into the Einstein Toolkit, yet also includes an active community maintaining and enhancing the foundational components. Broader impacts are enhanced through training, documentation and a support infrastructure that reduces the barrier to adoption by the community. The team is also creating a science portal with additional educational and showcase resources. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the National Science Foundation's Big Idea activities in Windows on the Universe (WoU).This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Internet of Samples: Toward an Interdisciplinary Cyberinfrastructure for Material Samples,OAC,2004839,Kerstin Lehnert,lehnert@ldeo.columbia.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Research frequently uses material samples as a basic element for reference, study, and experimentation in many scientific disciplines, especially in the natural and environmental sciences, material sciences, agriculture, physical anthropology, archaeology, and biomedicine. Observations made on samples collected in the field and in the laboratory constitute a critical data resource for research that addresses grand challenges of our planet's future sustainability, from environmental change; to food, energy, and water resources; to natural hazards and their mitigation; to public health. The large investments of public funds being made to curate huge volumes of samples acquired over decades or even centuries, and to collect and analyze new samples, demand that these samples be openly accessible, easily discoverable, and documented with sufficient information to make them reusable. The current ecosystem of sample and sample data management in the U.S. and globally is highly fragmented across stakeholders, including museums, federal agencies, academic institutions, and individual researchers, with a multitude of institutional and discipline-specific catalogs, practices for sample identification, and protocols for describing samples. The iSamples project is a multi-disciplinary collaboration that will develop a national digital infrastructure to provide services for globally unique, consistent, and convenient identification of material samples; metadata about them; and linking them to other samples, derived data, and research results published in the literature. iSamples builds on previous initiatives to achieve these goals by providing material samples with globally unique, persistent identifiers that reliably link to landing pages with metadata describing the sample and its provenance, and which allow unambiguously linking samples with data and publications. Leveraging significant national investments, iSamples provides the missing link among (i) physical collections (e.g., natural history museums, herbaria, biobanks), (ii) field stations, marine laboratories, long-term ecological research sites, and observatories, and (iii) data repositories and cyberinfrastructure. iSamples delivers enhanced infrastructure for STEM research and education, decision-makers, and the general public. iSamples benefits national security and resource management by offering a means to assure sample provenance, improving scientific reproducibility and demonstrating compliance with ethical standards, national regulations, and international treaties.The Internet of Samples (iSamples) is a multi-disciplinary and multi-institutional project to design, develop, and promote service infrastructure to uniquely, consistently, and conveniently identify material samples, record metadata about them, and persistently link them to other samples and derived digital content, including images, data, and publications. The project will create a flexible and scalable architecture to ensure broad adoption and implementation by diverse stakeholders. iSamples will build upon existing identifier infrastructure such as IGSNs (Global Sample Number;) and ARKs (Archival Resource Keys), but is agnostic to identifier type. Likewise, iSamples will encourage a high-level metadata standard for natural history samples (across biosciences, geosciences, and archaeology), while supporting community-developed metadata standards in specialist domains. Through integration with established discipline-specific infrastructure at the System for Earth Sample Registration SESAR (geoscience), CyVerse (bioscience), and Open Context (archaeology), iSamples will extend existing capabilities, enhance consistency, and expand their reach to serve science and society much more broadly. The project includes three main objectives: 1) Design and develop iSamples infrastructure (iSamples in a Box and iSamples Central); 2) Build four initial implementations of iSamples for adoption and use case testing (Open Context, GEOME, SESAR, and Smithsonian Institution); and 3) Conduct outreach and community engagement to developers, individual researchers, and international organizations concerned with material samples. The project will follow an agile development process that includes community engagement as an important element of creating software requirements and an implementation timeline.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Basic ALgebra LIbraries for Sustainable Technology with Interdisciplinary Collaboration (BALLISTIC),OAC,2004235,Michael Mahoney,mmahoney@icsi.berkeley.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","Scientific software libraries have long provided a large and growing resource for high-quality, reusable software components upon which applications from science and engineering can be rapidly constructed ? with improved robustness, portability, and sustainability. For this, a team of researchers from four collaborating organizations proposed to develop BALLISTIC (Basic ALgebra Libraries for Sustainable Technology with Interdisciplinary Collaboration). The BALLISTIC project, through the leading-edge research it channels into its software deliverables, will lead to the introduction of tools that will simplify the transition to the next generation of extreme-scale computer architectures. The main impact of the project will be to develop, push, and deploy software into the scientific community to make it competitive on a world-wide scale and to contribute to standardization efforts in the area. BALLISTIC has the potential to become the community standard for dense linear algebra and be adopted and/or supported by a large community of users, computing centers, and High-Performance Computing (HPC) vendors. Learning to use numerical libraries is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. BALLISTIC will have a correspondingly large impact on the research and education community, government laboratories, and private industry and support national efforts to build a workforce capable of employing state of the art tools in pursuit of science and engineering discoveries.The goal of BALLISTIC is to create a layered package of software components that is capable of running at every level of the platform deployment pyramid and achieves three complementary objectives: (1) deliver seamless access to the most up-to-date algorithms, numerics, and performance via familiar Sca/LAPACK interfaces, wherever possible; (2) make advanced algorithms, numerics, and performance capabilities available through new interface extensions, wherever necessary; and (3) provide a well-engineered conduit through which new discoveries at the frontiers of research in these areas can be channeled as quickly as possible to all the applications from science and engineering communities that depend on high-performance linear algebra libraries.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: SHF: Medium: Practical and Rigorous Correctness Checking and Correctness Preservation for Irregular Parallel Programs,CCF,1955367,Martin Burtscher,burtscher@txstate.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Many important ? and in some cases, lifesaving ? computations are performed on graph structures consisting of millions of vertices and edges. For example, such graphs might represent medical information, protein interactions, or taxonomies of diseases. Since these graphs tend to be large, they are processed in parallel to fully harness the speed offered by modern computers, which use multicore processors and often general-purpose Graphics Processing Units (GPUs). Unfortunately, parallelizing graph computations is difficult, especially for GPUs,and often leads to accidental uncoordinated accesses known as data races. Data races can be hard to track down as they only sometimes corrupt the result. The project's novelties are the development of scalable and mathematically sound methods for data-race and other bug detection on graph computations. The project's main impact is the elimination of many human programming errors to improve the trust in computations carried out on life-critical and other data.The project develops generic symbolic representations of allowed concurrent operations on primitive data operations. This provides theability to easily boil down new concurrency models into this semantic base to quickly create new analysis tools, thus counteracting verification tool obsolescence. It augments the power of small-scope symbolic-analysis methods with execution-based dynamic-analysis methods that scale to realistic code and data sizes. The project derives real-world case studies from high-performance CUDA and OpenMP implementations of important graph algorithms developed over a decade. The project plans to publicly release the new data-race checking tools as well as verification micro-benchmarks and rigorously verified parallel graph codes. It is also training students whose education is advanced by teaching them modern program analysis methods.This award is co-funded by the Software & Hardware Foundations Program in the Division of Computer & Computing Foundations, and the NSF Office of Advanced Cyberinfrastructure.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Software: Accelerating Discovery of the First Stars through a Robust Software Testing Infrastructure,OAC,1835120,Jonathan Pober,Jonathan_Pober@brown.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","The birth of the first stars and galaxies 13 billions years ago -- our ""Cosmic Dawn"" -- is one of the last unobserved periods in the history of the Universe. Scientists are working to observe the 21 cm radio light emitted by the primeval neutral hydrogen fog as the first stars formed. These observations are considered one of the grand challenges of modern astrophysics. This project will provide critical software infrastructure for the field of 21 cm cosmology, enabling rapid vetting of the new analyses and techniques developed for these observations and increasing their robustness, rigor, and reproducibility. Under this project The invetigators will train students in the best practices for software and code development, preparing them to develop robust, reproducible software for their own research, contribute to large open source projects, and develop software in a professional setting.One of the biggest challenges for the detection of the Epoch of Reionization is the presence of bright astrophysical foregrounds that obscures the signal of interest, requiring extraordinarily precise modeling and calibration of the radio telescopes performing these observations. The 21 cm cosmology community is rapidly developing new techniques for instrument calibration, foreground removal, and analysis, but thorough testing and integration into existing data analysis pipelines has been slow. This project will provide a software infrastructure that can enable rigorous, seamless testing of novel algorithmic developments within a unified framework. This infrastructure will ensure a level of reliability and reproducibility not possible with current tools and accelerate the speed at which developments become integrated into production level code, providing an invaluable foundation for bringing our field into the next decade and for leveraging the current NSF investments in these experiments.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Elements: libkrylov, a Modular Open-Source Software Library for Extremely Large Eigenvalue and Linear Problems",OAC,1835909,Filipp Furche,filipp.furche@uci.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Strongly coupled linear equation systems or eigenvalue problems with extremely large numbers of unknowns are a critical bottleneck for computational solutions to many grand challenges in science and engineering. For example, computational design of light emitting or photovoltaic materials from first principles requires the solution of tens of millions of strongly coupled linear equations within minutes to be practical. This project aims to develop, implement, test, and deploy libkrylov, a robust, efficient, and general open-source library of ""on-the-fly"" Krylov space methods suitable for solving such extremely large, dense problems. libkrylov will deliver the latest innovations in Krylov-space methods to the scientific and engineering communities by providing a uniform, reproducible, and user-friendly software standard. Coupled with electronic structure codes, the library will enable large-scale simulations of molecular time-dependent X-ray absorption spectra of organometallic and bio-inorganic systems. This project will promote computational literacy through student training and workforce education at University of California, Irvine and San Diego State University, and enhance national software infrastructure through collaboration with the NSF-funded Molecular Sciences Software Institute (MolSSI) in Blacksburg, VA.The PI and his group have recently developed nonorthonormal Krylov space methods for solving extremely large dense eigenvalue and linear problems ""on-the-fly"", i.e., without explicit storage or access of coefficient matrices, with demonstrated efficiency and stability. This project aims to transform this methodology into robust, efficient, and sustainable software infrastructure freely accessible to the public. Key features include (i) unique capability to solve extremely large problems, (ii) a highly flexible interface to matrix-vector multiplication ""engines"" (iii) ultrahigh efficiency by minimizing the number and cost of matrix-vector multiplications, (iv) outstanding robustness by dynamic control of errors and condition, and stabilization methods, (v) versatility by exploiting symmetry and special structure, real and complex arithmetic, (vi) configurable precision, convergence control, preconditioning, memory and disk usage, (vii) portability to broad range of platforms, environments, and languages, (viii) flexible and user-friendly generic interfaces, documentation, and testing capabilities, (ix) extensibility through object orientation and modularity, (x) reproducibility through a dedicated test suite, (xi) community involvement and sustainability by collaboration with MolSSI and deployment of a public issue and feature request tracker.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Data: NSCI: HDR: GeoSCIFramework: Scalable Real-Time Streaming Analytics and Machine Learning for Geoscience and Hazards Research,OAC,1835566,Kristy Tiampo,kristy.tiampo@colorado.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","This project develops a real-time processing system capable of handling a large mix of sensor observations. The focus of this system is automation of the detection of natural hazard events using machine learning, as the events are occurring. A four-organization collaboration (UNAVCO, University of Colorado, University of Oregon, and Rutgers University) develops a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research. This work will support rapid analysis and understanding of data associated with hazardous events (earthquakes, volcanic eruptions, tsunamis). This project uses a collaboration between computer scientists and geoscientists to develop a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research. It focuses on the aggregation and integration of a large number of data streams into a coherent system that supports analysis of the data streams in real-time. The framework will offer machine-learning-based tools designed to detect signals of events, such as earthquakes and tsunamis, that might only be detectable when looking at a broad selection of observational inputs. The architecture sets up a fast data pipeline by combining a group of open source components that make big data applications viable and easier to develop. Data sources for the project draw primarily upon the 1500+ sensors from the EarthScope networks currently managed by UNAVCO and the Incorporated Research Institutions for Seismology (IRIS), as well as the Ocean Observatories Initiative (OOI) cabled array data managed by Rutgers University. Machine learning (ML) algorithms will be researched and applied to the tsunami and earthquake use cases. Initially, the project plans to employ an advanced convolutional neural network method in a multi-data environment. The method has only been applied to seismic waveforms, so the project will explore extending the method to a multi-data environment. The approach is expected to be extensible beyond detection and characterization of earthquakes to include the onset of other geophysical signals such as slow-slip events or magmatic intrusion, expanding the potential for new scientific discoveries. The framework is applied to use cases in the Cascadia subduction zone and Yellowstone: these locations combine the expertise of the science team with locations where EarthScope and OOI have the greatest concentration of instruments. The architecture will be transportable and scalable, running in a Docker environment on laptops, local clusters and the cloud. Integral to the project will be development, documentation and training using collaborative online resources such as GitLab and Jupyter Notebooks, and utilizing NSF XSEDE resources to make larger datasets and computational resources more widely available.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program within the NSF Directorate for Geosciences, the Big Data Science and Engineering Program within the Directorate for Computer and Information Science and Engineering, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Software: Accelerating Discovery of the First Stars through a Robust Software Testing Infrastructure,OAC,1835421,Bryna Hazelton,brynah@phys.washington.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","The birth of the first stars and galaxies 13 billions years ago -- our ""Cosmic Dawn"" -- is one of the last unobserved periods in the history of the Universe. Scientists are working to observe the 21 cm radio light emitted by the primeval neutral hydrogen fog as the first stars formed. These observations are considered one of the grand challenges of modern astrophysics. This project will provide critical software infrastructure for the field of 21 cm cosmology, enabling rapid vetting of the new analyses and techniques developed for these observations and increasing their robustness, rigor, and reproducibility. Under this project The investigators will train students in the best practices for software and code development, preparing them to develop robust, reproducible software for their own research, contribute to large open source projects, and develop software in a professional setting.One of the biggest challenges for the detection of the Epoch of Reionization is the presence of bright astrophysical foregrounds that obscures the signal of interest, requiring extraordinarily precise modeling and calibration of the radio telescopes performing these observations. The 21 cm cosmology community is rapidly developing new techniques for instrument calibration, foreground removal, and analysis, but thorough testing and integration into existing data analysis pipelines has been slow. This project will provide a software infrastructure that can enable rigorous, seamless testing of novel algorithmic developments within a unified framework. This infrastructure will ensure a level of reliability and reproducibility not possible with current tools and accelerate the speed at which developments become integrated into production level code, providing an invaluable foundation for bringing our field into the next decade and for leveraging the current NSF investments in these experiments.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: ELEMENTS: Tuning-free Anomaly Detection Service,OAC,2103832,Elke Rundensteiner,rundenst@wpi.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Finding and understanding anomalous behavior in data is important in many applications. A large number of anomaly detection algorithms exist, and it can be difficult to determine which algorithm is best suited to a particular domain. And once an algorithm is selected, users must tune many parameters manually to get the algorithm to perform well; this requires in-depth knowledge of the machine learning process and an understanding of the trade-offs among different algorithms to select the best performing approach. To address these difficulties, this team develops a package that can test a range of unsupervised anomaly detection techniques on a dataset, explore options to identify best-fit, and classify anomalies with higher accuracy than manual tuning.The project will automatically test a range of unsupervised anomaly techniques on a data set, extract knowledge from the combined detection results to reliably distinguish between anomalies and normal data, and use this knowledge as labels to train an anomaly classifier; the goal is to classify anomalies with an accuracy higher than what is achievable by thorough manual tuning. The approach can be applied across of a range of data types and domains. The resulting cyberinfrastructure provides tuning-free anomaly detection capabilities while making it easy to incorporate domain-specific requirements. It enables scientists and engineers having little experience with anomaly detection techniques to steer the anomaly detection process with domain expertise. Evaluation of the unsupervised anomaly detection package will use data sets and partnerships with collaborators from the Massachusetts General Hospital/Harvard Medical School, Cyber Security research, and Signify (formerly Philips Lighting) to ensure that the utility and usability of the package is verified throughout the development process. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Division of Information and Intelligent Systems within the Directorate for Computer and Information Science and Engineering.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Framework: Software: HDR Globus Automate: A Distributed Research Automation Platform,OAC,1835890,Ian Foster,foster@uchicago.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Rapid increases in data volumes and velocities are overwhelming finite human capabilities. Continued progress in science and engineering demands that we automate a broad spectrum of currently manual research data manipulation tasks, from transfer and sharing to acquisition, publication, indexing, analysis, and inference. To address this need, which arises across essentially all scientific disciplines, this project will work with scientists in astronomy, engineering, geosciences, materials science, and neurosciences to develop and apply Globus Automate, a distributed research automation platform. Its purpose is to increase productivity and research quality across many science disciplines by allowing scientists to offload the management of a broad range of data acquisition, manipulation, and analysis tasks to a cloud-hosted distributed research automation platform. By thus enabling scientists to hand off responsibility for managing frequently performed tasks, such as acquiring, analyzing, and storing data, Globus Automate will increase the productivity of scientific instruments and the scientists that use them.This project will expand the capabilities and reach of the highly successful Globus research data management platform. Globus combines a professionally operated cloud-hosted management service with Globus Connect software deployed on more than 12,000 storage system endpoints, spanning most research universities, NSF-funded compute facilities, and NSF disciplines. Users employ Globus web interfaces and APIs to drive data movement, synchronization, and sharing tasks at and among endpoints. This ability to hand off responsibility for such tasks to cloud-hosted management logic has enabled substantial increases in data management efficiency, and spurred development of a wide range of innovative data management applications. Globus Automate will extend Globus capabilities to produce a full-featured distributed research automation platform that will enable the reliable, secure, and efficient automation of a wide range of research data management and manipulation activities. It will extend intuitive trigger-action programming models, suitable for non-programming users, to enable the specification and execution of a series of actions. It will provide for the detection of data events both at Globus storage system endpoints (e.g., creation or modification of new data files, extraction of new metadata) and at other sources (e.g., completion or failure of Globus transfer tasks); the propagation of such events to a cloud-hosted orchestration engine for reliable, efficient, and secure processing; and the invocation of remote actions on Globus endpoints and other resources. The project will leverage these basic event mechanisms to implement solutions to challenging science problems associated with partner science projects, and create a library of automation flows, both general-purpose (e.g., data publication and data replication) and domain-specific (e.g., feature detection in experimental data). These data event mechanisms will be made available on all storage systems relevant to research (Globus already supports most on-premises and cloud systems) and integrated with the Python language and JupyterLab environment that have become popular in science, so that researchers can define and share data automation behaviors as simple Python programs. A quantitative and qualitative research agenda will analyze the usability and adoption of both the platform and the research automation paradigm.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: HDR: Building the Twenty-First Century Citizen Science Framework to Enable Scientific Discovery Across Disciplines,OAC,1835272,Laura Trouille,ltrouille@adlerplanetarium.org,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","A team of experts from five institutions (University of Minnesota, Adler Planetarium, University of Wyoming, Colorado State University, and UC San Diego) links field-based and online analysis capabilities to support citizen science, focusing on three research areas (cell biology, ecology, and astronomy). The project builds on Zooniverse and CitSci.org, leverages the NSF Science Gateways Community Institute, and enhances the quality of citizen science and the experience of its participants.This project creates an integrated Citizen Science Cyberinfrastructure (CSCI) framework that expands the capacity of research communities across several disciplines to use citizen science as a suitable and sustainable research methodology. CSCI produces three improvements to the infrastructure for citizen science already provided by Zooniverse and CitSci.org: - Combining Modes - connecting the process of data collection and analysis; - Smart Assignment - improving the assignment of tasks during analysis; and - New Data Models - exploring the Data-as-Subject model. By treating time series data as data, this model removes the need to create images for classification and facilitates more complex workflows. These improvements are motivated and investigated through three distinct scientific cases: - Biomedicine (3D Morphology of Cell Nucleus). Currently, Zooniverse 'Etch-a-Cell' volunteers provide annotations of cellular components in images from high-resolution microscopy, where a single cell provides a stack containing thousands of sliced images. The Smart Task Assignment capability incorporates this information, so volunteers are not shown each image in a stack where machines or other volunteers have already evaluated some subset of data. - Ecology (Identifying Individual Animals). When monitoring wide-ranging wildlife populations, identification of individual animals is needed for robust estimates of population sizes and trends. This use case combines field collection and data analysis with deep learning to improve results. - Astronomy (Characterizing Lightcurves). Astronomical time series data reveal a variety of behaviors, such as stellar flares or planetary transits. The existing Zooniverse data model requires classification of individual images before aggregation of results and transformation back to refer to the original data. By using the Data-as-Subject model and the Smart Task Assignment capability, volunteers will be able to scan through the entire time series in a machine-aided manner to determine specific light curve characteristics.The team explores the use of recurrent neural networks (RNNs) to determine automated learning architectures best suited to the projects. Of particular interest is how the degree to which neighboring subjects are coupled affects performance. The integration of existing tools, which is based on application programming interfaces (APIs), also facilitates further tool integration. The effort creates a citizen science framework that directly advances knowledge for three science use cases in biomedicine, ecology, and astronomy, and combines field-collected data with data analysis. This has the ability to solve key problems in the individual applications, as well as benefiting the research of the dozens of projects on the Zooniverse platform. It provides benefits to researchers using citizen scientists, and to the nearly 1.6 million citizen scientists themselves.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Research on Learning in Formal and Informal Settings, within the NSF Directorate for Education and Human Resources.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Ghub as a Community-Driven Data-Model Framework for Ice-Sheet Science,OAC,2004302,Abani Patra,abani.patra@tufts.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Sea level rise is challenging societies around the globe. Planning for future sea level rise in the US is critical for national security, public health, and socioeconomic stability. However, current predictions of sea level rise remain uncertain, because the future behavior of melting ice sheets - a primary cause of sea level rise - is not well understood. A recent United Nations report (IPCC Special Report on the Ocean and Cryosphere in a Changing Climate) summarized two startling facts: (i) Recent sea level rise acceleration is due to increased ice loss from the Greenland and Antarctic ice sheets; and (ii) Uncertainty related to ice-sheet instability arises from limited observations, incomplete representation of ice-sheet processes in current models, and evolving understanding of the complex interactions between the atmosphere, ocean and ice sheets. Improving our ability to forecast the health of ice sheets and hence, predictions of future sea level rise, requires a large, long-lasting collective effort among ice sheet scientists working closely with scientists from the modeling and remote sensing disciplines. One challenge in this collective effort is the range of disciplines and approaches to ice-sheet science - the degree of specialization is an obstacle to efficient collaborative work. This project will lower the barriers among sub-disciplines in ice-sheet science by creating and promoting a centralized web-based hub, called ?Ghub,? where datasets and tools will be made accessible to the full range of ice sheet science fields of study. Ghub is accessible to all interested scientists and lay personnel. Use of Ghub includes access to datasets, analysis tools, and cloud computing power, as well as the ability to develop and share new tools within the Ghub environment. Several avenues of outreach and education as part of the Ghub project are specifically aimed at framing ice-sheet science for general audiences, and including students from underrepresented groups.The urgency in reducing uncertainties of near-term sea level rise relies on improved modeling of ice-sheet response to climate change. Predicting future ice-sheet change requires a tremendous effort across a range of disciplines in ice-sheet science including expertise in observational data, paleoglaciology (""paleo"") data, numerical ice sheet modeling, and widespread use of emerging methodologies for learning from the data, such as machine learning. However, significant knowledge and disciplinary barriers make collaboration between data and model groups the exception rather than the norm. Most modeling groups write their own tools to ingest data and analyze output, newer and larger observational datasets are not being fully taken advantage of by the modeling community, and paleo data critical for constraining model representation of ice sheet history are largely inaccessible to modelers. The diverse disciplinary approaches to ice-sheet science has led to bottlenecks that slow the response to the developing crisis. Coordination between data generators and modelers is critical for testing data-driven hypotheses, providing mechanistic explanations for past ice-sheet change, and incorporating newly understood physical processes and validating models to improve their predictive ability. Solving the urgent problem of unoptimized collaboration requires a novel, integrated, trans-disciplinary program that lowers barriers across the distinct approaches to ice-sheet science. Fostering collaboration between disciplines will lead to a transformational leap in ice-sheet and sea-level science. To make the leap, we must improve the efficiency in collaboration among traditionally disparate approaches to the problem. We will develop a community-building scientific and educational cyberinfrastructure framework including models and data processing tools, to enable coordination and synergistic exchange between ice-sheet scientific communities. The new cyberinfrastructure will be a significant bridge that connects the numerical ice-sheet modeling community with rapidly growing observational datasets of past and present ice-sheet states that will ultimately improve predictions of sea level rise. The GHub cyberinfrastructure will also be a template for organizing disparate scientific communities to address urgent societal needs in a timely fashion.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Elements: Cyberinfrastructure for streamlining coupled, simplified climate modeling within the Community Earth System Model",OAC,2004575,Scott Bachman,bachman@ucar.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Global climate models have increased dramatically in their complexity over the last few decades, and now represent a massive array of coupled atmospheric, oceanic, cryospheric, biospheric and chemical processes. However, this complexity incurs substantial computational expense, and creates a challenge for researchers to perform targeted experiments and investigate specific dynamical processes. Idealizing the climate system through the creation of simplified models is a way of performing these types of highly focused studies while avoiding complexity and expense. However, the climate modeling community presently lacks the software infrastructure to easily configure these models using minimal physics packages, simplified domain geometries, or more streamlined model versions, within a single framework that bridges the gap between these simplified models and the fully coupled system. This project aims to fill this gap by developing a software toolchain to enable fast and seamless setup of coupled simplified models within NCAR?s Community Earth System Model (CESM). This Simpler Models toolchain will be officially supported as part of the CESM modeling framework, and will serve the needs of the research and academic communities across all climate science disciplines.The overarching goal for the Simpler Models toolchain is to streamline the user workflow and enable research-ready coupled modeling at minimal expense in terms of human or computing time. The first component of the toolchain, the Simpler Models Query Tool, will advise users on the compatibility and configuration of different earth system model components based on their XML metadata. This tool will essentially let the user hone their modeling strategy without having to consult each component?s documentation or resorting to a trial and error approach. The toolchain will then allow for the configuration of model resolution and idealized domain geometries in such a way as to ensure compatibility across components. The workflow enabled by the toolchain will be structured so that compatibility of the coupled system is guaranteed first, followed by customizations at progressively higher levels of granularity within individual model components. The toolchain will be accompanied by user documentation that will be hosted on the CESM Simpler Models website.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Data:  Integrating Human and Machine for Post-Disaster Visual Data Analytics:  A Modern Media-Oriented Approach,OAC,1835473,Shirley Dyke,sdyke@purdue.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","This project creates a science-oriented visual data service that facilitates the query of datasets based on visual content. The approach allows a user to search for data based on visual similarity, even in cases where a term for the failure or observation does not yet have a scientific name. The visual analysis data and application services will be deployed on a cloud-based platform. The results will produce a framework enabling access to and analysis of a large amount of imagery from diverse sources. The research team creates VISER (Visual Structural Expertise Replicator), which will serve as a comprehensive cloud-based data analytics service and will facilitate the use of and integrate data and applications most needed by the user. The framework will implement two novel concepts: data-as-a-service and applications-as-a-service, which will bring data and applications to the user without the need to configure software systems or packages. The approach also employs artificial intelligence to interpret the contents of the images. VISER will use convolutional neural networks (CNNs) to train custom classifiers for new categories. Three applications will be developed and deployed within VISER: App1 will extract relevant visual context, App2 will facilitate similarity-based visual searching (through the use of a Siamese CNN), and App3 will help perform automatic extraction of pre-event/pre-disaster images based on Google Street View. The application of these tools would advance both the science of automated pattern recognition and of more effective construction techniques.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Software: Software Health Monitoring and Improvement Framework,OAC,1835747,Marouane Kessentini,kessentini@oakland.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Software underpins every aspects of modern life, with significant impact in society. Poor quality software can cause huge financial losses, even threatening people's lives. Software quality is even more critical within the scientific community. The reproducibility of research results and sustainability of the research itself, heavily depend on the quality of the software developed by scientists, who usually acquire basics of software programming but are not aware of the best design practices. As a consequence, several existing open access scientific software packages are known to be hard to use and evolve due to their poor quality, as highlighted in recent studies. This project will integrate and enhance recent advances in software issue detection and refactoring techniques, created by the PIs and sponsored by NSF, in order to serve diverse scientific and engineering domains, detecting and fixing software quality issues effectively. This proposal seeks to bridge the gap between software engineering community and other science and engineering community in general. It will provide quantitative comparisons of software projects against an industrial benchmark, enable users to pinpoint software issues responsible for high maintenance costs, visualize the severity of the detected issues, and refactor them using the proposed interactive refactoring framework. The proposed framework will bring together software users and software developers by enabling non software experts to post software challenges for the software community to solve, which will, in turn, boost the research and advances in software research.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: ROCCI: Integrated Cyberinfrastructure for In Situ Lossy Compression Optimization Based on Post Hoc Analysis Requirements,OAC,2247080,Dingwen Tao,ditao@iu.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Today?s simulations and advanced instruments are producing vast volumes of data, presenting a major storage and I/O burden for scientists. Error-bounded lossy compressors, which can significantly reduce the data volume while controlling data distortion with a constant error bound, have been developed for years. However, a significant gap still remains in practice. On the one hand, the impact of the compression errors on scientific research is not well understood, so how to set an appropriate error bound for lossy compression is very challenging. On the other hand, how to select the best fit compression technology and run it automatically in scientific application codes is non-trivial because of strengths and weaknesses of different compression techniques and diverse characteristics of applications and datasets. This project aims to develop a Requirement-Oriented Compression Cyber-Infrastructure (ROCCI) for data-intensive domains such as astrophysics and materials science, which can select and run the best fit lossy compressor automatically at runtime, in terms of user's requirement on their post hoc analysis.The overarching goal of this project is to offer a complete series of automatic functions and services allowing users to transparently run the best fit compressor at runtime during the scientific simulations or data acquisition. This project advances knowledge and understanding with three key thrusts: (1) it builds an efficient layer to interoperate with different lossy compressors and diverse post hoc analysis requirements on data fidelity by leveraging an existing compression adaptor library (LibPressio) and compression assessment library (Z-checker); (2) it develops an efficient engine to determine the best fit compressor with optimized settings based on user?s post-hoc analysis requirements; and (3) it develops a user-friendly infrastructure that integrates compression optimization and execution via the HDF5 dynamic filter mechanism. This project particularly targets cosmology and materials science applications and their specific requirements of using lossy compressors in practice.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Ghub as a Community-Driven Data-Model Framework for Ice-Sheet Science,OAC,2004826,Jason Briner,jbriner@buffalo.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Sea level rise is challenging societies around the globe. Planning for future sea level rise in the US is critical for national security, public health, and socioeconomic stability. However, current predictions of sea level rise remain uncertain, because the future behavior of melting ice sheets - a primary cause of sea level rise - is not well understood. A recent United Nations report (IPCC Special Report on the Ocean and Cryosphere in a Changing Climate) summarized two startling facts: (i) Recent sea level rise acceleration is due to increased ice loss from the Greenland and Antarctic ice sheets; and (ii) Uncertainty related to ice-sheet instability arises from limited observations, incomplete representation of ice-sheet processes in current models, and evolving understanding of the complex interactions between the atmosphere, ocean and ice sheets. Improving our ability to forecast the health of ice sheets and hence, predictions of future sea level rise, requires a large, long-lasting collective effort among ice sheet scientists working closely with scientists from the modeling and remote sensing disciplines. One challenge in this collective effort is the range of disciplines and approaches to ice-sheet science - the degree of specialization is an obstacle to efficient collaborative work. This project will lower the barriers among sub-disciplines in ice-sheet science by creating and promoting a centralized web-based hub, called ?Ghub,? where datasets and tools will be made accessible to the full range of ice sheet science fields of study. Ghub is accessible to all interested scientists and lay personnel. Use of Ghub includes access to datasets, analysis tools, and cloud computing power, as well as the ability to develop and share new tools within the Ghub environment. Several avenues of outreach and education as part of the Ghub project are specifically aimed at framing ice-sheet science for general audiences, and including students from underrepresented groups.The urgency in reducing uncertainties of near-term sea level rise relies on improved modeling of ice-sheet response to climate change. Predicting future ice-sheet change requires a tremendous effort across a range of disciplines in ice-sheet science including expertise in observational data, paleoglaciology (""paleo"") data, numerical ice sheet modeling, and widespread use of emerging methodologies for learning from the data, such as machine learning. However, significant knowledge and disciplinary barriers make collaboration between data and model groups the exception rather than the norm. Most modeling groups write their own tools to ingest data and analyze output, newer and larger observational datasets are not being fully taken advantage of by the modeling community, and paleo data critical for constraining model representation of ice sheet history are largely inaccessible to modelers. The diverse disciplinary approaches to ice-sheet science has led to bottlenecks that slow the response to the developing crisis. Coordination between data generators and modelers is critical for testing data-driven hypotheses, providing mechanistic explanations for past ice-sheet change, and incorporating newly understood physical processes and validating models to improve their predictive ability. Solving the urgent problem of unoptimized collaboration requires a novel, integrated, trans-disciplinary program that lowers barriers across the distinct approaches to ice-sheet science. Fostering collaboration between disciplines will lead to a transformational leap in ice-sheet and sea-level science. To make the leap, we must improve the efficiency in collaboration among traditionally disparate approaches to the problem. We will develop a community-building scientific and educational cyberinfrastructure framework including models and data processing tools, to enable coordination and synergistic exchange between ice-sheet scientific communities. The new cyberinfrastructure will be a significant bridge that connects the numerical ice-sheet modeling community with rapidly growing observational datasets of past and present ice-sheet states that will ultimately improve predictions of sea level rise. The GHub cyberinfrastructure will also be a template for organizing disparate scientific communities to address urgent societal needs in a timely fashion.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: ROCCI: Integrated Cyberinfrastructure for In Situ Lossy Compression Optimization Based on Post Hoc Analysis Requirements,OAC,2104024,Dingwen Tao,ditao@iu.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Today?s simulations and advanced instruments are producing vast volumes of data, presenting a major storage and I/O burden for scientists. Error-bounded lossy compressors, which can significantly reduce the data volume while controlling data distortion with a constant error bound, have been developed for years. However, a significant gap still remains in practice. On the one hand, the impact of the compression errors on scientific research is not well understood, so how to set an appropriate error bound for lossy compression is very challenging. On the other hand, how to select the best fit compression technology and run it automatically in scientific application codes is non-trivial because of strengths and weaknesses of different compression techniques and diverse characteristics of applications and datasets. This project aims to develop a Requirement-Oriented Compression Cyber-Infrastructure (ROCCI) for data-intensive domains such as astrophysics and materials science, which can select and run the best fit lossy compressor automatically at runtime, in terms of user's requirement on their post hoc analysis.The overarching goal of this project is to offer a complete series of automatic functions and services allowing users to transparently run the best fit compressor at runtime during the scientific simulations or data acquisition. This project advances knowledge and understanding with three key thrusts: (1) it builds an efficient layer to interoperate with different lossy compressors and diverse post hoc analysis requirements on data fidelity by leveraging an existing compression adaptor library (LibPressio) and compression assessment library (Z-checker); (2) it develops an efficient engine to determine the best fit compressor with optimized settings based on user?s post-hoc analysis requirements; and (3) it develops a user-friendly infrastructure that integrates compression optimization and execution via the HDF5 dynamic filter mechanism. This project particularly targets cosmology and materials science applications and their specific requirements of using lossy compressors in practice.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science,OAC,1916805,Madhav Marathe,mvm7hz@virginia.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth. Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn. The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above. Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities. Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks. Compositions of these diverse capabilities are rare. Furthermore, many researchers who study networks are not computer scientists. As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming. The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use. What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science. CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software. CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks. The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program. It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions. CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science. Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science,OAC,1835660,Madhav Marathe,mvm7hz@virginia.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth. Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn. The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above. Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities. Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks. Compositions of these diverse capabilities are rare. Furthermore, many researchers who study networks are not computer scientists. As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming. The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use. What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science. CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software. CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks. The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program. It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions. CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science. Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Software: NSCI: Chrono-An open-source simulation platform for computational dynamics problems,OAC,1835727,Arman Pazouki,apazouk@calstatela.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","This project seeks to augment modeling and solution methods employed by Chrono, an open-source computer simulation platform for multi-body dynamics (MBD) and fluid-solid interaction (FSI) problems. Chrono will be able to capture dynamics at various size and time scales spanning from millisecond (impact phenomena) to decades (geophysics). These performance levels open up new directions of research in several fields. Chrono is widely used and further developed by other users and has an active forum with more than 250 registered users currently. This project will enhance the richness of Chrono's modeling features, sound numerical solution foundation, and leverage of emerging hardware architectures to elevate this simulation capability to the status of ready-to-use, open-source, best-in-class computational dynamics platform. Chrono has been used by universities, national labs, and industry. Over the past two years, various groups have used Chrono in extraterrestrial applications, machine learning in robotics, image processing, pattern recognition and computer vision, mechanical watch design, architectural studies, autonomous vehicles, fluid-solid interaction applications, wind turbine dynamics, next generation space suit design, oil extraction and accident mitigation, hardware-in-the-loop simulation, etc. Finally, this project will engage high-school students from under-represented groups in a six-day residential camp run (now at its 12th edition) and will train a group of undergraduate students from California State University at University of Wisconsin-Madison through a new residential program that will introduce them to the use of Chrono in simulation-based robotics design.This project seeks to augment modeling and solution methods employed by Chrono, a BSD3 open-source simulation platform for multi-body dynamics (MBD) and fluid-solid interaction (FSI) problems. The software infrastructure enhancements in this project aim at sustaining teraflops-grade simulation of MBD and FSI systems with more than ten billion degrees of freedom; i.e., two to three orders of magnitude beyond conventional simulations today. In order to increase adoption and impact, the performance levels aimed at will be reached on budget/affordable hardware that leverages GPU computing. Chrono will be able to capture micro-, meso- and macro-scale dynamics on time scales spanning from millisecond (impact phenomena) to decades (geophysics). The intellectual merit of this project stems from the following key ideas: (i) with an eye towards the sunsetting of Moore's law, the software design solution embraces a scalable multi-GPU hardware layout poised to solve effectively large multi-physics problems; (ii) a hardware-aware software design paradigm, which aggressively reduces data storage and movement, will allow budget-conscious hardware systems to run billion-degree-of-freedom models, or, for models of similar size, accomplish a two orders of magnitude speedup when compared to the state of the art; (iii) a unified Lagrangian formulation for both solid and fluid dynamics is implemented in one software platform that can simulate complex multi-physics (coupled) problems; and (iv) Chrono promotes an alternative approach for handling friction and contact that revolves around the concept of differential variational inequality and thus avoids the small integration time step and numerical instability issues that hinder most of the existing many-body dynamics simulators. In relation to its educational and outreach initiatives, this project: (a) will be instrumental in establishing a new University of Wisconsin-Madison undergraduate course that introduces students to computing concepts subsequently refined in a graduate advanced computing class; (b) will promote the discipline of Computational Science and Computational Dynamics at high-school and undergraduate levels via two yearly residential summer programs for under-represented students; (c) will expand an advanced computing forum that facilitates technology transfer to industry and promotes Chrono adoption; and, (d) will strengthen ongoing collaborations that critically depend on Chrono in robotics, geomechanics, and soft-matter physics. Chrono is presently cloned on average 10 times every day, has been forked from its public repository by more than 150 parties, and has an active forum with more than 250 registered users. This project will enhance the richness of Chrono modeling features, improve its numerical solution foundation, and leverage emerging hardware architectures to elevate this simulation capability to the status of ready-to-use, open-source, best-in-class computational dynamics platform.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Data: HDR: Nanocomposites to Metamaterials: A Knowledge Graph Framework,OAC,1835735,Chiara Daraio,Daraio@caltech.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","A team of experts from four universities (Duke, RPI, Caltech and Northwestern) creates an open source data resource for the polymer nanocomposites and metamaterials communities. A broad spectrum of users will be able to query the system, identify materials that may have certain characteristics, and automatically produce information about these materials. The new capability (MetaMine) is based on previous work by the research team in nanomaterials (NanoMine). The effort focuses upon two significant domain problems: discovery of factors controlling the dissipation peak in nanocomposites, and tailored mechanical response in metamaterials motivated by an application to personalize running shoes. The project will significantly improve the representation of data and the robustness with which user communities can identify promising materials applications. By expanding interaction of the nanocomposite and metamaterials communities with curated data resources, the project enables new collaborations in materials discovery and design. Strong connections with the National Institute of Standards and Technology (NIST), the Air Force Research Laboratory (AFRL), and Lockheed Martin facilitate industry and government use of the resulting knowledge base. The project develops an open source Materials Knowledge Graph (MKG) framework. The framework for materials includes extensible semantic infrastructure, customizable user templates, semi-automatic curation tools, ontology-enabled design tools and custom user dashboards. The work generalizes a prototype data resource (NanoMine) previously developed by the researchers, and demonstrates the extensibility of this framework to metamaterials. NanoMine enables annotation, organization and data storage on a wide variety of nanocomposite samples, including information on composition, processing, microstructure and properties. The extensibility will be demonstrated through creation of a MetaMine module for metamaterials, parallel to the NanoMine module for nanocomposites. The frameworks will allow for curation of data sets and end-user discovery of processing-structure-property relationships. The work supports the Materials Genome Initiative by creating an extensible data ecosystem to share and re-use materials data, enabling faster development of materials via robust testing of models and application of analysis tools. The capability will be compatible with the NIST Material Data Curator System, and the team also engages both AFRL and Lockheed Martin to facilitate industry and government use of the resulting knowledge base. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate for Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Development of Assumption-Free Parallel Data Curing Service for Robust Machine Learning and Statistical Predictions,OAC,1931380,In Ho Cho,icho@iastate.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Large, incomplete datasets create major challenges for statistical prediction in research. This project will develop a data curing service that is able to manage large, incomplete, and diverse datasets, and would provide uncertainty measures for the cured data. The project identifies and collaborates with several communities where this data service is central to scientific research, including civil engineering, building science, urban energy, and social science. The effort creates a parallel data curing service, provides uncertainty measures for the cured data, and develops supplementary imputing algorithms. The team develops a data curing platform with imputation for incomplete, heterogeneous data; robust machine learning (ML) and statistical predictions would be established by developing an easy-to-use, general-purpose, large data-friendly imputation program. The focus is on a novel combination of three established imputation methods: two-level finite mixture model-based imputation (FMMI), fractional hot deck imputation (FHDI), and Gaussian mixture model-based imputation (GMMI), for which parallel implementations in R would also be provided. This award by the NSF Office of Advanced Cyberinfrastructure is jointly funded by the Established Program to Stimulate Competitive Research (EPSCoR).This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks for Intelligent Adaptive Experimentation: Enhancing and Tailoring Digital Education,OAC,2209821,Jeffrey Carver,carver@cs.ua.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","People are constantly learning ? whether formal education of homework problems & videos, or reading websites like Wikipedia. This project develops the Experiments As a Service Infrastructure (EASI), which lowers the barriers to conducting randomized experiments that compare alternative ways of designing digital learning experiences, as well as analyzing the data derived from the systems to rapidly change what future people receive. It does this by bringing together multidisciplinary researchers around the shared problem of testing ideas for improving and personalizing educational resources. The research also advances (1) the science of learning and instruction; (2) methods for analyzing complex educational data, and (3) machine learning algorithms that use data to improve educational experiences. Improving learning and teaching increases people's knowledge and gives them the ability to solve problems they care about, driving their personal and career success and increasing society's human capital.Instructional decisions about digital educational resources impact all students, from practice problems in K12 systems to tutorial webpages in university and community college online courses. The current versions of resources are too infrequently compared against alternative resources, which may provide better learning. With this in mind, the project has the goal of using data to test hypotheses about what is most helpful to students, and then use that data to change the experience for future students. The Experiments-As-a-Service-Infrastructure supports three complementary types of multi-disciplinary, collaborative research. A?Design: the infrastructure helps researchers investigate theories of learning and discover how to improve instruction by designing randomized field experiments on components of real-world digital educational resources. This provides more ecologically valid research on learning and instruction, in subfields of education, psychology, policy and discipline-based education research. B?Analysis: the infrastructure facilitates sophisticated analysis of experiments in the context of large-scale data about student profiles, such as to discover which interventions are effective for different subgroups of students. This can advance the use of innovative data-intensive methods for gaining actionable knowledge in education, learning analytics, educational data mining, and applied statistics. C?Adaptation: the infrastructure enables research into adaptive experimentation by providing a testbed for algorithms that dynamically analyze data from experiments, to enhance learning by presenting future students with whichever version of a resource (condition) is more effective, or to personalize learning by presenting different subgroups of future students with the version of a resource that is most effective for their subgroup. The infrastructure provides a testbed for empirical evaluation of which algorithms enact effective adaptive experimentation in education to inspire the development of new algorithms. Finally, the work aligns many educational communities around the shared problem of enhancing and personalizing education through experimentation and spurs multidisciplinary research by providing extensive support for collaboration and sharing of designs, data, analysis scripts and algorithms while fostering an online community for training and collaborations, to promote high-quality, innovative, impactful experiments.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Data: Toward Exascale Community Ocean Circulation Modeling,OAC,1835640,Thomas Haine,Thomas.Haine@jhu.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","This project designs and implements a software framework for handling petabyte-scale datasets; the focus is on global ocean circulation. A team of three universities (Johns Hopkins University, MIT, and Columbia University) builds a unified data system that is capable of delivering global ocean circulation model output at 1 km horizontal resolution. The product will be hosted in an open portal, providing the community with scalable software tools to enable analysis of the dataset. The team will use this data to answer specific questions about mixing and dissipation processes in the ocean. The goal of this effort is the creation and demonstration of a complete and replicable cyberinfrastructure for sharing and analysis of massive simulations. The focus is on high resolution ocean circulation modeling, with software tools that will enable efficient storage. Two major challenges to the study of ocean and climate dynamics are addressed: handling large datasets from high-resolution simulations, and understanding the role of small-scale ocean processes in large-scale ocean/climate systems. Resolving the first challenge would significantly facilitate ongoing and future studies of the ocean/atmosphere/climate system; addressing the second challenge would profoundly improve understanding of ocean/climate dynamics. The project builds a unified data system consisting of high-resolution global ocean circulation simulations, a petascale portal for data sharing, and scalable software tools for interactive analysis. The software framework from this project is expected to handle petascale to exascale datasets for users. Several pre-existing capabilities are leveraged for this project: the JHU regional numerical model of the Spill Jet on the East Greenland continental slope, software from the Pangeo project, the SciServer data-intensive software infrastructure, and lessons learned from the North East Storage Exchange multi-petabyte regional data store. The broader target is next generation simulation software in the geosciences and other disciplines. This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Ocean Sciences and the Integrative and Collaborative Education and Research Program within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Advancing Data Science and Analytics for Water (DSAW),OAC,1931297,Jeffery Horsburgh,jeff.horsburgh@usu.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Scientific challenges in hydrology and water resources such as understanding impacts of variable climate, sustainability of water supply with population growth and land use change, and impacts of hydrologic change on ecosystems and humans are increasingly data intensive. The volume of data produced by environmental scientists to study hydrologic systems requires advanced software tools for effective data visualization, analysis, and modeling. Scientists spend much of their time accessing, organizing, and preparing datasets for analyses, which can be a barrier to efficient analyses and hinders scientific inquiries and advances. This project will develop new software that will enhance scientists' ability to apply advanced data visualization and analysis methods (collectively referred to as ""data science"" methods) in the hydrology and water resources domain. The project will promote standardized software tools and data formats to help scientists enhance the consistency, share-ability, and reproducibility of the analyses they perform - all of which are important in building trust in scientific results. The software developed in the project will make data loading and organization for analysis easier, reducing the time spent by scientists in choosing appropriate data structures and writing computer code to read and parse data. It will enable users to automatically retrieve data from the HydroShare system, which is a hydrology domain data repository, as well as from important national water data sources like the United States Geological Survey's National Water Information System. The software will automatically load data from these sources into standardized and high performance data structures targeted to specific scientific data types and that integrate with visualization, analysis, and other data science capabilities commonly used by scientists in the hydrology and water resources domains. The project will also reduce the technical burden for water scientists associated with creating a computational environment within which to execute their analyses by installing and maintaining the Python packages developed within the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI) HydroShare-linked JupyterHub environment. Finally, the project will demonstrate the functionality and use of the software by producing a set of educational modules based on real water-data science applications that provide a specific mechanism for delivering the software to the community and promoting its use in classroom and research environments.Scientific and related management challenges in the water domain are inherently multi-disciplinary, requiring synthesis of data of multiple types from multiple domains. Many data manipulation, visualization, and analysis tasks performed by water scientists are difficult because (1) datasets are becoming larger and more complex; (2) standard data formats for common data types are not always agreed upon, and, when they are, they are not always mapped to an efficient structure for visualization and/or analysis within an analytical environment; and (3) water scientists generally lack training in data intensive scientific methods that would enable them to use new and existing tools to efficiently tackle large and complex datasets. This project will advance Data Science and Analytics for Water (DSAW) by developing: (1) an advanced object data model that maps common water-related data types to high performance data structures within the object-oriented Python language and analytical environment based upon standard file, data, and content types established by the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI) HydroShare system; (2) two new Python packages that enable users to write Python code for automating retrieval of desired water data, loading it into high performance memory objects specified by the object data model designed in the project, and performing analysis in a reproducible way that can be shared, collaborated around, and formally published for reuse. The project will use domain-specific data science applications to demonstrate how the new Python packages can be paired with the powerful data science capabilities of existing Python packages like Pandas, numpy, and scikit-learn to develop advanced analytical workflows within cloud and desktop environments. The project aims to extend the data access, collaboration, and archival capabilities of the HydroShare data and model repository and promote its use as a platform for reproducible water-data science. The project also aims to overcome barriers associated with accessing, organizing, and preparing datasets for data science intensive analyses. Overcoming these barriers will be an enabler for transforming scientific inquiries and advancing application of data science methods in the hydrology and water resources domains.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Scalable Modular Software and Methods for High-Accuracy Materials and Condensed Phase Chemistry Simulation,OAC,1931258,Edgar Solomonik,solomon2@illinois.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","How electrons are arranged in materials gives rise to a large variety of different behaviors. We can observe these behaviors and use them in various technologies. However, the prediction of these behaviors is a serious challenge. This makes the successful design of new materials harder. The goal of the Materials Genome Initiative is to use computer simulations to model electrons according to the laws of quantum physics. This will allow researchers to design new materials with desired properties. This project aims to build fast and accurate computer programs which simulate those new materials. These programs combine advances in computer science, quantum chemistry, and condensed-matter physics. They will be implemented in an open-source Python-based community code. This distribution model allows other researchers to use this code and to contribute new features.This research addresses gaps in existing software cyberinfrastructure in quantum materials simulation, by developing novel parallel implementations of low-scaling, high-accuracy methods. In particular, new techniques for mean-field calculations will be developed, which will act as groundwork for periodic coupled-cluster and quantum Monte Carlo methods. State-of-the-art techniques in sparsity and tensor decomposition will be employed to achieve good system-size scaling while retaining accuracy within each of these numerical schemes. Critically, the methods will be developed using efficient high-level software abstractions, implemented as Python-level modules within PySCF that leverage the Cyclops library for massively-parallel execution. The library software infrastructure will also be extended to maximize productivity via source-to-source automatic differentiation, as well as to enable execution of sparse kernels on emerging GPU-based supercomputing architectures. This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure, and the Division of Materials Research and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Adaptive End-to-End Parallelism for Distributed Science Workflows,OAC,2209955,Engin Arslan,earslan@unr.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Technological advancements in sensing and computing technologies have led to an unprecedented increase in the amount of data generated by scientific applications. As science projects are increasingly distributed in nature, the increase in data sizes in turn results in an increased volume of traffic that needs to be moved across geographically distributed locations. Although significant investments have been made to build high-speed networks to facilitate data movements between research and education institutions, it is difficult for domain scientists to efficiently utilize this available capacity mainly due to the lack of scalable data transfer services. This project addresses this need by developing a scalable and reliable data transfer service. It further integrates the data transfer service into elastic workflow management systems to achieve end-to-end optimization for distributed science workflows. This project makes three novel contributions to the field: (i) it innovates scalable integrity verification and encryption for file transfers to ensure the reliability of file transfers without sacrificing performance. It takes advantage of computing resources available at data transfer nodes to scale the performance of integrity verification and channel encryption features. (ii) It innovates end-to-end parallelism for distributed workflows by integrating an online transfer optimization service into elastic workflow management tools. Unlike existing workflow management solutions, which merely focus on the optimization of computing tasks, the proposed integration of online transfer optimization services into elastic workflow schedulers enables true end-to-end parallelism for distributed workflows. (iii) Finally, it demonstrates the performance of the developed service on a real-world bioscience workflow that streams a large volume of sequence read archive data from the NCBI database to extract computation-ready SAM/BAM files.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: The ThYme database and identifying representative amino acid sequences that originate thioester-active enzyme families,OAC,2001385,David Cantu,dcantu@unr.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","The ThYme database includes most enzymes involved in the formation of fatty acids and polyketides. These are ultimately converted into valuable products. Such products include cosmetics, detergents, insecticides, fungicides, antibiotics, and other medicinal compounds. The updated ThYme database will provide vital support to research related to these products. Metabolic engineers, plant biologists, natural products and medicinal chemists will all benefit from improved access to enzyme structure and function. High school students from underrepresented populations will be recruited and trained in various aspects of coding. They will then have the opportunity to work with graduate students and postdocs to contribute to the improvement of the platform. This should ultimately strengthen the STEM workforce in Nevada, and nationally.The ThYme database contains most known sequences and structures of enzymes that act on thioesters, classified by sequence similarity into families. The advantage of classifying enzymes by sequence similarity is that one can infer that all enzymes in a family will have very similar structures and nearly identical catalytic residues and mechanisms. The goal of this project is to launch a new and updated ThYme database by identifying the current families of thioester active enzymes, developing a new approach to identify representative sequences, improving the database management scheme, and modernizing the online user interface. We will develop an efficient method, using submodular functions, to select the representative sequence(s) of an enzyme family among sequences experimentally verified to have a particular enzymatic activity. The database will be disseminated by a website, where every enzyme family will have its own webpage including relevant knowledge and an open query field where users will be able to search by organism, sequence accession code, function, name, sequence, or crystal structure. With input from the user community, the new website will have features to make the content more interactive and allow automated data querying. The user community will be engaged and supported with a forum page to pose questions and begin discussions to which both developers and users can contribute, as well as with tutorials of useful website functionalities.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the ivision of Chemical, Bioengineering, Environmental and Transport Systems within the NSF Directorate of Engineering.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Cyberloop for Accelerated Bionanomaterials Design,OAC,1931587,Hendrik Heinz,hendrik.heinz@colorado.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","The evolution of biological and materials systems must be understood at many scales in order to achieve groundbreaking advances. Areas that are impacted include the health sciences, materials sciences, energy conversion, sustainability, and overall quality of life. Molecular simulations using complex models and configurations play an increasing role in such efforts. They address the limitations of experiments which study events over very small time and length scales. Such simulations require great expertise due to the complexity of the systems being studied. and the tools being used. This is particularly true for systems containing both inorganic and biological materials. This project will help researchers to quickly set up complex simulations, carry out the simulations with high accuracy, and assess uncertainties in the results. They will help develop the Cyberloop computational infrastructure. Cyberloop will dramatically reduce the time required to perform state-of-the-art simulations. They will also help to educate the next generation of researchers in this important field.Cyberloop will integrate three existing successful platforms for soft matter and solid state simulations (IFF, OpenKIM, and CHARMM-GUI) into a single unified framework. These systems will work together to enable users to set up complex bionanomaterial configurations, select reliable validated force fields, generate input scripts for popular simulation platforms, and assess the uncertainty in the results. The integration of these tools requires a host of technological and scientific innovations including: automated charge assignment protocols and file conversions, expansion of the Interface force field (IFF) to new systems, generation of new surface models, extension of the Open Knowledgebase of Interatomic Models (OpenKIM) to bonded force fields, development of machine learning based force field selection and uncertainty tools, and development of new Nanomaterial Builder and Bionano Builder modules in CHARMM-GUI. Cyberloop fulfils a critical need in the user community to discover and engineer new multi-component bionanomaterials to create the next generation of therapeutics, materials for energy conversion, and ultrastrong composites. The project will facilitate the training of graduate students, undergraduate students, and postdoctoral scholars, including underrepresented and minority students, at the participating institutions to prepare an interdisciplinary scientific workforce with significant experience in cyber-enabled technology. Online educational materials and tutorials will help increase participation in bionanomaterial research across academia and government. This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure, and the Division of Materials Research and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Frameworks: Collaborative Proposal: Software Infrastructure for Transformative Urban Sustainability Research,OAC,1931363,Shrideep Pallickara,shrideep@cs.colostate.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","The United States is highly urbanized with more than 80% of the population residing in cities. Cities draw from and impact natural resources and ecosystems while utilizing vast, expensive infrastructures to meet economic, social, and environmental needs. The National Science Foundation has invested in several strategic research efforts in the area of urban sustainability, all of which generate, collect, and manage large volumes of spatiotemporal data. Voluminous datasets are also made available in domains such as climate, ecology, health, and census. These data can spur exploration of new questions and hypotheses, particularly across traditionally disparate disciplines, and offer unprecedented opportunities for discovery and innovation. However, the data are encoded in diverse formats and managed using a multiplicity of data management frameworks -- all contributing to a break-down of the observational space that inhibits discovery. A scientist must reconcile not only the encoding and storage frameworks, but also negotiate authorizations to access the data. A consequence is that data are locked in institutional silos, each of which represents only a sliver of the observational space. This project, SUSTAIN (Software for Urban Sustainability to Tailor Analyses over Interconnected Networks), facilitates and accelerates discovery by significantly alleviating data-induced inefficiencies. This effort has deep, far-reaching impact. It transforms urban sustainability science by establishing a community of interdisciplinary researchers and catalyzing their collaborative capacity. Hundreds of researchers from over 150 universities are members of our collaborating organizations and will immediately benefit from SUSTAIN. Domains where spatiotemporal phenomena must be analyzed benefit from this innovative research; the partnership with ESRI and Google Earth amplify the impact of SUSTAIN, giving the project a global reach and enabling international collaborative initiatives. The direct engagement with middle school students in computer science and STEM disciplines has well-known benefits and, combined with graduate training, produces a diverse, globally competitive STEM workforce. SUSTAIN targets transformational capabilities for feature space exploration, hypotheses formulation, and model creation and validation over voluminous, high-dimensional spatiotemporal data. These capabilities are deeply aligned with the urban sustainability community's needs, and they address challenges that preclude effective research. SUSTAIN accomplishes these interconnected goals by enabling holistic visibility of the observational space, interactive visualizations of multidimensional information spaces using overlays, fast evaluation of expressive queries tailored to the needs of the discovery process, generation of custom exploratory datasets, and interoperation with diverse analyses software frameworks - all leading to better science. SUSTAIN fosters deep explorations through its transformative visibility of the federated information space. The project reconciles the fragmentation and diversity of siloed data to provide seamless, unprecedented visibility of the information space. A novel aspect of the project's methodology is the innovative use of the Synopsis, a spatiotemporal sketching algorithm that decouples data and information. The methodology extracts and organizes information from the data and uses the information (or sketches of the data) as the basis for explorations. The project also incorporates a novel algorithm for imputations at the sketch level at myriad spatiotemporal scopes. The effort creates a collaborative community of multidisciplinary researchers to build an enduring software infrastructure for urban sustainability.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Building a Collaboration Infrastructure: CyberWater2 -- A Sustainable Data/Model Integration Framework,OAC,2209835,Yao Liang,yliang@cs.iupui.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Natural hazards, such as coastal and inland flooding caused by Hurricanes and severe drought and its associated wildfire, have been occurring with unprecedented frequency, induced by climate changes that encompass hydrological, biological, environmental, atmospheric, ocean, and other geosciences. Such hazards have caused not only profound damages to our environment and required tremendous efforts to recover, but also cost people's lives. To mitigate these potential disasters, it is a critical time to tackle their associated scientific questions both fundamental and large-scale that impact on the health, resilience, and sustainability of the Earth system we live in. The problems are complex and multidisciplinary, and researchers and practitioners from diverse fields must work together to find solutions. By its nature, Earth system models are comprised of component models ? from land surface, to rivers, coastal regions, ocean, sea ice, and atmosphere, where each component model is coupled with one another. As science advances, a component model or its subsystems may have to be replaced because of new understanding, or because different perspectives must be explored and tested for the credence of different combinations to find the most credible predictions for different conditions at different locations. Such tasks often require substantial efforts and time and can become a bottleneck. This project is aimed at developing a new open-source cyberinfrastructure framework, Cyberwater2, in which model coupling is shifted from the current ""code-coupling"" approach to a new ""information coupling"" approach, and can be configured without writing glue code. This minimizes the need to access and modify each participating model's original code, and removes a major obstacle for large-scale cross-institutional collaborations and scientific investigations across disciplines and geographic boundaries. CyberWater2 is designed for diverse research communities including water, climate, coastal, engineering, and beyond. With our framework, researchers can devote their collaborative energy on problem solving and exploration of new frontiers, while using CyberWater2 to effectively achieve two-way open model couplings across platforms, model parameter calibration, data assimilation, testing/validations/comparisons, etc.The goal of this project is to make it easier to conduct large scale collaboration on complex problems and solve them efficiently, accurately and in-depth by developing a cyberinfrastructure, CyberWatyer2, that (1) significantly eliminates ""glue"" coding for two-way couplings across heterogeneous computing platforms, disciplines, and organizations; (2) automates complex model calibration and facilitates data assimilation processes applicable to various models; (3) supports task-based and in-situ hybrid workflow for greatly improved efficiency on two-way coupling across heterogeneous platforms; (4) provides a CyberWater2 server and web service framework for users in addition to the standalone systems; (5) enables sustainable data access from diverse sources by automatically adapting data agents to the changes (e.g., API interfaces) made to external data sources by providers; and (6) enables automated resource planning with intelligent site recommendation for High Performance Computing (HPC)/Cloud access on demand to maximize users' benefits.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Earth Sciences in the Directorate of Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Frameworks: Collaborative Research: Extensible and Community-Driven Thermodynamics, Transport, and Chemical Kinetics Modeling with Cantera: Expanding to Diverse Scientific Domains",OAC,1931397,Claude Goldsmith,franklin_goldsmith@brown.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Modeling and simulation play key enabling roles in aiding and accelerating discovery connecting to energy and chemical research. In applications such as energy storage and conversion, atmospheric chemistry, and catalytic chemical processing, modeling and simulation software helps facilitate technological advances. However, in recent years the available software has not kept pace with the increasing chemical complexity and interdisciplinarity of advanced technology solutions. This project addresses this gap by developing and promoting new state-of-the-art modeling capabilities for diverse scientific fields in the existing Cantera software platform. Cantera is an extensible, open-source framework that enables researchers to study basic science and support new technology development and enables teachers to demonstrate concepts and applications in their classrooms. This project extends Cantera to provide new cross-disciplinary research capabilities and provides a foundation for further community-driven improvements to the Cantera framework. Simultaneous development of the open-source platform and outreach to new user communities will facilitate both fundamental scientific insight and practical technology design and analysis, train the next generation of researchers in both software-development best practices and scientific knowledge, and generate reusable and open educational materials. In addition to work on the framework development, the project includes training of graduate students as well as education, outreach and scientific community engagement activities.This work will develop the Cantera software platform in service of three objectives: (i) extend Cantera?s scientific capabilities to support the development of transformative technologies; (ii) expand Cantera?s user base in fields including electrochemistry, heterogeneous catalysis, and atmospheric chemistry; and (iii) broaden participation in the software?s development and management to improve Cantera?s sustainability and usability. These will be achieved by developing new scientific modeling capabilities, conducting outreach to new user communities, and improving Cantera?s architecture and software development practices. The new scientific modeling capabilities will focus on four content areas: thermodynamics, chemical kinetics, transport, and multi-phase capabilities. Outreach activities, including publications and presentations, conference workshops, and domain-specific software toolkits with examples to demonstrate Cantera operation and functionality, will engage new and existing communities. The project will establish a scientific advisory board, consisting of experts from diverse fields and backgrounds who will help guide software development and outreach. Finally, the architectural and software engineering changes in this work will improve extensibility and interoperability and implement advanced numerical algorithms to enable the application of Cantera to new types of problems. These changes will also make it easier for users to contribute to Cantera, ensure software correctness, and provide new ways of accessing Cantera?s functionality. The resulting software framework will aid in scientific discovery and development of key enabling technologies with broad societal impacts. These impacts include next-generation batteries and fuel cells for clean energy storage and conversion, catalytic and membrane reactors for electrolysis, novel fuel generation, chemical processing, environmentally conscious combustion applications, and understanding and addressing anthropogenic challenges in atmospheric chemistry.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Data: HDR: Nanocomposites to Metamaterials: A Knowledge Graph Framework,OAC,1835648,Linda Schadler,linda.schadler@uvm.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","A team of experts from four universities (Duke, RPI, Caltech and Northwestern) creates an open source data resource for the polymer nanocomposites and metamaterials communities. A broad spectrum of users will be able to query the system, identify materials that may have certain characteristics, and automatically produce information about these materials. The new capability (MetaMine) is based on previous work by the research team in nanomaterials (NanoMine). The effort focuses upon two significant domain problems: discovery of factors controlling the dissipation peak in nanocomposites, and tailored mechanical response in metamaterials motivated by an application to personalize running shoes. The project will significantly improve the representation of data and the robustness with which user communities can identify promising materials applications. By expanding interaction of the nanocomposite and metamaterials communities with curated data resources, the project enables new collaborations in materials discovery and design. Strong connections with the National Institute of Standards and Technology (NIST), the Air Force Research Laboratory (AFRL), and Lockheed Martin facilitate industry and government use of the resulting knowledge base. The project develops an open source Materials Knowledge Graph (MKG) framework. The framework for materials includes extensible semantic infrastructure, customizable user templates, semi-automatic curation tools, ontology-enabled design tools and custom user dashboards. The work generalizes a prototype data resource (NanoMine) previously developed by the researchers, and demonstrates the extensibility of this framework to metamaterials. NanoMine enables annotation, organization and data storage on a wide variety of nanocomposite samples, including information on composition, processing, microstructure and properties. The extensibility will be demonstrated through creation of a MetaMine module for metamaterials, parallel to the NanoMine module for nanocomposites. The frameworks will allow for curation of data sets and end-user discovery of processing-structure-property relationships. The work supports the Materials Genome Initiative by creating an extensible data ecosystem to share and re-use materials data, enabling faster development of materials via robust testing of models and application of analysis tools. The capability will be compatible with the NIST Material Data Curator System, and the team also engages both AFRL and Lockheed Martin to facilitate industry and government use of the resulting knowledge base. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate for Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: SI2-SSI: ELSI-Infrastructure for Scalable Electronic Structure Theory,OAC,1450280,Volker Blum,volker.blum@duke.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Predictive, so-called ab initio electronic structure calculations, particularly those based on the Kohn-Sham density functional theory (DFT) are now a widely used scientific workhorse with applications in virtually all sciences, and increasingly in engineering and industry. In materials science, they enable the computational (""in silico"") design of new materials with improved properties. In biological or pharmacological research, they provide molecular-level insights into the function of macromolecules or drugs. In the search for new energy solutions, they give molecular-level insights into new solar cell designs, catalytic processes, and many others. A key bottleneck in many applications and calculations is the ""cubic scaling wall"" of the so-called Kohn-Sham eigenvalue problem with system size (i.e., the effort increases by a factor of 1,000 if the model size increases by a factor of 10). This project will establish an open source software infrastructure ""ELSI"" that offers a common, practical interface to initially three complementary solution strategies to alleviate or overcome the difficulty associated with solving the Kohn-Sham eigenvalue problem. ELSI will enable a broad range of end user communities, centered around different codes with, often, unique features that tie a specialized group of scientists to that particular solution, to easily incorporate state-of-the-art solution strategies for a key problem they all share. By providing these effective, accessible solution strategies, we will open up major areas for electronic structure theory where DFT based predictive methodologies are not applicable today. This will in turn open doors for new development in materials science, chemistry, and all related areas. Commitments to support ELSI exist from some of the most important electronic structure developer communities, as well as from industry and government leaders in high-performance computing. Thus, we will create a strong U.S. based infrastructure that leverages the large user and developer base from a globally active community developing DFT methods for materials research.ELSI will support and enhance three state-of-the-art approaches, each best suited for a specific problem range: (i) The ELPA (EigensoLvers for Petascale Applications) library, a leading library for efficient, massively parallel solution of eigenvalue problems (for small- and mid-sized problems up to several 1,000s of atoms), (ii) the OMM (Orbital Minimization Method) in a recent re-implementation, which circumvents the eigenvalue problem by focusing on a reduced, auxiliary problem (for systems in the several 1,000s of atoms range), and (iii) the PEXSI (Pole EXpansion and Selective Inversion) library, a proven reduced scaling (at most quadratic scaling) solution for general systems (for problems with 1,000s of atoms and beyond). By establishing standardized interfaces in a style already familiar to many electronic structure developers, ELSI will enable production electronic structure codes that use it to significantly reduce the ""scaling wall"" of the eigenvalue problem. First, ELSI will help them make efficient use of the most powerful computational platforms available. The target platforms are current massively parallel computers and multicore architectures, GPU based systems and future manycore processors. Second, the project will make targeted methodological improvements to ELPA, OMM, and PEXSI, e.g., a more effective use of matrix sparsity towards very large systems. The focus on similar computational architectures and similar methodological enhancements will lead to significant cross-fertilization and synergy between these approaches."
Collaborative Research: Elements: EXHUME: Extraction for High-Order Unfitted Finite Element Methods,OAC,2104106,John Evans,john.a.evans@colorado.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","Unfitted finite element methods allow for the simulation of physical systems that are difficult if not impossible to simulate using classical finite element methods requiring body-fitted meshes. For instance, unfitted finite element methods can be directly applied to the simulation of physical systems exhibiting a change of domain topology, such as the movement of blood cells through a human capillary or the flow of blood past the heart valves between the four main chambers of the human heart. Unfitted finite element methods also streamline the construction of computational design optimization technologies that optimize the geometry and material layout of an engineered system based on prescribed performance metrics. However, the computer implementation of an unfitted finite element method remains a challenging and time-consuming task even for domain experts. The overarching objective of this project is to construct a novel software library, EXHUME (EXtraction for High-order Unfitted finite element MEthods), to enable the use of classical finite element codes for unfitted finite element analysis. EXHUME will empower a large community of scientists and engineers to employ unfitted finite element methods in their own work, allowing them to carry out biomedical, materials science, and geophysical simulations that have been too expensive or too unstable to realize using classical finite element methods. EXHUME will also improve the fidelity of design optimizations being performed in academia, national laboratories, and industry on a near daily basis.Unfitted finite element methods simplify the finite element solution of PDEs (Partial Differential Equations) on complex and/or deforming domain geometries by relaxing the requirement that the finite element approximation space be defined on a body-fitted mesh whose elements satisfy restrictive shape and connectivity constraints. Early unfitted finite element methods exhibited low-order convergence rates, but recent progress has led to high-order methods. The key ingredient to success of a high-order unfitted finite element method is accurate numerical integration over cut cells (i.e, unfitted elements cut by domain boundaries). EXHUME uses the concept of extraction to express numerical integration over cut cells in terms of basic operations already implemented in typical finite element codes, an integration mesh, and extraction operators expressing unfitted finite element basis functions in terms of canonical shape functions. EXHUME generates integration meshes and extraction operators outside of the confines of a particular finite element code so it may be paired with existing codes with little implementation effort. A key goal of the project is demonstration of EXHUME by connecting it to existing research codes and the popular FEniCS toolchain for finite element analysis. An effort parallel to software development explores accuracy versus efficiency trade-offs associated with 1) approximations made during extraction and 2) novel numerical quadrature schemes for cut cells. The breadth of EXHUME's technical impact is ensured by several factors: 1) the ubiquity of PDEs across nearly all disciplines of science and engineering, 2) the library's interoperability with existing finite element codes, and 3) the generic nature of the EXHUME+FEniCS demonstrative example, which can be applied to arbitrary systems of PDEs. By simplifying the setup of PDE-based computational models, EXHUME+FEniCS enables classroom demonstrations simulating complicated physical scenarios without letting the technical details of numerical methods distract from the scientific principles being taught.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: NSCI : Computational and data innovation implementing a national community hydrologic modeling framework for scientific discovery,OAC,2054506,Reed Maxwell,reedmaxwell@princeton.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Machine Learning Materials Innovation Infrastructure,OAC,1931298,Dane Morgan,ddmorgan@wisc.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Machine learning is rapidly changing our society, with computers recently gaining skills in many new tasks. These tasks range from understanding language to driving cars. Materials science and engineering is also being transformed. Many tasks are becoming increasingly accessible to machine learning algorithms. These range from predicting new data to analyzing images. Many basic machine learning algorithms are readily available. However the overall workflow involved in the application of machine learning for materials problems is still largely executed by hand. Getting results out is still done by traditional methods like publishing articles. There is an enormous opportunity to accelerate the growth and impact of machine learning in materials research. This requires improved cyberinfrastructure. This project will develop an approach to accelerate the entire machine learning workflow. Its output will include tools to easily develop datasets, manage model development, and output models. These will be reusable and reproducible for future use. This project will enable materials scientists and engineers to rapidly develop and deploy machine learning models. More importantly, the entire materials community will be able to quickly access these models. It will transform how we discover and develop advanced materials.The project will have three major technical components: (i) A MAterials Simulation Toolkit for Machine Learning (MAST-ML) with workflow tools that will enable local or cloud-based multistep, automated execution of complex machine learning data analysis and model training, codified best practices, increased access to machine learning methods for non-experts, and accelerated model development; (ii) The Foundry Materials Informatics Environment that will provide flexible, integrated, cloud-based management of machine learning materials science and engineering projects, from organizing data to developing models to disseminating results that are machine and human accessible and reproducible in ways that support a networked materials innovation ecosystem, (iii) Representative science applications of machine learning materials science and engineering projects that will support infrastructure development and promotion, as well as demonstrate best practices on state-of-the-art materials science and engineering problems. In addition to its impact on materials science and engineering, this project will develop students and young researchers with the interdisciplinary skills of machine learning and materials science and engineering, and promote these new ideas to the broader materials community. This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure, and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Frameworks: Collaborative Research: Extensible and Community-Driven Thermodynamics, Transport, and Chemical Kinetics Modeling with Cantera: Expanding to Diverse Scientific Domains",OAC,1931592,Kyle Niemeyer,kyle.niemeyer@oregonstate.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Modeling and simulation play key enabling roles in aiding and accelerating discovery connecting to energy and chemical research. In applications such as energy storage and conversion, atmospheric chemistry, and catalytic chemical processing, modeling and simulation software helps facilitate technological advances. However, in recent years the available software has not kept pace with the increasing chemical complexity and interdisciplinarity of advanced technology solutions. This project addresses this gap by developing and promoting new state-of-the-art modeling capabilities for diverse scientific fields in the existing Cantera software platform. Cantera is an extensible, open-source framework that enables researchers to study basic science and support new technology development and enables teachers to demonstrate concepts and applications in their classrooms. This project extends Cantera to provide new cross-disciplinary research capabilities and provides a foundation for further community-driven improvements to the Cantera framework. Simultaneous development of the open-source platform and outreach to new user communities will facilitate both fundamental scientific insight and practical technology design and analysis, train the next generation of researchers in both software-development best practices and scientific knowledge, and generate reusable and open educational materials. In addition to work on the framework development, the project includes training of graduate students as well as education, outreach and scientific community engagement activities.This work will develop the Cantera software platform in service of three objectives: (i) extend Cantera?s scientific capabilities to support the development of transformative technologies; (ii) expand Cantera?s user base in fields including electrochemistry, heterogeneous catalysis, and atmospheric chemistry; and (iii) broaden participation in the software?s development and management to improve Cantera?s sustainability and usability. These will be achieved by developing new scientific modeling capabilities, conducting outreach to new user communities, and improving Cantera?s architecture and software development practices. The new scientific modeling capabilities will focus on four content areas: thermodynamics, chemical kinetics, transport, and multi-phase capabilities. Outreach activities, including publications and presentations, conference workshops, and domain-specific software toolkits with examples to demonstrate Cantera operation and functionality, will engage new and existing communities. The project will establish a scientific advisory board, consisting of experts from diverse fields and backgrounds who will help guide software development and outreach. Finally, the architectural and software engineering changes in this work will improve extensibility and interoperability and implement advanced numerical algorithms to enable the application of Cantera to new types of problems. These changes will also make it easier for users to contribute to Cantera, ensure software correctness, and provide new ways of accessing Cantera?s functionality. The resulting software framework will aid in scientific discovery and development of key enabling technologies with broad societal impacts. These impacts include next-generation batteries and fuel cells for clean energy storage and conversion, catalytic and membrane reactors for electrolysis, novel fuel generation, chemical processing, environmentally conscious combustion applications, and understanding and addressing anthropogenic challenges in atmospheric chemistry.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science,OAC,1835631,Geoffrey Fox,gcfexchange@gmail.com,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth. Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn. The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above. Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities. Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks. Compositions of these diverse capabilities are rare. Furthermore, many researchers who study networks are not computer scientists. As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming. The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use. What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science. CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software. CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks. The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program. It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions. CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science. Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Improving the Understanding and Representation of Atmospheric Gravity Waves using High-Resolution Observations and Machine Learning,OAC,2004512,M Joan Alexander,alexand@nwra.com,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Geophysical gravity waves are a ubiquitous phenomenon in Earth?s atmosphere and ocean, made possible by the interaction of gravity with a stratified, or layered fluid. They are excited in the atmosphere when winds flow over mountains, by thunderstorms and other strong convective systems, and when winter storms intensify. Gravity waves play an important role in the momentum and energy balance of the atmosphere, with direct impacts on surface weather and climate through their effect on the variability of key features of the climate system such as the jet streams and stratospheric polar vortices. These waves present a challenge to weather and climate prediction: waves on scales of 100 meters to 100 kilometers can neither be systematically measured with conventional observational systems, nor properly resolved in global atmospheric models. As a result, these waves must be represented, or approximated, based on the resolved flow that can be directly simulated. Current representations of gravity waves are severely limited by computational necessity and the scarcity of observations, leading to inaccuracies or uncertainties in short term weather and long term climate predictions. The objective of this project is to leverage unprecedented observations from Loon high altitude balloons and use specialized high resolution computer simulations and machine learning techniques to develop accurate, data-informed representation of gravity waves. The outcomes of this project are expected to result in better weather and climate models, thus improving short term forecasts of weather extremes and long term climate change projections, which have substantial societal benefits. Furthermore, the project will support the training of 3 Ph.D. students, 4 postdocs, and 10 undergraduate summer researchers to work at the intersection of atmospheric dynamics, climate modeling, and data science, thus preparing the next generation of scientists for interdisciplinary careers.The project will deliver two key advances. First, it will open up a new data source to constrain gravity wave momentum transport in the atmosphere. Loon LLC has been launching super pressure balloons since 2013 to provide global internet coverage. Very high resolution position, temperature, and pressure observations (taken every 60 seconds) are available from thousands of flights. This provides an unprecedented source of high resolution observations to constrain gravity wave sources and propagation. The project will process the balloon measurements and, in concert with novel high resolution simulations, establish a publicly available dataset to open up a potentially transformational resource for observationally constrained assessment of gravity wave sources, propagation, and breaking. The second transformation will be using machine learning techniques to develop computationally feasible representations of momentum deposition by gravity waves. Current physics-based representations only account for vertical propagation of the waves (i.e., they are one dimensional) and ignore their horizontal propagation. Using the data based on the Loon measurements and high resolution models, one and three dimensional data driven representations will be developed to more accurately and efficiently represent the effects of gravity waves in weather and climate models. These novel representations will be implemented in idealized atmospheric models to study the role of gravity waves in the variability of the extratropical jet streams, the Quasi Biennial Oscillation (a slow variation of the winds in the tropical stratosphere) and the polar vortex of the winter stratosphere, enabling better understanding their response to increased atmospheric greenhouse gas concentrations.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Convergence of Bayesian inverse methods and scientific machine learning in Earth system models through universal differentiable programming,OAC,2104009,Mathieu Morlighem,Mathieu.Morlighem@dartmouth.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Understanding and quantifying parameter sensitivity of simulated systems, such as the numerical models of physical systems and mathematical renderings of neural networks, are essential in simulation-based science (SBS) and scientific machine learning (SciML). They are the key ingredients in Bayesian inference and neural network training. Seizing on the opportunity of emerging open-source Earth system model development in the Julia high-level programming language, this project is endowing these open-source models with automatic differentiation (AD) enabled derivative information, making these converging data science and simulation-based science tools available to a much broader research and data science community. Enabling a general-purpose AD framework which can handle both large-scale Earth system models as well as SciML algorithms, such as physics-informed neural networks or neural differential equations, will enable seamless integration of these approaches for hybrid Bayesian inversion and Bayesian machine learning. It merges big data science, in which available data enable model discovery with sparse data science, and the model structure is exploited in the selection of surrogate models representing data-informed subspaces and fulfilling conservation laws. The emerging Julia language engages a new generation of researchers and software engineers, channeling much needed talent into computational science approaches to climate modeling. Through dedicated community outreach programs (e.g., Hackathons, Minisymposia, Tutorials) the project team will be working toward increasing equity, diversity, and inclusion across the participating disciplines.The project is developing a framework for universal differentiable programming and open-source, general-purpose AD that unifies these algorithmic frameworks within Julia programming language. The general-purpose AD framework in Julia leverages the composability of Julia software packages and the differentiable programming approach that underlies many of the SciML and high-performance scientific computing packages. Compared to most current modeling systems targeted for HPC, Julia is ideally suited for heterogeneous parallel computing hardware (e.g., CUDA, ROCm, oneAPI, ARM, PowerPC, x86 64, TPUs). The project is bringing together expertise in AD targeted at Earth system data assimilation in high performance computing environments with SciML expertise. The project team is working with the Julia Computing organization and package developers to ensure sustainability of the developed frameworks. The project?s Earth system flagship applications consist of (i) an open-source, AD-enabled ocean general circulation model that is being developed separately as part of the Climate Modelling Alliance (CliMA), and (2) an open-source, AD-enabled ice flow model. Each of these application frameworks is being made available to the community for science application, in which derivative (gradient or Hessian) information represent key algorithmic enabling tools. These include SciML-based training of surrogate models (data-driven and/or model-informed), parameter and state estimation, data assimilation for model initialization, uncertainty quantification (Hessian-based and gradient-informed MCMC) and quantitative observing system design. Academic and industry partners are involved, who are using the frameworks for developing efficient power grids, personalized precision pharmacometrics, and improved EEG design.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Designing Next-Generation MPI Libraries for Emerging Dense GPU Systems,OAC,1931537,Dhabaleswar Panda,panda@cse.ohio-state.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","The extremely high compute and communication capabilities offered by modern Graphics Processing Units (GPUs) and high-performance interconnects have led to the creation of High-Performance Computing (HPC) platforms with multiple GPUs and high-performance interconnects per node. Unfortunately, state-of-the-art production quality implementations of the popular Message Passing Interface (MPI) programming model do not have the appropriate support to deliver the best performance and scalability for applications on such dense GPU systems. These developments in High-End Computing (HEC) technologies and associated middleware issues lead to the following broad challenge: How can existing production quality MPI middleware be enhanced to take advantage of emerging networking technologies to deliver the best possible scale-up and scale-out for HPC and Deep Learning (DL) applications on emerging dense GPU systems? A synergistic and comprehensive research plan, involving computer scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) and computational scientists from the Texas Advanced Computing Center (TACC), and San Diego Supercomputer Center (SDSC) and University of California San Diego (UCSD), is proposed to address the above broad challenges with innovative solutions. The proposed framework will be made available to collaborators and the broader scientific community to understand the impact of the proposed innovations on next-generation HPC and DL frameworks and applications in various science domains. Multiple graduate and undergraduate students will be trained under this project as future scientists and engineers in HPC. The proposed work will enable curriculum advancements via research in pedagogy for key courses in the new Data Science programs at OSU, SDSC and TACC. The established national-scale training and outreach programs at TACC, SDSC and OSC will be used to disseminate the results of this research to XSEDE users. Tutorials and workshops will be organized at PEARC, SC and other conferences to share the research results and experience with the community. The project is aligned with the National Strategic Computing Initiative (NSCI) to advance US leadership in HPC and the recent initiative of the US Government to maintain leadership in Artificial Intelligence (AI.)The proposed innovations include: 1) Designing high-performance and scalable point-to-point, and collective communication operations that fully utilize multiple network adapters and advanced in-network computing features for GPU and CPU buffers within and across nodes; 2) Designing novel datatype processing and unified memory management to improve application performance; 3) Designing CUDA-aware I/O subsystem to accelerate MPI I/O and checkpoint-restart for HPC and DL applications; 4) Designing support for containerized environments to better enable easy deployment of proposed solutions on modern cloud environments; and 5) Carry out integrated development and evaluation to ensure proper integration of proposed designs with the driving applications. The proposed designs will be integrated into the widely-used MVAPICH2 library and made available. The project team members will work closely with internal and external collaborators to facilitate wide deployment and adoption of released software. The proposed solutions will be targeted to enable scale-up and scale-out of the driving science domains (molecular dynamics, lattice QCD, seismology, image classification, and fusion research) on emerging dense GPU platforms. The transformative impact of the proposed development effort is to achieve scalability, performance, and portability out of HPC and DL frameworks and applications to take advantage of emerging dense GPU platforms and hence, leading to significant advancements in science and engineering.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Frameworks: Advanced Cyberinfrastructure for Sustainable Community Usage of Big Data from Numerical Fluid Dynamics Simulations,OAC,2103874,Charles Meneveau,meneveau@jhu.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","In most computer simulation-based research, the prevailing approach has been to perform large simulations that generate so much data that only some quantities can be computed during the simulation runs while at most a few representative snapshots are stored and shared for subsequent analysis. Storing the entire time evolution creates access and distribution bottlenecks that have been a pervasive challenge. This project addresses the challenge and broadens the impact of high-performance scientific computing in one of the disciplines at the frontier of high-performance scientific computing: fluid turbulence. Improved tools for turbulence research are required to model many natural processes in atmospheric and ocean sciences and to develop engineering applications. The project builds easily accessible and useable databases from world-class turbulence simulations that help bridge the increasing resource gap between top computer simulators and the wider turbulence data user community. The project develops and implements an advanced cyberinfrastructure framework for turbulence databases that enables ground-breaking research on fluid turbulence in various engineering, atmospheric and ocean flows. Novel services include user-programmable server computation, efficient batch processing tools, easy-to-use inspections of the data that also allow users to store and query the locations of specific flow patterns. The system extends the applicability of notebook-based, AI-augmented data analyses accessing a diverse federation of structured databases with more complex data objects, and includes datasets for various new flows of engineering and geophysical interest at increasing Reynolds numbers. It contains backwards compatible elements with a legacy system, to support and maintain compatibility with an existing active community of users. Research training and mentoring focus on the interplay between physical concepts and computer and computational science aspects of massive simulation-based datasets. Over 2.5 Petabytes of data will become available to support breakthroughs in turbulence research. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the National Science Foundation Division of Chemical, Bioengineering, Environmental and Transport Systems within the Directorate for Engineering; and by the Physical Oceanography Program, Physical and Dynamic Meteorology Program, and the Division of Integrative and Collaborative Education and Research within the Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Element: Computational Toolkit to Discover Peptides that Self-assemble into User-selected Structures,OAC,1931430,Carol Hall,hall@ncsu.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Peptides are short chains (sequences) of naturally-occurring amino acids. They are found in all living cells and tissues, where they perform vital biological functions. Peptides are now being considered for use in nanotechnology as they are able to assemble to form a variety of nanostructures - nanofibers, nanosheets, and nanoparticles. Such structures have potential applications in a wide variety of fields including medicine, electronics, enzyme catalysis and drug release. The goal of this project is to develop an open software toolkit that enables the identification of peptide sequences that are capable of assembling into user-selected fiber-like structures. Users will be able to screen potentially thousands of peptide sequences that assemble into the nanostructure of their choosing, and rank order them according to their stability. An algorithm, PepAD (Peptide Assembly Design) will be developed that searches for sequences that assemble into structures specified by the user. An accompanying software package will allow further analysis of the relative speed at which a large number of these peptide sequences form the desired structure. To establish efficacy and a basis for future improvement of computational tools, selected designs will be validated experimentally using advanced biophysical characterization techniques and solid-state nuclear magnetic resonance spectroscopy. PepAD will be open source and easy to run. Its use by the developers and by members of the scientific and engineering communities should lead to the ability to design the next generation of complex nanostructures. The toolkit, which will be the first of its kind for these types of assemblies, will be available on GitHub and on the NSF-sponsored Molecular Simulation and Design Framework (MoSDeF).Many peptides are known to adopt beta strand conformations and assemble spontaneously into a variety of nanostructures--- nanofibers, nanosheets, nanoparticles, etc. - with applications in a wide variety of fields including nanomedicine, electronics, drug release, and hydrogels. The goal of this project is to develop an open software toolkit that enables the identification of peptide sequences that are capable of assembling into user-selected beta-sheet-based structures. An algorithm, PepAD (Peptide Assembly Design) will be developed that searches for sequences that assemble into structurers specified by the user. PepAD will allow users to screen potentially thousands of peptide sequences that assemble spontaneously into the structure of their choosing, and rank order them according to their stability. Discontinuous molecular dynamics (DMD) simulation software along with the PRIME20 force field will also be made available to enable analysis of the designed structures? assembly kinetics. To establish efficacy and a basis for future improvement of computational tools, selected designs will be validated experimentally using biophysical characterization techniques and solid-state nuclear magnetic resonance (ssNMR) spectroscopy. There are four objectives: (1) develop an algorithm, PepAD, that identifies short peptide sequences that are capable of self-assembling into user-determined amyloid structures; (2) perform DMD/PRIME20 simulations to examine assembly kinetics, (3) synthesize and test the peptide designs using biophysical characterization experiments and ssNMR, and (4) community test and refine the PepAD software and then install it on GitHub and on MoSDeF as a plugin. The toolkit, which will be the first of its kind for beta-sheet assemblies, will be open source and easy to use. Successful implementation of this software will pave the way for the computational design of nanostructures that self-assemble: (a) in response to a trigger such as a change in temperature, pH, or specific ions, and (b) when the peptides are conjugated to functionalities like small molecules, recognition elements, fluorophores or enzymes. Outreach activities include the creation of a video for general audiences that describes how molecular-level computer simulations can be used in the design of new materials and an iPad app that allows users to computationally design model proteins and then watch movies of them as they fold. The project will use the concept of harnessing self-assembly and related ideas to design educational activities for undergraduate STEM students. The project will work to broaden opportunities for women and minorities, and to increase science awareness in K-12 students.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Seismic COmputational Platform for Empowering Discovery (SCOPED),OAC,2103621,Hatice Bozdag,bozdag@mines.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Seismology is the most powerful tool for investigating the interior structure of Earth?from its surface down to the inner core?and its wide range of processes, including earthquakes, volcanic activity, glacial processes, oceanic and environmental processes, and human-caused processes such as nuclear explosions or hydraulic fracturing in oil and gas exploration. Seismology cannot achieve its greatest potential without harnessing state-of-the-art computing capabilities for the dual purpose of scientific modeling and analysis of rapidly increasing data sets. The SCOPED (Seismic COmputational Platform for Empowering Discovery) project establishes a computing platform that delivers data, computation, and service to the seismological community in a way that promotes education, innovation, and discovery, and enables efficient solutions to outstanding scientific problems in geophysics. By focusing on openly available data, openly available software, and virtual training, SCOPED opens seismological research to a broad range of users. Four research components emphasize openly available software for the purpose of characterizing Earth's subsurface structure and the wide range of natural and man-made events that are recorded by seismometers every day. Training of seismologists is a central focus of the project. SCOPED training workshops (seismoHackweeks) are open to the community. Emphasis on virtual research and training diversifies strategies to engage minority groups entering computational geosciences. The project trains a new generation of seismologists to harness the latest capabilities for processing and modeling large data sets. The SCOPED project establishes cyberinfrastructure that provides fast access to large seismic archives from a suite of containerized open-source computational tools for big data analysis, machine learning, and high-performance simulations. The implementation focuses on four interconnected, compute- and data-intensive research components: seismic imaging of Earth?s interior, waveform modeling of earthquakes and Earth structure, monitoring of Earth structure using ambient noise, and precision monitoring of earthquakes and faults. Each research component is enabled by open-source codes that meet, or aspire to meet, best practices for software development. The project contains several transformative components. First, it offers compute performance for both model- and data-driven seismological problems. Hundreds of terabytes of waveform data are directly accessible both to modelers?for data assimilation problems?and to data scientists for processing, analysis, and exploration. Second, it establishes a direct collaborative link among four teams of seismologists at four institutions and a team of computational scientists at Texas Advanced Computing Center. This unity reflects the necessity of both groups to achieve research-ready codes that can exploit high-performance computing (HPC) and Cloud systems. Third, it establishes a gateway with ready-to-run (or adapt) container images and data as a service for the seismological community. Fourth, it develops computational tools that promote the democratization of HPC/Cloud with cutting-edge data processing and modeling software through their scalability from laptops to HPC or Cloud systems and through their portability with containerization. Finally, although the development of cyberinfrastructure is the main priority, ancillary scientific results from advanced techniques are expected to offer insights into fundamental seismological problems. The project has the potential for discoveries across fields (seismology, Earth science, computer science, data science, material science), as well as societal relevance in the realms of seismic hazard assessment, environmental science, cryosphere, earthquake early warning, energy systems, and geophysical detection of nuclear proliferation.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Software. icepack: an open-source glacier flow modeling library in Python,OAC,1835321,Daniel Shapero,shapero@uw.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","This project supports the development of a software package named ""icepack"", that will enable simulations of how glaciers, such as those in Greenland, Antarctica, and mountain ranges around the world, will flow in response to the environment around them. Glaciologists use software tools to run simulations so that they can make predictions of how large the Greenland and Antarctic ice sheets will be in the future. With these predictions, scientists can give policy-makers and the public better predictions on the sea level rise in the coming decades. While the ability to run simulations is essential for advancing our understanding of science, doing so requires a significant programming and scientific expertise. The goal of this project is to lower this barrier to entry. Led by an early career scientist, the team, from University of Washington will develop a tool that is easier to use for researchers and students, whether they are experts or novices. The software applications will be freely available and an open source license.icepack allows for estimating parameters, such as a basal friction or internal rheology, that are not observable via remote sensing. Glaciologists use simulation tools like icepack for (1) exploring aspects of the physics of ice sheets that are not completely understood, (2) drawing inferences from observational data, and (3) making predictions of the future state of the ice sheets in order to estimate future sea-level rise. While modeling is an essential tool for practicing glaciologists, it is still a complex endeavor. In addition to supporting development of more features and improvements to icepack, we will create an extensive set of tutorial materials for a workshop aimed at graduate students and early-career researchers on how to use icepack. Additionally, the investigators will implement novel algorithms for parameter estimation and uncertainty quantification in icepack. These will allow the investigators to leverage the entire time series of observations of the ice sheets, while current algorithms are limited in how much data they can use, and to get a better idea of the statistical spread on estimates of the current and future states of the ice sheets.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program within the NSF Directorate for Geosciences, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
RAPID: ReCOVER: Accurate Predictions and Resource Allocation for COVID-19 Epidemic Response,OAC,2027007,Viktor Prasanna,prasanna@usc.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","The recent outbreak of COVID-19 and its world-wide impact calls for urgent measures to contain the epidemic. Predicting the speed and severity of infectious diseases like COVID-19 and allocating medical resources appropriately is central to dealing with epidemics. Epidemics like COVID-19 not only affect world-wide health, but also have profound economic and social impact. Containing the epidemic, providing informed predictions and preventing future epidemics is essential for the global population to resume their day-to-day work and travel without fear. Shortage of resources puts undue stress on healthcare system further risking health of the community. Preparedness and better management of available resources would require specific predictions at the level of cities and counties around the world rather than solely at the level of countries. The project will provide a predictive understanding of the spread of the virus by developing machine learning based computational models to study the transmission of the virus and evaluate the impact of various interventions on disease spread. The project will learn infection prediction models for COVID-19 considering the following. (i) Predicting at state/county/city-level rather than country-level as finer granularity is essential in planning and managing resources. (ii) How infectious a person is changes over time. Learning the model through observed data will help in understanding of the temporal nature of the virality. (iii) At such granularity travel is a significant reason for the spread and needs to be accounted for. (iv) Available data needs to be ?corrected? by finding the number of underlying unreported cases that are not observed and yet influence the epidemic dynamics. The project will also solve the resource allocation problem based on the prediction ? for instance if a certain number of masks will be available next week in a certain state, how should they be distributed across different hospitals in the state (which hospitals and how many in each state)?Proposed project ReCOVER will use a novel fine-grained, heterogeneous infection rate model to perform predictions at various granularities (hospital/airports, city, state, country) while accounting for human mobility. ReCOVER will integrate data from various sources to build highly accurate models for prediction of the epidemic across the world at various granularity. Due to the ability to capture temporal heterogeneity in infection rate, the approach has the potential to provide insights into infectious nature of COVID-19 which are not fully understood yet. The project will address the issue of unreported cases through temporal analysis of historical infections and correct the data. The right granularities of modeling will be automatically identified, e.g., when to model a state over its cities to trade-off precision for higher reliability in predictions. The proposed project also formulates and solves a resource allocation problem that can guide the response to contain the epidemic and prevent future outbreaks. This is provided by optimal solutions to resource allocation over a network where each node (representing a region) has a function that captures probabilistic response. While the project obtains data with COVID-19 in consideration, the model and algorithms developed under the project are applicable to a wide class of contagious diseases. The project will culminate into an interactive customizable tool that can be used to perform predictions and resource management by a qualified user such as a government entity tasked with managing the epidemic response. The data and code will also be shared with research community.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Frameworks: Software NSCI-Open OnDemand 2.0: Advancing Accessibility and Scalability for Computational Science through Leveraged Software Cyberinfrastructure,OAC,1835725,David Hudak,dhudak@osc.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Reducing barriers that limit the adoption of high performance computing (HPC) addresses an important problem that broadly affects the science, engineering, and humanities communities. This effort builds on existing capabilities with large and varied user communities, and on national scale cyberinfrastructure and high-performance computing resources. The approach has several benefits: - It increases the use of HPC resources among communities that are not well represented on HPC yet, but have growing needs for HPC. - It is also beneficial to HPC providers, by supporting advanced features for monitoring and visualization of the states of systems. - The resulting framework will be used for training and workforce development, expanding the future ability to use advanced cyberinfrastructure for science.This project builds on the strengths of existing efforts, and has the potential to benefit a broad user community. The project develops Open OnDemand 2.0, an open-source software that enables access to high-performance computing, cloud, and remote computing resources via the web, and lower the barriers to access HPC systems. The project combines two widely used HPC resources: - Open OnDemand 1.0 - an existing open-source, web-based project for accessing HPC services; and - Open XDMoD - an open-source tool that facilitates the management of high performance computing resources.Project activities include enhancing an existing web portal-to-HPC system (OnDemand), integrating XDMoD, extending the portal to provide other methods of access for other science domains, and improving the scaling of the system. The software employs a unique per-user web server architecture. This gives a user full system-level access to an HPC cluster through a web browser. Job performance visibility is provided by XDMoD, which enables users to make more efficient usage of HPC resources. Innovation and discovery will be integrated through a study which investigates ways to leverage the system-level access provided by Open OnDemand with science gateways. The integrated platform will enhance resource utilization visibility, extend to more resource types and institutions, and support a smooth and easy utilization of HPC resources with intuitive web interfaces.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Seismic COmputational Platform for Empowering Discovery (SCOPED),OAC,2103701,Marine Denolle,marinedenolle@gmail.com,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Seismology is the most powerful tool for investigating the interior structure of Earth?from its surface down to the inner core?and its wide range of processes, including earthquakes, volcanic activity, glacial processes, oceanic and environmental processes, and human-caused processes such as nuclear explosions or hydraulic fracturing in oil and gas exploration. Seismology cannot achieve its greatest potential without harnessing state-of-the-art computing capabilities for the dual purpose of scientific modeling and analysis of rapidly increasing data sets. The SCOPED (Seismic COmputational Platform for Empowering Discovery) project establishes a computing platform that delivers data, computation, and service to the seismological community in a way that promotes education, innovation, and discovery, and enables efficient solutions to outstanding scientific problems in geophysics. By focusing on openly available data, openly available software, and virtual training, SCOPED opens seismological research to a broad range of users. Four research components emphasize openly available software for the purpose of characterizing Earth's subsurface structure and the wide range of natural and man-made events that are recorded by seismometers every day. Training of seismologists is a central focus of the project. SCOPED training workshops (seismoHackweeks) are open to the community. Emphasis on virtual research and training diversifies strategies to engage minority groups entering computational geosciences. The project trains a new generation of seismologists to harness the latest capabilities for processing and modeling large data sets. The SCOPED project establishes cyberinfrastructure that provides fast access to large seismic archives from a suite of containerized open-source computational tools for big data analysis, machine learning, and high-performance simulations. The implementation focuses on four interconnected, compute- and data-intensive research components: seismic imaging of Earth?s interior, waveform modeling of earthquakes and Earth structure, monitoring of Earth structure using ambient noise, and precision monitoring of earthquakes and faults. Each research component is enabled by open-source codes that meet, or aspire to meet, best practices for software development. The project contains several transformative components. First, it offers compute performance for both model- and data-driven seismological problems. Hundreds of terabytes of waveform data are directly accessible both to modelers?for data assimilation problems?and to data scientists for processing, analysis, and exploration. Second, it establishes a direct collaborative link among four teams of seismologists at four institutions and a team of computational scientists at Texas Advanced Computing Center. This unity reflects the necessity of both groups to achieve research-ready codes that can exploit high-performance computing (HPC) and Cloud systems. Third, it establishes a gateway with ready-to-run (or adapt) container images and data as a service for the seismological community. Fourth, it develops computational tools that promote the democratization of HPC/Cloud with cutting-edge data processing and modeling software through their scalability from laptops to HPC or Cloud systems and through their portability with containerization. Finally, although the development of cyberinfrastructure is the main priority, ancillary scientific results from advanced techniques are expected to offer insights into fundamental seismological problems. The project has the potential for discoveries across fields (seismology, Earth science, computer science, data science, material science), as well as societal relevance in the realms of seismic hazard assessment, environmental science, cryosphere, earthquake early warning, energy systems, and geophysical detection of nuclear proliferation.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: DeCODER (Democratized Cyberinfrastructure for Open Discovery to Enable Research),OAC,2209863,Kenton McHenry,kmchenry@ncsa.uiuc.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Scientific research in most scientific fields today involves significant amounts of data as well as software to operate on that data. Furthermore, research increasingly requires data from across a number of sources, perhaps even fields. This is a significant challenge for the scientific community, and better solutions are needed in order to enable new discoveries and accelerate the societal impacts from those discoveries. The approach in this project is to follow in the footsteps of the web and it aims to standardize how scientific data is described, allowing for tools addressing the above challenges, such as search engines for scientific data that not only support discoverability but also facilitate the usage of the data.The DeCODER project will expand and extend the successful EarthCube GeoCODES platform and community to unify data and tool description and re-use across geoscience domains. Building on the NSF CIF21 vision, the EarthCube program was formed to address the technological challenges surrounding data and software within the geosciences. Through extensive interaction with the community this culminated in two key activities around data discovery and reuse. First, the promotion, refinement, and adoption of schema.org to annotate geosciences metadata within distributed repositories so that datasets can be crawled. Second, the promotion and support for the adoption of notebooks to document, share, and reuse software as peer reviewed scholarly objects. A rallying point around these activities was the GeoCODES platform, which allows communities to stand up instances of scientific search engines specific to their domains, while building a community of geoscience data users and developers and, ultimately, reducing the time to science. This project will leverage this effort in the DeCODER platform to enable similar activities and outcomes across scientific communities. This work will continue the endeavor to support the scientific community in the adoption of schema.org and notebooks, facilitating this by providing DeCODER as an open source resource that can be customized by a given scientific community to create lightweight scientific gateways that bring together relevant distributed resources.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: DeCODER (Democratized Cyberinfrastructure for Open Discovery to Enable Research),OAC,2209866,Robert Thomas,rqthomas@vt.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Scientific research in most scientific fields today involves significant amounts of data as well as software to operate on that data. Furthermore, research increasingly requires data from across a number of sources, perhaps even fields. This is a significant challenge for the scientific community, and better solutions are needed in order to enable new discoveries and accelerate the societal impacts from those discoveries. The approach in this project is to follow in the footsteps of the web and it aims to standardize how scientific data is described, allowing for tools addressing the above challenges, such as search engines for scientific data that not only support discoverability but also facilitate the usage of the data.The DeCODER project will expand and extend the successful EarthCube GeoCODES platform and community to unify data and tool description and re-use across geoscience domains. Building on the NSF CIF21 vision, the EarthCube program was formed to address the technological challenges surrounding data and software within the geosciences. Through extensive interaction with the community this culminated in two key activities around data discovery and reuse. First, the promotion, refinement, and adoption of schema.org to annotate geosciences metadata within distributed repositories so that datasets can be crawled. Second, the promotion and support for the adoption of notebooks to document, share, and reuse software as peer reviewed scholarly objects. A rallying point around these activities was the GeoCODES platform, which allows communities to stand up instances of scientific search engines specific to their domains, while building a community of geoscience data users and developers and, ultimately, reducing the time to science. This project will leverage this effort in the DeCODER platform to enable similar activities and outcomes across scientific communities. This work will continue the endeavor to support the scientific community in the adoption of schema.org and notebooks, facilitating this by providing DeCODER as an open source resource that can be customized by a given scientific community to create lightweight scientific gateways that bring together relevant distributed resources.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Software Infrastructure for Programming and Architectural Exploration of Neuromorphic Computing Systems,OAC,2209745,Nagarajan Kandasamy,kandasamy@coe.drexel.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Machine learning has proved to be immensely successful across a range of social domains such as healthcare, environment, education, infrastructure, and cybersecurity. Computing platforms currently used to run machine-learning tasks have a high carbon footprint associated with them. Neuromorphic computing systems, which mimic biological neurons and synapses can implement these tasks in a highly energy-efficient fashion. Major challenges for neuromorphic computing, however, lie in its adoption by users and from a system developer's perspective, to cope with faster time-to-market pressure for new neuromorphic chip designs. This project develops a software infrastructure called NeuroXplorer, which helps both end-users as well as developers of neuromorphic systems: it allows for machine-learning tasks to be mapped onto neuromorphic chips in the most efficient way possible; and provides analysis, simulation, and synthesis tools that can be used to explore new chip designs to meet the needs of emerging machine-learning workloads. NeuroXplorer is distributed under an open-source license to promote the adoption of neuromorphic computing as well as the development and commercialization of neuromorphic systems in the United States.The intellectual merits of the project lie in the development of compiler backends within NeuroXplorer to generate executable code for neuromorphic chips such as Loihi, Dynamic Neurormorphic Asynchronous Processor, and Microbrain from a high-level specification of the machine-learning task; development of mapping and synthesis tools to execute machine-learning tasks on novel neuromorphic architectures built using Field-Programmable Gate Array (FPGA); and development of high-performance software for hardware/software design-space exploration of new neuromorphic architectures. NeuroXplorer is built to be modular and extensible such that developers can easily contribute new features to the software. The capabilities of NeuroXplorer are accessible over the Internet. The end-user trains the machine-learning model using a standard workflow and uploads it, upon which the appropriate code is automatically generated and executed on neuromorphic architecture. The neuromorphic program and bitstream files for the final FPGA design can be freely downloaded. Design-space exploration tools within NeuroXplorer efficiently tackle the growing complexity of neuromorphic systems and challenges in integrating emerging design technologies into these systems. From an educational perspective, the project involves both graduate and undergraduate students at Drexel University in the development of the software. Collaborators from academia and industry deliver guest lectures on current developments in neuromorphic hardware, system software, and applications, with these lectures being integrated within relevant courses.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Sustainability: A Community-Centered Approach for Supporting and Sustaining Parsl,OAC,2209919,Kyle Chard,chard@uchicago.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","Modern research computing requires orchestration of complex computational workflows across diverse research computing infrastructure. To meet this requirement, Parsl, an open source software package, enables scientists and engineers to parallelize Python programs and run them scalably and efficiently on small to very large local and remote resources (e.g., laptops, clusters, clouds, and supercomputers), while also moving data as needed. Parsl effectively democratizes access to NSF's increasingly broad and powerful cyberinfrastructure resources by enabling researchers to work in the familiar and productive Python programming language and environment while also allowing them to easily scale and distribute their work to fully use today's cutting-edge cyberinfrastructure. Parsl is a key tool used by researchers to tackle some of the world's most significant challenges: from understanding the beginning of the universe to exploring an enormous molecular search space to identify viable therapeutics. This project is laying the foundation for the sustainability of this crucial tool, enabling new long-term science advances and thereby benefiting society. Participation in several education programs is exposing a diverse group of students to the increasingly important area of parallel and distributed computing. This project is transitioning Parsl to a community-governed and community-supported open source project, with future income to be managed by a nonprofit organization under the direction of an elected Parsl Coordination Committee. The project is delivering a sustainable Parsl community by a) targeted technical activities that reduce costs and barriers for contribution, reducing future maintenance costs; b) building the Parsl community via outreach, engagement, and education programs, increasing potential future contributors; and c) establishing pathways and incentives to convert users to contributors and contributors to leaders, growing the next generation of the community. The project takes a data-driven approach by measuring effort and funding across the project to reveal insights about where effort is spent, how it is supported, and how these factors can be balanced. The lessons learned in this project are being converted to a blueprint for similar projects.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Elements: Making Ice Penetrating Radar More Accessible: A tool for finding, downloading and visualizing georeferenced radargrams within the QGIS ecosystem",OAC,2209726,Laura Lindzey,lindzey@uw.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","Ice penetrating radar is one of the primary tools that researchers use to study ice sheets and glaciers. With radar, it is possible to see a cross-section of the ice, revealing internal layers and the shape of the rocks under the ice. Among other things, this is important for calculating how much potential sea level change is locked up in the polar ice sheets, and how stable the ice sheets are likely to be in a warming world. This type of data is logistically challenging and expensive to collect. Historically, individual research groups have obtained funding to collect these data sets, and then the data largely stayed within that institution. There has been a recent push to make more and more data openly available, enabling the same datasets to be used by multiple research groups. However, it is still difficult to figure out what data is available because there is no centralized index. Additionally, each group releases data in a different format, which creates an additional hurdle to its use. This project addresses both of those challenges to data reuse by providing a unified tool for discovering where ice penetrating radar data already exists, then allowing the researcher to download and visualize the data. It is integrated into open-source mapping software that many in the research community already use, and makes it possible for non-experts to explore these datasets. This is particularly valuable for early-career researchers and for enabling interdisciplinary work.The US alone has spent many tens of millions of dollars on direct grants to enable the acquisition and analysis of polar ice penetrating radar data, and even more on the associated infrastructure and support costs. Unfortunately, much of these data is not publicly released, and even the data that has been released is not easily accessible. There is significant technical work involved in figuring out how to locate, download and view the data. This project is developing a tool that will both lower the barrier to entry for using this data and improve the workflows of existing users. Quantarctica and QGreenland have rapidly become indispensable tools for the polar research community, making diverse data sets readily available to researchers. However, ice penetrating radar is a major category of data that is not currently supported ? it is possible to see the locations of existing survey lines, and the ice thickness maps that have been interpreted from their data, but it is not readily possible to see the radargrams themselves in context with all of the other information. This capability is important because there is far more visual information contained in a radargram than simply its interpreted basal elevation or ice thickness. This project is developing software that will enable researchers to to view radargram images and interpreted surface and basal horizons in context with the existing map-view datasets in Quantarctica and QGreenland. A data layer shows the locations of all known ice penetrating radar surveys, color-coded based on availability. This layer enables data discovery and browsing. The plugin itself interacts with the data layer, first to download selected data, then to visualize the radargrams along with a cursor that moves simultaneously along the radargram and along the map view, making it straightforward to determine the precise geolocation of radar features.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: TRAnsparency CErtified (TRACE): Trusting Computational Research Without Repeating It,OAC,2209630,Thu-Mai Christian,thumai@email.unc.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Research communities across the natural and social sciences are increasingly concerned about the transparency and reproducibility of results obtained by computational means. Calls for increased transparency can be found in the policies of peer-reviewed journals and processing pipelines employed in the creation of research data products made available through science gateways, data portals, and statistical agencies. These communities recognize that the integrity of published results and data products is uncertain when it is not possible to trace their lineage or validate their production. Verifying the transparency or reproducibility of computational artifacts?by repeating computations and comparing results?is expensive, time-consuming, and difficult, and may be infeasible if the research products rely on resources that are subject to legitimate restrictions such as the use of sensitive or proprietary data; streaming, transient, or ephemeral data; and large-scale or specialized computational resources available only to approved or authorized users. The TRACE project is addressing this problem through an approach called certified transparency - a trustworthy record of computations signed by the systems within which they were performed. Using TRACE, system owners and operators certify the original execution of a computational workflow that produces findings or data products. By using a TRACE-enabled system, researchers produce transparent computational artifacts that no longer require verification, reducing burden on journal editors and reviewers seeking to ensure reproducibility and transparency of computational results. TRACE presents an innovative and efficient approach to ensuring the transparency of research that uses computational methods, is consistent with the vision outlined by the National Academies, and enables evidence-based policymaking based on transparent and trustworthy science.The central goal of the TRACE project is the development, validation, and implementation of a technical model of certified transparency. This includes a set of infrastructure elements that can be employed by system owners to (1) declare the dimensions of computational transparency supported by their platforms; (2) certify that a specific computational workflow was executed on the platform; and (3) bundle artifacts, records of their execution, technical metadata about their contents, and certify them for dissemination. The first phase of the project focuses on the development of a conceptual model and technical specification that can be used to certify the description of a system, termed a Transparency-Certified System (TRACE system), and the aggregation of artifacts along with records of their execution, termed Transparency-Certified Research Objects (TROs). The second phase focuses on the development of reusable software components implementing the TRACE model and approach. To demonstrate certified transparency, the toolkit is used to TRACE-enable existing platforms including Whole Tale, SKOPE, and the SLURM workload manager. These TRACE-enabled systems produce certified TROs that can be trusted and do not need to be repeated or re-executed to verify that results were obtained as claimed.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Social and Economic Sciences within the Directorate for Social, Behavioral and Economic Sciences; and by the Division of Information and Intelligent Systems within the Directorate for Computer and Information Science and Engineering.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Elements: An ML Ecosystem of Filament Detection: Classification, Localization, and Segmentation",OAC,2209912,azim Ahmadzadeh,aahmadzadeh1@gsu.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","Since object-detection algorithms outperformed humans, one decade has passed. During this period, the unprecedented achievements of the Computer Vision domain made many believe that object detection is a solved problem. However, when it comes to scientific imagery such as microscopic, telescopic, aerial, satellite, and medical images, the general-purpose object-detection algorithms are far from perfect. A pixel-precise segmentation of objects and identification of their physical properties based on their texture features are still outstanding challenges in many interdisciplinary areas of research. Space Weather is one such area. Extreme space-weather events, similar to extreme terrestrial events, can have drastic economic and collateral impacts on mankind. Continuous and automatic monitoring of solar filaments plays an integral role in achieving reliable space-weather forecast/prediction systems, which consequently results in the technical preparedness much needed in many infrastructural aspects of the society, such as the power grid and the GPS systems. Our Machine Learning Ecosystem brings automatic, accurate, and reliable analyses of filaments? dynamic behavior to the experts? fingertips. The main contributions of this ecosystem are two data products of annotated filaments, and four software products which carry out the annotation (localization, identification, and segmentation) of these filaments. This modular ecosystem can be easily expanded in the future, beyond the lifetime of the award, as faster and more efficient modules are expected to be implemented by the community and replace the existing ones. Throughout the development of this project, we consult with the instrument/data experts from the National Solar Observatory (NSO) for proper utilization of the observation images and metadata we integrate from the six ground-based observatories of the Global Oscillation Network Group, that together provide a full-disk and continuous (24/7) coverage of the Sun.The primary focus of this project is on the localization and segmentation of a specific solar event, called a filament, and the identification of its magnetic field chirality. That said, the novel concepts investigated in this project, such as the detection algorithm, the augmentation engine, and the segmentation loss function which is sensitive to granularities of objects, remain agnostic to the type of the event/object of interest. Moreover, the released datasets of annotated filaments can serve the Computer Vision community as a testbed for algorithms that aim at high-precision segmentation of objects. Our Machine Learning Ecosystem consists of two data products and four software products. The largest collection of manually annotated filaments data, and a continuously-growing collection of automatically annotated filaments are the two main data products. The main software products are (1) an augmentation engine that provides users with practically unlimited semi-real filament instances, (2) a deep neural network algorithm for localization, segmentation, and classification of filaments, (3) a high-precision segmentation loss function (sensitive to granularities of the observed filaments) that guides the segmentation task, and (4) a deployable detection module which carries out the localization, segmentation, and classification tasks in real time.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Scalable and Automated Atomic Portal - Bridging the Gap Between Research Codes and User Community,OAC,2209639,Marianna Safronova,msafrono@udel.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","In a number of present applications, ranging from studies of fundamental interactions to the development of future technologies, accurate atomic theory is indispensable to the design and interpretation of experiments, with direct experimental measurement of relevant parameters being impossible or infeasible. These data are also in high demand by broader atomic, plasma, astrophysics, and nuclear physics communities. The need for high-precision atomic modeling has increased significantly in recent years with the development of atomic-base quantum technologies for a wide range of fundamental and practical applications. Starting from the prototype codes developed by our group, we will develop open-access atomic software with a user-friendly interface capable of calculating a large volume of high-quality atomic data for various atoms and ions. We will develop a scalable and automated data portal with a convenient interface that will allow for easy addition of data for new elements and updates of data already provided by the portal. Beyond the immediate research and cyberinfrastructure aims of the proposed effort, this project will impact student learning, the broader knowledge base of atomic physicists, the productivity of the larger science community, and the competitiveness of the private sector in atomic physics engineering.This project aims to bridge the gap between the development of atomic physics research codes and the need for data and software by the user community. Further rapid advances in applications involving complex atoms will require accurate knowledge of basic atomic properties, most of which remain highly uncertain and difficult to measure experimentally. Moreover, the lack of a reliable theoretical framework hinders the search for further applications of rich, complex atomic structures. This project provides high-quality atomic data and software in several scientific communities. To meet the needs of the community, we will (1) develop a scalable and sustainable online data portal with an automated interface for easy update and addition of new data, (2) continue the development of open-access atomic software based on our research codes that allow generating large volumes of data with automated accuracy assessments. The portal will provide energies, wavelengths, transition matrix elements, and rates for various transition types, branching ratios, lifetimes, polarizabilities, hyperfine constants, and other data. We plan to make data for over 100 atoms and ions, including high-charged ions, available for the user community by the end of this project. Significant scaling of the portal will require a new level of both atomic code and portal automatization. The new interface will allow easy addition of data for new elements and updates of data already provided by the portal by the physics team of the PI and collaborators to ensure that the portal is sustainable beyond the funding period.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Physics at the Information Frontier in the Division of Physics within the Directorate for Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Proposal: Frameworks: Sustainable Open-Source Quantum Dynamics and Spectroscopy Software,OAC,2103717,Xiaosong Li,xsli@uw.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","With support from the Office of Advanced Cyberinfrastructure and the Division of Chemistry at NSF, Professor Li and his team will work to expand the capabilities of the open-source software platform, Chronus Quantum (ChronusQ), to include time-resolved spectroscopy for supporting quantum chemistry calculations. The new capabilities will include novel computational methods to provide unprecedented capabilities to simulate chemical processes of electrons and nuclei that exhibit quantum behaviors. The physical insights gleaned through applications of ChronusQ underlie the advancement of new technologies that are crucial to sustainable energy, catalysis, quantum computing, and other applications that can immediately impact society. This project provides a mechanism for advanced interdisciplinary education and training in the areas of inorganic, theoretical, physical, and materials chemistry. The collaborative academic research serves as a test ground for identifying and deploying ways that the scientific community as a whole can both increase awareness of the importance of active engagement in professional skill development for graduate students and post-doctoral scholars, and develop tools to facilitate professional development in an academic setting.ChronusQ seamlessly integrates time-dependent quantum mechanical theories, spectral analysis tools, and modular high-performance numerical libraries that are highly parallelized, extensible, reusable, community-driven, and open-sourced. The Team develops in ChronusQ the complete time-dependent quantum description of coupled nuclear and electronic dynamics within the time-dependent density functional theory and equation-of-motion coupled cluster framework. The project enables computational studies of ultrafast time-resolved spectroscopies and simulations of chemical processes in the strongly nonadiabatic regime. Software modules are bolstered by algebraic and integral acceleration engines that can make it feasible to simulate fully quantum mechanically molecular dynamics. The collaborative project advances the theoretical description of quantum dynamics across time scales, bridging the attosecond and subnanosecond regimes, enabling the development of spectroscopic technologies to probe molecular and materials properties with state specificity that is beyond the Born-Oppenheimer approximation.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: OpenEarthscape - Transformative Cyberinfrastructure for Modeling and Simulation in the Earth-Surface Science Communities,OAC,2104102,Gregory Tucker,gtucker@colorado.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","The landscape around us changes constantly. Sometimes change is slow: a river bend migrates, soil erodes from a field, a waterfall carves its way upstream. Sometimes change happens fast: a landslide, a flood, a winter storm eroding beaches. To understand, manage, and forecast such processes, scientists rely on computer simulation models. This project develops software tools to make simulation modeling easier, more accessible, and more efficient. Among the products is a website through which researchers and students alike can learn about and experiment with a variety of environmental simulation models, without needing to install anything on their own computers. This web portal takes advantage of a technology that combines text, pictures, and computer code in a single online document. The project also develops improved computer-programming libraries designed to make it easier and more efficient for researchers to create new simulation models. The project contributes computing-skills training for college students enrolled in Colorado-based summer programs that serve traditionally underrepresented student populations. The project also promotes public education in geology, by creating an online animated simulation illustrating how landscapes evolve in response to various geologic events.As the sciences that probe Earth's changing surface become more quantitative and prediction-oriented, they increasingly rely on computational modeling and model-data integration. This project develops OpenEarthscape: an integrated suite of community-developed cyber resources for simulation and model-data integration, focusing on nine high-priority geoscience frontiers. Products and activities include EarthscapeHub: a JupyterHub server providing easy access to models, tools, and libraries; new capacity for creating and sharing reproducible analyses; and major enhancements to current programming libraries for model construction and coupling. OpenEarthscape catalyzes efficiency by building new technology to improve performance and developing an extended version of the Basic Model Interface API standard to address parallel architecture and coupling. OpenEarthscape fosters research productivity with improved library capabilities for data I/O and visualization, and with community resources for efficient software distribution and cross-platform compatibility. Broader impacts include partnership with undergraduate research programs that support traditionally underrepresented student populations, with the project team contributing introductory training in scientific computing. A novel educational element is the OpenEarthscape Simulator: a web-hosted visual simulation of a micro-continent evolving in response to various geologic events. The simulator provides students and the general public with an intriguing visualization of Earthscape dynamics and provides a template for the research community to identify defects in our current understanding.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements:Open-source hardware and software evaluation system for UAV,OAC,2103951,Hyesoon Kim,hyesoon@cc.gatech.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","The usage of Robots and Unmanned Aerial Vehicles (UAVs) is increasing, and they are becoming a part of our everyday lives. In the process, robotic systems have ceased to be solely a mechanical design problem but now include substantial computing power as well. However, little system or architecture research has been done for robotics workloads, especially in realistic environments. The existing infrastructure is focused on drone flight algorithm tests, with little flexibility to configure the computing capability. To this end, this project develops a software-hardware co-design framework that includes an end-to-end, vertically integrated stack that enables reconfigurable hardware to be programmed and exposed to virtual reality environments for realistic drone navigation problems. It will establish a flexible software-hardware infrastructure of drones and open up exciting possibilities for more research in academia and industry. Since drones are nowadays widely used, the research has potential for impact across a wide range of applications. The project develops an end-to-end hardware and software infrastructure that can evaluate the computing requirements and test out new architectures and technologies. This includes: (1) profiling the workload for UAVs, which include both model-based as well as learning-based workloads; (2) developing an open-source hardware/software drone platform that can be used for testing computing architecture/systems with a real system; (3) developing a cyber-physical system infrastructure that can be connected with drones and flight simulation; and (4) developing a simulation infrastructure to offload high-end computation to other accelerators while running drone flight scenarios. The project will advance the state of the art in implementation of processors for robotics/UAV workloads. It will offer new opportunities in power-constrained platforms for applications including surveillance, automotive, environment, military, and disaster to name a few.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Collaborative Research: Community-driven Environment of AI-powered Noise Reduction Services for Materials Discovery from Electron Microscopy Data,OAC,2103936,Carlos Fernandez Granda,cfg3@nyu.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","The goal of this project is to create cyberinfrastructure (CI) powered by artificial intelligence (AI) for sustained innovation in materials science. Deep understanding of materials is critical for progress in technologies related to energy, communication, construction, transportation and human health. The revolutionary progress of deep learning has been enabled by the availability of open-source AI models and open-access benchmark databases. However, the existing codebases and datasets relevant to image processing focus mostly on photographic images. In order to promote the sustained development of AI technology that can have significant impact in materials science, it is critical to provide data and AI models that are tailored to this domain. The developed CI will address this need by providing software to process images obtained from electron-microscopes, a technique enabling atoms to be visualized, and has the potential to enable transformative breakthroughs in varied and important areas of materials science. The CI is explicitly designed to foster the growth of a sustainable community of users and developers of AI technology at the intersection of the materials and data science communities, and to empower materials scientists to simulate their own datasets and develop their own AI models for scientific discovery. The developed AI-powered CI will therefore enable transformative progress in atomic-level understanding of materials, which will have broader impacts in health, energy, environment, and biotechnology. The CI environment will contribute to training materials scientists in AI technology, connecting them to the AI community, and providing software, data, and support materials to initiate them in AI-powered research. Educational and outreach plans are designed to facilitate interactions between the materials science and AI communities. Outreach activities specifically targeted to the general public, and to high-school teachers and their students, will expose them to materials science, electron microscopy, and AI. The project is committed to providing opportunities to women and underrepresented groups and will prioritize diversity in collaboration with the NYU Center for Data Science diversity committee.Developing a fundamental understanding of atomic level structure and dynamics is critical for transformative advances in materials science. Aberration-corrected transmission electron microscopy is a primary tool to accomplish this goal. Unfortunately, the information content of microscopy data may be severely limited by poor signal-to-noise ratios. This is particularly true for radiation sensitive materials and experiments where high time resolution is required to investigate dynamic kinetic processes. AI methodology can exploit prior information about material structure by training deep neural nets with extensive simulations. These approaches may significantly outperform existing state-of-the-art methods, especially for non-periodic structures, including defects, interfaces, and surfaces. The developed CI will provide AI noise reduction services which will yield immediate advances and impacts for zeolites, metal organic frameworks, protein-material interfaces, liquid phase nucleation and growth, liquid-solid interfaces, and fluxional behavior in catalytic nanoparticles. In addition, the project will advance methodology for the design of AI-oriented CI. The CI is strategically designed to create a holistic environment for the use and development of AI technology in a specific scientific domain. It will attract domain scientists with little AI expertise, by providing software where the AI technology is transparent to the end user. Exposure to the technology will motivate the scientific community to design and train their own models, which will be facilitated by the open-source codebase in the AI repository. The open-access database combined with the repository will attract AI practitioners with little domain expertise, by giving them access to well-curated data and a clear specification of the relevant AI tasks. These services will be jump-started and supported through multiple educational and outreach activities.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Community-Based Weather and Climate Simulation With a Global Storm-Resolving Model,OAC,2005137,David Randall,randall@atmos.colostate.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","Global Earth System Models (ESMs) use mathematical equations to simulate both weather and climate. ESMs include the dynamics of the atmosphere, oceans, land surface, ice, and vegetation. They can be used to make predictions of use to the public and policymakers. Today?s ESMs use coarse grids with cells about 100 km wide. Important weather systems like thunderstorms are too small to be simulated with such grids. One way to improve ESMs is to use finer grids that can directly simulate thunderstorms, but such models can only be run on very powerful computers. This project, called EarthWorks, will create an ESM capable of resolving storms by taking advantage of recent developments in high performance computing. EarthWorks will also use artificial intelligence to improve and speed up the model, and state-of-the-art methods to limit the amount of data produced as the model runs. The EarthWorks ESM will be built by spinning off and modifying a copy of the most recent version of the widely used Community Earth System Model. The modified model will represent the atmosphere, the oceans, and the land surface on a single very high-resolution grid, with grid cells about 4 km wide. It will have improved forecast skill, and produce more realistic simulations of past, present, and future climates. The project will make the model and its output openly available for use by all scientists.The open-source Community Earth System Model (CESM) is both developed and applied to scientific problems by a large community of researchers. It is critical infrastructure for the U.S. climate research community. In the atmosphere and ocean components of the CESM, the adiabatic terms of the partial differential equations that express conservation of mass, momentum, and thermodynamic energy are solved numerically using what is called a dynamical core. Atmosphere and ocean models also include parametric representations, called parameterizations, that are designed to include the effects of storm and cloud processes that occur on scales too small to be represented on the model's grid. Despite decades of work by many scientists, today's parameterizations are still problematic and limit the utility of ESMs for many applications of societal relevance. Fortunately, recent advances in computer power have made it possible to parameterize less, by using grid spacings on the order of a few kilometers over the entire globe. These ""global storm-resolving models"" (GSRMs) can only be run on today's fastest computers. GSRMs are under very active development at a dozen or so modeling centers around the world. Unfortunately, however, the current formulation of the CESM prevents it from being run as a GSRM. This project, called EarthWorks, will create a new, openly available GSRM by spinning off and intensively modifying a copy of the CESM. To accomplish this goal, the researchers will use recently developed and closely related dynamical cores for the atmosphere and ocean. All components of the model will use the same very high-resolution grid. This high resolution will make it possible to eliminate the particularly troublesome parameterization of deep cumulus convection (i.e., thunderstorms), and thereby reduce systematic biases that plague current ESMs. Earthworks will exploit the pre-exascale and exascale technologies now being brought to market by high performance computing vendors. The new exascale ESM will run the most computationally intensive components on powerful graphics processor units (GPUs), and exploit node-level task parallelism to execute the rest of the model asynchronously. The component model codes are close to completion and are currently being tested on GPUs. EarthWorks will use a simplified component-coupling approach, incorporate machine learning where feasible, and leverage lossy compression techniques and parallel I/O tools to deal with the enormous data volumes that will be generated as the model runs. The completed model will be simple, powerful, and well documented. The project will apply it to pressing scientific problems in both numerical weather prediction and climate simulation. The model and its input datasets will be made openly available to the broad research community, via GitHub.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Data-Science Methods for Resource Allocation During Characterization of Dynamic Systems,OAC,2005012,Michael Groeber,groeber.9@osu.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","The project develops the software infrastructure and its integration with physical infrastructure that is required to bring state-of-the-art data science, machine learning, and artificial Intelligence (AI) tools to novel materials science experiments at a national user facility. The work will create an openly available control package that will enable dynamic experiments informed by modeling in real-time. The developments will make more efficient use of scarce beam-time at national synchrotron user facilities, enabling higher scientific throughput for in-situ experiments probing the mechanical response of materials under load.The effort evaluates the hypothesis that rare material failure events can be predicted from a small number of features that describe evolving local material states using machine learning solutions. The project is focusing on synchrotron x-ray scattering measurements of materials under mechanical load, and specifically integrating new and existing toolsets into a control package capable of dynamic resource allocation for hyper-efficient data collection at the Cornell High Energy Synchrotron Source (CHESS), a national user facility. The project integrates these toolsets to detect precursor signatures through real-time processing of data from user facilities such as CHESS, and suggests resource allocations to facilitate study of early stages of stochastic events in dynamic materials systems. The goal is to develop machine learning (ML) techniques and software infrastructure to inform the best, in a probabilistic sense, allocation of limited detector resources at material testing facilities, to better capture early stages of rare events in materials and the key factors for these events. The effort is also interested in applying the same resource allocation strategies to computational resource allocation in simulations of materials systems. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Civil, Mechanical and Manufacturing Innovation (CMMI) within the NSF Directorate for Engineering, and the Division of Materials Research (DMR) within the Directorate for Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Cyber-infrastructure for Interactive Computation and Display of Materials Datasets,OAC,2004693,JoAnn Kuchera-Morin,jkm@create.ucsb.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","This project will accelerate the progress of scientific research by creating a software infrastructure named TINC (The Toolkit for Interactive Computation). TINC will allow scientists to interactively work with very complex information generated by high performance computing (HPC) clusters. It will provide software tools to assist scientists by facilitating virtual experimentation through interactive visualization, speeding up time to discovery. By tying together the scientists' workflow of computation, scientific data analysis in scripting languages, and visualization, TINC will enable new ways of sharing and disseminating results by allowing researchers to share not only their results, but also their interactive workflow as part of their publications. This research will begin its focus on an important and essential need in the materials science community, speeding up time to discovery of new materials through rapid prototyping using computation. The basic science to be generated through the application of the TINC infrastructure is the study of the electrochemical properties of electrode materials for Sodium ion batteries that will help overcome materials challenges that are preventing the commercialization of this promising technology for large-scale grid storage applications. This important proof of concept will facilitate delving deep into the science while focusing on the generalization of the tool to other disciplines as well. The ultimate goal of TINC is to create a new paradigm for high performance computing, facilitating ease of use by tying together interactive visualization with computation. This paradigm shift may facilitate a connection not only to a wider scientific community but also to an informed general public as well through TINC's focus on reproducibility and provenance tracing. TINC is a computational toolkit that expedites data discovery by improving the interaction workflow in complex data analysis. This improvement is achieved by tightly integrating interactivity, computation and visualization with complex scientific data. By managing the connection between data parameters and on the fly computation, TINC simultaneously tackles the issues of reproducibility and interactive control in the exploration of data with large parameter spaces. With the integration of scripting languages and data notebooks, scientists can study their data with the ease of interactive computation and display. Through a robust caching mechanism it will enable new ways of sharing and disseminating results by allowing researchers to share not only their results, but also their interactive workflow as part of their publications. TINC will tightly integrate interactivity, computation and visualization in the research loop, allowing scientists to more quickly and more deeply understand, compare and validate their data. Thus, TINC will facilitate the merging of complex scientific computational models with high performance interactive visualization and will enable real-time exploration of empirical and theoretical models and large experimental datasets. Provided as a set of python and C++ libraries, TINC will handle parameter space mapping to data, interactive triggering of computation on this parameter space and caching to enable scalability, performance, full reproducibility and data provenance tracking. TINC will be applied to a statistical mechanics study of ionic transport mechanisms and ionic insertion processes in layered intercalation compounds that are candidate electrode materials for Sodium ion batteries. This is of critical importance to the area of materials simulation that focuses on the study of transport mechanisms in alloy systems, where visualizing specific mechanisms experimentally is difficult. TINC will allow computational researchers to propose and verify transport mechanisms in a way not previously possible.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Machine Learning Materials Innovation Infrastructure,OAC,1931306,Benjamin Blaiszik,blaiszik@uchicago.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Machine learning is rapidly changing our society, with computers recently gaining skills in many new tasks. These tasks range from understanding language to driving cars. Materials science and engineering is also being transformed. Many tasks are becoming increasingly accessible to machine learning algorithms. These range from predicting new data to analyzing images. Many basic machine learning algorithms are readily available. However the overall workflow involved in the application of machine learning for materials problems is still largely executed by hand. Getting results out is still done by traditional methods like publishing articles. There is an enormous opportunity to accelerate the growth and impact of machine learning in materials research. This requires improved cyberinfrastructure. This project will develop an approach to accelerate the entire machine learning workflow. Its output will include tools to easily develop datasets, manage model development, and output models. These will be reusable and reproducible for future use. This project will enable materials scientists and engineers to rapidly develop and deploy machine learning models. More importantly, the entire materials community will be able to quickly access these models. It will transform how we discover and develop advanced materials.The project will have three major technical components: (i) A MAterials Simulation Toolkit for Machine Learning (MAST-ML) with workflow tools that will enable local or cloud-based multistep, automated execution of complex machine learning data analysis and model training, codified best practices, increased access to machine learning methods for non-experts, and accelerated model development; (ii) The Foundry Materials Informatics Environment that will provide flexible, integrated, cloud-based management of machine learning materials science and engineering projects, from organizing data to developing models to disseminating results that are machine and human accessible and reproducible in ways that support a networked materials innovation ecosystem, (iii) Representative science applications of machine learning materials science and engineering projects that will support infrastructure development and promotion, as well as demonstrate best practices on state-of-the-art materials science and engineering problems. In addition to its impact on materials science and engineering, this project will develop students and young researchers with the interdisciplinary skills of machine learning and materials science and engineering, and promote these new ideas to the broader materials community. This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure, and the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements:Software:NSCI: Chrono - An Open-Source Simulation Platform for Computational Dynamics Problems,OAC,1835674,Dan Negrut,negrut@wisc.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","This project seeks to augment modeling and solution methods employed by Chrono, an open-source computer simulation platform for multi-body dynamics (MBD) and fluid-solid interaction (FSI) problems. Chrono will be able to capture dynamics at various size and time scales spanning from millisecond (impact phenomena) to decades (geophysics). These performance levels open up new directions of research in several fields. Chrono is widely used and further developed by other users and has an active forum with more than 250 registered users currently. This project will enhance the richness of Chrono's modeling features, sound numerical solution foundation, and leverage of emerging hardware architectures to elevate this simulation capability to the status of ready-to-use, open-source, best-in-class computational dynamics platform. Chrono has been used by universities, national labs, and industry. Over the past two years, various groups have used Chrono in extraterrestrial applications, machine learning in robotics, image processing, pattern recognition and computer vision, mechanical watch design, architectural studies, autonomous vehicles, fluid-solid interaction applications, wind turbine dynamics, next generation space suit design, oil extraction and accident mitigation, hardware-in-the-loop simulation, etc. Finally, this project will engage high-school students from under-represented groups in a six-day residential camp run (now at its 12th edition) and will train a group of undergraduate students from California State University at University of Wisconsin-Madison through a new residential program that will introduce them to the use of Chrono in simulation-based robotics design.This project seeks to augment modeling and solution methods employed by Chrono, a BSD3 open-source simulation platform for multi-body dynamics (MBD) and fluid-solid interaction (FSI) problems. The software infrastructure enhancements in this project aim at sustaining teraflops-grade simulation of MBD and FSI systems with more than ten billion degrees of freedom; i.e., two to three orders of magnitude beyond conventional simulations today. In order to increase adoption and impact, the performance levels aimed at will be reached on budget/affordable hardware that leverages GPU computing. Chrono will be able to capture micro-, meso- and macro-scale dynamics on time scales spanning from millisecond (impact phenomena) to decades (geophysics). The intellectual merit of this project stems from the following key ideas: (i) with an eye towards the sunsetting of Moore's law, the software design solution embraces a scalable multi-GPU hardware layout poised to solve effectively large multi-physics problems; (ii) a hardware-aware software design paradigm, which aggressively reduces data storage and movement, will allow budget-conscious hardware systems to run billion-degree-of-freedom models, or, for models of similar size, accomplish a two orders of magnitude speedup when compared to the state of the art; (iii) a unified Lagrangian formulation for both solid and fluid dynamics is implemented in one software platform that can simulate complex multi-physics (coupled) problems; and (iv) Chrono promotes an alternative approach for handling friction and contact that revolves around the concept of differential variational inequality and thus avoids the small integration time step and numerical instability issues that hinder most of the existing many-body dynamics simulators. In relation to its educational and outreach initiatives, this project: (a) will be instrumental in establishing a new University of Wisconsin-Madison undergraduate course that introduces students to computing concepts subsequently refined in a graduate advanced computing class; (b) will promote the discipline of Computational Science and Computational Dynamics at high-school and undergraduate levels via two yearly residential summer programs for under-represented students; (c) will expand an advanced computing forum that facilitates technology transfer to industry and promotes Chrono adoption; and, (d) will strengthen ongoing collaborations that critically depend on Chrono in robotics, geomechanics, and soft-matter physics. Chrono is presently cloned on average 10 times every day, has been forked from its public repository by more than 150 parties, and has an active forum with more than 250 registered users. This project will enhance the richness of Chrono modeling features, improve its numerical solution foundation, and leverage emerging hardware architectures to elevate this simulation capability to the status of ready-to-use, open-source, best-in-class computational dynamics platform.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Software: Future Proofing the Finite Element Library Deal.II -- Development and Community Building,OAC,1835673,Wolfgang Bangerth,bangerth@colostate.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","Partial differential equations (PDEs) are used as mathematical models throughout the natural sciences, engineering, and more recently also in the biomedical and social sciences as well as in finance. Their numerical solution is, consequently, of great relevance in understanding, accurately simulating, and optimizing natural, human, and engineered systems. In many applications, finite element methods (FEM) are the method of choice converting the PDE into finite dimensional, computationally solvable problems. The deal.II project is an open source FEM software library that enables scientists to solve PDEs across many disciplines, that supports simulation and computational discovery in virtually all parts of the sciences and engineering by providing tools to solve essentially all PDEs amenable to the FEM. In this project new capabilities will be added and the user and contributor community expanded to include additional science domains.Deal.II is a project with a thriving, world-wide user and developer community. This project will further enable its community of users and developers, by undertaking specifically for work that can either not be expected of volunteers, or that is necessary to strengthen the long-term independent sustainability of the project. Based on a recent user survey, the following work items in the following four categories will be addressed: 1. Foundational features too large or complicated to be tackled by volunteers: the team will research and implement efficient and scalable approaches to support parallel, adaptive multigrid and hp FEM. 2. Expand documentation and training modules through more tutorial programs and YouTube-hosted video lectures: This will further broaden the reach of the project and extend the education for the computational science community. 3. Continuous integration and packaging infrastructure to better support the pace of development. 4. Support and expand deal.II's thriving communities through a summer school, workshops, hackathons, and careful mentoring of newcomers.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Cybershuttle: An end-to-end Cyberinfrastructure Continuum to accelerate Discovery in Science and Engineering,OAC,2209873,Anton Arkhipov,antona@alleninstitute.org,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","Science depends critically on accessing scientific and engineering software, data repositories, storage resources, analytical tools, and a wide range of advanced computing resources, all of which must be integrated into a cohesive scientific research environment. The Cybershuttle project is creating a seamless, secure, and highly usable scientific research environment that integrates all of a scientist?s research tools and data, which may be on the scientist?s laptop, a computing cloud, or a university supercomputer. These research environments can further support scientific research by enabling scientists to share their research with collaborators and the broader scientific community, supporting replicability and reuse. The Cybershuttle team integrates biophysicists, neuroscientists, engineers, and computer scientists into a single team pursuing the project goals with a grounding in cutting-edge research problems such as understanding how spike proteins in viruses work, how the brain functions during sleep, and how artificial intelligence techniques can be applied to modeling engineering materials. To meet its ambitious goals, the project is building on over a decade of experience in developing and operating the open-source Apache Airavata software framework for creating science-centric distributed systems. Cybershuttle is providing a system that can be used as a training ground to educate students in concepts of open-source software development and applied distributed systems, fostering a globally competitive workforce who can move easily between academic and non-academic careers. Cybershuttle is creating a new type of user-facing cyberinfrastructure that will enable seamless access to a continuum of CI resources usable for all researchers, increasing their productivity. The core of the Cybershuttle framework is a hybrid distributed system, based on open-source Apache Airavata software. This system integrates locally deployed agent programs with centrally hosted middleware to enable an end-to-end integration of computational science and engineering research on resources that span users? local resources, centralized university computing and data resources, computational clouds, and NSF-funded, national-scale computing centers. Scientists and engineers access this system using scientific user environments designed from the beginning with the best user-centered design practices. Cybershuttle uses a spiral approach for developing, deploying, and increasing usage and usability, beginning with on-team scientists and expanding to larger scientific communities. The project engages the larger community of scientists, cyberinfrastructure experts, and other stakeholders in the creation and advancement of Cybershuttle through a stakeholder advisory board. Cybershuttle's team includes researchers from Indiana University, the University of Illinois at Urbana-Champaign, the University of California San Diego, the San Diego Supercomputer Center, and the Allen Institute.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Enriching Scholarly Communication with Augmented Reality,OAC,2209625,Joshua E.G. Peek,jegpeek@stsci.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Today?s online shoppers can use augmented reality (AR) to aim their smartphones at an empty corner of their living room to see how a particular new lamp might look there, and diners can instantly see a restaurant's menu on their phones just by scanning a QR code posted at their table. This project will leverage the tremendous investments in AR made by the corporate world over the past several years, and the familiar ease of QR codes, to allow astronomers to see and explore the 3D Universe just as easily as they might shop for a new couch. Building on their 2021 success in publishing the first AR-enhanced figure in an American Astronomical Society Journal, the funding from this award will be used to create a robust system allowing any author to publish figures showcasing high-dimensional data in augmented reality environments. No expensive equipment beyond the same smartphones and tablets used by online shoppers will be needed. Astronomers will be able to see and explore their data in ""3D"" by walking around projections of it hovering above flat surfaces, or holding in their hands using AR target devices. Imagine, for example, a jet from a black hole, spewing out material from the center of a simulated galaxy, projected just above a researcher?s kitchen table, etc.Over the course of the project, the team will design, repeatedly test, and ultimately deploy an efficient and effective end-to-end system for embedding augmented reality figures in scholarly journals. By enriching scholarly communication, this new AR-based system is expected to accelerate the pace of scientific discovery. The system created will extend across multiple modular cyberinfrastructure components, including: data format standards; data analysis software; 3D conversion tooling; AR integration pipelines; visual ID encoding infrastructure; and the publication process. The system for authoring and deploying AR figures created and tested under this proposal represents cyberinfrastructure innovation that will ultimately open completely new channels for communication amongst all who rely on effective communication of high-dimensional data.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate for Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: A Software Ecosystem for Plasma Science and Space Weather Applications,OAC,2209471,Amitava Bhattacharjee,amitava@princeton.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","This project advances the frontiers of global simulations of space and laboratory plasma systems, and makes these advances accessible to a broad community of users. The plasma states in such systems are often in weakly collisional regimes where traditional fluid models break down. To describe such systems, the project team is developing Gkeyll, a multi-fluid, multi-moment model that incorporates important kinetic effects. The model can be used for high-fidelity and accurate predictions for space weather events, and for the design and study of laboratory experiments engaged in discovery plasma science. The project will develop comprehensive cyberinfrastructure (CI) to support professional scientists in the execution of space weather and basic plasma physics simulations over a wide variety of computing platforms from local clusters to high-performance computing facilities. Gkeyll CI to be developed includes (1) providing web-based user interfaces for defining, executing, and managing simulations, (2) workflows consisting of of pre-defined sequences of tasks, and (3) user-friendly tools that implement the workflow tasks. Gkeyll science will be facilitated by algorithmic advances. Gkeyll is open-source and provides the space and laboratory plasma science communities with a modern and versatile software ecosystem. The project team is strongly interdisciplinary, including space and laboratory plasma scientists, applied mathematicians, computer scientists and software engineers, and the project will promote diversity in professional training and education.The CI to be built in the project is centered on the Gkeyll framework that consists of advanced plasma simulation models, tools, and automated continuous integration of new software. The project will provide a uniform and complete open-source solution with workflow control to model plasma systems from terrestrial magnetospheres to laboratory experiments in collisionless or weakly collisional plasma regimes, which are essential for space weather and laboratory plasma experiments. The project will extend and enhance the core algorithms in Gkeyll and integrate the software with the NSF Science Gateway Platform as a Service (SciGaP). Gkeyll science is facilitated by core modeling and PDE discretization advances, including automated and adaptive gridding for relevant sets of parameterized geometries that are properly matched to the equation discretization methods. The project will employ modern software engineering practices to produce scalable, secure, and high-fidelity physics-based software.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Physics within the NSF Directorate for Mathematical and Physical Sciences, and the Division of Atmospheric and Geospace Sciences and the Division of Research, Innovation, Synergies, and Education within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Frameworks: Target and Observation Manager Systems for Multi-Messenger and Time Domain Science,OAC,2209852,Rachel Street,rstreet@lcogt.net,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Current and near-future U.S.-led astronomy surveys offer a revolutionary opportunity to explore physical phenomena through multiple messengers and instruments, associating gravitational wave detections with neutrino signatures and electromagnetic counterparts. The combination provides powerful new insights relevant to a broad range of research in the astrophysical and physics communities. The scale of these surveys means that unprecedented, petabyte scale data volumes and their rapid delivery must be managed effectively to fully realize their scientific potential, but these data alone cannot fully characterize the discoveries. Additional observations from facilities with complementary instrumentation are normally required, but obtaining these in a timely manner can present serious technical challenges, both to researchers trying to identify and characterize time-sensitive targets of interest, and to the infrastructure of these facilities. Innovative new software tools enable researchers to easily build a database to manage all aspects of their science projects, including conducting observations, through a single programmatic interface. The capabilities of these tools are expanding to ensure that they are seamlessly integrated with other, rapidly developing linchpin scientific software services, especially the range of facilities where follow-up observations are conducted. This provides a critical link in the chain of discovery and characterization for many landmark surveys. Yet historically, such sophisticated database-driven systems have only been available to larger projects and institutions with in-house software teams. This perpetuates inequitable access to the scientific potential of major taxpayer-funded facilities. The well-documented open source software package to be developed in this project, called the TOM (Target and Observation Manager) toolkit, and an associated community development program, will make these professional research tools accessible to everyone, including educators and citizen scientists. Ensuring that researchers can receive information on new discoveries rapidly, manage and evaluate all relevant data on targets of interest, and conduct follow-up observations in a timely and effective manner, is essential to realizing the scientific potential of modern major astronomical surveys and missions. Petabyte-scale data products, delivered in real-time, are useful only if we can adequately handle the data rate. Recognizing this, much effort has been invested in generating the survey data products, but the tools used by most researchers to do science investigations with these products are often an ad hoc combination of text files, spreadsheets and other applications that do not scale well and are not designed to operate with other key services. The Big Data era demands scientific project management software that interface with existing services, such as discovery alert brokers and telescopes, to enable scientists to effectively collate information relevant to their interests, and to manage all aspects of follow-up observing programs. Modern, browser-based data exploration and visualization tools enable a diverse range of science, and well-designed public software can ensure that researchers spend time using these tools for science instead of repeatedly re-inventing them. A partnership between researchers and software engineers ensures that science can take advantage of the latest developments from industries where handling Big Data is already standard. The TOM software package provides this critical tool set, ensures that it remains fully integrated with the evolving services in the field, and expands its capabilities to meet the needs of users. This project advances the goals of the NSF Windows on the Universe Big Idea.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Simulating Autonomous Agents and the Human-Autonomous Agent Interaction,OAC,2209795,Chen Li,lichen98@gmail.com,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","This project augments the Chrono computer simulation platform in transformative ways. Chrono's purpose is to predict through simulation the interplay between mechatronic systems, the environment they operate in, and humans with whom they might interact. The open-source simulation platform is slated to become a community-shared virtual investigation tool used to probe competing engineering designs and test hypotheses that would be too dangerous, difficult, or costly to verify through physical experiments. Chrono has been and will continue to be used in multiple fields and disciplines, e.g., terramechanics, astrophysics; soft matter physics; biomechanics; mechanical engineering; civil engineering; industrial engineering; and computer science. Specifically, it is used to engineer the 2023 VIPER lunar rover; relied upon by US Army experts in evaluating its wheeled and tracked vehicle designs; used in the US and Germany in the wind turbine industry; and involved in designing wave energy conversion solutions in Europe. Upon project completion, Chrono will become a simulation engine in Gazebo, which is widely used in robotics research; operate on the largest driving simulator in the US; empower research in the bio-robotics and field-robotics communities; and assist efforts in the broad area of automotive research carried out by a consortium of universities and companies under the umbrella of the Automotive Research Center. The educational impact of this project is threefold: training undergraduate, graduate, and post-doctoral students in a multi-disciplinary fashion that emphasizes advanced computing skills development; anchoring two new courses in autonomous vehicle control and simulation in robotics; and broadening participation in computing through a residential program on the campus of the University of Wisconsin-Madison that engages teachers and students from rural high-schools. Innovation and discovery are fueled by quality data. At its core, this project seeks to increase the share of this data that has simulation as its provenance. In this context, a multi-disciplinary team of 40 researchers augments and validates a physics-based simulation framework that empowers research in autonomous agents (AAs). The AAs operate in complex and unstructured dynamic environments and might engage in two-way interaction with humans or other AAs. This project enables Chrono to generate machine learning training data quickly and inexpensively; facilitates comparison of competing designs for assessing trade-offs; and gauges candidate design robustness via testing in simulation of corner-case scenarios. These tasks are accomplished by upgrading and extending Chrono to leverage recent computational dynamics innovations, e.g., a faster index 3 differential algebraic equations solver; a new approach to solving frictional contact problems; a real-time solver for handling flexible-body dynamics in soft robotics via nonlinear finite element analysis; a best-in-class simulator for terradynamics applications; reliance on just-in-time compiling for producing executables that are both problem- and hardware-optimized; a novel way for using mixed data representations for parsimonious storing of state information; and a scalable multi-agent framework that enables geographically-distributed, over the Internet, real-time simulation of human-AA interaction.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Proposal: Frameworks: Sustainable Open-Source Quantum Dynamics and Spectroscopy Software,OAC,2103902,Sharon Hammes-Schiffer,sharon.hammes-schiffer@yale.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","With support from the Office of Advanced Cyberinfrastructure and the Division of Chemistry at NSF, Professor Li and his team will work to expand the capabilities of the open-source software platform, Chronus Quantum (ChronusQ), to include time-resolved spectroscopy for supporting quantum chemistry calculations. The new capabilities will include novel computational methods to provide unprecedented capabilities to simulate chemical processes of electrons and nuclei that exhibit quantum behaviors. The physical insights gleaned through applications of ChronusQ underlie the advancement of new technologies that are crucial to sustainable energy, catalysis, quantum computing, and other applications that can immediately impact society. This project provides a mechanism for advanced interdisciplinary education and training in the areas of inorganic, theoretical, physical, and materials chemistry. The collaborative academic research serves as a test ground for identifying and deploying ways that the scientific community as a whole can both increase awareness of the importance of active engagement in professional skill development for graduate students and post-doctoral scholars, and develop tools to facilitate professional development in an academic setting.ChronusQ seamlessly integrates time-dependent quantum mechanical theories, spectral analysis tools, and modular high-performance numerical libraries that are highly parallelized, extensible, reusable, community-driven, and open-sourced. The Team develops in ChronusQ the complete time-dependent quantum description of coupled nuclear and electronic dynamics within the time-dependent density functional theory and equation-of-motion coupled cluster framework. The project enables computational studies of ultrafast time-resolved spectroscopies and simulations of chemical processes in the strongly nonadiabatic regime. Software modules are bolstered by algebraic and integral acceleration engines that can make it feasible to simulate fully quantum mechanically molecular dynamics. The collaborative project advances the theoretical description of quantum dynamics across time scales, bridging the attosecond and subnanosecond regimes, enabling the development of spectroscopic technologies to probe molecular and materials properties with state specificity that is beyond the Born-Oppenheimer approximation.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Frameworks: Collaborative Research: Integrative Cyberinfrastructure for Next-Generation Modeling Science,OAC,2103878,Mark Piper,mark.piper@colorado.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","This project is designed to support and advance next generation, interdisciplinary science of the complexly interacting societal and natural processes that are critical to human life and well-being. Computational models are powerful scientific tools for understanding these coupled social-natural systems and forecasting their future conditions for evidenced-based planning and policy-making. This project is led by the Network for Computational Modeling in Social and Ecological Sciences (CoMSES.Net). CoMSES.Net's science gateway promotes knowledge sharing among scientists and with the general public, and enables open, online access to sophisticated computational models of social and ecological systems. CoMSES.Net's partners in this project (the Community Surface Dynamics Modeling System and Consortium of Universities for the Advancement of Hydrologic Science) also enable knowledge sharing and provide open, online repositories of models in the earth sciences. This project will enhance these science gateways and create online educational materials to make these critical technologies easier to find, understand, and use for scientists and non-scientists alike. By integrating innovative technology with training and incentives to engage in best practice standards, this project will stimulate innovation and diversity in modeling science. It will enable researchers to build on each other's work and combine it in new ways to address societal and environmental challenges. The cybertools and educational programs developed in the project will be openly accessible not just to research institutions but also to smaller colleges, state and local governments, and a broader audience beyond the science community. The project will give decision-makers and the data scientists who support them access to a larger and more varied toolkit with which to explore potential solutions to societal and environmental policy issues. A long-term aim of the project is to support an evolving ecosystem of diverse, reusable, and combinable models that are transparently accessible to anyone in the world. Sustainable planetary care and management is a challenge that confronts all of humanity, and requires knowledge, histories, methods, perspectives, and engagement of researchers, decision-makers, and private citizens across the country and throughout the world.The project will develop an Integrative Cyberinfrastructure Framework (ICF) to enable innovative next-generation modeling of human and natural systems, and build capacity in modeling science. It will support a set of activities that integrate the human and technological components of cyberinfrastructure. 1) Software tools will be developed that augment model codebases with modern software development scaffolding to facilitate reuse, integration, and validation of model code. 2) The project will provide high-throughput computing (HTC) resources for simultaneously running numerous iterations of models needed to capture stochastic variability, explore a parameter space, and generate alternative scenarios; 3) Online training activities will build expertise and capacity to make effective use of the cybertools and the HTC resources; 4) The ICF will engage a global modeling science community to provide professional incentives that encourage researchers to adopt best practices and catalyze innovative science. Leveraging existing NSF investments, the ICF will be developed and deployed by the Network for Computational Modeling in Social and Ecological Sciences (CoMSES.Net), in partnership with the Community Surface Dynamics Modeling System (CSDMS), Consortium of Universities for the Advancement of Hydrologic Science (CUAHSI), Open Science Grid, Big Data Hub/Spoke network, and Science Gateways Community Institute. Computational models have emerged as powerful scientific tools for understanding coupled social-biogeophysical systems and generating forecasts about future conditions under a range of climate, biogeophysical, and socioeconomic conditions. CoMSES.Net, CSDMS, and CUASI are scientific networks, with online science gateways and code archives that enable open access to computational models for an international community of social, ecological, environmental, and geophysical scientists. However, the full value of accessible, well-documented models only can be realized if their code is also widely reproducible and reusable, with a potential for integration with other models. In order to confront critical challenges for understanding the coupled human and natural systems of today's world, modeling scientists also need HTC environments for upscaling models and exploring high-dimensional parameter spaces inherent in representing these systems. The ICF is designed to meet these challenges. By integrating technology with intellectual capacity-building, the ICF will stimulate innovation and diversity in modeling science by letting creative researchers build on each other's work more readily and combine it in new ways to address societal-environmental challenges we have not yet perceived. The tools and training resources will be openly accessible not just to leading research institutions but also to the many smaller colleges, state and local governments, and a broader audience beyond science. They will provide decision-makers and the data scientists who support them access to a much larger and more varied toolkit with which to explore potential solution spaces to social and environmental policy issues. The proposed ICF is also designed to help transform scientific modeling practice, including incentives that can help early career researchers shift from creating models to solve problems specific to a particular project to models that are also useful for others. The project will help support a future evolving ecosystem of diverse, reusable, and integrable models that are transparently accessible to the broader community.This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering, with the Division of Social and Economic Sciences in the Directorate for Social, Behavioral & Economic Sciences also contributing funds.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Convergence of Bayesian inverse methods and scientific machine learning in Earth system models through universal differentiable programming,OAC,2147601,Mathieu Morlighem,Mathieu.Morlighem@dartmouth.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Understanding and quantifying parameter sensitivity of simulated systems, such as the numerical models of physical systems and mathematical renderings of neural networks, are essential in simulation-based science (SBS) and scientific machine learning (SciML). They are the key ingredients in Bayesian inference and neural network training. Seizing on the opportunity of emerging open-source Earth system model development in the Julia high-level programming language, this project is endowing these open-source models with automatic differentiation (AD) enabled derivative information, making these converging data science and simulation-based science tools available to a much broader research and data science community. Enabling a general-purpose AD framework which can handle both large-scale Earth system models as well as SciML algorithms, such as physics-informed neural networks or neural differential equations, will enable seamless integration of these approaches for hybrid Bayesian inversion and Bayesian machine learning. It merges big data science, in which available data enable model discovery with sparse data science, and the model structure is exploited in the selection of surrogate models representing data-informed subspaces and fulfilling conservation laws. The emerging Julia language engages a new generation of researchers and software engineers, channeling much needed talent into computational science approaches to climate modeling. Through dedicated community outreach programs (e.g., Hackathons, Minisymposia, Tutorials) the project team will be working toward increasing equity, diversity, and inclusion across the participating disciplines.The project is developing a framework for universal differentiable programming and open-source, general-purpose AD that unifies these algorithmic frameworks within Julia programming language. The general-purpose AD framework in Julia leverages the composability of Julia software packages and the differentiable programming approach that underlies many of the SciML and high-performance scientific computing packages. Compared to most current modeling systems targeted for HPC, Julia is ideally suited for heterogeneous parallel computing hardware (e.g., CUDA, ROCm, oneAPI, ARM, PowerPC, x86 64, TPUs). The project is bringing together expertise in AD targeted at Earth system data assimilation in high performance computing environments with SciML expertise. The project team is working with the Julia Computing organization and package developers to ensure sustainability of the developed frameworks. The project?s Earth system flagship applications consist of (i) an open-source, AD-enabled ocean general circulation model that is being developed separately as part of the Climate Modelling Alliance (CliMA), and (2) an open-source, AD-enabled ice flow model. Each of these application frameworks is being made available to the community for science application, in which derivative (gradient or Hessian) information represent key algorithmic enabling tools. These include SciML-based training of surrogate models (data-driven and/or model-informed), parameter and state estimation, data assimilation for model initialization, uncertainty quantification (Hessian-based and gradient-informed MCMC) and quantitative observing system design. Academic and industry partners are involved, who are using the frameworks for developing efficient power grids, personalized precision pharmacometrics, and improved EEG design.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Crowdsourced Materials Data Engine for Unpublished XRD Results,OAC,2104007,Yinghui Wu,yinghuiwu.ed@gmail.com,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Although data-driven analysis has been heralded as a new paradigm in fundamental material science such as X-ray Diffraction (XRD) analysis, high-value material datasets are often not made public and are underutilized. This project designs and develops CRUX, a crowdsourced data infrastructure and services to curate, discover, share, and recommend unpublished XRD data and analytical results. CRUX promotes underutilized high-quality material science data by allowing the sharing and exploration of unpublished data with state-of-the-art crowdsourcing, knowledge harvesting, and machine learning techniques. CRUX provides a crowdsourced knowledge base to allow scientists and the general public to share and access unpublished data resources. It also provides (a) a novel search engine that supports simple keyword search, can provide relevant data resources when the exact keyword matching does not exist, and self-evolves to improve the search quality, and (b) a ""data feed"" service to allow users to easily receive and track updates of specific data resources of interest. The developed infrastructure and tools enable an open, collaborative, and sustainable platform that can facilitate exchanging of unpublished XRD data and discoveries, unlock new research problems (e.g., predictive analysis of materials compositions with multi-phase data), and inspire the novel design of machine learning pipelines (e.g., deep neural networks) for data-driven materials science. CRUX will make materials data resources available and shareable for a broad community including materials scientists, data analysts, software developers, and the general public, and thus promote long-term collaborative research, software development, and education. The developed CRUX system enables (1) coherent representation of materials data, metadata, and knowledge in terms of a three-tier knowledge graph model; (2) scalable XRD metadata curation and information extraction techniques to promote high-value unpublished XRD data sources for data-driven materials research; (3) adaptive, self-improving search and recommendation techniques to recommend relevant datasets upon user requests and feedback, with sustainability beyond the time of the project; and (4) interactive and exploratory search techniques to explain and recommend the relevant datasets beyond the scope of initial queries. CRUX will be evaluated with established human-in-the-loop knowledge bases and active machine learning algorithms by cornerstone materials research such as the discovery of new high-temperature ferroelectrics. The research community will be able to share XRD data resources (analytical results, machine learning models, processing data) via ""one-click"" upload, search for high-quality data resources, and (re)discover new resources for machine learning pipelines. CRUX enables several components to advance data-driven materials research, including a materials knowledge graph model, automatic data integration, and exploratory query engine that support ""Why"" and ""What-if"" analysis for XRD analysis. Developed solutions will benefit data-driven material science in general. For example, researchers can make use of unpublished two-phase data to predict new materials compositions, identify solubility limits through parameterization by machine learning tools, and refine machine learning models with more sophisticated techniques such as deep neural networks.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: SciMem: Enabling High Performance Multi-Scale Simulation on Big Memory Platforms,OAC,2103967,Zhen Li,zli7@clemson.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Increasing system scalability is crucial to improving nation?s computation capabilities for scientific applications. However, some applications often face the scalability challenge from the perspective of memory capacity. This is especially true in multi-scale simulations when handling massive simulation data from different scales. The emerging big memory infrastructures have shown great potential to increase the simulation scale and solve larger numerical problems. However, using big memory architectures for the multi-scale simulation is challenging, because of limited computing capability in the big memory machines and memory heterogeneity introduced by big memory. There is a lack of a software infrastructure that can release the full power of big memory to accelerate multi-scale simulation. This project aims to create a capability and a software package (named SciMem) that enables high performance multi-scale simulation on big memory platforms. The techniques presented offer a path for general use of this structure for a wide variety of applications having a broad impact on science and engineering. There will be impact on the students through their direct involvement with the project and through the integration with the educational activities.The project will enable high performance multi-scale simulations on big memory platforms through more efficient utilization of large and heterogeneous memory machines. Specifically, it will replace computations with pre-computed and stored in memory data on a heterogeneous computing systems. The developed tool, SciMem, will be integrated and tested with the popular parallel molecular dynamics simulator, LAMMPS (Large-scale Atomic/Molecular Massively Parallel Simulator). The developed improvements in the use of computational resources will allow more accurate models of complex physical phenomena to be carried out on the emerging hardware systems. SciMem aims to bring a 10x performance improvement for certain larger-scale multi-scale simulations widely applied in the fields of computational chemistry and material science, e.g., quantum mechanical/molecular mechanical-based molecular dynamics (MD) simulation of catalysis.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: SENSORY: Software Ecosystem for kNowledge diScOveRY - a data-driven framework for soil moisture applications,OAC,2103845,Michela Taufer,taufer@utk.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Tools for gathering soil moisture data (such as in situ soil sensors and satellites) have differing capabilities. In situ soil moisture data has fine-grained spatial and high temporal resolution, but is only available in limited areas; satellite data is available globally, but is more coarse in resolution. Existing software tools for studying the dynamic characteristics of soil moisture data are limited in their ability to model soil moisture at multiple spatial and temporal scales, and these limitations hamper scientists? ability to address urgent practical problems such as wildfire management and food and water security. Accurate gathering and effective modeling of soil moisture data are essential to address pressing environmental challenges. This interdisciplinary project designs, builds, and shares a data-driven software ecosystem for soil moisture applications. This software ecosystem models and predicts soil moisture at scales suitable to support studies in forestry, precision agriculture, and earth surface hydrology.This project connects multi-disciplinary advances across the scientific community (such as generating datasets at scale and supporting cloud-based cyberinfrastructures) to develop a data-driven software ecosystem for analyzing, visualizing, and extracting knowledge from the growing data collections (from fine-grained, in situ soil sensor information to coarse-grained, global satellite measurements) and releasing this knowledge to applications in environmental sciences. Specifically, this project (a) develops scalable methodologies to integrate and analyze soil moisture data at multiple spatial and temporal scales; (b) implements a data-driven software ecosystem to access complex information and provide basic and applied knowledge to inform researchers and stakeholders interested in soil moisture dynamics (scientists, educators, government agencies, policy makers); and (c) builds cyberinfrastructures to support discovery on cloud platforms, lowering resource barriers to improve accessibility and interoperability.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Hydrologic Sciences Program, the Division of Earth Sciences, and the Division of Integrative and Collaborative Education and Research within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Elements: Developing an integrated modeling platform for tectonics, earthquake cycles and surface processes",OAC,2104002,Eunseo Choi,echoi2@memphis.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","This project develops user-friendly and sustainable code that can simulate lithospheric deformations coupled with landscape evolution and earthquake cycles. Natural hazards such as earthquakes, flooding, landslides, and volcanoes sit at the intersection of geologic and human time scales; the ability to simulate interactions among diverse geologic processes provides a baseline to study how global changes are perturbing these processes on human time scales. The goal of the project is to view earthquake cycles and landscape evolution as a system coupled with long-term lithospheric deformation. Multiple research communities including geomorphology, structural geology and earthquake physics are expected to benefit from the new modeling capability this project enables. The product software extends existing open-source code, DES3D (Dynamic Earth Solver in 3D). With internal enhancements and scalable performance enabled by parallel computing technology, the software functions as a reliable integrated modeling platform that can explore interplay between tectonics, surface processes and earthquake cycles. The project includes measures to improve user experience with the product code; Web-based input file generation, useful documentation and tutorials readable and executable online will make the product code accessible and easy to use. To facilitate sustained development and maintenance of the product code, modern software engineering practices are adopted. Container-based technology is built into the product code to create reproducible packages of a model without user intervention. To ensure that the product code will be a valuable addition to the existing cyberinfrastructure, this project will actively leverage expertise from collaborators who have conducted related research. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Geophysics Program, the Tectonics Program, the Geomorphology and Land-Use Dynamics Program of the Division of Earth Sciences, and the Division of Integrative and Collaborative Education and Research within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Framework: Awkward Arrays - Accelerating scientific data analysis on irregularly shaped data,OAC,2103945,Jim Pivarski,pivarski@princeton.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","Nowadays, scientists need to be programmers as well as experts in their fields. Any software that simplifies the computational gymnastics of analyzing data is welcomed, as it allows scientists to focus more of their attention on what they want to compute, rather than how, but there is usually a tension between ease of use and computational speed. Faster computation means more data analyzed, but error-free code matters most. Awkward Array is a software library that performs calculations on data that do not fit into neat rows. For instance, a fleet of undersea probes may each measure ocean temperatures at a different number of depths, but data analysis tools like Excel, Pandas, and SQL require measurements to be arranged in rectangular tables with the same number of columns in each row. The problem is more acute when variable numbers of measurements are nested within variable numbers of entities. Traditionally, scientists have either used slow scripting languages or unforgiving ""bare metal"" languages to perform calculations on these non-tabular datasets. Awkward Array generalizes array concepts so that the easy and fast expressions that once applied only to rectangular tables now apply to irregularly shaped data, allowing bare metal speed in a high-level scripting language. This project broadens the applicability of Awkward Array beyond the problem domain it was originally intended for, particle physics, to a wide variety of scientific fields, including oceanography, astronomy, genetics, chemistry, and health care. It also integrates the Awkward Array library with popular data science and machine learning tools and extends the implementation to GPUs for extremely fast processing of the same array idioms.Scientists use NumPy-based tools, such as Pandas, to analyze large and regularly shaped data efficiently. Packing tables of numbers into contiguous arrays allows operations to be precompiled and fast; and expressing operations as concise commands with implicit loops makes them easier to read and quicker to type during data exploration. However, scientific data often has a complexity that does not fit well into tabular format. This forces scientists to write programs with explicit loops, which are slow in Python. But what about analysis of large and irregularly shaped data? Awkward Array is a generalization of NumPy; it is a Python library defining arrays of objects with arbitrary types and a suite of generic operations on them. Like NumPy arrays, Awkward Arrays are packed into contiguous buffers for efficiency, but unlike NumPy arrays, they can include variable-length lists, nested fields, missing values, and mixed types. Awkward Arrays use 10 times less memory than equivalent Python objects and are up to 100 times faster in computations. This project generalizes Awkward Array as a foundational library for science. It is a cross-disciplinary effort, extending Awkward Array to efficiently solve challenges faced by scientists in a variety of fields and data science in industry. The project members work with scientific collaborators to solve specific problems, adding features to Awkward Array if necessary, and industry collaborators at Anaconda and Nvidia update Python?s standard tools to recognize these new array types for CPU and GPU workloads. It is also an educational project, teaching new approaches to complex problems using array idioms, both to practicing scientists and to students.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Bifrost - A CPU/GPU Pipeline Framework for High Throughput Data Acquisition and Analysis,OAC,2103771,Christopher League,christopher.league@liu.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Modern computers, including cell phones and tablets, have sophisticated Graphics Processing Units (GPUs) that render the beautiful graphic displays in games. We are developing software that takes advantage of these same GPUs for capturing and processing data from astronomical telescopes. This allows us to benefit from all the years of effort spent developing these powerful computational tools. This software, known as Bifrost, is currently in use at the Long Wavelength Array (LWA), a radio telescope for exploration of a broad scientific portfolio ranging from the study of Cosmic Dawn when the first stars and galaxies lit up the Universe, to understanding the properties of the Earth's ionosphere. We are actively developing Bifrost to make it both more powerful and easier to use for other telescopes. Eventually we aim for Bifrost to be available as a more general purpose framework that can be applied to research projects beyond astronomy. About 5 years ago we adopted a commodity equipment design for the second LWA station (LWA-SV) which makes use of computing servers with GPUs to handle the data capture, beamforming, and correlation at the station level. Previously these functions were taken on by dedicated hardware referred to as the Digital Processor. However, this custom-hardware design was expensive to build and maintain, lacks flexibility, and cannot be easily replicated for future LWA stations. In contrast the commodity approach is easier to maintain, much more flexible and expandable, and can be readily adapted to new LWA stations. We are engaged in a concentrated effort to improve the underpinnings of Bifrost. This involves increasing the data rates that Bifrost is capable of handling, improving the application programming interface, and providing tools to make it easier for users to develop and test new pipelines. Through this award we are working with collaborators to incorporate Bifrost in telescopes and instruments currently under development. The availability of Bifrost will increase the scientific return of not only radio astronomy but also other areas where high throughput data processing is needed.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: EdgeVPN: Seamless Secure VirtualNetworking for Edge and Fog Computing,OAC,2004323,Cayelan Carey,cayelan@vt.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","Edge computing encompasses a variety of technologies that are poised to enable new applications across the Internet that support data capture, storage, processing and communication near the edge of the Internet. Edge computing environments pose new challenges, as devices are heterogeneous, widely distributed geographically, and physically closer to end users, such as mobile and Internet-of-Things (IoT) devices. This project develops EdgeVPN, a software element that addresses a fundamental challenge of networking for edge computing applications: establishing Virtual Private Networks (VPNs) to logically interconnect edge devices, while preserving privacy and integrity of data as it flows through Internet links. More specifically, the EdgeVPN software developed in this project addresses technical challenges in creating virtual networks that self-organize into scalable, resilient systems that can significantly lower the barrier to entry to deploying a private communication fabric in support of existing and future edge applications. There are a wide range of applications that are poised to benefit from EdgeVPN; in particular, this project is motivated by use cases in ecological monitoring and forecasting for freshwater lakes and reservoirs, situational awareness and command-and-control in defense applications, and smart and connected cities. Because EdgeVPN is open-source and freely available to the public, the software will promote progress of science and benefit society at large by contributing to the set of tools available to researchers, developers and practitioners to catalyze innovation and future applications in edge computing.Edge computing applications need to be deployed across multiple network providers, and harness low-latency, high-throughput processing of streams of data from large numbers of distributed IoT devices. Achieving this goal will demand not only advances in the underlying physical network, but also require a trustworthy communication fabric that is easy to use, and operates atop the existing Internet without requiring changes to the infrastructure. The EdgeVPN open-source software developed in this project is an overlay virtual network that allows seamless private networking among groups of edge computing resources, as well as cloud resources. EdgeVPN is novel in how it integrates: 1) a flexible group management and messaging service to create and manage peer-to-peer VPN tunnels grouping devices distributed across the Internet, 2) a scalable structured overlay network topology supporting primitives for unicast, multicast and broadcast, 3) software-defined networking (SDN) as the control plane to support message routing through the peer-to-peer data path, and 4) network virtualization and integration with virtualized compute/storage endpoints with Docker containers to allow existing Internet applications to work unmodified. EdgeVPN self-organizes an overlay topology of tunnels that enables encrypted, authenticated communication among edge devices connected across disparate providers in the Internet, possibly subject to mobility and constraints imposed by firewalls and Network Address Translation, NATs. It builds upon standard SDN interfaces to implement packet manipulation primitives for virtualization supporting the ubiquitous Ethernet and IP-layer protocols.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: funcX: A Function Execution Service for Portability and Performance,OAC,2004894,Ian Foster,foster@uchicago.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","The funcX project is developing, deploying, and operating a new distributed computing cyberinfrastructure platform to enable researchers to build applications from programming functions that execute on different computing resources, from laptops to supercomputers. This cloud-hosted service democratizes access to advanced computing by providing intuitive interfaces for both registering remote computers as function executors and executing functions on these computers reliably, securely, and with high performance. Researchers can thus decompose monolithic applications into collections of reusable lightweight functions that can be run wherever makes the most sense, for example where data reside or where excess capacity is available. By simplifying access to specialized and high performance cyberinfrastructure and decreasing the time to discovery, the project serves the national interest, as stated in NSF's mission, by promoting the progress of science. A total of 33 diverse science, cyberinfrastructure, and software institute partners working with cutting-edge science applications and research cyberinfrastructure will directly benefit from the funcX platform.This project develops funcX, a scalable and high-performance federated platform for managing the remote execution of (often short-duration) functions across diverse cyberinfrastructure systems, from edge accelerators to clusters, supercomputers, and clouds. funcX allows developers to decompose applications into collections of functions that can each be executed in the best location, in terms of cost, execution time, data movement costs, and/or energy consumption. It thus integrates the extreme convenience of the function as a service (FaaS) model, developed in industry for specific industry applications, with support for the specialized needs of scientific research. funcX addresses important barriers to these new uses of research cyberinfrastructure systems, by enabling the intuitive, flexible, and scalable execution of functions without regard to physical location, scheduler architecture, virtualization technology, administrative domain, or data location. Flexible open-source funcX agent software makes it easy to expose arbitrary computing systems as funcX computing platforms, thereby transforming existing cyberinfrastructure systems into high-performance function serving environments (endpoints). The cloud-hosted funcX service provides a REST interface for registering functions, discovering available endpoints, and managing the execution of functions on endpoints, all via a universal trust fabric and standard web authentication and authorization mechanisms. It dynamically creates and deploys containers that incorporate function dependencies and provide a secure and isolated environment for safe function execution. The project engages a diverse set of 11 science partners, 18 research computing and cyberinfrastructure projects, and 4 NSF Software Institutes, each supporting many NSF-funded researchers, to provide use cases for funcX, shape its design, and evaluate its implementation.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: funcX: A Function Execution Service for Portability and Performance,OAC,2004932,Daniel Katz,dskatz@illinois.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","The funcX project is developing, deploying, and operating a new distributed computing cyberinfrastructure platform to enable researchers to build applications from programming functions that execute on different computing resources, from laptops to supercomputers. This cloud-hosted service democratizes access to advanced computing by providing intuitive interfaces for both registering remote computers as function executors and executing functions on these computers reliably, securely, and with high performance. Researchers can thus decompose monolithic applications into collections of reusable lightweight functions that can be run wherever makes the most sense, for example where data reside or where excess capacity is available. By simplifying access to specialized and high performance cyberinfrastructure and decreasing the time to discovery, the project serves the national interest, as stated in NSF's mission, by promoting the progress of science. A total of 33 diverse science, cyberinfrastructure, and software institute partners working with cutting-edge science applications and research cyberinfrastructure will directly benefit from the funcX platform.This project develops funcX, a scalable and high-performance federated platform for managing the remote execution of (often short-duration) functions across diverse cyberinfrastructure systems, from edge accelerators to clusters, supercomputers, and clouds. funcX allows developers to decompose applications into collections of functions that can each be executed in the best location, in terms of cost, execution time, data movement costs, and/or energy consumption. It thus integrates the extreme convenience of the function as a service (FaaS) model, developed in industry for specific industry applications, with support for the specialized needs of scientific research. funcX addresses important barriers to these new uses of research cyberinfrastructure systems, by enabling the intuitive, flexible, and scalable execution of functions without regard to physical location, scheduler architecture, virtualization technology, administrative domain, or data location. Flexible open-source funcX agent software makes it easy to expose arbitrary computing systems as funcX computing platforms, thereby transforming existing cyberinfrastructure systems into high-performance function serving environments (endpoints). The cloud-hosted funcX service provides a REST interface for registering functions, discovering available endpoints, and managing the execution of functions on endpoints, all via a universal trust fabric and standard web authentication and authorization mechanisms. It dynamically creates and deploys containers that incorporate function dependencies and provide a secure and isolated environment for safe function execution. The project engages a diverse set of 11 science partners, 18 research computing and cyberinfrastructure projects, and 4 NSF Software Institutes, each supporting many NSF-funded researchers, to provide use cases for funcX, shape its design, and evaluate its implementation.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: SHF: Medium: Practical and Rigorous Correctness Checking and Correctness Preservation for Irregular Parallel Programs,CCF,1955852,Stephen Siegel,siegel@udel.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Many important ? and in some cases, lifesaving ? computations are performed on graph structures consisting of millions of vertices and edges. For example, such graphs might represent medical information, protein interactions, or taxonomies of diseases. Since these graphs tend to be large, they are processed in parallel to fully harness the speed offered by modern computers, which use multicore processors and often general-purpose Graphics Processing Units (GPUs). Unfortunately, parallelizing graph computations is difficult, especially for GPUs,and often leads to accidental uncoordinated accesses known as data races. Data races can be hard to track down as they only sometimes corrupt the result. The project's novelties are the development of scalable and mathematically sound methods for data-race and other bug detection on graph computations. The project's main impact is the elimination of many human programming errors to improve the trust in computations carried out on life-critical and other data.The project develops generic symbolic representations of allowed concurrent operations on primitive data operations. This provides theability to easily boil down new concurrency models into this semantic base to quickly create new analysis tools, thus counteracting verification tool obsolescence. It augments the power of small-scope symbolic-analysis methods with execution-based dynamic-analysis methods that scale to realistic code and data sizes. The project derives real-world case studies from high-performance CUDA and OpenMP implementations of important graph algorithms developed over a decade. The project plans to publicly release the new data-race checking tools as well as verification micro-benchmarks and rigorously verified parallel graph codes. It is also training students whose education is advanced by teaching them modern program analysis methods.This award is co-funded by the Software & Hardware Foundations Program in the Division of Computer & Computing Foundations, and the NSF Office of Advanced Cyberinfrastructure.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: SHF: Medium: Bringing Python Up to Speed,CCF,1955565,Benjamin Pierce,bcpierce@cis.upenn.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","The Python programming language is among today's most popular computer programming languages and is used to write software in a wide variety of domains, from web services to data analysis to machine learning. Unfortunately, Python?s lightweight and flexible nature -- a major source of its appeal -- can cause significant performance and correctness problems - Python programs can suffer slowdowns as high as 60,000x over optimized code written in traditional programming languages like C and C++, and can require an order-of-magnitude more memory. Python's flexible, ?dynamic? features also make its programs error-prone, with many coding errors only being discovered late in development or after deployment. Python?s frequent use as a ""glue language"" -- to integrate and interact with different components written in C or C++ -- exposes many Python programs to the unique dangers of those languages, including susceptibility to memory corruption-based security vulnerabilities. This project aims to remedy these problems by developing new technology for Python in the form of novel performance analysis tools, memory-reduction and speed-improving optimizations (including support for multi-core execution), automated software testing frameworks, and common benchmarks to drive their evaluation.This project will develop (1) performance analysis tools that help Python programmers accurately identify the sources of slowdowns; (2) techniques for automatically identifying code that can be replaced by calls to C/C++ libraries; (3) an approach to unlocking parallelism in Python threads, which currently must execute sequentially due to a global interpreter lock; and (4) automatic techniques to drastically reduce the memory footprints of Python applications. To improve the correctness of Python applications, the project will develop novel automated testing techniques that (1) augment property-based random testing with coverage-guided fuzzing; (2) employ concolic execution for smarter test generation and input minimization; (3) synthesize property-specific generator functions; (4) leverage statistical clustering techniques to reduce duplicated failure-inducing inputs; and (5) leverage parallelism and adaptive scheduling algorithms to increase testing throughput. The project will develop a set of ""bug benchmarks"" -- indeed, a novel benchmark-producing methodology -- to evaluate these techniques. The twin threads of performance and correctness are synergistic and complementary: automatic testing drives performance analysis, while performance optimizations (like parallelism) speed automatic testing.This award is co-funded by the Software & Hardware Foundations Program in the Division of Computer & Computing Foundations, and the NSF Office of Advanced Cyberinfrastructure.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Cyberloop for Accelerated Bionanomaterials Design,OAC,1931343,Wonpil Im,woi216@lehigh.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","The evolution of biological and materials systems must be understood at many scales in order to achieve groundbreaking advances. Areas that are impacted include the health sciences, materials sciences, energy conversion, sustainability, and overall quality of life. Molecular simulations using complex models and configurations play an increasing role in such efforts. They address the limitations of experiments which study events over very small time and length scales. Such simulations require great expertise due to the complexity of the systems being studied. and the tools being used. This is particularly true for systems containing both inorganic and biological materials. This project will help researchers to quickly set up complex simulations, carry out the simulations with high accuracy, and assess uncertainties in the results. They will help develop the Cyberloop computational infrastructure. Cyberloop will dramatically reduce the time required to perform state-of-the-art simulations. They will also help to educate the next generation of researchers in this important field.Cyberloop will integrate three existing successful platforms for soft matter and solid state simulations (IFF, OpenKIM, and CHARMM-GUI) into a single unified framework. These systems will work together to enable users to set up complex bionanomaterial configurations, select reliable validated force fields, generate input scripts for popular simulation platforms, and assess the uncertainty in the results. The integration of these tools requires a host of technological and scientific innovations including: automated charge assignment protocols and file conversions, expansion of the Interface force field (IFF) to new systems, generation of new surface models, extension of the Open Knowledgebase of Interatomic Models (OpenKIM) to bonded force fields, development of machine learning based force field selection and uncertainty tools, and development of new Nanomaterial Builder and Bionano Builder modules in CHARMM-GUI. Cyberloop fulfils a critical need in the user community to discover and engineer new multi-component bionanomaterials to create the next generation of therapeutics, materials for energy conversion, and ultrastrong composites. The project will facilitate the training of graduate students, undergraduate students, and postdoctoral scholars, including underrepresented and minority students, at the participating institutions to prepare an interdisciplinary scientific workforce with significant experience in cyber-enabled technology. Online educational materials and tutorials will help increase participation in bionanomaterial research across academia and government. This award is jointly supported by the NSF Office of Advanced Cyberinfrastructure, and the Division of Materials Research and the Division of Chemistry within the NSF Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: GPU-accelerated First-Principles Simulation of Exciton Dynamics in Complex Systems,OAC,2209857,Andre Schleife,schleife@illinois.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","The development of societally-important energy harvesting approaches, such as photocatalysis or photovoltaics, requires detailed understanding of fast dynamics of electrons coupled with ions in novel materials. Modern supercomputers can help obtain such an understanding using sophisticated quantum-mechanical simulations. However, such a scientific effort requires accurate simulation techniques to be developed and efficient use of the underlying supercomputer hardware is crucial. This project meets these outstanding challenges by implementing novel techniques to describe the quantum-mechanical electron-electron interaction and interactions of electron dynamics with ions. Using and testing these new developments on graphical processing units advances science as it prepares quantum-mechanical simulations for next-generation supercomputers. Applying these advanced simulations to model complex systems of great importance for energy harvesting furthers the computational science community towards the goal of advancing national prosperity and welfare. The project makes these techniques freely available for a broad community, including documentation and tutorials, and trains the next generation of computational researchers through organizing summer schools and workshops.This project leverages a multi-organizational team to benefit from synergies that emerge from two possible solutions to current scientific barriers: Descriptions of exchange and correlation based on a long-range correction and on hybrid functionals that can scale favorably even for large systems with thousands of electrons are implemented and applied. Descriptions of non-adiabatic dynamics are implemented and applied to study long-term dynamics of excitons during which the interaction with the nuclei becomes important. Doing so within a cutting-edge electronic-structure code that runs efficiently on graphics processing units, provides a unique opportunity to compare accuracy, applicability to a broad range of systems, and computational cost. Knowledge on the reliability of these approximations and their computational cost for extended systems of practical relevance, including complex heterogeneous systems like semiconductor-molecule interfaces, are advanced by this research. The efforts include building, increasing, and growing a skilled community especially of US based researchers in a recurrent summer school.This proposal receives funds through the Office of Advanced Cyberinfrastructure in the Computer and Information Science and Engineering Directorate and the Division of Materials Research in the Mathematical and Physical Sciences Directorate.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: The PERTURBO Package: A Community Code for Electron Interactions and Dynamics in Materials,OAC,2209262,Marco Bernardi,bmarco@caltech.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Scientists in academia and industry are studying increasingly complex materials. Meanwhile, novel experimental probes enable unprecedented characterization of the microscopic interactions governing materials properties. This project supports the development of future versions of PERTURBO, an open-source software to study how electrons interact and move in the next generation of complex materials, which takes into account their complex atomic and electronic structures and the effects of electric and magnetic fields in the experimental probes. PERTURBO provides a set of accurate computational tools that are robust, extensively validated, sustainable, and user-oriented. Its workflows can predict with quantitative accuracy the electron interactions and dynamics in a wide range of conventional and quantum materials, with applications in electronics, optoelectronics, energy, sensing, computing, and quantum technologies. The software targets users in academia, national laboratories and industry, and leverages the current and future generations of high-performance computers. PERTURBO fills an important gap in the scientific software needed for the future development of science and technology in the United States, thus serving NSF's science mission and placing the country at the leading edge of materials and device research.PERTURBO project enables accurate studies of electron dynamics in complex materials and provides tools to interpret state-of-the-art transport and spectroscopy experiments. This project develops new theory and computational methods to predict the interactions and dynamics of electrons, phonons, spin, and excited states, in both conventional and quantum materials, using a unified framework. It additionally improves PERTURBO?s performance and memory usage, expands community engagement and integration with the cyberinfrastructure, and develops GPU parallelization and machine-learning functionalities. These efforts make PERTURBO efficient, sustainable, compatible with modern paradigms of software engineering, and prepared for the next generation of exascale and GPU supercomputers. The vision is to ensure that PERTURBO becomes the computational tool of choice for first-principles studies of electron dynamics in materials, transitioning during this 3-year project to a community code with thousands of users and many co-developers. The new workflows provided by PERTURBO advance both basic sciences (materials science, physics and chemistry) and applications in electronics, optoelectronics, energy, sensing, computing, and quantum technologies.This proposal receives funds through the Office of Advanced Cyberinfrastructure in the Computer and Information Science and Engineering Directorate and the Division of Materials Research in the Mathematical and Physical Sciences Directorate.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Community portal for high-precision atomic physics data and computation,OAC,1931339,Marianna Safronova,msafrono@udel.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Many engineering products and science projects depend on knowledge of exact characteristics of the atoms that make up the used materials. Today's Global Positioning Systems (GPS) are an example of such engineering products. This project will develop an easy-to-use web portal that provides the atomic physics and other communities with much-needed precision information about atomic properties. Research in quantum information, degenerate quantum gases, atomic clocks, precision measurements of atoms and molecules, plasma physics, astrophysics, and studies of fundamental physics all rely on precise knowledge of atomic properties. Experimental measurements are impossible or infeasible in many cases. Releasing all codes to the public and creating a portal with easy access to high-precision computations, now limited to only group of experts, will enable and accelerate a broad range of physics research and engineering projects.The portal is powered by a database of high-precision information, including matrix elements and polarizabilities of frequently used atoms. The database is coupled with a package of computational applications capable of computing on-demand a wider range of properties and for atoms not yet in the database. The code package has already demonstrated these capabilities, but will be developed into community codes that run efficiently on available high-performance computing systems. For atoms with many valence electrons, computing precise properties is not yet feasible, as the computational complexity is exponential. This project will develop new methods and algorithms for computing these properties, enabling precision atomic research to be done with atoms not possible thus far. Finally, the development of the portal and parallel applications addresses important computer science challenges, such as the efficient coupling of databases with complex computations that automatically ingest new data. Thus, this project will also create new knowledge in the key issue of combining data and simulation.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Physics and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Sustainability: A Community-Centered Approach for Supporting and Sustaining Parsl,OAC,2209920,Daniel Katz,dskatz@illinois.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","Modern research computing requires orchestration of complex computational workflows across diverse research computing infrastructure. To meet this requirement, Parsl, an open source software package, enables scientists and engineers to parallelize Python programs and run them scalably and efficiently on small to very large local and remote resources (e.g., laptops, clusters, clouds, and supercomputers), while also moving data as needed. Parsl effectively democratizes access to NSF's increasingly broad and powerful cyberinfrastructure resources by enabling researchers to work in the familiar and productive Python programming language and environment while also allowing them to easily scale and distribute their work to fully use today's cutting-edge cyberinfrastructure. Parsl is a key tool used by researchers to tackle some of the world's most significant challenges: from understanding the beginning of the universe to exploring an enormous molecular search space to identify viable therapeutics. This project is laying the foundation for the sustainability of this crucial tool, enabling new long-term science advances and thereby benefiting society. Participation in several education programs is exposing a diverse group of students to the increasingly important area of parallel and distributed computing. This project is transitioning Parsl to a community-governed and community-supported open source project, with future income to be managed by a nonprofit organization under the direction of an elected Parsl Coordination Committee. The project is delivering a sustainable Parsl community by a) targeted technical activities that reduce costs and barriers for contribution, reducing future maintenance costs; b) building the Parsl community via outreach, engagement, and education programs, increasing potential future contributors; and c) establishing pathways and incentives to convert users to contributors and contributors to leaders, growing the next generation of the community. The project takes a data-driven approach by measuring effort and funding across the project to reveal insights about where effort is spent, how it is supported, and how these factors can be balanced. The lessons learned in this project are being converted to a blueprint for similar projects.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: A Software Ecosystem for Plasma Science and Space Weather Applications,OAC,2209472,Mark Shephard,shephard@rpi.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","This project advances the frontiers of global simulations of space and laboratory plasma systems, and makes these advances accessible to a broad community of users. The plasma states in such systems are often in weakly collisional regimes where traditional fluid models break down. To describe such systems, the project team is developing Gkeyll, a multi-fluid, multi-moment model that incorporates important kinetic effects. The model can be used for high-fidelity and accurate predictions for space weather events, and for the design and study of laboratory experiments engaged in discovery plasma science. The project will develop comprehensive cyberinfrastructure (CI) to support professional scientists in the execution of space weather and basic plasma physics simulations over a wide variety of computing platforms from local clusters to high-performance computing facilities. Gkeyll CI to be developed includes (1) providing web-based user interfaces for defining, executing, and managing simulations, (2) workflows consisting of of pre-defined sequences of tasks, and (3) user-friendly tools that implement the workflow tasks. Gkeyll science will be facilitated by algorithmic advances. Gkeyll is open-source and provides the space and laboratory plasma science communities with a modern and versatile software ecosystem. The project team is strongly interdisciplinary, including space and laboratory plasma scientists, applied mathematicians, computer scientists and software engineers, and the project will promote diversity in professional training and education.The CI to be built in the project is centered on the Gkeyll framework that consists of advanced plasma simulation models, tools, and automated continuous integration of new software. The project will provide a uniform and complete open-source solution with workflow control to model plasma systems from terrestrial magnetospheres to laboratory experiments in collisionless or weakly collisional plasma regimes, which are essential for space weather and laboratory plasma experiments. The project will extend and enhance the core algorithms in Gkeyll and integrate the software with the NSF Science Gateway Platform as a Service (SciGaP). Gkeyll science is facilitated by core modeling and PDE discretization advances, including automated and adaptive gridding for relevant sets of parameterized geometries that are properly matched to the equation discretization methods. The project will employ modern software engineering practices to produce scalable, secure, and high-fidelity physics-based software.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Physics within the NSF Directorate for Mathematical and Physical Sciences, and the Division of Atmospheric and Geospace Sciences and the Division of Research, Innovation, Synergies, and Education within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Collaborative Research: Frameworks: Multiphase Fluid-Structure Interaction Software Infrastructure to Enable Applications in Medicine, Biology, and Engineering",OAC,1931524,Matthew Knepley,knepley@buffalo.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Physical systems in which fluid flows interact with immersed structures are found in a wide range of areas of science and engineering. Such fluid-structure interactions are ubiquitous in biological systems, including blood flow in the heart, the ingestion of food, and mucus transport in the lung. Fluid-structure interaction is also a crucial aspect of new approaches to energy harvesting, such as wave-energy converters that extract energy from the motion of sea or ocean waves, and in advanced approaches to manufacturing, such as 3D printing. This award supports the development of an advanced computer simulation infrastructure for modeling this full range of application areas. Computer models advanced by this project could ultimately lead to improved diagnostics and treatments for human disease, optimized designs of novel approaches to renewable energy, and reduced manufacturing costs through improved production times in 3D printing.This project aims to enhance the IBAMR computer modeling and simulation infrastructure that provides advanced implementations of the immersed boundary (IB) method and its extensions with support for adaptive mesh refinement (AMR). IBAMR is designed to simulate large-scale fluid-structure interaction models on distributed memory-parallel systems. Most current IBAMR models assume that the properties of the fluid are uniform, but many physical systems involve multiphase fluid models with inhomogeneous properties, such as air-water interfaces or the complex fluid environments of biological systems. This project aims to extend recently developed support in IBAMR for treating multiphase flows by improving the accuracy and efficiency of IBAMR's treatment of multiphase Newtonian flows, and also by extending this multiphase flow modeling capability to treat multiphase complex (polymeric) fluid flows, which are commonly encountered in biological systems, and to treat reacting flows with complex chemistry, which are relevant to models of combustion, astrophysics, and additive manufacturing using stereolithography (3D printing). This project also aims to re-engineer IBAMR for massive parallelism, so that it may effectively use very large computational resources in service of applications that require very high fidelity. The project will also develop modules that will facilitate the use of image-derived geometries, and it will develop novel fluid-structure coupling schemes that will facilitate the use of independent fluid and solid solvers. These capabilities are motivated within this project by models of cardiac, gastrointestinal, and lung physiology; renewable energy; and advanced manufacturing. This software will be used in courses developed by the members of the project team. The project also aims to grow the community of IBAMR users by enhancing project documentation and training materials, hosting user group meetings, and offering short courses.This award by the NSF Office of Advanced Cyberinfrastructure is co funded by the Division of Civil, Mechanical, and Manufacturing Innovation to provide enabling tools to advance potentially transformative fundamental research, particularly in biomechanics and mechanobiology.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Simplifying Compiled Python Packaging in the Sciences,OAC,2209877,Henry Schreiner,henryfs@princeton.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Python is the most popular language in the sciences, and compiled extensions for it are at the core of almost all scientific packages for Python. The process to build an extension has traditionally been complex. Even a simple extension is challenging and making a complex extension like NumPy requires thousands of lines of code just for compilation. Scikit-build provides access to CMake, the most popular and powerful build system for compiled languages, for Python users in a native and natural way. Scikit-build is being redesigned on top of standardized packaging procedures that were not yet written when scikit-build was started in 2014. This will establish Scikit-build as the reference scientific Python community solution for the future, as well as provide many new features to users, including much simpler configuration, much better stability, cross-platform compilation, and much more. A collection of popular scientific libraries are adapting the new Scikit-build core infrastructure and providing feedback on what features need to be made available. Extensive tutorials, examples, and training sessions are being held during the course of the project to make binary extensions available to far more users than ever before.Scikit-build was designed as a wrapper around distutils, the standard library package for building extensions, which was the only option in 2014. It is being rewritten on published PEPs (Python Enhancement Proposals) to avoid usage of distutils before distutils is removed in 2023 in Python 3.12. The rewritten Scikit-build will also provide a configuration system that conforms to modern best practices, an improved developer experience, and increased stability by avoiding the fragile distutils private internals. New features include a new library discovery system integrated with CMake to make reusable compiled libraries shareable through Python distribution channels like PyPI and conda-forge, cached builds, better CUDA and Fortran support, cross-compilation, and more. Scikit-build is collaborating with NumPy and SciPy on documentation, US-RSE for tutorials and workshops, and RAPIDS, PyArrow, PySTAN, Awkward Array, ITK, 3D Slicer, CERN ROOT, ATLAS, OSQP, and Bzier for initial integration and testing of Scikit-Build?s redesigned infrastructure.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Simulating Autonomous Agents and the Human-Autonomous Agent Interaction,OAC,2209792,Daniel Goldman,daniel.goldman@physics.gatech.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","This project augments the Chrono computer simulation platform in transformative ways. Chrono's purpose is to predict through simulation the interplay between mechatronic systems, the environment they operate in, and humans with whom they might interact. The open-source simulation platform is slated to become a community-shared virtual investigation tool used to probe competing engineering designs and test hypotheses that would be too dangerous, difficult, or costly to verify through physical experiments. Chrono has been and will continue to be used in multiple fields and disciplines, e.g., terramechanics, astrophysics; soft matter physics; biomechanics; mechanical engineering; civil engineering; industrial engineering; and computer science. Specifically, it is used to engineer the 2023 VIPER lunar rover; relied upon by US Army experts in evaluating its wheeled and tracked vehicle designs; used in the US and Germany in the wind turbine industry; and involved in designing wave energy conversion solutions in Europe. Upon project completion, Chrono will become a simulation engine in Gazebo, which is widely used in robotics research; operate on the largest driving simulator in the US; empower research in the bio-robotics and field-robotics communities; and assist efforts in the broad area of automotive research carried out by a consortium of universities and companies under the umbrella of the Automotive Research Center. The educational impact of this project is threefold: training undergraduate, graduate, and post-doctoral students in a multi-disciplinary fashion that emphasizes advanced computing skills development; anchoring two new courses in autonomous vehicle control and simulation in robotics; and broadening participation in computing through a residential program on the campus of the University of Wisconsin-Madison that engages teachers and students from rural high-schools. Innovation and discovery are fueled by quality data. At its core, this project seeks to increase the share of this data that has simulation as its provenance. In this context, a multi-disciplinary team of 40 researchers augments and validates a physics-based simulation framework that empowers research in autonomous agents (AAs). The AAs operate in complex and unstructured dynamic environments and might engage in two-way interaction with humans or other AAs. This project enables Chrono to generate machine learning training data quickly and inexpensively; facilitates comparison of competing designs for assessing trade-offs; and gauges candidate design robustness via testing in simulation of corner-case scenarios. These tasks are accomplished by upgrading and extending Chrono to leverage recent computational dynamics innovations, e.g., a faster index 3 differential algebraic equations solver; a new approach to solving frictional contact problems; a real-time solver for handling flexible-body dynamics in soft robotics via nonlinear finite element analysis; a best-in-class simulator for terradynamics applications; reliance on just-in-time compiling for producing executables that are both problem- and hardware-optimized; a novel way for using mixed data representations for parsimonious storing of state information; and a scalable multi-agent framework that enables geographically-distributed, over the Internet, real-time simulation of human-AA interaction.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Sustainable Open-Source Quantum Dynamics and Spectroscopy Software,OAC,2103738,Edward Valeev,evaleev@vt.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","With support from the Office of Advanced Cyberinfrastructure and the Division of Chemistry at NSF, Professor Li and his team will work to expand the capabilities of the open-source software platform, Chronus Quantum (ChronusQ), to include time-resolved spectroscopy for supporting quantum chemistry calculations. The new capabilities will include novel computational methods to provide unprecedented capabilities to simulate chemical processes of electrons and nuclei that exhibit quantum behaviors. The physical insights gleaned through applications of ChronusQ underlie the advancement of new technologies that are crucial to sustainable energy, catalysis, quantum computing, and other applications that can immediately impact society. This project provides a mechanism for advanced interdisciplinary education and training in the areas of inorganic, theoretical, physical, and materials chemistry. The collaborative academic research serves as a test ground for identifying and deploying ways that the scientific community as a whole can both increase awareness of the importance of active engagement in professional skill development for graduate students and post-doctoral scholars, and develop tools to facilitate professional development in an academic setting.ChronusQ seamlessly integrates time-dependent quantum mechanical theories, spectral analysis tools, and modular high-performance numerical libraries that are highly parallelized, extensible, reusable, community-driven, and open-sourced. The Team develops in ChronusQ the complete time-dependent quantum description of coupled nuclear and electronic dynamics within the time-dependent density functional theory and equation-of-motion coupled cluster framework. The project enables computational studies of ultrafast time-resolved spectroscopies and simulations of chemical processes in the strongly nonadiabatic regime. Software modules are bolstered by algebraic and integral acceleration engines that can make it feasible to simulate fully quantum mechanically molecular dynamics. The collaborative project advances the theoretical description of quantum dynamics across time scales, bridging the attosecond and subnanosecond regimes, enabling the development of spectroscopic technologies to probe molecular and materials properties with state specificity that is beyond the Born-Oppenheimer approximation.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: OpenEarthscape - Transformative Cyberinfrastructure for Modeling and Simulation in the Earth-Surface Science Communities,OAC,2103632,Erkan Istanbulluoglu,erkani@u.washington.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","The landscape around us changes constantly. Sometimes change is slow: a river bend migrates, soil erodes from a field, a waterfall carves its way upstream. Sometimes change happens fast: a landslide, a flood, a winter storm eroding beaches. To understand, manage, and forecast such processes, scientists rely on computer simulation models. This project develops software tools to make simulation modeling easier, more accessible, and more efficient. Among the products is a website through which researchers and students alike can learn about and experiment with a variety of environmental simulation models, without needing to install anything on their own computers. This web portal takes advantage of a technology that combines text, pictures, and computer code in a single online document. The project also develops improved computer-programming libraries designed to make it easier and more efficient for researchers to create new simulation models. The project contributes computing-skills training for college students enrolled in Colorado-based summer programs that serve traditionally underrepresented student populations. The project also promotes public education in geology, by creating an online animated simulation illustrating how landscapes evolve in response to various geologic events.As the sciences that probe Earth's changing surface become more quantitative and prediction-oriented, they increasingly rely on computational modeling and model-data integration. This project develops OpenEarthscape: an integrated suite of community-developed cyber resources for simulation and model-data integration, focusing on nine high-priority geoscience frontiers. Products and activities include EarthscapeHub: a JupyterHub server providing easy access to models, tools, and libraries; new capacity for creating and sharing reproducible analyses; and major enhancements to current programming libraries for model construction and coupling. OpenEarthscape catalyzes efficiency by building new technology to improve performance and developing an extended version of the Basic Model Interface API standard to address parallel architecture and coupling. OpenEarthscape fosters research productivity with improved library capabilities for data I/O and visualization, and with community resources for efficient software distribution and cross-platform compatibility. Broader impacts include partnership with undergraduate research programs that support traditionally underrepresented student populations, with the project team contributing introductory training in scientific computing. A novel educational element is the OpenEarthscape Simulator: a web-hosted visual simulation of a micro-continent evolving in response to various geologic events. The simulator provides students and the general public with an intriguing visualization of Earthscape dynamics and provides a template for the research community to identify defects in our current understanding.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Bringing Montage To Cutting Edge Science Environments,OAC,1835379,Graham Berriman,gbb@ipac.caltech.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Astronomy is undergoing a transformation in the way data are acquired, and this is driving a corresponding transformation in the way astronomers process these data. Telescopes and sky surveys that are operating now or will begin to operate in the coming years will deliver data that are too large and complex to analyze by the traditional method of downloading data to desktops and local clusters. Thus a transformation is underway to use new technologies to process data. Astronomers have embraced the Python language for analysis because it provides the necessary flexible building blocks to handle complex data, and are embracing new Python-based technologies to manage and control processing that allows the software to run next to the data themselves. The Montage image mosaic engine, a toolkit already used widely by astronomers, will join this transformative community and deliver high-performance, next generation image processing capabilities for astronomers and computer scientists. It will allow astronomers to create large-scale images of the sky, and study these images with the many powerful tools available in Python.Python has become the language of choice for astronomy, and environments such as JupyterLabs and JupyterHub are almost certainly the science environments of the future. The LSST is committed to using such an environment for its science platform, which will be the primary way LSST users will discover, access and analyze data. Astronomy science archives are actively building similar platforms. NOAO has deployed their DataLab, which supports datasets acquired at Kitt Peak and CTIO. We will incorporate the functionality of the Montage image mosaic engine - a scalable toolkit written in ANSI-C and in wide use in astronomy and information technology - into environments such as these to unleash its full power when applied to large and complex new datasets. Moreover, the same functionality can be incorporated into a single desktop platform, or into a new scalable environment built to support a new project or mission, or into a distributed scalable environment such the Amazon Elastic Cloud (""bringing the code to the data""). As a component-based toolkit, Montage will be well positioned to respond to the rapid changes expected as these new platforms develop and contribute substantially to understanding their performance and usefulness.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Open-source tools for block polymer phase behavior,OAC,2103627,Kevin Dorfman,dorfman@umn.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Block polymers are molecules formed by bonding one or more different polymer molecules at their ends. When a liquid of block polymers is cooled, it spontaneously forms an ordered structure with nanoscale domain sizes and a structure that is dictated by the chemistry of the block polymer. Understanding the selection of different ordered states has both fundamental and practical implications. On the fundamental side, block polymers provide an important model system for examining self-assembly and the formation of crystalline order in soft matter. At a practical level, the ability to construct materials with a controllable arrangement of components with dramatically different properties (e.g., glassy spheres embedded in a rubbery matrix) enables novel applications. Computation plays an important role for both fundamental studies and these technological applications by providing guidance towards the selection of materials to analyze and a framework for interpreting the results. This project will produce an open-source software package that enables block polymer researchers to easily use state-of-the-art computational tools in their own research. The long-term goal of the project is to achieve, within the context of the block polymer theory and experimental community, a widespread use of field-based simulation tools that naturally complement particle-based molecular dynamics simulation methods for studying block polymer properties. This project also provides graduate students and undergraduates with the opportunities to work on state-of-the-art simulation methods and training in high-performance scientific computing.The proposed open-source software package will enable self-consistent field theory (SCFT) and field theoretic simulations (FTS) under a common framework implemented in C++. The package will include SCFT tools that build on the successful Polymer Self Consistent Field (PSCF) software package for periodic systems to provide the full suite of functionality required for advanced polymer physics research. This new package, entirely rewritten in a new language and with a more flexible design, will enable accelerated calculations on either GPUs or multicore CPUs, efficient treatment of problems with special symmetries, and treatment of complex boundaries such as polymer-grafted nanoparticles and patterned surfaces. A separate set of FTS programs will allow the user to implement either complex Langevin sampling of the fully fluctuating model or a more efficient form of stochastic simulation that involves an approximate treatment of non-specific steric interactions that maintain a uniform density. Input/output tools will also be developed to lower the barrier to usage of the software. Dissemination of the work will include a project website, which will provide background for potential users and links to documentation, and an online tutorial workshop that will introduce the software and its capabilities to the broader community. This project is funded by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering, with the Division of Materials Research and the Division of Chemistry in the Directorate for Mathematical and Physical Sciences and the Division of Chemical, Bioengineering, Environmental and Transport Systems in the Directorate for Engineering also contributing funds.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Framework: Software: NSCI: Collaborative Research: Hermes: Extending the HDF Library to Support Intelligent I/O Buffering for Deep Memory and Storage Hierarchy Systems,OAC,1835669,Jian Peng,jianpeng@illinois.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","Modern high performance computing (HPC) applications generate massive amounts of data. However, the performance improvement of disk based storage systems has been much slower than that of memory, creating a significant Input/Output (I/O) performance gap. To reduce the performance gap, storage subsystems are under extensive changes, adopting new technologies and adding more layers into the memory/storage hierarchy. With a deeper memory hierarchy, the data movement complexity of memory systems is increased significantly, making it harder to utilize the potential of the deep memory and storage hierarchy (DMSH) design. As we move towards the exascale era, I/O bottleneck is a must to solve performance bottleneck facing the HPC community. DMSHs with multiple levels of memory/storage layers offer a feasible solution but are very complex to use effectively. Ideally, the presence of multiple layers of storage should be transparent to applications without having to sacrifice I/O performance. There is a need to enhance and extend current software systems to support data access and movement transparently and effectively under DMSHs. Hierarchical Data Format (HDF) technologies are a set of current I/O solutions addressing the problems in organizing, accessing, analyzing, and preserving data. HDF5 library is widely popular within the scientific community. Among the high level I/O libraries used in DOE labs, HDF5 is the undeniable leader with 99% of the share. HDF5 addresses the I/O bottleneck by hiding the complexity of performing coordinated I/O to single, shared files, and by encapsulating general purpose optimizations. While HDF technologies, like other existing I/O middleware, are not designed to support DMSHs, its wide popularity and its middleware nature make HDF5 an ideal candidate to enable, manage, and supervise I/O buffering under DMSHs. This project proposes the development of Hermes, a heterogeneous aware, multi tiered, dynamic, and distributed I/O buffering system that will significantly accelerate I/O performance. This project proposes to extend HDF technologies with the Hermes design. Hermes is new, and the enhancement of HDF5 is new. The deliveries of this research include an enhanced HDF5 library, a set of extended HDF technologies, and a group of general I/O buffering and memory system optimization mechanisms and methods. We believe that the combination of DMSH I/O buffering and HDF technologies is a reachable practical solution that can efficiently support scientific discovery. Hermes will advance HDF5 core technology by developing new buffering algorithms and mechanisms to support 1) vertical and horizontal buffering in DMSHs: here vertical means access data to/from different levels locally and horizontal means spread/gather data across remote compute nodes; 2) selective buffering via HDF5: here selective means some memory layer, e.g. NVMe, only for selected data; 3) dynamic buffering via online system profiling: the buffering schema can be changed dynamically based on messaging traffic; 4) adaptive buffering via Reinforcement Learning: by learning the application's access pattern, we can adaptprefetching algorithms and cache replacement policies at runtime. The development Hermes will be translated into high quality dependable software and will be released with the core HDF5 library.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: HDR: Reproducible Visual Analysis of Multivariate Networks with MultiNet,OAC,1835893,Luke Harmon,lukeh@uidaho.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","Multivariate networks -- datasets that link together entities that are associated with multiple different variables -- are a critical data representation for a range of high-impact problems, from understanding how our bodies work to uncovering how social media influences society. These data representations are a rich and complex reflection of the multifaceted relationships that exist in the world. Reasoning about a problem using a multivariate network allows an analyst to ask questions beyond those about explicit connectivity alone: Do groups of social-media influencers have similar backgrounds or experiences? Do species that co-evolve live in similar climates? What patterns of cell-types support different types of brain functions? Questions like these require understanding patterns and trends about entities with respect to both their attributes and their connectivity, leading to inferences about relationships beyond the initial network structure. As data continues to become an increasingly important driver of scientific discovery, datasets of networks have also become increasingly complex. These networks capture information about relationships between entities as well as attributes of the entities and the connections. Tools used in practice today provide very limited support for reasoning about networks and are also limited in the how users can interact with them. This lack of support leaves analysts and scientists to piece together workflows using separate tools, and significant amounts of programming, especially in the data preparation step. This project aims fill this critical gap in the existing cyber-infrastructure ecosystem for reasoning about multivariate networks by developing MultiNet, a robust, flexible, secure, and sustainable open-source visual analysis system. MultiNet aims to change the landscape of visual analysis capabilities for reasoning about and analyzing multivariate networks. The web-based tool, along with an underlying plug-in-based framework, will support three core capabilities: (1) interactive, task-driven visualization of both the connectivity and attributes of networks, (2) reshaping the underlying network structure to bring the network into a shape that is well suited to address analysis questions, and (3) leveraging provenance data to support reproducibility, communication, and integration in computational workflows. These capabilities will allow scientists to ask new classes of questions about network datasets, and lead to insights about a wide range of pressing topics. To meet this goal, we will ground the design of MultiNet in four deeply collaborative case studies with domain scientists in biology, neuroscience, sociology, and geology.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Software NSCI: Constitutive Relation Inference Toolkit (CRIKit),OAC,1835825,Jed Brown,Jed.Brown@colorado.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","Constitutive relations are mathematical models that describe the way materials respond to local stimuli such as stress or temperature change, and are essential to the study of biological tissues in biomechanics, ice and rock in geosciences, plasmas in high-energy physics and many other science and engineering applications. This project seeks to infer constitutive relations from practical observations without requiring isolation of the material in conventional laboratory experiments, which are often expensive and difficult to apply to volatile materials such as liquid foams or materials such as sea ice that exhibit homogenized behavior only at large scales. The investigators and their students will develop underlying algorithms and the Constitutive Relation Inference Toolkit (CRIKit), a new community software package to leverage recent progress in machine learning and physically-based modeling to infer constitutive relations from noisy, indirect observations, and disseminate the results as citable research products for use in a range of open source and extensible commercial simulation environments. This development will create new opportunities and increase accessibility at the confluence of data science and high-fidelity physical modeling, which the investigators will highlight through community outreach and educational activities.The CRIKit software will integrate parallel partial differential equation (PDE) solvers like FEniCS/dolfin-adjoint with machine learning (ML) packages like TensorFlow to infer constitutive relations from noisy indirect or in-situ observations of material responses. The forward simulation is post-processed to create synthetic observations which are compared to real observations by way of a loss function, which may range from simple least squares to advanced techniques such as ML-based image analysis. This approach results in a nonlinear regression problem for the constitutive relation (formulated to satisfy invariants and free energy compatibility requirements) and relies on well-behaved and efficiently computable gradients provided by PDE solvers using compatible discretizations with adjoint capability. The inference problem exposes parallelism within each forward model and across different experimental realizations and facilitates research in optimization. The research enables constitutive models to be readily updated with new experimental data as well as reproducibility and validation studies. CRIKit's models will improve simulation capability for scientists and engineers by providing ready access to the cutting edge of constitutive modeling.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Convergence of Bayesian inverse methods and scientific machine learning in Earth system models through universal differentiable programming,OAC,2104068,Sri Hari Krishn Narayanan,snarayan@mcs.anl.gov,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Understanding and quantifying parameter sensitivity of simulated systems, such as the numerical models of physical systems and mathematical renderings of neural networks, are essential in simulation-based science (SBS) and scientific machine learning (SciML). They are the key ingredients in Bayesian inference and neural network training. Seizing on the opportunity of emerging open-source Earth system model development in the Julia high-level programming language, this project is endowing these open-source models with automatic differentiation (AD) enabled derivative information, making these converging data science and simulation-based science tools available to a much broader research and data science community. Enabling a general-purpose AD framework which can handle both large-scale Earth system models as well as SciML algorithms, such as physics-informed neural networks or neural differential equations, will enable seamless integration of these approaches for hybrid Bayesian inversion and Bayesian machine learning. It merges big data science, in which available data enable model discovery with sparse data science, and the model structure is exploited in the selection of surrogate models representing data-informed subspaces and fulfilling conservation laws. The emerging Julia language engages a new generation of researchers and software engineers, channeling much needed talent into computational science approaches to climate modeling. Through dedicated community outreach programs (e.g., Hackathons, Minisymposia, Tutorials) the project team will be working toward increasing equity, diversity, and inclusion across the participating disciplines.The project is developing a framework for universal differentiable programming and open-source, general-purpose AD that unifies these algorithmic frameworks within Julia programming language. The general-purpose AD framework in Julia leverages the composability of Julia software packages and the differentiable programming approach that underlies many of the SciML and high-performance scientific computing packages. Compared to most current modeling systems targeted for HPC, Julia is ideally suited for heterogeneous parallel computing hardware (e.g., CUDA, ROCm, oneAPI, ARM, PowerPC, x86 64, TPUs). The project is bringing together expertise in AD targeted at Earth system data assimilation in high performance computing environments with SciML expertise. The project team is working with the Julia Computing organization and package developers to ensure sustainability of the developed frameworks. The project?s Earth system flagship applications consist of (i) an open-source, AD-enabled ocean general circulation model that is being developed separately as part of the Climate Modelling Alliance (CliMA), and (2) an open-source, AD-enabled ice flow model. Each of these application frameworks is being made available to the community for science application, in which derivative (gradient or Hessian) information represent key algorithmic enabling tools. These include SciML-based training of surrogate models (data-driven and/or model-informed), parameter and state estimation, data assimilation for model initialization, uncertainty quantification (Hessian-based and gradient-informed MCMC) and quantitative observing system design. Academic and industry partners are involved, who are using the frameworks for developing efficient power grids, personalized precision pharmacometrics, and improved EEG design.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Convergence of Bayesian inverse methods and scientific machine learning in Earth system models through universal differentiable programming,OAC,2103791,Nora Loose,nora.loose@colorado.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Understanding and quantifying parameter sensitivity of simulated systems, such as the numerical models of physical systems and mathematical renderings of neural networks, are essential in simulation-based science (SBS) and scientific machine learning (SciML). They are the key ingredients in Bayesian inference and neural network training. Seizing on the opportunity of emerging open-source Earth system model development in the Julia high-level programming language, this project is endowing these open-source models with automatic differentiation (AD) enabled derivative information, making these converging data science and simulation-based science tools available to a much broader research and data science community. Enabling a general-purpose AD framework which can handle both large-scale Earth system models as well as SciML algorithms, such as physics-informed neural networks or neural differential equations, will enable seamless integration of these approaches for hybrid Bayesian inversion and Bayesian machine learning. It merges big data science, in which available data enable model discovery with sparse data science, and the model structure is exploited in the selection of surrogate models representing data-informed subspaces and fulfilling conservation laws. The emerging Julia language engages a new generation of researchers and software engineers, channeling much needed talent into computational science approaches to climate modeling. Through dedicated community outreach programs (e.g., Hackathons, Minisymposia, Tutorials) the project team will be working toward increasing equity, diversity, and inclusion across the participating disciplines.The project is developing a framework for universal differentiable programming and open-source, general-purpose AD that unifies these algorithmic frameworks within Julia programming language. The general-purpose AD framework in Julia leverages the composability of Julia software packages and the differentiable programming approach that underlies many of the SciML and high-performance scientific computing packages. Compared to most current modeling systems targeted for HPC, Julia is ideally suited for heterogeneous parallel computing hardware (e.g., CUDA, ROCm, oneAPI, ARM, PowerPC, x86 64, TPUs). The project is bringing together expertise in AD targeted at Earth system data assimilation in high performance computing environments with SciML expertise. The project team is working with the Julia Computing organization and package developers to ensure sustainability of the developed frameworks. The project?s Earth system flagship applications consist of (i) an open-source, AD-enabled ocean general circulation model that is being developed separately as part of the Climate Modelling Alliance (CliMA), and (2) an open-source, AD-enabled ice flow model. Each of these application frameworks is being made available to the community for science application, in which derivative (gradient or Hessian) information represent key algorithmic enabling tools. These include SciML-based training of surrogate models (data-driven and/or model-informed), parameter and state estimation, data assimilation for model initialization, uncertainty quantification (Hessian-based and gradient-informed MCMC) and quantitative observing system design. Academic and industry partners are involved, who are using the frameworks for developing efficient power grids, personalized precision pharmacometrics, and improved EEG design.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research:Framework:Software:NSCI:Enzo for the Exascale Era (Enzo-E),OAC,1835213,John Wise,jwise@physics.gatech.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","The earliest stages of the formation of galaxies and quasars in the universe are soon to be explored with a powerful new generation of ground and space-based observatories. Broad and deep astronomical surveys of the early universe beginning in the next decade using the Large Synoptic Survey Telescope and the James Webb Space Telescope will revolutionize our understanding of the origin of galaxies and quasars, and help constrain the nature of the dark matter which is the dominant matter constituent in the universe. Detailed physical simulations that model the formation of these objects are an indispensible aid to understanding the coming glut of observational data, and to maximize the scientific return of these instruments. In this project, investigators at the Univ. California San Diego, Columbia Univ., Georgia Tech, and Michigan State Univ. are collaborating with the goal of developing a next generation community simulation software framework for the coming generation of supercomputers for cosmological simulations of the young universe. Undergraduate and graduate students will be directly involved in the software development as well as its application to several frontier cosmological research topics. The software framework that will be produced will be disseminated as open source software to enable a much broader range of scientific explorations of astrophysical topics. The project brings together the key developers of the open source Enzo adaptive mesh refinement (AMR) hydrodynamic cosmology code, who will port its software components to a newly developed AMR software framework called Cello. Cello implements the highly scalable array-of-octrees AMR algorithm on top of the powerful Charm++ parallel object system. Designed to be extensible and scalable to millions of processors, the new framework, called Enzo-E, will target exascale high performance computing (HPC) systems of the future. Through this project, the entire Enzo community will have a viable path to exascale simulations of unprecedented size and scope. The investigators have chosen three frontier problems in cosmology to drive the development of the Enzo-E framework: (1) the assembly of the first generation of stars and black holes into the first galaxies; (2) the role of cosmic rays in driving galactic outflows; and (3) the evolution of the intergalactic medium from cosmic dawn to the present day. Annual developer workshops and software releases will keep the broader research community informed and involved in the developments.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Simulation-driven Evaluation of Cyberinfrastructure Systems,OAC,2103508,Loic Pottier,lpottier@isi.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Most scientific breakthroughs and discoveries are now preconditioned on performing complex processing of vast amounts of data as conveniently, reliably, and efficiently as possible. This requires high-end interconnected compute and storage resources, as well as software systems to automate the processing on these resources. An enormous amount of effort has been invested in producing such ""cyberinfrastructure"" software systems. And yet, developing and evolving these systems so that they are as efficient as possible, while anticipating future cyberinfrastructure opportunities and needs, is an open challenge. This project transforms the way in which these systems are evaluated, so that their capabilities can be developed and evolved judiciously. The traditional evaluation approach is to observe executions of these systems on real-world hardware resources. Although seemingly natural, this approach suffers from many shortcomings. Instead, this project focuses on simulating these executions. Simulation has tremendous, and untapped, potential for transforming the development cycle of cyberinfrastructure systems. Specifically, this project produces software elements that can be easily integrated into existing and future systems to afford them with simulation capabilities. These capabilities make it possible for developers to put their systems through the wringer and observe their behaviors for arbitrary operating conditions, including ones that go beyond current hardware platforms and scientific applications. Simply put, these capabilities will make it possible to establish a solid experimental science approach for the development of cyberinfrastructure systems that support current and future scientific endeavors that are critical to the development of our society.The cyberinfrastructure has been the object of intensive research and development, resulting in a rich set of interoperable software systems that are used to support science. A key challenge is the development of systems that can execute application workloads efficiently, while anticipating future cyberinfrastructure opportunities and needs. This project aims to transform the way in which these systems are evaluated, so that their capabilities can be evolved based on a sound, quantitative experimental science approach. The traditional evaluation approach is to use full-fledged software stacks to execute application workloads on actual cyberinfrastructure deployments. Unfortunately, this approach suffers from several shortcomings: real-world experiments are time- and labor-intensive, and they are limited to currently available hardware and software configurations. An alternative to real-world experiments that does not suffer from these shortcomings is simulation, i.e., the implementation and use of a software artifact that models the functional and performance behaviors of software and hardware stacks of interest. This project uses simulation to transform the way in which cyberinfrastructure systems are evaluated as part of their long-term development cycles. This is achieved via software elements for enhancing production cyberinfrastructure systems with simulation capabilities so as to enable quantitative evaluation of these systems for arbitrary execution scenarios. Creating these scenarios requires little labor, and executions can be simulated accurately and orders of magnitude faster than their real-world counterparts. Furthermore, simulations are perfectly reproducible and observable. While this approach is general, its effectiveness will be demonstrated by applying it to a number of production systems, namely, workflow management systems. This project capitalizes on the years of development invested in the SimGrid and WRENCH simulation frameworks.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Machine Learning Quark Hadronization,OAC,2103889,Jure Zupan,zupanje@ucmail.uc.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","Particle physics explores the fundamental building blocks of nature, and their interactions. This is accomplished by colliding particles like protons together at high energies, producing many particles which are then measured by experimental detectors. To interpret the data, detailed simulations of particle collisions are needed. Monte Carlo event generators provide the simulations and are at the core of particle physics discoveries, such as the discovery of the Higgs boson at the Large Hadron Collider in 2012, forming a keystone of the cyberinfrastructure for particle physics. This project addresses a bottle-neck in Monte Carlo event generators ? how quarks are bound together inside protons and other composite particles through the process of hadronization. Hadronization models are improved by this project, including the speed of simulating the process of hadronization, fulfilling a critical step for understanding the scientific impact of upcoming large-scale particle, neutrino, and nuclear physics projects. Event generators simulate particle collisions in three steps: a high energy collision at the small distances, evolution of the event into a large distance (low energy) configuration, and finally a hadronization into observable particles. The hadronization step is particularly challenging, because it cannot be directly calculated from first principles. Since hundreds of particles must be produced for each particle collision, algorithmic efficiency is necessary and requires innovative approaches. This project is applying Machine Learning (ML) techniques to hadronization in order to: connect to existing models of hadronization, understand the underlying description of data by current event generators augmented with ML, and provide a fast and accurate ML based simulation of hadronization. As part of the project, a platform is being built where user submitted modules, such as this ML hadronization module, can be incorporated in the most widely used event generator in the particle physics community, Pythia.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Shared Data-Delivery Infrastructure to Enable Discovery with Next Generation Dark Matter and Computational Astrophysics Experiments,OAC,2104003,Amy Roberts,amy.roberts@ucdenver.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Modern laboratories provide unprecedented sensitivity to the many different galactic-messengers that stream through our planet by the minute: cosmic rays, light from distant galaxies, elusive neutrinos, and possibly dark matter. Combining this information with models and data from simulations provides insight into how our universe began and continues to evolve -- the scales at which objects first collapsed, the development of stars and galaxies, and the dynamics within our own galaxy. However, this data is often inaccessible: scientists within an experiment or community struggle with the complex, custom-built programs they use to access the data. And switching to a standard format is usually not an option: these data formats are designed for requirements that often do not include cross-experiment synthesis or linking. Junior scientists - let alone the public - can struggle to generate new insights from the data because the data is difficult to access, understand and analyze. The cross-cutting inquiry that could arise from clever reuse and combination of data from different experiments and simulations is rarely conducted. This project makes data accessible both within and across collaborations, providing the infrastructure to search for signals in detectors across the globe. Extending existing efforts to improve data access makes this project possible: yt is software that provides uniform access to simulation data; Kaitai is a data-description language that enables easy access to any data format; Rucio and other tools provide a standard interface that allows data downloads; and ServiceX can identify, subset and process data with little effort from the end user. Scientists have built experiments that offer an incredible wealth of information about our world. This project works to make that information accessible to everyone.Technical DescriptionThe Personal Data-Delivery infrastructure (PONDD) addresses the data challenges of existing dark matter and astrophysics experiments while requiring no changes to existing data formats. This non-invasive, no-changes-necessary support for any file format provides opportunities to expand beyond our two identified use cases, dark matter searches and astrophysics simulations, into many other data-driven science domains that rely on custom file formats. This work delivers an infrastructure that seamlessly delivers data in a well-supported format (such as Parquet) from multiple sources. To successfully deliver cross-experiment data to end users, we bring together ongoing projects from High Energy Physics and the broader NSF community; while this project will involve development of software products (yt and Kaitai) it will also include synthesis of existing investments in cyberinfrastructure and efforts to improve their long-term sustainability. This project is supported by the Office of Advanced Infrastructure in the Directorate for Computer and Information Science and Engineering and the Division of Physics in the Directorate for Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Shared Data-Delivery Infrastructure to Enable Discovery with Next Generation Dark Matter and Computational Astrophysics Experiments,OAC,2103778,Matthew Turk,mjturk@illinois.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","Modern laboratories provide unprecedented sensitivity to the many different galactic-messengers that stream through our planet by the minute: cosmic rays, light from distant galaxies, elusive neutrinos, and possibly dark matter. Combining this information with models and data from simulations provides insight into how our universe began and continues to evolve -- the scales at which objects first collapsed, the development of stars and galaxies, and the dynamics within our own galaxy.However, this data is often inaccessible: scientists within an experiment or community struggle with the complex, custom-built programs they use to access the data. And switching to a standard format is usually not an option: these data formats are designed for requirements that often do not include cross-experiment synthesis or linking.Junior scientists - let alone the public - can struggle to generate new insights from the data because the data is difficult to access, understand and analyze. The cross-cutting inquiry that could arise from clever reuse and combination of data from different experiments and simulations is rarely conducted.This project makes data accessible both within and across collaborations, providing the infrastructure to search for signals in detectors across the globe. Extending existing efforts to improve data access makes this project possible: yt is software that provides uniform access to simulation data; Kaitai is a data-description language that enables easy access to any data format; Rucio and other tools provide a standard interface that allows data downloads; and ServiceX can identify, subset and process data with little effort from the end user.Scientists have built experiments that offer an incredible wealth of information about our world. This project works to make that information accessible to everyone.Technical DescriptionThe Personal Data-Delivery infrastructure (PONDD) addresses the data challenges of existing dark matter and astrophysics experiments while requiring no changes to existing data formats. This non-invasive, no-changes-necessary support for any file format provides opportunities to expand beyond our two identified use cases, dark matter searches and astrophysics simulations, into many other data-driven science domains that rely on custom file formats.This work delivers an infrastructure that seamlessly delivers data in a well-supported format (such as Parquet) from multiple sources. To successfully deliver cross-experiment data to end users, we bring together ongoing projects from High Energy Physics and the broader NSF community; while this project will involve development of software products (yt and Kaitai) it will also include synthesis of existing investments in cyberinfrastructure and efforts to improve their long-term sustainability.This project is supported by the Office of Advanced Infrastructure in the Directorate for Computer and Information Science and Engineering and the Division of Physics in the Directorate for Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Ghub as a Community-Driven Data-Model Framework for Ice-Sheet Science,OAC,2004510,William Lipscomb,lipscomb@ucar.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Sea level rise is challenging societies around the globe. Planning for future sea level rise in the US is critical for national security, public health, and socioeconomic stability. However, current predictions of sea level rise remain uncertain, because the future behavior of melting ice sheets - a primary cause of sea level rise - is not well understood. A recent United Nations report (IPCC Special Report on the Ocean and Cryosphere in a Changing Climate) summarized two startling facts: (i) Recent sea level rise acceleration is due to increased ice loss from the Greenland and Antarctic ice sheets; and (ii) Uncertainty related to ice-sheet instability arises from limited observations, incomplete representation of ice-sheet processes in current models, and evolving understanding of the complex interactions between the atmosphere, ocean and ice sheets. Improving our ability to forecast the health of ice sheets and hence, predictions of future sea level rise, requires a large, long-lasting collective effort among ice sheet scientists working closely with scientists from the modeling and remote sensing disciplines. One challenge in this collective effort is the range of disciplines and approaches to ice-sheet science - the degree of specialization is an obstacle to efficient collaborative work. This project will lower the barriers among sub-disciplines in ice-sheet science by creating and promoting a centralized web-based hub, called ?Ghub,? where datasets and tools will be made accessible to the full range of ice sheet science fields of study. Ghub is accessible to all interested scientists and lay personnel. Use of Ghub includes access to datasets, analysis tools, and cloud computing power, as well as the ability to develop and share new tools within the Ghub environment. Several avenues of outreach and education as part of the Ghub project are specifically aimed at framing ice-sheet science for general audiences, and including students from underrepresented groups.The urgency in reducing uncertainties of near-term sea level rise relies on improved modeling of ice-sheet response to climate change. Predicting future ice-sheet change requires a tremendous effort across a range of disciplines in ice-sheet science including expertise in observational data, paleoglaciology (""paleo"") data, numerical ice sheet modeling, and widespread use of emerging methodologies for learning from the data, such as machine learning. However, significant knowledge and disciplinary barriers make collaboration between data and model groups the exception rather than the norm. Most modeling groups write their own tools to ingest data and analyze output, newer and larger observational datasets are not being fully taken advantage of by the modeling community, and paleo data critical for constraining model representation of ice sheet history are largely inaccessible to modelers. The diverse disciplinary approaches to ice-sheet science has led to bottlenecks that slow the response to the developing crisis. Coordination between data generators and modelers is critical for testing data-driven hypotheses, providing mechanistic explanations for past ice-sheet change, and incorporating newly understood physical processes and validating models to improve their predictive ability. Solving the urgent problem of unoptimized collaboration requires a novel, integrated, trans-disciplinary program that lowers barriers across the distinct approaches to ice-sheet science. Fostering collaboration between disciplines will lead to a transformational leap in ice-sheet and sea-level science. To make the leap, we must improve the efficiency in collaboration among traditionally disparate approaches to the problem. We will develop a community-building scientific and educational cyberinfrastructure framework including models and data processing tools, to enable coordination and synergistic exchange between ice-sheet scientific communities. The new cyberinfrastructure will be a significant bridge that connects the numerical ice-sheet modeling community with rapidly growing observational datasets of past and present ice-sheet states that will ultimately improve predictions of sea level rise. The GHub cyberinfrastructure will also be a template for organizing disparate scientific communities to address urgent societal needs in a timely fashion.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Morpho-Cyberinfrastructure for scientists and engineers studying shape change,OAC,2003820,Timothy Atherton,timothy.atherton@tufts.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","The goal of this project is to create an accessible open-source software package, Morpho, able to solve a wide variety of shape optimization, evolution and shapeshifting problems. These occur in numerous systems that cut across multiple NSF programs involving soft matter, an umbrella term for readily deformable materials, and includes soft robots, plastics, complex fluids, textiles, particulate media, glasses and biological materials as well as other applications in mathematics and computer science involving computational geometry. Shape change is an important feature of these systems, or a goal of the envisioned applications, but predicting their behavior is very challenging. There is presently a lack of appropriate simulation tools readily available to practitioners working in these domains inhibiting quantitative, mechanistic understanding of their behavior and optimization for applications. With Morpho, domain scientists gain a powerful new simulation tool that enables them to tackle larger and more complex shape evolution problems than presently possible. The project also creates a user community by providing extensive training opportunities including an immersive annual workshop, high quality documentation and a virtual community.Shape optimization and evolution problems are numerically extremely challenging because the final shape is not known ahead of time: The numerical representation must be continuously monitored to ensure the solution obtained is correct and of high quality. The central innovation of Morpho is to regularize ill-posed shape problems by introducing auxiliary functionals that capture some notion of mesh quality. The aim of this work is to extend the range and complexity of problems Morpho can solve in two ways. The first is to allow the user to incorporate arbitrary types of manifolds, field quantities defined on the manifolds, discretizations, functionals and constraints pertinent to their problem. The second is to leverage multilevel algorithms and GPU computing to accelerate the simulations and enable the software to predict the dynamical response of the system given an initial configuration. The project also engages domain scientists in creating and using Morpho through a user-centered development process and a community driven science and education program, incorporating documentation, a repository of example code and tutorials, a virtual community and an annual training workshop. The resulting software and educational materials enable other researchers to simulate shape evolution in several emerging fields involving soft matter and other areas including active materials, soft robots, programmable materials and extreme mechanics. This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: An open source software ecosystem for plasma physics,OAC,1931435,Bennett Maruca,bmaruca@udel.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Software is crucial to all areas of modern plasma physics research. Plasma physicists use software for activities such as analyzing data from laboratory experiments and simulating the behavior of plasmas. Research groups often use software developed independently within their own group, which leads to unnecessary duplication of functionality and a lack of interoperability between different software packages. The lack of interoperability is compounded by different groups writing software using different coding styles and conventions. Much of the research software in plasma physics is not openly available to the public, which makes it harder for other scientists to reproduce scientific results. The team will develop PlasmaPy: a community-wide open source software package for plasma physics research and education. PlasmaPy will be written using the freely available Python programming language which is commonly used in related fields like astronomy. PlasmaPy itself will contain the general functionality needed by most plasma physicists, whereas community-developed affiliated software packages will contain more specialized functionality. The team will seek feedback from plasma physicists, hold annual workshops, and actively support new users and contributors.The research team will lead the development of PlasmaPy and affiliated packages to foster the creation of an open source software ecosystem for plasma physics. The PlasmaPy core package will contain functionality needed by plasma physicists across disciplines, whereas affiliated package will contain more specialized functionality. At the beginning of the project, the research team will formalize the software architecture, refactor existing code, improve tests, and improve base data structures to provide a solid foundation for future development. Subsequent code development priorities include a dispersion relation solver for plasma waves and instabilities, the groundwork for a flexible framework for plasma simulation, time series turbulence analysis tools, classes for the analysis of plasma diagnostics, and tools to provide access to atomic and physical data. They will make base data structures compatible with open source packages for data science to enable future data science studies. The research team will actively seek feedback from the plasma physics community, and adjust code development priorities based on this feedback. The team will hold workshops each year and actively support new users and contributors to grow PlasmaPy into a self-sustaining project.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering, the Division of Physics in the Directorate of Mathematical and Physical Sciences, and the Division of Atmospheric and Geospace Sciences in the Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Frameworks: Collaborative Proposal: Software Infrastructure for Transformative Urban Sustainability Research,OAC,1931283,Claire Welty,weltyc@umbc.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","The United States is highly urbanized with more than 80% of the population residing in cities. Cities draw from and impact natural resources and ecosystems while utilizing vast, expensive infrastructures to meet economic, social, and environmental needs. The National Science Foundation has invested in several strategic research efforts in the area of urban sustainability, all of which generate, collect, and manage large volumes of spatiotemporal data. Voluminous datasets are also made available in domains such as climate, ecology, health, and census. These data can spur exploration of new questions and hypotheses, particularly across traditionally disparate disciplines, and offer unprecedented opportunities for discovery and innovation. However, the data are encoded in diverse formats and managed using a multiplicity of data management frameworks -- all contributing to a break-down of the observational space that inhibits discovery. A scientist must reconcile not only the encoding and storage frameworks, but also negotiate authorizations to access the data. A consequence is that data are locked in institutional silos, each of which represents only a sliver of the observational space. This project, SUSTAIN (Software for Urban Sustainability to Tailor Analyses over Interconnected Networks), facilitates and accelerates discovery by significantly alleviating data-induced inefficiencies. This effort has deep, far-reaching impact. It transforms urban sustainability science by establishing a community of interdisciplinary researchers and catalyzing their collaborative capacity. Hundreds of researchers from over 150 universities are members of our collaborating organizations and will immediately benefit from SUSTAIN. Domains where spatiotemporal phenomena must be analyzed benefit from this innovative research; the partnership with ESRI and Google Earth amplify the impact of SUSTAIN, giving the project a global reach and enabling international collaborative initiatives. The direct engagement with middle school students in computer science and STEM disciplines has well-known benefits and, combined with graduate training, produces a diverse, globally competitive STEM workforce. SUSTAIN targets transformational capabilities for feature space exploration, hypotheses formulation, and model creation and validation over voluminous, high-dimensional spatiotemporal data. These capabilities are deeply aligned with the urban sustainability community's needs, and they address challenges that preclude effective research. SUSTAIN accomplishes these interconnected goals by enabling holistic visibility of the observational space, interactive visualizations of multidimensional information spaces using overlays, fast evaluation of expressive queries tailored to the needs of the discovery process, generation of custom exploratory datasets, and interoperation with diverse analyses software frameworks - all leading to better science. SUSTAIN fosters deep explorations through its transformative visibility of the federated information space. The project reconciles the fragmentation and diversity of siloed data to provide seamless, unprecedented visibility of the information space. A novel aspect of the project's methodology is the innovative use of the Synopsis, a spatiotemporal sketching algorithm that decouples data and information. The methodology extracts and organizes information from the data and uses the information (or sketches of the data) as the basis for explorations. The project also incorporates a novel algorithm for imputations at the sketch level at myriad spatiotemporal scopes. The effort creates a collaborative community of multidisciplinary researchers to build an enduring software infrastructure for urban sustainability.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: Production quality Ecosystem for Programming and Executing eXtreme-scale Applications (EPEXA),OAC,1931347,Edward Valeev,evaleev@vt.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","A team of researchers from three institutions will work collaboratively to design and develop a software framework that implements high-performance methods for irregular and dynamic computations that are poorly supported by current programming paradigms. The framework, titled EPEXA (Ecosystem for Programming and Executing eXtreme Applications), will create a production-quality, general-purpose, community-supported, open-source software ecosystem that attacks the twin challenges of programmer productivity and portable performance for advanced scientific applications on modern high-performance computers. Employing science-driven co-design, the team will transition into production a successful research prototype of a new programming model and accelerate the growth of the community of computer scientists and domain scientists employing these tools for their research. The project bridges the so-called ""valley of death"" between successful proofs of principle to an implementation with enough quality, performance, and community support to motivate application scientists and other researchers to adopt the tools and invest their own effort into the community. In addition to work on the framework development, the project includes training of postdoctoral scholars, graduate and undergraduate students as well as education, outreach and scientific community engagement activities.Specifically, the new powerful data-flow programming model and associated parallel runtime directly address multiple challenges faced by scientists as they attempt to employ rapidly changing computer technologies including current massively-parallel, hybrid, and many-core systems. Both data-intensive and compute-intensive applications are enabled in part by the general programming model and through the ability to target multiple backends or runtime systems. Also enabled is the creation by domain scientists of new domain-specific languages (DSLs) for both shared and distributed-memory computers. EPEXA contributes to the design and development of state-of-the-art software environments that leverage the National Science Foundation's investments in cyberinfrastructure to enable scientific discovery across all disciplines.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Software: HDR: A knowledge base of deep time to facilitate automated workflows in studying the co-evolution of the geosphere and biosphere,OAC,1835717,Xiaogang Ma,xgmachina@gmail.com,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","This project will result in the creation of a software that will support research in the Earth's deep time history. The co-evolution of the geosphere and biosphere is one of the fundamental questions for the 21st century Earth science. The multi-disciplinary characteristics of the research questions on co-evolution are reflected in the various subjects of datasets that need to be integrated. In the past decades, many open data facilities have been built through the support from NSF and other sources. However, the shortage of efficient methods for accessing and synthesizing multi-source datasets hamper the data-intensive co-evolution research. Geologic time is an essential topic in the co-evolving geosphere and biosphere, and can be used as a common reference to connect various parameters among the data silos. This project will improve the machine readability and alignment of various global, local and regional geologic time standards and build a knowledge base of deep time and its service on the Web. All the deliverables will be well-documented and offered under open-access to promote a national cyberinfrastructure ecosystem. The planned tasks and activities will leverage the usage of existing data facilities, facilitate executable and reproducible workflows, generate best practices of cross-disciplinary data science, generate state-of-the-art materials to education programs, and engage the participation of female and underrepresented groups. Shared in the national cyberinfrastructure, the knowledge base built in the project will be able to support a broad range of research, education and outreach programs, which will benefit not only science and engineering but also the society at large.The research question to be addressed is the heterogeneity of geologic time concepts that hamper the data synthesis among multiple data facilities. Accordingly, the objective of this project is to build a knowledge base of deep time to automate geoscience data access and integration in the open data environment, and to support data synthesis in executable workflows for data-intensive scientific discovery. The development approach will include both top-down and bottom-up tracks to leverage previous works on geologic time ontologies and address end user needs through use case analyses. With carefully designed activities and work plan, deliverables from this project will include a machine-readable knowledge base of aligned geologic time standards, services and packages for accessing and querying the knowledge base, and best practices of data synthesis in workflow platforms for studying the co-evolution. The developed knowledge base of deep time will provide powerful support to co-evolution researchers to tackle data heterogeneity issues. Robust services the knowledge base will be built to support automated data synthesis in workflow platforms to advance the co-evolution research. The source code and metadata of the knowledge base will be released on GitHub and registered on community repositories to enable reuse and adaptation. This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences, and the OAC Cyberinfrastructure for Emerging Science and Engineering Research (CESER) program.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements:Software:Open-Source Robust Geometry Toolkit for Black-Box Finite Element Analysis,OAC,1835712,Daniele Panozzo,panozzo@nyu.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","The numerical solution of partial differential equations (PDEs) is ubiquitous in science and engineering applications, including simulation of elastoplastic deformations, fluids, and light scattering. The finite element method (FEM) is the most commonly used discretization of PDEs, especially in the context of structural and thermal analysis, due to its generality and rich selection of off-the-shelf commercial implementations. Ideally, a PDE solver should be a ``black box'': the user provides as input the domain boundary, boundary conditions, and the governing equations, and the code computes the value of the solution at a set of user-specified points of the input domain. This is surprisingly far from being the case for all existing open-source or commercial software, despite the research efforts in this direction and the large academic and industrial interest. To a large extent, this is due to treating meshing and FEM basis construction as two disjoint problems, often exposing the user to the technical issues of interfacing the meshing software with FEM basis construction, both of which, strictly speaking, are technical issues internal to the solver. This state of matters presents a fundamental problem for applications that require fully automatic, robust processing of large collections of meshes of varying sizes, an increasingly common situation as large collections of geometric data become available. This proposal introduces an integrated pipeline, considering meshing and element design as a single challenge, and developing a software platform to enable black box analysis on complex geometric models represented as point clouds, triangle meshes, or CAD (Computer Aided Design) models, opening the door to new shape design technique to a wide range of new applications in sciences and engineering.This project proposes to develop a set of software components based on a set of novel approaches the investigators have developed combined with ""filtered"" use of rational or multi-precision numerical representations to handle robustness problems while maintaining practical performance. The proposed set of geometry processing techniques, while slower than existing ones, are fully robust in a sense of always produce a valid result with minimal assumptions on the input. The geometric toolkit will allow to automatically convert geometrical data in the form of range scans, CAD models, or voxel grids into a surface or volumetric representation, directly usable in widely used open-source finite element method (FEM) packages. It will include mesh generation, in addition to tetrahedral meshes, for other common types of discretizations: hexahedral meshes, and hex-dominant hybrid meshes. The key innovation is to achieve numerical robustness with minimal added algorithmic complexity by carefully mixing higher precision representations for the critical part, while relying on standard fixed-precision floating point representation for the rest and designing algorithms amenable to this approach. As in overwhelming majority of cases higher accuracy is needed for a vanishingly small fraction of computation, this approach allows the users to achieve sensible running time while ensuring output validity and algorithmic correctness on imperfect, real world data. Secondly, the invetigators will integrate FEM basis construction with meshing decoupling accuracy from mesh quality. The software toolkit developed in this proposal has potential for a major impact in all domains that require computational simulation of physical phenomena in complex geometries, enabling the automation of data acquisition, reconstruction, and simulation pipelines. The expectation of this project is that the outcome will not only be a reduction in human time, but the opportunity to fully automate this pipeline will open new research venues. The release of all the software with a MPL2 license will facilitate integration of the results of the work into commercial software, in addition to academic/non-profit research use.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"Framework: Data: HDR: Extensible Geospatial Data Framework towards FAIR (Findable, Accessible, Interoperable, Reusable) Science",OAC,1835822,Xiaohui Carol Song,cxsong@purdue.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","This project provides seamless connections among platforms, data and tools, making large scientific and social geospatial datasets directly usable in scientific models and tools. Users with little or no programming experience will be able to create and build data pipelines that collect and process data at multiple scales and convert such data into usable results. Four case studies demonstrate the capability of the data framework: flood hazard prediction, plant phenotyping, water quality monitoring and sustainable development.The project creates an extensible geospatial data framework (GeoEDF) to address prevalent geospatial data challenges, support domain science needs, and contribute to a national geospatial data and software ecosystem. Specific objectives include: - development of a plug-and-play data framework (GeoEDF), - use of the framework to address domain science needs, - development of interoperability with other cyberinfrastructures, and - dissemination of GeoEDF to the broader community. Use of modular plug-and-play application programming interfaces (APIs) would enable integration of domain-specific geospatial data in a meaningful and accessible manner, leveraging an existing cyberinfrastructure capability (HUBzero) to facilitate adoption and dissemination without reinventing existing components. The project engages a substantial number of domain scientists from a variety of stakeholder communities; by incorporating the framework into HUBzero-powered sites, the team anticipates having access to more than 750,000 users. Training will be provided to the next-generation of researchers and professionals; internship programs are planned for undergraduate and underrepresented groups. The project allows users / scientists to connect a range of data sources, potentially increasing interdisciplinary work. The ultimate goal is to put easy-to-use tools and platforms into the hands of researchers and students to conduct scientific investigations using findable, accessible, interoperable, and reusable (FAIR) science principles.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative:RAPID:Leveraging New Data Sources to Analyze the Risk of COVID-19 in Crowded Locations.,OAC,2027518,Sirish Namilae,namilaes@erau.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","The goal of this project is to create a software infrastructure that will help scientists investigate the risk of the spread of COVID-19 and analyze future epidemics in crowded locations using real-time public webcam videos and location based services (LBS) data. It is motivated by the observation that COVID-19 clusters often arise at sites involving high densities of people. Current strategies suggest coarse scale interventions to prevent this, such as cancellation of activities, which incur substantial economic and social costs. More detailed fine scaled analysis of the movement and interaction patterns of people at crowded locations can suggest interventions, such as changes to crowd management procedures and the design of built environments, that yield social distance without being as disruptive to human activities and the economy. The field of pedestrian dynamics provides mathematical models that can generate such detailed insight. However, these models need data on human behavior, which varies significantly with context and culture. This project will leverage novel data streams, such as public webcams and location based services, to inform the pedestrian dynamics model. Relevant data, models, and software will be made available to benefit other researchers working in this domain, subject to privacy restrictions. The project team will also perform outreach to decision makers so that the scientific insights yield actionable policies contributing to public health. The net result will be critical scientific insight that can generate a transformative impact on the response to the COVID-19 pandemic, including a possible second wave, so that it protects public health while minimizing adverse effects from the interventions.We will accomplish the above work through the following methods and innovations. LBS data can identify crowded locations at a scale of tens of meters and help screen for potential risk by analyzing the long range movement of individuals there. Worldwide video streams can yield finer-grained details of social closeness and other behavioral patterns desirable for accurate modeling. On the other hand, the videos may not be available for potentially high risk locations, nor can they directly answer ?what-if? questions. Videos from contexts similar to the one being modeled will be used to calibrate pedestrian dynamics model parameters, such as walking speeds. Then the trajectories of individual pedestrians will be simulated in the target locations to estimate social closeness. An infection transmission model will be applied to these trajectories to yield estimates of infection spread. This will result in a novel methodology to include diverse real time data into pedestrian dynamics models so that they can quickly and accurately capture human movement patterns in new and evolving situations. The cyberinfrastructure will automatically discover real-time video streams on the Internet and analyze them to determine the pedestrian density, movements, and social distances. The pedestrian dynamics model will be reformulated from the current force-based definition to one that uses pedestrian density and individual speed, both of which can be measured effectively through video analysis. The revised model will be used to produce scientific insight to inform policies, such as steps to mitigate localized outbreaks of COVID-19 and for the systematic reopening, potential re-closing, and permanent changes to economic and social activities.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: CRISPS: Cell-Centric Recursive Image Similarity Projection Searching,OAC,2209135,Joshua Agar,jca92@drexel.edu,"['project', 'data', 'software', 'science', 'research', 'support', 'model', 'material']","Materials scientists use microscopy to determine the structure, order, and periodicity that affect properties. The core challenge is that only a fraction of microscopy data is published. Thus, most of the information from these costly experiments is lost to an abyss. CRISPS - Cell-Centric Recursive Image Similarity Projection Searching ? is a computing infrastructure to make high-value materials microscopy Findable, Accessible, Interoperable, and Reproducible (FAIR). CRISPS is a multitouch, interactive research ""buddy"" that uses artificial intelligence to form associations like the human mind to identify similarities between images. It has permanent, immutable recollection to search and discover collections of scientific images based on labels and associations. CRISPS will be made openly available and will be promoted at conferences, in user facilities, and online to foster a community of developers and users. Public scientific literacy will be enforced through interactive museum exhibits that use CRISPS to explore and discover materials microscopy. The program also supports a first summer research experience for six under-represented persons in STEM.CRISPS is a full-stack software solution for materials microscopy that seamlessly integrates three novel software concepts. 1. DataFed: a federated scientific database for collecting, collating, and searching scientific data and metadata. 2. Schema-Free Search: a tool for cell-centric indexing and searching metadata without schemas. 3. Recursive Image Similarity Projections: a tool to interactively explore image similarity. Each of these efforts is intellectually innovative. DataFed provides an automated, secure, scalable generalized scientific data repository that supports metadata schemas, searches, and provenance graphs. DataFed removes barriers to collaborative science through trusted authentication and secure managed file transfers using GridFTP. Schema-Free Search: an innovative index and ML-tokenization methodology to search unstructured metadata efficiently using ElasticSearch. Scientists will discover schemas and ontologies through an interactive graphical user interface (GUI). Recursive Image Similarity Projections: a collection of deep learning models to conduct automatic symmetry-aware microscopy featurization. When coupled with manifold learning and a GUI, this software tool will enable filtering, similarity exploration, and rapid labeling of materials microscopy. Combining these tools will facilitate creative inquiry into unpublished microscopy, accelerating the discovery of new materials with novel functionalities. CRISPS will be documented and released under a non-restrictive license which allows its reuse, modification, and commercialization. The PIs work with stakeholders in academia and industry to implement CRISPS for typical experiments in optical, electron, and scanning probe microscopies. It provides public access to a 0.5 PB allocation on a DataFed server with indexing and image similarity searching functionality through CRISPS. Interdisciplinary concepts, including scientific data management, search ontologies, and machine learning, are being integrated into courses in materials and computer science; and will be broadly shared through the Lehigh Microscopy School and conference tutorials.This proposal receives funds through the Office of Advanced Cyberinfrastructure in the Computer and Information Science and Engineering Directorate and the Division of Materials Research in the Mathematical and Physical Sciences Directorate.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
III: Large: Collaborative Research: Analysis Engineering for Robust End-to-End Data Science,IIS,1901386,Jeffrey Heer,jheer@cs.washington.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","From poor statistical practices leading to retractions of scientific ""discoveries"" to low-level spreadsheet errors subverting high-stakes analyses, failures of data analysis can have catastrophic consequences. The rapid growth of data science practice in the last decade has led to large collaborative efforts to develop new data processing, machine learning, and analytics tools that put more advanced data analysis into the hands of a wider audience of practitioners, from students to scientists to designers. The most dominant tool for data science is code, where cutting-edge algorithms can be applied from an existing libraries. However, as this democratization of data science has lowered the barrier to using advanced methods, safely using these tools under sound statistical practice remains as difficult as ever. To facilitate more robust data science, this project investigates models and tools for analysis engineering by data scientists who write programs. The focus is on the complete end-to-end process of data analysis performed with code: the iterative, and often exploratory, steps that analysts go through to turn data into This project will contribute insights and characterizations of analytic work, novel methods for capturing and analyzing data science activities, and develop new programming tools and visualization methods for authoring and validating analyses. If successful, this project will augment people's ability to conduct and assess data analyses, promoting more robust results and reducing the gap between novice and expert analysts. The findings and tools from the project will be incorporated into educational efforts, including classroom teaching and tutorials and available as open source software integrated into popular analytical environments (e.g., Jupyter).Data analysis is a central activity to scientific research, yet is too often conducted in an undisciplined fashion. This project treats the entire analytic process as our central phenomenon of study. The project will employ mixed methods to study and characterize common analysis practices and pitfalls, including direct observations of data analysts, large-scale analysis of computational notebooks, and instrumentation of analytic programming environments like JupyterLab. The project will contribute new methods for specifying and safeguarding analyses, including domain-specific languages and program synthesis methods to guide users to preferred next steps. It will also explore ""multiverse"" workflows to manage and assess a diversity of analysis decisions. Analogues of debugging and testing tools will be developed to flag problems and perform error analysis, while the capture and visualization of analytic provenance to aid reproducibility, verification, and collaborative review. The work will be evaluated through controlled studies, classroom use, and open-source deployment for wide-scale field use.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Phonon-Assisted Diffusion in Solids from First-Principles: Unraveling a New Mechanism for Fast Diffusion,DMR,1954621,Sara Kadkhodaei,sarakad@uic.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Nontechnical SummaryDiffusion of molecules in liquids and gases is familiar: perfume, for example, can diffuse in air. Diffusion also takes place in high-temperature solids, and understanding how will be key to new applications. Many technologically-relevant materials, in particular those with high operating temperatures, experience different modes of collective atomic vibrations that are highly correlated. The effect of these correlated vibrations on diffusion is poorly understood, and existing theoretical models are extremely limited in incorporating them into the diffusion description. This project addresses this limitation by (a) devising a theoretical framework that describes the mass-transport behavior affected by the correlated collective atomic vibrations, and (b) implementing a computational tool to predict the diffusion coefficient without requiring experimental input.The end product will be an open-access software toolkit with predictive capability for diffusivity, which will substantially accelerate the discovery and design of advanced materials for applications such as solid-state batteries and fuel cells. Educational activities include the development of a thorough tutorial on advanced topics in the kinetics of high-temperature materials, which will be distributed on the PI's website to provide a world-wide educational platform for students. Also, a graduate-level course about first-principles modeling for engineers will be developed by the PI.Technical SummaryThere currently exist substantial gaps in fundamental understanding and theoretical modeling of diffusion phenomena in strongly anharmonic systems. Namely, (i) the effect of anharmonic vibrations on diffusion phenomena is not well understood, and (ii) existing computational models based on the harmonic approximation of zero-temperature energy fall short of predicting diffusivity in these systems. The goal of this project is to address these gaps by introducing a theoretical framework that combines stochastic sampling techniques and ab-initio calculations to identify the diffusion pathways on an effective energy surface.The findings of this project will advance the fundamental understanding of anharmonic vibration effects on diffusion and will significantly expand the current limits of diffusion modeling capabilities. Additionally, it will provide a-priori predictive capability without requiring experimental input, which will substantially accelerate the optimization and design of new materials. The outcomes of this project will contribute to the advancement of two lines of materials research: (i) predicting diffusion properties for novel high-temperature solid phases and (ii) gaining new understanding of various diffusion-controlled processes (e.g., phase transformation, precipitate growth and coarsening, oxidation, and creep) by providing accurate diffusive mobility data for their simulation.The university is a Hispanic-serving institution, and the PI will participate in two existing programs for recruiting minority students and women to engineering at UIC. She will train undergraduates and graduate students in research, and she will give introductory lectures at local high schools and develop a new graduate-level engineering course in first-principles modeling. The PI will release open-source software as well as data on diffusivity using standard repositories.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: NSCI : Computational and data innovation implementing a national community hydrologic modeling framework for scientific discovery,OAC,1835903,Reed Maxwell,reedmaxwell@princeton.edu,"['model', 'community', 'data', 'hydrologic', 'software', 'science', 'project', 'framework']","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: The Einstein Toolkit ecosystem: Enabling fundamental research in the era of multi-messenger astrophysics,OAC,2004311,Zachariah Etienne,zbetienne@mail.wvu.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","A team of experts from five institutions (University of Illinois Urbana-Champaign, Georgia Institute of Technology, Rochester Institute of Technology, Louisiana State University, and West Virginia University) are collaborating on further development of the Einstein Toolkit, a community-driven, open-source cyberinfrastructure ecosystem providing computational tools supporting research in computational astrophysics, gravitational physics, and fundamental science. The new tools address current and future challenges in gravitational wave source modeling, improve the scalability of the code base, and support an expanded science and user community around the Einstein Toolkit.The Einstein Toolkit is a community-driven suite of research-grade Python codes for performing astrophysics and gravitational wave calculations. The code is open-source, accessible via Conda (an open source package management system) and represents a long-term investment by NSF in providing such computational infrastructure. The software is designed to simulate compact binary stars as sources of gravitational waves. This project focuses on the sustainability of the Einstein Toolkit; specific research efforts center around the development of three new software capabilities for the toolkit: ? CarpetX -- a new mesh refinement driver and interface between AMReX, a software framework containing the functionality to write massively parallel block-structured adaptive mesh refinement (AMR) code, and Cactus, a framework for building a variety of computing applications in science and engineering;? NRPy+ -- a user-friendly code generator based on Python; and ? Canuda -- a new physics library to probe fundamental physics. Integration of graphics processing units (GPUs) will incorporate modern heterogeneous computing devices into the system and will enhance the capability of the toolkit. The end product is sustainable through integration into the Einstein Toolkit, yet also includes an active community maintaining and enhancing the foundational components. Broader impacts are enhanced through training, documentation and a support infrastructure that reduces the barrier to adoption by the community. The team is also creating a science portal with additional educational and showcase resources. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the National Science Foundation's Big Idea activities in Windows on the Universe (WoU).This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Towards A Scalable Infrastructure for Archival and Reproducible Scientific Visualizations,IIS,2209767,Jian Huang,huangj@eecs.utk.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","Today?s science revolves around leading edge datasets ? data that scientists need to carefully analyze so that they can draw reliable scientific conclusions. The rate at which these leading-edge datasets are becoming larger and more complex is accelerating every day. In many ways, having access to a dataset does not equal to, or even come close to, having access to the insights in the dataset. This nuanced but crucial difference in accessibility creates a deep barrier to making scientific results reproducible. To this end, ?Accessible Reproducible Research?, published by Science in 2010, presented a system for reproducible research. A decade later, unfortunately, accessible reproducible research is still in its infancy. It turns out that this barrier is much more fundamental than previously believed, even though on the surface it seems solvable by investing resources and setting guidelines and policies. The real challenge is that the computing toolsets, the working environments, and the work processes of the original team of scientists are very difficult for a different team of scientists to recreate with precision. Such difficulty stems from the rapid speed at which computing technology is advancing; so that freezing a computing environment in a practical manner is nearly impossible. In addition, scientific intuition is difficult to codify, simply documenting a new idea is not enough to communicate what a scientist saw before pursuing that idea. From that respect, making accessible reproducible research a reality requires better methods and tools. In this project, the investigators will focus on the visualization step of data analysis, which is a central component of scientific discovery. This project?s aim is to develop an Archiving Infrastructure for Reproducible Interactive Visualization (AIRIV). Through this infrastructure, the investigators will demonstrate how visual explorations of large and complex data can be reliably captured, efficiently stored, easily shared, and freely reused by any user. This project will improve accessibility of reproducible research and promote the progress of science. For areas such as medicine and pharmaceutical research, this project will provide an unprecedented channel to accelerate translational research and advance the national health.This project will build upon research funded by a prior NSF CISE Research Infrastructure award. In that previous project, the investigators found a method to capture interactive user experience of visualization tools, and to share the captured experience without the need to share the original software or the original data. Furthermore, during the reuse of a captured experience, the user has freedom to explore beyond the exact sequence of how the previous user has used the tool with a method called Loom. In this new project to create AIRIV, the investigators will focus on web-based visualization dashboards, which represent the standard way for scientists around the world to interact with their data and derive insights. This project will first build a general AIRIV Javascript library that can be imported by any web browser-based application. Using the AIRIV library, developers of web-based visual dashboards can easily implement automatic generation of Loom objects into their dashboards. Developers will be able to instrument their applications to store new provenance information with Loom objects as well. The investigators will then conduct performance and scaling tests to understand the tradeoffs between hosting choices under settings of local, institutional clusters, and community shared data infrastructures. Operators of scientific facilities can use the findings to help science communities make informed choices as to where and how to host scientific visualization archives for better share-ability and cost efficiency. The investigators will also develop machine learning methods that can compare Loom objects and externalize commonalities and patterns in an entire archive of Loom objects. Such new methods will lead to creating a search by example functionality for AIRIV archives. For requirements collection, continuous improvement, and deployment testing, the investigators will engage the Mayo Clinic & Illinois Alliance, which serves as a framework for several technologies in healthcare, many of which center around the research and development of dashboard/analytical tools. We target two such analytics efforts, OmiX and KnowEnG, both of which are developed at National Center for Supercomputing Applications (NCSA).This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Towards A Scalable Infrastructure for Archival and Reproducible Scientific Visualizations,IIS,2209768,Roberto Sisneros,sisneros@illinois.edu,"['project', 'data', 'research', 'model', 'science', 'community', 'support', 'simulation']","Today?s science revolves around leading edge datasets ? data that scientists need to carefully analyze so that they can draw reliable scientific conclusions. The rate at which these leading-edge datasets are becoming larger and more complex is accelerating every day. In many ways, having access to a dataset does not equal to, or even come close to, having access to the insights in the dataset. This nuanced but crucial difference in accessibility creates a deep barrier to making scientific results reproducible. To this end, ?Accessible Reproducible Research?, published by Science in 2010, presented a system for reproducible research. A decade later, unfortunately, accessible reproducible research is still in its infancy. It turns out that this barrier is much more fundamental than previously believed, even though on the surface it seems solvable by investing resources and setting guidelines and policies. The real challenge is that the computing toolsets, the working environments, and the work processes of the original team of scientists are very difficult for a different team of scientists to recreate with precision. Such difficulty stems from the rapid speed at which computing technology is advancing; so that freezing a computing environment in a practical manner is nearly impossible. In addition, scientific intuition is difficult to codify, simply documenting a new idea is not enough to communicate what a scientist saw before pursuing that idea. From that respect, making accessible reproducible research a reality requires better methods and tools. In this project, the investigators will focus on the visualization step of data analysis, which is a central component of scientific discovery. This project?s aim is to develop an Archiving Infrastructure for Reproducible Interactive Visualization (AIRIV). Through this infrastructure, the investigators will demonstrate how visual explorations of large and complex data can be reliably captured, efficiently stored, easily shared, and freely reused by any user. This project will improve accessibility of reproducible research and promote the progress of science. For areas such as medicine and pharmaceutical research, this project will provide an unprecedented channel to accelerate translational research and advance the national health.This project will build upon research funded by a prior NSF CISE Research Infrastructure award. In that previous project, the investigators found a method to capture interactive user experience of visualization tools, and to share the captured experience without the need to share the original software or the original data. Furthermore, during the reuse of a captured experience, the user has freedom to explore beyond the exact sequence of how the previous user has used the tool with a method called Loom. In this new project to create AIRIV, the investigators will focus on web-based visualization dashboards, which represent the standard way for scientists around the world to interact with their data and derive insights. This project will first build a general AIRIV Javascript library that can be imported by any web browser-based application. Using the AIRIV library, developers of web-based visual dashboards can easily implement automatic generation of Loom objects into their dashboards. Developers will be able to instrument their applications to store new provenance information with Loom objects as well. The investigators will then conduct performance and scaling tests to understand the tradeoffs between hosting choices under settings of local, institutional clusters, and community shared data infrastructures. Operators of scientific facilities can use the findings to help science communities make informed choices as to where and how to host scientific visualization archives for better share-ability and cost efficiency. The investigators will also develop machine learning methods that can compare Loom objects and externalize commonalities and patterns in an entire archive of Loom objects. Such new methods will lead to creating a search by example functionality for AIRIV archives. For requirements collection, continuous improvement, and deployment testing, the investigators will engage the Mayo Clinic & Illinois Alliance, which serves as a framework for several technologies in healthcare, many of which center around the research and development of dashboard/analytical tools. We target two such analytics efforts, OmiX and KnowEnG, both of which are developed at National Center for Supercomputing Applications (NCSA).This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Frameworks: The Einstein Toolkit Ecosystem: Enabling fundamental research in the era of multi-messenger astrophysics,OAC,2003893,Pablo Laguna,pablo.laguna@austin.utexas.edu,"['software', 'project', 'code', 'community', 'science', 'research', 'new', 'physic']","A team of experts from five institutions (University of Illinois Urbana-Champaign, Georgia Institute of Technology, Rochester Institute of Technology, Louisiana State University, and West Virginia University) are collaborating on further development of the Einstein Toolkit, a community-driven, open-source cyberinfrastructure ecosystem providing computational tools supporting research in computational astrophysics, gravitational physics, and fundamental science. The new tools address current and future challenges in gravitational wave source modeling, improve the scalability of the code base, and support an expanded science and user community around the Einstein Toolkit.The Einstein Toolkit is a community-driven suite of research-grade Python codes for performing astrophysics and gravitational wave calculations. The code is open-source, accessible via Conda (an open source package management system) and represents a long-term investment by NSF in providing such computational infrastructure. The software is designed to simulate compact binary stars as sources of gravitational waves. This project focuses on the sustainability of the Einstein Toolkit; specific research efforts center around the development of three new software capabilities for the toolkit: ? CarpetX -- a new mesh refinement driver and interface between AMReX, a software framework containing the functionality to write massively parallel block-structured adaptive mesh refinement (AMR) code, and Cactus, a framework for building a variety of computing applications in science and engineering;? NRPy+ -- a user-friendly code generator based on Python; and ? Canuda -- a new physics library to probe fundamental physics. Integration of graphics processing units (GPUs) will incorporate modern heterogeneous computing devices into the system and will enhance the capability of the toolkit. The end product is sustainable through integration into the Einstein Toolkit, yet also includes an active community maintaining and enhancing the foundational components. Broader impacts are enhanced through training, documentation and a support infrastructure that reduces the barrier to adoption by the community. The team is also creating a science portal with additional educational and showcase resources. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the National Science Foundation's Big Idea activities in Windows on the Universe (WoU).This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"CAREER: Understanding Sensitization and Corrosion Mechanisms in Additively Manufactured Metals for Improved Surface Finish, Mechanical Properties and Corrosion Resistance",CMMI,1944516,Owen Hildreth,ohildreth@mines.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","This Faculty Early Career Development (CAREER) grant focuses on identifying, quantifying, and exploiting sensitization and corrosion mechanisms in metals and alloys produced using powder-bed fusion additive manufacturing. Sensitization refers to the precipitation of carbides at grain boundaries in a metal alloy, causing the alloy to be susceptible to intergranular corrosion. Metal additive manufacturing or three-dimensional printing allows companies to manufacture complex parts with improved performance and shorter production times. To reduce the costs of additive manufacturing and expand design freedom, this project establishes the understanding necessary to exploit corrosion phenomena to create a scalable and uniform etching process to dissolve supports and trapped powder while improving surface finish. Specifically, this research project establishes the fundamental relationships between the as-printed microstructure, sensitization kinetics and corrosion mechanisms in these materials. This new understanding allows manufacturers to control the amount of material removed while improving surface finish and mechanical properties. By replacing expensive post-process machining operations with simple chemical dissolution to remove support structures, this new approach reduces manufacturing costs and provides U.S. manufacturing with a competitive advantage. The award?s STEM educational components include curriculum development and K-12 outreach partnerships with groups serving underrepresented minorities and students with disabilities.The specific goal of this research is to understand how the microstructure of powder-bed fusion-processed metals and alloys control sensitization kinetics and corrosion mechanisms. To achieve this understanding, the research objectives of this project are to: (1) understand microstructure evolution during sensitization; (2) understand how the depth-dependent sensitized microstructure changes the corrosion behavior and associated self-terminating etch-stop mechanism; and (3) quantify the impacts that sensitization and dissolution have on mechanical properties and corrosion performance. To achieve these research objectives, this project explores carburization and sulfidation-based sensitization of stainless steel, nickel-based superalloys, and titanium alloys as model systems to test the following hypotheses: (i) carbon-based sensitization depth decreases as the diffusion path for passivating elements increases; (ii) dealloying increases with decreasing diffusion path; and (iii) chromium depletion zone decreases faster with increasing sensitization rate. The overarching focus is to obtain a better understanding of the kinetics of microstructure evolution in high temperature corrosive environments along with increased understanding of the microstructure-dependent corrosion mechanisms in sensitized metals in aqueous environments. This new knowledge is used to guide the design and manufacturing of powder-bed fusion metallic parts towards efficient support structure removal, improved surface finish and increased fracture and fatigue resistance. This project allows the PI to advance the knowledge base in materials science, corrosion, and mechanical behavior while supporting a career in advanced manufacturing.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Software: NSCI: HDR: Building An HPC/HTC Infrastructure For The Synthesis And Analysis Of Current And Future Cosmic Microwave Background Datasets,OAC,1835526,Thomas Crawford,tcrawfor@kicp.uchicago.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","The photons created in the Big Bang have experienced the entire history of the Universe, and every step in the evolution of the Universe has left its mark on their statistical properties. Observations of these photons have the potential to unlock the secrets of fundamental physics and cosmology, and to provide key insights into the formation and evolution of cosmic structures such as galaxies and galaxy clusters. Since the traces of these processes are so faint, one must gather enormous datasets to be able to detect them above the unavoidable instrumental and environmental noise. This in turn means that one must be able to use the most powerful computing resources available to be able to process the volume of data. These computing resources include both highly localized supercomputers and widely distributed grid and cloud systems. The PI and Co-Is will develop a common computing infrastructure able to take advantage of both types of resource, and demonstrate its suitability for ongoing and planned experiments by adapting the analysis pipelines of four leading Big Bang observatories to run within it. In addition to enabling the full scientific exploitation of these extraordinarily rich data sets, the investigators will mentor students engaged in this research and run summer schools in applied supercomputing.This project seeks to enable the detection of the faintest signals in Cosmic Microwave Background radiation, and in particular the pattern of peaks and troughs in the angular power spectra of its polarization field. In order to obtain these spectra one must first reduce the raw observations to maps of the sky in a way the preserve the correlations in the signal and characterizes the correlation in the noise. While the algorithms to perform this reduction are well-understood, applying them to data sets with quadrillions to quintillions of observations is a very serious computational challenge. The computational resources available to the project to address this include both high performance and high throughput computing systems, and one will need to take advantage of both of them. This project will develop a joint high performance/high throughput computational framework, and deploy within it analysis pipelines currently being fielded by the ongoing Atacama Cosmology Telescope, BICEP/Keck Array, POLARBEAR, and South Pole Telescope experiments. By doing so one will also demonstrate the frameworks efficacy for the planned Simons Observatory and CMB-S4 experiments.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Framework: Software: HDR: Building the Twenty-First Century Citizen Science Framework to Enable Scientific Discovery Across Disciplines,OAC,1835410,Sarah Benson-Amram,sbensona@uwyo.edu,"['data', 'project', 'science', 'system', 'research', 'new', 'software', 'learning']","A team of experts from five institutions (University of Minnesota, Adler Planetarium, University of Wyoming, Colorado State University, and UC San Diego) links field-based and online analysis capabilities to support citizen science, focusing on three research areas (cell biology, ecology, and astronomy). The project builds on Zooniverse and CitSci.org, leverages the NSF Science Gateways Community Institute, and enhances the quality of citizen science and the experience of its participants.This project creates an integrated Citizen Science Cyberinfrastructure (CSCI) framework that expands the capacity of research communities across several disciplines to use citizen science as a suitable and sustainable research methodology. CSCI produces three improvements to the infrastructure for citizen science already provided by Zooniverse and CitSci.org: - Combining Modes - connecting the process of data collection and analysis; - Smart Assignment - improving the assignment of tasks during analysis; and - New Data Models - exploring the Data-as-Subject model. By treating time series data as data, this model removes the need to create images for classification and facilitates more complex workflows. These improvements are motivated and investigated through three distinct scientific cases: - Biomedicine (3D Morphology of Cell Nucleus). Currently, Zooniverse 'Etch-a-Cell' volunteers provide annotations of cellular components in images from high-resolution microscopy, where a single cell provides a stack containing thousands of sliced images. The Smart Task Assignment capability incorporates this information, so volunteers are not shown each image in a stack where machines or other volunteers have already evaluated some subset of data. - Ecology (Identifying Individual Animals). When monitoring wide-ranging wildlife populations, identification of individual animals is needed for robust estimates of population sizes and trends. This use case combines field collection and data analysis with deep learning to improve results. - Astronomy (Characterizing Lightcurves). Astronomical time series data reveal a variety of behaviors, such as stellar flares or planetary transits. The existing Zooniverse data model requires classification of individual images before aggregation of results and transformation back to refer to the original data. By using the Data-as-Subject model and the Smart Task Assignment capability, volunteers will be able to scan through the entire time series in a machine-aided manner to determine specific light curve characteristics.The team explores the use of recurrent neural networks (RNNs) to determine automated learning architectures best suited to the projects. Of particular interest is how the degree to which neighboring subjects are coupled affects performance. The integration of existing tools, which is based on application programming interfaces (APIs), also facilitates further tool integration. The effort creates a citizen science framework that directly advances knowledge for three science use cases in biomedicine, ecology, and astronomy, and combines field-collected data with data analysis. This has the ability to solve key problems in the individual applications, as well as benefiting the research of the dozens of projects on the Zooniverse platform. It provides benefits to researchers using citizen scientists, and to the nearly 1.6 million citizen scientists themselves.This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Research on Learning in Formal and Informal Settings, within the NSF Directorate for Education and Human Resources.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Elements: Software: The Integrated Geoscience Observatory (InGeO),OAC,1835573,Asti Bhatt,asti.bhatt@sri.com,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","This award will support the development of the Integrated Geoscience Observatory (InGeO). InGeO is an online platform that integrates data and associated software tools contributed by researchers into a unified toolset to enable studying the convergent, systems science. Complex processes in the Sun-Earth system affect the habitability of earth. As technological progress continues, our society also becomes increasingly vulnerable to the disruptions introduced by these complex processes. Study of the Sun-Earth system is traditionally broken-up into separate geoscience disciplines, typically focusing on a specific region of the system. However, often the interaction between several regions end up significantly affecting terrestrial systems. For example, a chain of processes occurring between the Sun, magnetosphere, and ionosphere often result in loss of GPS signal that severely affects all the communication/navigation and infrastructure that depends on this technology. To broach the larger question of the interaction of the subsystems studied by the separate communities, it is necessary to overcome the barriers of communication posed by different observational instruments, software tools for interpreting data, and modeling methods.To promote systems science, InGeO creates an integrated package of software tools specifically designed to help researchers find and integrate diverse observational data and distributes this toolkit to the community in the most flexible way possible. Building on the pilot project supported by the NSF EarthCube program, the specific features of the toolkit include: (a) linking diverse data sets from multiple observational data repositories; (b) using these data within a shareable computing environment, complete with a full selection of standard numerical and data-processing tools; and (c) enabling an ever growing assortment of targeted, user-contributed tools, such as assimilative models or interpolation methods. The toolkit can be accessed and used either through a web-based computing environment or by downloading Docker containers for local installation, with a nearly seamless transition between the two. The toolkit is open-source and ensures credit attribution to data and software contributors, and the InGeO team works with members of scientific community (especially students and early career researchers) to improve the toolkit and build a sustainable, more connected user-base.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
NSCI: SI2-SSE: An Extensible Model to Support Scalable Checkpoint-Restart for DMTCP Across Multiple Disciplines,OAC,1740218,Gene Cooperman,gene@ccs.neu.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","Checkpointing is a technique that periodically saves the state of a long-running computer program to disk. If a computer crash occurs during the running of the program, one can then restart the program state from a previously saved ""checkpoint"" file on disk. The goal of this project is to discover, implement and deploy novel techniques for adapting checkpointing so as to provide a more robust capability easily usable across applications supporting the research of a variety of scientific and engineering disciplines. In particular, a problem with the classic (transparent) checkpoint model is that these packages do not model, and hence cannot recreate upon restart, communications between the original program and other external processes or programs. In this project, a virtualization model for commonly used mechanisms for communication will be developed so that on restart, external communications are emulated. Checkpointing is used across academia, industry, and government, particularly by those with long-running high performance computing programs. Thus, the project outcomes have broad applicability and value. The project has the added benefit of educating the next generation of students in valuable and highly transferable system skills. Today, transparent checkpoint-restart today is used primarily for fault tolerance, and primarily in closed systems with no external communication. DMTCP is a twelve-year old open source checkpointing project. Its currently evolving process virtualization model of checkpointing enables an application to support complex applications that interact with external subsystems. The project explores and extends a model of process virtualization in order to adapt checkpoint-restart to multiple, novel applications, and to extend its use across multiple scientific and engineering disciplines. Example disciplines that will benefit include: supercomputing (and in particular, forging a path toward practical exascale checkpointing); novel strategies for flexible resource managers (batch queues) for computer clusters that adapt to the current workload; and better support for hardware circuit emulators for Electronic Design Automation (EDA). Example challenges include the need to support transparent checkpointing over the newer low-latency networks such as Omni-Path, integration of application-specific checkpointing with transparent DMTCP-style checkpointing, the need to avoid ""flooding"" back-end storage during checkpointing in high-end clusters, and new types of resource managers that benefit from the flexibility of arbitrarily suspending running jobs through checkpointing. Rather than build ad hoc solutions for each of the above, this work will provide a simple model allowing end users to easily build their own extensions to support checkpointing of the external subsystems. The simple model will be derived by generalizing over solutions to many of the example challenges described above. In addition to fault tolerance, the technology holds advantages for: fast startup (checkpoint after process initialization,in order to restart and skip this phase in future sessions); debugging (e.g. checkpoint every 30 seconds); reproducible bug reports; extended interactive sessions (e.g. checkpoint before dinner and restart the next day); and so on.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Cyberinfrastructure for Sustained Scientific Innovation - Software Elements: Cloud WRF for the Atmospheric Research and Education Communities,OAC,1835511,Jordan Powers,powers@ucar.edu,"['data', 'software', 'project', 'community', 'new', 'nsf', 'scientific', 'science']","This award supports the establishment of an officially-supported version of the Weather and Research Forecast (WRF) model in the cloud environment. WRF is the world's most popular numerical weather prediction model and is supported by the National Center for Atmospheric Research (NCAR) for a community of users across universities, research labs, and operational weather centers. This project will address fundamental issues such as modeling system accessibility, improvement of model support to the research and educational user communities, student and scientist training, and facilitation of model development. Given WRF's prominence in both atmospheric research and real-time weather forecasting, this work will not only promote the advancement of science, but also will contribute to the vigor of development and application of one of the nation's cyberinfrastucture (CI) assets, a key weather prediction model used at operational centers and for public purposes. Furthermore, this project will contribute to education at minority-serving institutions and will yield tools to better the training of new generations of the nation's atmospheric scientists. Lastly, the project will leverage the resources and support of commercial cloud service providers to serve these national interests, in a collaboration of the principal researchers with industry.The viability of running the WRF in the cloud has been previously demonstrated, and this project would advance the field by configuring and supporting the official version of the WRF in the cloud, with an up-to-date, cloud-configured version of the WRF system code synced to the WRF GitHub repository. Other materials that will be available include the WRF tutorial materials and system documentation, WRF input and output datasets, and the WRF Testing Framework code analysis package. The project will produce a configured and documented cloud WRF system that will open a new arena for model use and that will enhance the efficiency of both user support and the integration of contributed model improvements. This cloud capability will exploit an emerging common cyber-ground to facilitate code development for the target model and allow for critical reproducibility in code analysis and testing. It will advance the delivery of modeling system tutorials, and thus scientist training and education, through establishing globally-accessible modeling spaces, enhancing instruction portability and access, decreasing costs for host institutions, and improving usability of on-line materials. These new cloud capabilities will be addressing bottlenecks in model support and development, and once developed, the CI can be adapted for other community models, thus benefiting other scientific disciplines and their tools via its reuse.This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program within the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Framework: Sofware: Collaborative Research: CyberWater -An open and sustainable framework for diverse data and model integration with provenance and access to HPC,OAC,1835656,Yang Zhang,ya.zhang@northeastern.edu,"['simulation', 'model', 'project', 'system', 'material', 'science', 'data', 'framework']","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities. The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery. The models and datasets cover fields such as hydrology, biology, environmental engineering and climate. The project also addresses one of the key issues for extreme-scale computing: scalable file systems. The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI). The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity. The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use. To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts. The project builds upon an existing prototype developed by the lead investigator; basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control. The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI. For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated. The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
Collaborative Research: Elements: Software: NSCI: HDR: Building An HPC/HTC Infrastructure For The Synthesis And Analysis Of Current And Future Cosmic Microwave Background Datasets,OAC,1835768,Matthew Hasselfield,mhasse@psu.edu,"['science', 'data', 'network', 'project', 'model', 'software', 'cyberinfrastructure', 'system']","The photons created in the Big Bang have experienced the entire history of the Universe, and every step in the evolution of the Universe has left its mark on their statistical properties. Observations of these photons have the potential to unlock the secrets of fundamental physics and cosmology, and to provide key insights into the formation and evolution of cosmic structures such as galaxies and galaxy clusters. Since the traces of these processes are so faint, one must gather enormous datasets to be able to detect them above the unavoidable instrumental and environmental noise. This in turn means that one must be able to use the most powerful computing resources available to be able to process the volume of data. These computing resources include both highly localized supercomputers and widely distributed grid and cloud systems. The PI and Co-Is will develop a common computing infrastructure able to take advantage of both types of resource, and demonstrate its suitability for ongoing and planned experiments by adapting the analysis pipelines of four leading Big Bang observatories to run within it. In addition to enabling the full scientific exploitation of these extraordinarily rich data sets, the investigators will mentor students engaged in this research and run summer schools in applied supercomputing.This project seeks to enable the detection of the faintest signals in Cosmic Microwave Background radiation, and in particular the pattern of peaks and troughs in the angular power spectra of its polarization field. In order to obtain these spectra one must first reduce the raw observations to maps of the sky in a way the preserve the correlations in the signal and characterizes the correlation in the noise. While the algorithms to perform this reduction are well-understood, applying them to data sets with quadrillions to quintillions of observations is a very serious computational challenge. The computational resources available to the project to address this include both high performance and high throughput computing systems, and one will need to take advantage of both of them. This project will develop a joint high performance/high throughput computational framework, and deploy within it analysis pipelines currently being fielded by the ongoing Atacama Cosmology Telescope, BICEP/Keck Array, POLARBEAR, and South Pole Telescope experiments. By doing so one will also demonstrate the frameworks efficacy for the planned Simons Observatory and CMB-S4 experiments.This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
